{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记希望复现 huang gao 组的 budgeted training of vision transformers。我们基于已有的 train.py 进行修改。\n",
    "\n",
    "首先是必要的头文件和一些 args 的处理。可以折叠该部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" ImageNet Training Script\n",
    "\n",
    "This is intended to be a lean and easily modifiable ImageNet training script that reproduces ImageNet\n",
    "training results with some of the latest networks and training techniques. It favours canonical PyTorch\n",
    "and standard Python style over trying to be able to 'do it all.' That said, it offers quite a few speed\n",
    "and training result improvements over the usual PyTorch example scripts. Repurpose as you see fit.\n",
    "\n",
    "This script was started from an early version of the PyTorch ImageNet example\n",
    "(https://github.com/pytorch/examples/tree/master/imagenet)\n",
    "\n",
    "NVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n",
    "(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ADDITIONAL NOTES BY fish:\n",
    "    - This script is a modified version of the original train.py script from Ross Wightman's timm library.\n",
    "    - for the current version, the \n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "import yaml\n",
    "from torch.nn.parallel import DistributedDataParallel as NativeDDP\n",
    "\n",
    "from timm import utils\n",
    "from timm.data import create_dataset, create_loader, resolve_data_config, Mixup, FastCollateMixup, AugMixDataset\n",
    "from timm.layers import convert_splitbn_model, convert_sync_batchnorm, set_fast_norm\n",
    "from timm.loss import JsdCrossEntropy, SoftTargetCrossEntropy, BinaryCrossEntropy, LabelSmoothingCrossEntropy\n",
    "from timm.models import create_model, safe_model_name, resume_checkpoint, load_checkpoint, model_parameters\n",
    "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
    "from timm.scheduler import create_scheduler_v2, scheduler_kwargs\n",
    "from timm.utils import ApexScaler, NativeScaler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    from apex.parallel import DistributedDataParallel as ApexDDP\n",
    "    from apex.parallel import convert_syncbn_model\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    has_apex = False\n",
    "\n",
    "has_native_amp = False\n",
    "try:\n",
    "    if getattr(torch.cuda.amp, 'autocast') is not None:\n",
    "        has_native_amp = True\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    has_wandb = True\n",
    "except ImportError:\n",
    "    has_wandb = False\n",
    "\n",
    "try:\n",
    "    from functorch.compile import memory_efficient_fusion\n",
    "    has_functorch = True\n",
    "except ImportError as e:\n",
    "    has_functorch = False\n",
    "\n",
    "# fish: secondorder\n",
    "from eva import KFAC as Eva\n",
    "from eva import KFACParamScheduler\n",
    "\n",
    "has_compile = hasattr(torch, 'compile')\n",
    "\n",
    "\n",
    "_logger = logging.getLogger('train')\n",
    "\n",
    "# The first arg parser parses out only the --config argument, this argument is used to\n",
    "# load a yaml file containing key-values that override the defaults for the main parser below\n",
    "config_parser = parser = argparse.ArgumentParser(description='Training Config', add_help=False)\n",
    "parser.add_argument('-c', '--config', default='', type=str, metavar='FILE',\n",
    "                    help='YAML config file specifying default arguments')\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "\n",
    "# Dataset parameters\n",
    "group = parser.add_argument_group('Dataset parameters')\n",
    "# Keep this argument outside the dataset group because it is positional.\n",
    "parser.add_argument('data', nargs='?', metavar='DIR', const=None,\n",
    "                    help='path to dataset (positional is *deprecated*, use --data-dir)')\n",
    "parser.add_argument('--data-dir', metavar='DIR',\n",
    "                    help='path to dataset (root dir)')\n",
    "parser.add_argument('--dataset', metavar='NAME', default='',\n",
    "                    help='dataset type + name (\"<type>/<name>\") (default: ImageFolder or ImageTar if empty)')\n",
    "group.add_argument('--train-split', metavar='NAME', default='train',\n",
    "                   help='dataset train split (default: train)')\n",
    "group.add_argument('--val-split', metavar='NAME', default='validation',\n",
    "                   help='dataset validation split (default: validation)')\n",
    "group.add_argument('--dataset-download', action='store_true', default=False,\n",
    "                   help='Allow download of dataset for torch/ and tfds/ datasets that support it.')\n",
    "group.add_argument('--class-map', default='', type=str, metavar='FILENAME',\n",
    "                   help='path to class to idx mapping file (default: \"\")')\n",
    "\n",
    "# Model parameters\n",
    "group = parser.add_argument_group('Model parameters')\n",
    "group.add_argument('--model', default='resnet50', type=str, metavar='MODEL',\n",
    "                   help='Name of model to train (default: \"resnet50\")')\n",
    "group.add_argument('--pretrained', action='store_true', default=False,\n",
    "                   help='Start with pretrained version of specified network (if avail)')\n",
    "group.add_argument('--initial-checkpoint', default='', type=str, metavar='PATH',\n",
    "                   help='Initialize model from this checkpoint (default: none)')\n",
    "group.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                   help='Resume full model and optimizer state from checkpoint (default: none)')\n",
    "group.add_argument('--no-resume-opt', action='store_true', default=False,\n",
    "                   help='prevent resume of optimizer state when resuming model')\n",
    "group.add_argument('--num-classes', type=int, default=None, metavar='N',\n",
    "                   help='number of label classes (Model default if None)')\n",
    "group.add_argument('--gp', default=None, type=str, metavar='POOL',\n",
    "                   help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\n",
    "group.add_argument('--img-size', type=int, default=None, metavar='N',\n",
    "                   help='Image size (default: None => model default)')\n",
    "group.add_argument('--in-chans', type=int, default=None, metavar='N',\n",
    "                   help='Image input channels (default: None => 3)')\n",
    "group.add_argument('--input-size', default=None, nargs=3, type=int,\n",
    "                   metavar='N N N',\n",
    "                   help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\n",
    "group.add_argument('--crop-pct', default=None, type=float,\n",
    "                   metavar='N', help='Input image center crop percent (for validation only)')\n",
    "group.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n",
    "                   help='Override mean pixel value of dataset')\n",
    "group.add_argument('--std', type=float, nargs='+', default=None, metavar='STD',\n",
    "                   help='Override std deviation of dataset')\n",
    "group.add_argument('--interpolation', default='', type=str, metavar='NAME',\n",
    "                   help='Image resize interpolation type (overrides model)')\n",
    "group.add_argument('-b', '--batch-size', type=int, default=128, metavar='N',\n",
    "                   help='Input batch size for training (default: 128)')\n",
    "group.add_argument('-vb', '--validation-batch-size', type=int, default=None, metavar='N',\n",
    "                   help='Validation batch size override (default: None)')\n",
    "group.add_argument('--channels-last', action='store_true', default=False,\n",
    "                   help='Use channels_last memory layout')\n",
    "group.add_argument('--fuser', default='', type=str,\n",
    "                   help=\"Select jit fuser. One of ('', 'te', 'old', 'nvfuser')\")\n",
    "group.add_argument('--grad-accum-steps', type=int, default=1, metavar='N',\n",
    "                   help='The number of steps to accumulate gradients (default: 1)')\n",
    "group.add_argument('--grad-checkpointing', action='store_true', default=False,\n",
    "                   help='Enable gradient checkpointing through model blocks/stages')\n",
    "group.add_argument('--fast-norm', default=False, action='store_true',\n",
    "                   help='enable experimental fast-norm')\n",
    "group.add_argument('--model-kwargs', nargs='*', default={}, action=utils.ParseKwargs)\n",
    "group.add_argument('--head-init-scale', default=None, type=float,\n",
    "                   help='Head initialization scale')\n",
    "group.add_argument('--head-init-bias', default=None, type=float,\n",
    "                   help='Head initialization bias value')\n",
    "\n",
    "# scripting / codegen\n",
    "scripting_group = group.add_mutually_exclusive_group()\n",
    "scripting_group.add_argument('--torchscript', dest='torchscript', action='store_true',\n",
    "                             help='torch.jit.script the full model')\n",
    "scripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',\n",
    "                             help=\"Enable compilation w/ specified backend (default: inductor).\")\n",
    "\n",
    "# Optimizer parameters\n",
    "group = parser.add_argument_group('Optimizer parameters')\n",
    "group.add_argument('--opt', default='sgd', type=str, metavar='OPTIMIZER',\n",
    "                   help='Optimizer (default: \"sgd\")')\n",
    "group.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',\n",
    "                   help='Optimizer Epsilon (default: None, use opt default)')\n",
    "group.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                   help='Optimizer Betas (default: None, use opt default)')\n",
    "group.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                   help='Optimizer momentum (default: 0.9)')\n",
    "group.add_argument('--weight-decay', type=float, default=2e-5,\n",
    "                   help='weight decay (default: 2e-5)')\n",
    "group.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
    "                   help='Clip gradient norm (default: None, no clipping)')\n",
    "group.add_argument('--clip-mode', type=str, default='norm',\n",
    "                   help='Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")')\n",
    "group.add_argument('--layer-decay', type=float, default=None,\n",
    "                   help='layer-wise learning rate decay (default: None)')\n",
    "group.add_argument('--opt-kwargs', nargs='*', default={}, action=utils.ParseKwargs)\n",
    "\n",
    "# fish: EVA preconditioner parameters\n",
    "group = parser.add_argument_group('Preconditioner parameters')\n",
    "\n",
    "# eva second conditioner arguments\n",
    "parser.add_argument('--use-eva', action='store_true',default=False,help='use Eva preconditioner')\n",
    "\n",
    "# KFAC Parameters\n",
    "# parser.add_argument('--kfac-name', type=str, default='inverse',\n",
    "#         help='choises: %s' % kfac.kfac_mappers.keys() + ', default: '+'inverse')\n",
    "parser.add_argument('--exclude-parts', type=str, default='',\n",
    "        help='choises: CommunicateInverse,ComputeInverse,CommunicateFactor,ComputeFactor')\n",
    "parser.add_argument('--kfac-update-freq', type=int, default=1,\n",
    "                    help='iters between kfac inv ops (0 = no kfac) (default: 1)')\n",
    "parser.add_argument('--kfac-cov-update-freq', type=int, default=1,\n",
    "                    help='iters between kfac cov ops (default: 1)')\n",
    "parser.add_argument('--kfac-update-freq-alpha', type=float, default=10,\n",
    "                    help='KFAC update freq multiplier (default: 10)')\n",
    "parser.add_argument('--kfac-update-freq-decay', nargs='+', type=int, default=None,\n",
    "                    help='KFAC update freq schedule (default None)')\n",
    "parser.add_argument('--stat-decay', type=float, default=0.95,\n",
    "                    help='Alpha value for covariance accumulation (default: 0.95)')\n",
    "parser.add_argument('--damping', type=float, default=0.001,\n",
    "                    help='KFAC damping factor (default 0.001)')\n",
    "parser.add_argument('--damping-alpha', type=float, default=0.5,\n",
    "                    help='KFAC damping decay factor (default: 0.5)')\n",
    "parser.add_argument('--damping-decay', nargs='+', type=int, default=None,\n",
    "                    help='KFAC damping decay schedule (default None)')\n",
    "parser.add_argument('--kl-clip', type=float, default=0.001,\n",
    "                    help='KL clip (default: 0.001)')\n",
    "parser.add_argument('--diag-blocks', type=int, default=1,\n",
    "                    help='Number of blocks to approx layer factor with (default: 1)')\n",
    "parser.add_argument('--diag-warmup', type=int, default=0,\n",
    "                    help='Epoch to start diag block approximation at (default: 0)')\n",
    "parser.add_argument('--distribute-layer-factors', action='store_true', default=None,\n",
    "                    help='Compute A and G for a single layer on different workers. '\n",
    "                            'None to determine automatically based on worker and '\n",
    "                            'layer count.')\n",
    "\n",
    "\n",
    "# Learning rate schedule parameters\n",
    "group = parser.add_argument_group('Learning rate schedule parameters')\n",
    "group.add_argument('--sched', type=str, default='cosine', metavar='SCHEDULER',\n",
    "                   help='LR scheduler (default: \"step\"')\n",
    "group.add_argument('--sched-on-updates', action='store_true', default=False,\n",
    "                   help='Apply LR scheduler step on update instead of epoch end.')\n",
    "group.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                   help='learning rate, overrides lr-base if set (default: None)')\n",
    "group.add_argument('--lr-base', type=float, default=0.1, metavar='LR',\n",
    "                   help='base learning rate: lr = lr_base * global_batch_size / base_size')\n",
    "group.add_argument('--lr-base-size', type=int, default=256, metavar='DIV',\n",
    "                   help='base learning rate batch size (divisor, default: 256).')\n",
    "group.add_argument('--lr-base-scale', type=str, default='', metavar='SCALE',\n",
    "                   help='base learning rate vs batch_size scaling (\"linear\", \"sqrt\", based on opt if empty)')\n",
    "group.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
    "                   help='learning rate noise on/off epoch percentages')\n",
    "group.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
    "                   help='learning rate noise limit percent (default: 0.67)')\n",
    "group.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
    "                   help='learning rate noise std-dev (default: 1.0)')\n",
    "group.add_argument('--lr-cycle-mul', type=float, default=1.0, metavar='MULT',\n",
    "                   help='learning rate cycle len multiplier (default: 1.0)')\n",
    "group.add_argument('--lr-cycle-decay', type=float, default=0.5, metavar='MULT',\n",
    "                   help='amount to decay each learning rate cycle (default: 0.5)')\n",
    "group.add_argument('--lr-cycle-limit', type=int, default=1, metavar='N',\n",
    "                   help='learning rate cycle limit, cycles enabled if > 1')\n",
    "group.add_argument('--lr-k-decay', type=float, default=1.0,\n",
    "                   help='learning rate k-decay for cosine/poly (default: 1.0)')\n",
    "group.add_argument('--warmup-lr', type=float, default=1e-5, metavar='LR',\n",
    "                   help='warmup learning rate (default: 1e-5)')\n",
    "group.add_argument('--min-lr', type=float, default=0, metavar='LR',\n",
    "                   help='lower lr bound for cyclic schedulers that hit 0 (default: 0)')\n",
    "group.add_argument('--epochs', type=int, default=300, metavar='N',\n",
    "                   help='number of epochs to train (default: 300)')\n",
    "group.add_argument('--epoch-repeats', type=float, default=0., metavar='N',\n",
    "                   help='epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).')\n",
    "group.add_argument('--start-epoch', default=None, type=int, metavar='N',\n",
    "                   help='manual epoch number (useful on restarts)')\n",
    "group.add_argument('--decay-milestones', default=[90, 180, 270], type=int, nargs='+', metavar=\"MILESTONES\",\n",
    "                   help='list of decay epoch indices for multistep lr. must be increasing')\n",
    "group.add_argument('--decay-epochs', type=float, default=90, metavar='N',\n",
    "                   help='epoch interval to decay LR')\n",
    "group.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n",
    "                   help='epochs to warmup LR, if scheduler supports')\n",
    "group.add_argument('--warmup-prefix', action='store_true', default=False,\n",
    "                   help='Exclude warmup period from decay schedule.'),\n",
    "group.add_argument('--cooldown-epochs', type=int, default=0, metavar='N',\n",
    "                   help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
    "group.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
    "                   help='patience epochs for Plateau LR scheduler (default: 10)')\n",
    "group.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
    "                   help='LR decay rate (default: 0.1)')\n",
    "\n",
    "# Augmentation & regularization parameters\n",
    "group = parser.add_argument_group('Augmentation and regularization parameters')\n",
    "group.add_argument('--no-aug', action='store_true', default=False,\n",
    "                   help='Disable all training augmentation, override other train aug args')\n",
    "group.add_argument('--scale', type=float, nargs='+', default=[0.08, 1.0], metavar='PCT',\n",
    "                   help='Random resize scale (default: 0.08 1.0)')\n",
    "group.add_argument('--ratio', type=float, nargs='+', default=[3. / 4., 4. / 3.], metavar='RATIO',\n",
    "                   help='Random resize aspect ratio (default: 0.75 1.33)')\n",
    "group.add_argument('--hflip', type=float, default=0.5,\n",
    "                   help='Horizontal flip training aug probability')\n",
    "group.add_argument('--vflip', type=float, default=0.,\n",
    "                   help='Vertical flip training aug probability')\n",
    "group.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n",
    "                   help='Color jitter factor (default: 0.4)')\n",
    "group.add_argument('--aa', type=str, default=None, metavar='NAME',\n",
    "                   help='Use AutoAugment policy. \"v0\" or \"original\". (default: None)'),\n",
    "group.add_argument('--aug-repeats', type=float, default=0,\n",
    "                   help='Number of augmentation repetitions (distributed training only) (default: 0)')\n",
    "group.add_argument('--aug-splits', type=int, default=0,\n",
    "                   help='Number of augmentation splits (default: 0, valid: 0 or >=2)')\n",
    "group.add_argument('--jsd-loss', action='store_true', default=False,\n",
    "                   help='Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.')\n",
    "group.add_argument('--bce-loss', action='store_true', default=False,\n",
    "                   help='Enable BCE loss w/ Mixup/CutMix use.')\n",
    "group.add_argument('--bce-target-thresh', type=float, default=None,\n",
    "                   help='Threshold for binarizing softened BCE targets (default: None, disabled)')\n",
    "group.add_argument('--reprob', type=float, default=0., metavar='PCT',\n",
    "                   help='Random erase prob (default: 0.)')\n",
    "group.add_argument('--remode', type=str, default='pixel',\n",
    "                   help='Random erase mode (default: \"pixel\")')\n",
    "group.add_argument('--recount', type=int, default=1,\n",
    "                   help='Random erase count (default: 1)')\n",
    "group.add_argument('--resplit', action='store_true', default=False,\n",
    "                   help='Do not random erase first (clean) augmentation split')\n",
    "group.add_argument('--mixup', type=float, default=0.0,\n",
    "                   help='mixup alpha, mixup enabled if > 0. (default: 0.)')\n",
    "group.add_argument('--cutmix', type=float, default=0.0,\n",
    "                   help='cutmix alpha, cutmix enabled if > 0. (default: 0.)')\n",
    "group.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
    "                   help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "group.add_argument('--mixup-prob', type=float, default=1.0,\n",
    "                   help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "group.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
    "                   help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "group.add_argument('--mixup-mode', type=str, default='batch',\n",
    "                   help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "group.add_argument('--mixup-off-epoch', default=0, type=int, metavar='N',\n",
    "                   help='Turn off mixup after this epoch, disabled if 0 (default: 0)')\n",
    "group.add_argument('--smoothing', type=float, default=0.1,\n",
    "                   help='Label smoothing (default: 0.1)')\n",
    "group.add_argument('--train-interpolation', type=str, default='random',\n",
    "                   help='Training interpolation (random, bilinear, bicubic default: \"random\")')\n",
    "group.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                   help='Dropout rate (default: 0.)')\n",
    "group.add_argument('--drop-connect', type=float, default=None, metavar='PCT',\n",
    "                   help='Drop connect rate, DEPRECATED, use drop-path (default: None)')\n",
    "group.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n",
    "                   help='Drop path rate (default: None)')\n",
    "group.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n",
    "                   help='Drop block rate (default: None)')\n",
    "\n",
    "# Batch norm parameters (only works with gen_efficientnet based models currently)\n",
    "group = parser.add_argument_group('Batch norm parameters', 'Only works with gen_efficientnet based models currently.')\n",
    "group.add_argument('--bn-momentum', type=float, default=None,\n",
    "                   help='BatchNorm momentum override (if not None)')\n",
    "group.add_argument('--bn-eps', type=float, default=None,\n",
    "                   help='BatchNorm epsilon override (if not None)')\n",
    "group.add_argument('--sync-bn', action='store_true',\n",
    "                   help='Enable NVIDIA Apex or Torch synchronized BatchNorm.')\n",
    "group.add_argument('--dist-bn', type=str, default='reduce',\n",
    "                   help='Distribute BatchNorm stats between nodes after each epoch (\"broadcast\", \"reduce\", or \"\")')\n",
    "group.add_argument('--split-bn', action='store_true',\n",
    "                   help='Enable separate BN layers per augmentation split.')\n",
    "\n",
    "# Model Exponential Moving Average\n",
    "group = parser.add_argument_group('Model exponential moving average parameters')\n",
    "group.add_argument('--model-ema', action='store_true', default=False,\n",
    "                   help='Enable tracking moving average of model weights')\n",
    "group.add_argument('--model-ema-force-cpu', action='store_true', default=False,\n",
    "                   help='Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.')\n",
    "group.add_argument('--model-ema-decay', type=float, default=0.9998,\n",
    "                   help='decay factor for model weights moving average (default: 0.9998)')\n",
    "\n",
    "# Misc\n",
    "group = parser.add_argument_group('Miscellaneous parameters')\n",
    "group.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                   help='random seed (default: 42)')\n",
    "group.add_argument('--worker-seeding', type=str, default='all',\n",
    "                   help='worker seed mode (default: all)')\n",
    "group.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                   help='how many batches to wait before logging training status')\n",
    "group.add_argument('--recovery-interval', type=int, default=0, metavar='N',\n",
    "                   help='how many batches to wait before writing recovery checkpoint')\n",
    "group.add_argument('--checkpoint-hist', type=int, default=10, metavar='N',\n",
    "                   help='number of checkpoints to keep (default: 10)')\n",
    "group.add_argument('-j', '--workers', type=int, default=4, metavar='N',\n",
    "                   help='how many training processes to use (default: 4)')\n",
    "group.add_argument('--save-images', action='store_true', default=False,\n",
    "                   help='save images of input bathes every log interval for debugging')\n",
    "group.add_argument('--amp', action='store_true', default=False,\n",
    "                   help='use NVIDIA Apex AMP or Native AMP for mixed precision training')\n",
    "group.add_argument('--amp-dtype', default='float16', type=str,\n",
    "                   help='lower precision AMP dtype (default: float16)')\n",
    "group.add_argument('--amp-impl', default='native', type=str,\n",
    "                   help='AMP impl to use, \"native\" or \"apex\" (default: native)')\n",
    "group.add_argument('--no-ddp-bb', action='store_true', default=False,\n",
    "                   help='Force broadcast buffers for native DDP to off.')\n",
    "group.add_argument('--synchronize-step', action='store_true', default=False,\n",
    "                   help='torch.cuda.synchronize() end of each step')\n",
    "group.add_argument('--pin-mem', action='store_true', default=False,\n",
    "                   help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "group.add_argument('--no-prefetcher', action='store_true', default=False,\n",
    "                   help='disable fast prefetcher')\n",
    "group.add_argument('--output', default='', type=str, metavar='PATH',\n",
    "                   help='path to output folder (default: none, current dir)')\n",
    "group.add_argument('--experiment', default='', type=str, metavar='NAME',\n",
    "                   help='name of train experiment, name of sub-folder for output')\n",
    "group.add_argument('--eval-metric', default='top1', type=str, metavar='EVAL_METRIC',\n",
    "                   help='Best metric (default: \"top1\"')\n",
    "group.add_argument('--tta', type=int, default=0, metavar='N',\n",
    "                   help='Test/inference time augmentation (oversampling) factor. 0=None (default: 0)')\n",
    "group.add_argument(\"--local_rank\", default=0, type=int)\n",
    "group.add_argument('--use-multi-epochs-loader', action='store_true', default=False,\n",
    "                   help='use the multi-epochs-loader to save time at the beginning of every epoch')\n",
    "group.add_argument('--log-wandb', action='store_true', default=False,\n",
    "                   help='log training and validation metrics to wandb')\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重写之后，我们尝试新建一个 budgeted training 的 model，设置为初始 stage，在 config 里我们已经将其设置为 baby model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data=None, data_dir='/home/fish/Documents/imagenet/', dataset='', train_split='train', val_split='validation', dataset_download=False, class_map='', model='bud_small_patch16_224_baby', pretrained=False, initial_checkpoint='', resume='', no_resume_opt=False, num_classes=None, gp=None, img_size=None, in_chans=None, input_size=None, crop_pct=None, mean=None, std=None, interpolation='', batch_size=128, validation_batch_size=None, channels_last=False, fuser='', grad_accum_steps=1, grad_checkpointing=False, fast_norm=False, model_kwargs={}, head_init_scale=None, head_init_bias=None, torchscript=False, torchcompile=None, opt='lion', opt_eps=None, opt_betas=None, momentum=0.9, weight_decay=0.001, clip_grad=None, clip_mode='norm', layer_decay=None, opt_kwargs={}, use_eva=False, exclude_parts='', kfac_update_freq=1, kfac_cov_update_freq=1, kfac_update_freq_alpha=10, kfac_update_freq_decay=None, stat_decay=0.95, damping=0.001, damping_alpha=0.5, damping_decay=None, kl_clip=0.001, diag_blocks=1, diag_warmup=0, distribute_layer_factors=None, sched='cosine', sched_on_updates=False, lr=None, lr_base=0.0001, lr_base_size=256, lr_base_scale='', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_decay=0.5, lr_cycle_limit=1, lr_k_decay=1.0, warmup_lr=1e-05, min_lr=0, epochs=15, epoch_repeats=0.0, start_epoch=None, decay_milestones=[90, 180, 270], decay_epochs=90, warmup_epochs=0, warmup_prefix=False, cooldown_epochs=0, patience_epochs=10, decay_rate=0.1, no_aug=False, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0.5, vflip=0.0, color_jitter=0.4, aa=None, aug_repeats=0, aug_splits=0, jsd_loss=False, bce_loss=False, bce_target_thresh=None, reprob=0.0, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, smoothing=0.1, train_interpolation='random', drop=0.0, drop_connect=None, drop_path=None, drop_block=None, bn_momentum=None, bn_eps=None, sync_bn=False, dist_bn='reduce', split_bn=False, model_ema=False, model_ema_force_cpu=False, model_ema_decay=0.9998, seed=42, worker_seeding='all', log_interval=50, recovery_interval=0, checkpoint_hist=10, workers=4, save_images=False, amp=True, amp_dtype='float16', amp_impl='native', no_ddp_bb=False, synchronize_step=False, pin_mem=False, no_prefetcher=False, output='./output', experiment='budgeted', eval_metric='top1', tta=0, local_rank=0, use_multi_epochs_loader=False, log_wandb=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_line_args = [\n",
    "    \"--model\", \"bud_small_patch16_224_baby\",\n",
    "    \"--batch-size\", \"128\",\n",
    "    \"--epochs\", \"15\",\n",
    "    \"--warmup-epochs\", \"0\",\n",
    "    \"--data-dir\", \"/home/fish/Documents/imagenet/\",\n",
    "    \"--output\", \"./output\",\n",
    "    \"--experiment\", \"budgeted\",\n",
    "    # \"--num-stages\", \"3\",\n",
    "    \"--amp\",\n",
    "    \"--opt\", \"lion\",\n",
    "    \"--lr-base\", \"1e-4\",\n",
    "    \"--weight-decay\", \"1e-3\",\n",
    "    \"--log-wandb\",\n",
    "]\n",
    "\n",
    "args = parser.parse_args(cmd_line_args)\n",
    "args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 logging，注意只运行一次该格，不然会有重复输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_default_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.init(project=\"jupyter-projo\",\n",
    "#            config={\n",
    "#                \"batch_size\": 128,\n",
    "#                \"learning_rate\": 0.01,\n",
    "#                \"dataset\": \"CIFAR-100\",\n",
    "#            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB_NOTEBOOK_NAME = 'budgeted_train.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码进行训练的初始设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with a single process on 1 device (cuda:0).\n",
      "Model bud_small_patch16_224_baby created, param count:6101224\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n",
      "Learning rate (5e-05) calculated from base learning rate (0.0001) and effective global batch size (128) with linear scaling.\n",
      "Using native Torch AMP. Training in mixed precision.\n",
      "ERROR: Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfabfish\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fish/Documents/GitHub/pytorch-image-models/wandb/run-20230827_153352-ltuo5m6f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabfish/timm/runs/ltuo5m6f' target=\"_blank\">budgeted</a></strong> to <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">https://wandb.ai/fabfish/timm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabfish/timm/runs/ltuo5m6f' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/ltuo5m6f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scheduled epochs: 15. LR stepped per epoch.\n"
     ]
    }
   ],
   "source": [
    "    # utils.setup_default_logging()\n",
    "    # args, args_text = _parse_args() # args 我们已经另外做了处理\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    args.prefetcher = not args.no_prefetcher\n",
    "    args.grad_accum_steps = max(1, args.grad_accum_steps)\n",
    "    device = utils.init_distributed_device(args)\n",
    "    if args.distributed:\n",
    "        _logger.info(\n",
    "            'Training in distributed mode with multiple processes, 1 device per process.'\n",
    "            f'Process {args.rank}, total {args.world_size}, device {args.device}.')\n",
    "    else:\n",
    "        _logger.info(f'Training with a single process on 1 device ({args.device}).')\n",
    "    assert args.rank >= 0\n",
    "\n",
    "    # resolve AMP arguments based on PyTorch / Apex availability\n",
    "    use_amp = None\n",
    "    amp_dtype = torch.float16\n",
    "    if args.amp:\n",
    "        if args.amp_impl == 'apex':\n",
    "            assert has_apex, 'AMP impl specified as APEX but APEX is not installed.'\n",
    "            use_amp = 'apex'\n",
    "            assert args.amp_dtype == 'float16'\n",
    "        else:\n",
    "            assert has_native_amp, 'Please update PyTorch to a version with native AMP (or use APEX).'\n",
    "            use_amp = 'native'\n",
    "            assert args.amp_dtype in ('float16', 'bfloat16')\n",
    "        if args.amp_dtype == 'bfloat16':\n",
    "            amp_dtype = torch.bfloat16\n",
    "\n",
    "    utils.random_seed(args.seed, args.rank)\n",
    "\n",
    "    if args.fuser:\n",
    "        utils.set_jit_fuser(args.fuser)\n",
    "    if args.fast_norm:\n",
    "        set_fast_norm()\n",
    "\n",
    "    in_chans = 3\n",
    "    if args.in_chans is not None:\n",
    "        in_chans = args.in_chans\n",
    "    elif args.input_size is not None:\n",
    "        in_chans = args.input_size[0]\n",
    "\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # create the train and eval datasets\n",
    "    if args.data and not args.data_dir:\n",
    "        args.data_dir = args.data\n",
    "    dataset_train = create_dataset(\n",
    "        args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split=args.train_split,\n",
    "        is_training=True,\n",
    "        class_map=args.class_map,\n",
    "        download=args.dataset_download,\n",
    "        batch_size=args.batch_size,\n",
    "        seed=args.seed,\n",
    "        repeats=args.epoch_repeats,\n",
    "    )\n",
    "\n",
    "    dataset_eval = create_dataset(\n",
    "        args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split=args.val_split,\n",
    "        is_training=False,\n",
    "        class_map=args.class_map,\n",
    "        download=args.dataset_download,\n",
    "        batch_size=args.batch_size,\n",
    "    )\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "    loader_train = create_loader(\n",
    "        dataset_train,\n",
    "        input_size=data_config['input_size'],\n",
    "        batch_size=args.batch_size,\n",
    "        is_training=True,\n",
    "        use_prefetcher=args.prefetcher,\n",
    "        no_aug=args.no_aug,\n",
    "        re_prob=args.reprob,\n",
    "        re_mode=args.remode,\n",
    "        re_count=args.recount,\n",
    "        re_split=args.resplit,\n",
    "        scale=args.scale,\n",
    "        ratio=args.ratio,\n",
    "        hflip=args.hflip,\n",
    "        vflip=args.vflip,\n",
    "        color_jitter=args.color_jitter,\n",
    "        auto_augment=args.aa,\n",
    "        num_aug_repeats=args.aug_repeats,\n",
    "        num_aug_splits=num_aug_splits,\n",
    "        interpolation=train_interpolation,\n",
    "        mean=data_config['mean'],\n",
    "        std=data_config['std'],\n",
    "        num_workers=args.workers,\n",
    "        distributed=args.distributed,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=args.pin_mem,\n",
    "        device=device,\n",
    "        use_multi_epochs_loader=args.use_multi_epochs_loader,\n",
    "        worker_seeding=args.worker_seeding,\n",
    "    )\n",
    "\n",
    "    eval_workers = args.workers\n",
    "    if args.distributed and ('tfds' in args.dataset or 'wds' in args.dataset):\n",
    "        # FIXME reduces validation padding issues when using TFDS, WDS w/ workers and distributed training\n",
    "        eval_workers = min(2, args.workers)\n",
    "    loader_eval = create_loader(\n",
    "        dataset_eval,\n",
    "        input_size=data_config['input_size'],\n",
    "        batch_size=args.validation_batch_size or args.batch_size,\n",
    "        is_training=False,\n",
    "        use_prefetcher=args.prefetcher,\n",
    "        interpolation=data_config['interpolation'],\n",
    "        mean=data_config['mean'],\n",
    "        std=data_config['std'],\n",
    "        num_workers=eval_workers,\n",
    "        distributed=args.distributed,\n",
    "        crop_pct=data_config['crop_pct'],\n",
    "        pin_memory=args.pin_mem,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm', name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 0\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 5 个 epoch 查看结果。下面第一格是训练代码，第二格是训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_one_epoch(\n",
    "        epoch,\n",
    "        model,\n",
    "        loader,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        args,\n",
    "        device=torch.device('cuda'),\n",
    "        lr_scheduler=None,\n",
    "        saver=None,\n",
    "        output_dir=None,\n",
    "        amp_autocast=suppress,\n",
    "        loss_scaler=None,\n",
    "        model_ema=None,\n",
    "        mixup_fn=None,\n",
    "        # fish: add preconditioner support\n",
    "        preconditioner=None,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n",
    "        if args.prefetcher and loader.mixup_enabled:\n",
    "            loader.mixup_enabled = False\n",
    "        elif mixup_fn is not None:\n",
    "            mixup_fn.mixup_enabled = False\n",
    "\n",
    "    second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "    has_no_sync = hasattr(model, \"no_sync\")\n",
    "    update_time_m = utils.AverageMeter()\n",
    "    data_time_m = utils.AverageMeter()\n",
    "    losses_m = utils.AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    accum_steps = args.grad_accum_steps\n",
    "    last_accum_steps = len(loader) % accum_steps\n",
    "    updates_per_epoch = (len(loader) + accum_steps - 1) // accum_steps\n",
    "    num_updates = epoch * updates_per_epoch\n",
    "    last_batch_idx = len(loader) - 1\n",
    "    last_batch_idx_to_accum = len(loader) - last_accum_steps\n",
    "\n",
    "    data_start_time = update_start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    update_sample_count = 0\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        last_batch = batch_idx == last_batch_idx\n",
    "        need_update = last_batch or (batch_idx + 1) % accum_steps == 0\n",
    "        update_idx = batch_idx // accum_steps\n",
    "        if batch_idx >= last_batch_idx_to_accum:\n",
    "            accum_steps = last_accum_steps\n",
    "\n",
    "        if not args.prefetcher:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            if mixup_fn is not None:\n",
    "                input, target = mixup_fn(input, target)\n",
    "        if args.channels_last:\n",
    "            input = input.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "        # multiply by accum steps to get equivalent for full update\n",
    "        data_time_m.update(accum_steps * (time.time() - data_start_time))\n",
    "\n",
    "        def _forward():\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "                loss = loss_fn(output, target)\n",
    "            if accum_steps > 1:\n",
    "                loss /= accum_steps\n",
    "            return loss\n",
    "\n",
    "        def _backward(_loss):\n",
    "            if loss_scaler is not None:\n",
    "                loss_scaler(\n",
    "                    _loss,\n",
    "                    optimizer,\n",
    "                    clip_grad=args.clip_grad,\n",
    "                    clip_mode=args.clip_mode,\n",
    "                    parameters=model_parameters(model, exclude_head='agc' in args.clip_mode),\n",
    "                    create_graph=second_order,\n",
    "                    need_update=need_update,\n",
    "                )\n",
    "            else:\n",
    "                _loss.backward(create_graph=second_order)\n",
    "                if need_update:\n",
    "                    if args.clip_grad is not None:\n",
    "                        utils.dispatch_clip_grad(\n",
    "                            model_parameters(model, exclude_head='agc' in args.clip_mode),\n",
    "                            value=args.clip_grad,\n",
    "                            mode=args.clip_mode,\n",
    "                        )\n",
    "                    # fish: step the optimizer\n",
    "                    if preconditioner is not None:\n",
    "                        # self._scaler.step(preconditioner)\n",
    "                        preconditioner.step()\n",
    "                    optimizer.step()\n",
    "\n",
    "        if has_no_sync and not need_update:\n",
    "            with model.no_sync():\n",
    "                loss = _forward()\n",
    "                _backward(loss)\n",
    "        else:\n",
    "            loss = _forward()\n",
    "            _backward(loss)\n",
    "\n",
    "        if not args.distributed:\n",
    "            losses_m.update(loss.item() * accum_steps, input.size(0))\n",
    "        update_sample_count += input.size(0)\n",
    "\n",
    "        if not need_update:\n",
    "            data_start_time = time.time()\n",
    "            continue\n",
    "\n",
    "        num_updates += 1\n",
    "        optimizer.zero_grad()\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        if args.synchronize_step and device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        time_now = time.time()\n",
    "        update_time_m.update(time.time() - update_start_time)\n",
    "        update_start_time = time_now\n",
    "\n",
    "        if update_idx % args.log_interval == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = utils.reduce_tensor(loss.data, args.world_size)\n",
    "                losses_m.update(reduced_loss.item() * accum_steps, input.size(0))\n",
    "                update_sample_count *= args.world_size\n",
    "\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\n",
    "                    f'Train: {epoch} [{update_idx:>4d}/{updates_per_epoch} '\n",
    "                    f'({100. * update_idx / (updates_per_epoch - 1):>3.0f}%)]  '\n",
    "                    f'Loss: {losses_m.val:#.3g} ({losses_m.avg:#.3g})  '\n",
    "                    f'Time: {update_time_m.val:.3f}s, {update_sample_count / update_time_m.val:>7.2f}/s  '\n",
    "                    f'({update_time_m.avg:.3f}s, {update_sample_count / update_time_m.avg:>7.2f}/s)  '\n",
    "                    f'LR: {lr:.3e}  '\n",
    "                    f'Data: {data_time_m.val:.3f} ({data_time_m.avg:.3f})'\n",
    "                    f'Time: {time.time() - start_time:.3f}s'\n",
    "                )\n",
    "\n",
    "                if args.save_images and output_dir:\n",
    "                    torchvision.utils.save_image(\n",
    "                        input,\n",
    "                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n",
    "                        padding=0,\n",
    "                        normalize=True\n",
    "                    )\n",
    "\n",
    "        if saver is not None and args.recovery_interval and (\n",
    "                (update_idx + 1) % args.recovery_interval == 0):\n",
    "            saver.save_recovery(epoch, batch_idx=update_idx)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "\n",
    "        update_sample_count = 0\n",
    "        data_start_time = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])\n",
    "\n",
    "\n",
    "def validate(\n",
    "        model,\n",
    "        loader,\n",
    "        loss_fn,\n",
    "        args,\n",
    "        device=torch.device('cuda'),\n",
    "        amp_autocast=suppress,\n",
    "        log_suffix=''\n",
    "):\n",
    "    batch_time_m = utils.AverageMeter()\n",
    "    losses_m = utils.AverageMeter()\n",
    "    top1_m = utils.AverageMeter()\n",
    "    top5_m = utils.AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            last_batch = batch_idx == last_idx\n",
    "            if not args.prefetcher:\n",
    "                input = input.to(device)\n",
    "                target = target.to(device)\n",
    "            if args.channels_last:\n",
    "                input = input.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "                if isinstance(output, (tuple, list)):\n",
    "                    output = output[0]\n",
    "\n",
    "                # augmentation reduction\n",
    "                reduce_factor = args.tta\n",
    "                if reduce_factor > 1:\n",
    "                    output = output.unfold(0, reduce_factor, reduce_factor).mean(dim=2)\n",
    "                    target = target[0:target.size(0):reduce_factor]\n",
    "\n",
    "                loss = loss_fn(output, target)\n",
    "            acc1, acc5 = utils.accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = utils.reduce_tensor(loss.data, args.world_size)\n",
    "                acc1 = utils.reduce_tensor(acc1, args.world_size)\n",
    "                acc5 = utils.reduce_tensor(acc5, args.world_size)\n",
    "            else:\n",
    "                reduced_loss = loss.data\n",
    "\n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "\n",
    "            losses_m.update(reduced_loss.item(), input.size(0))\n",
    "            top1_m.update(acc1.item(), output.size(0))\n",
    "            top5_m.update(acc5.item(), output.size(0))\n",
    "\n",
    "            batch_time_m.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if utils.is_primary(args) and (last_batch or batch_idx % args.log_interval == 0):\n",
    "                log_name = 'Test' + log_suffix\n",
    "                _logger.info(\n",
    "                    f'{log_name}: [{batch_idx:>4d}/{last_idx}]  '\n",
    "                    f'Time: {batch_time_m.val:.3f} ({batch_time_m.avg:.3f})  '\n",
    "                    f'Loss: {losses_m.val:>7.3f} ({losses_m.avg:>6.3f})  '\n",
    "                    f'Acc@1: {top1_m.val:>7.3f} ({top1_m.avg:>7.3f})  '\n",
    "                    f'Acc@5: {top5_m.val:>7.3f} ({top5_m.avg:>7.3f})'\n",
    "                )\n",
    "\n",
    "    metrics = OrderedDict([('loss', losses_m.avg), ('top1', top1_m.avg), ('top5', top5_m.avg)])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fish/anaconda3/envs/timm/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Train: 0 [   0/10009 (  0%)]  Loss: 6.97 (6.97)  Time: 2.181s,   58.68/s  (2.181s,   58.68/s)  LR: 5.000e-05  Data: 0.790 (0.790)Time: 2.182s\n",
      "Train: 0 [  50/10009 (  0%)]  Loss: 6.95 (6.94)  Time: 0.109s, 1177.51/s  (0.173s,  739.27/s)  LR: 5.000e-05  Data: 0.005 (0.039)Time: 8.831s\n",
      "Train: 0 [ 100/10009 (  1%)]  Loss: 6.91 (6.92)  Time: 0.110s, 1167.40/s  (0.154s,  833.77/s)  LR: 5.000e-05  Data: 0.006 (0.031)Time: 15.506s\n",
      "Train: 0 [ 150/10009 (  1%)]  Loss: 6.82 (6.89)  Time: 0.123s, 1038.01/s  (0.146s,  878.58/s)  LR: 5.000e-05  Data: 0.007 (0.027)Time: 22.000s\n",
      "Train: 0 [ 200/10009 (  2%)]  Loss: 6.88 (6.87)  Time: 0.114s, 1117.97/s  (0.141s,  905.56/s)  LR: 5.000e-05  Data: 0.006 (0.024)Time: 28.412s\n",
      "Train: 0 [ 250/10009 (  2%)]  Loss: 6.84 (6.86)  Time: 0.110s, 1163.33/s  (0.140s,  914.06/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 35.149s\n",
      "Train: 0 [ 300/10009 (  3%)]  Loss: 6.80 (6.84)  Time: 0.109s, 1170.01/s  (0.138s,  926.56/s)  LR: 5.000e-05  Data: 0.006 (0.023)Time: 41.582s\n",
      "Train: 0 [ 350/10009 (  3%)]  Loss: 6.65 (6.83)  Time: 0.110s, 1158.70/s  (0.137s,  931.93/s)  LR: 5.000e-05  Data: 0.005 (0.023)Time: 48.210s\n",
      "Train: 0 [ 400/10009 (  4%)]  Loss: 6.81 (6.82)  Time: 0.109s, 1177.90/s  (0.137s,  931.14/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 55.124s\n",
      "Train: 0 [ 450/10009 (  4%)]  Loss: 6.77 (6.81)  Time: 0.111s, 1154.55/s  (0.137s,  934.94/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 61.746s\n",
      "Train: 0 [ 500/10009 (  5%)]  Loss: 6.69 (6.80)  Time: 0.109s, 1175.27/s  (0.137s,  935.73/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 68.533s\n",
      "Train: 0 [ 550/10009 (  5%)]  Loss: 6.67 (6.79)  Time: 0.111s, 1155.10/s  (0.137s,  937.18/s)  LR: 5.000e-05  Data: 0.006 (0.024)Time: 75.256s\n",
      "Train: 0 [ 600/10009 (  6%)]  Loss: 6.60 (6.77)  Time: 0.111s, 1157.79/s  (0.136s,  940.08/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 81.832s\n",
      "Train: 0 [ 650/10009 (  6%)]  Loss: 6.58 (6.76)  Time: 0.110s, 1165.46/s  (0.136s,  941.44/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 88.512s\n",
      "Train: 0 [ 700/10009 (  7%)]  Loss: 6.61 (6.75)  Time: 0.113s, 1134.75/s  (0.135s,  945.86/s)  LR: 5.000e-05  Data: 0.007 (0.024)Time: 94.864s\n",
      "Train: 0 [ 750/10009 (  7%)]  Loss: 6.59 (6.74)  Time: 0.111s, 1154.67/s  (0.135s,  948.57/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 101.340s\n",
      "Train: 0 [ 800/10009 (  8%)]  Loss: 6.60 (6.73)  Time: 0.111s, 1157.71/s  (0.135s,  950.92/s)  LR: 5.000e-05  Data: 0.007 (0.023)Time: 107.820s\n",
      "Train: 0 [ 850/10009 (  8%)]  Loss: 6.47 (6.72)  Time: 0.109s, 1176.78/s  (0.135s,  950.43/s)  LR: 5.000e-05  Data: 0.006 (0.023)Time: 114.609s\n",
      "Train: 0 [ 900/10009 (  9%)]  Loss: 6.50 (6.71)  Time: 0.110s, 1160.61/s  (0.135s,  950.33/s)  LR: 5.000e-05  Data: 0.006 (0.024)Time: 121.356s\n",
      "Train: 0 [ 950/10009 (  9%)]  Loss: 6.50 (6.70)  Time: 0.124s, 1032.16/s  (0.135s,  946.23/s)  LR: 5.000e-05  Data: 0.006 (0.024)Time: 128.646s\n",
      "Train: 0 [1000/10009 ( 10%)]  Loss: 6.62 (6.69)  Time: 0.110s, 1163.48/s  (0.135s,  947.74/s)  LR: 5.000e-05  Data: 0.006 (0.024)Time: 135.194s\n",
      "Train: 0 [1050/10009 ( 10%)]  Loss: 6.40 (6.68)  Time: 0.193s,  664.79/s  (0.135s,  946.51/s)  LR: 5.000e-05  Data: 0.090 (0.024)Time: 142.131s\n",
      "Train: 0 [1100/10009 ( 11%)]  Loss: 6.46 (6.67)  Time: 0.110s, 1165.85/s  (0.136s,  944.28/s)  LR: 5.000e-05  Data: 0.006 (0.024)Time: 149.243s\n",
      "Train: 0 [1150/10009 ( 11%)]  Loss: 6.40 (6.66)  Time: 0.223s,  575.21/s  (0.135s,  945.18/s)  LR: 5.000e-05  Data: 0.117 (0.024)Time: 155.874s\n",
      "Train: 0 [1200/10009 ( 12%)]  Loss: 6.34 (6.65)  Time: 0.287s,  446.72/s  (0.135s,  945.57/s)  LR: 5.000e-05  Data: 0.183 (0.024)Time: 162.577s\n",
      "Train: 0 [1250/10009 ( 12%)]  Loss: 6.37 (6.63)  Time: 0.108s, 1182.66/s  (0.135s,  945.36/s)  LR: 5.000e-05  Data: 0.005 (0.024)Time: 169.382s\n",
      "Train: 0 [1300/10009 ( 13%)]  Loss: 6.33 (6.62)  Time: 0.141s,  909.46/s  (0.136s,  944.53/s)  LR: 5.000e-05  Data: 0.037 (0.024)Time: 176.307s\n",
      "Train: 0 [1350/10009 ( 13%)]  Loss: 6.35 (6.61)  Time: 0.142s,  903.28/s  (0.136s,  940.00/s)  LR: 5.000e-05  Data: 0.006 (0.025)Time: 183.966s\n",
      "Train: 0 [1400/10009 ( 14%)]  Loss: 6.38 (6.60)  Time: 0.112s, 1144.96/s  (0.136s,  938.73/s)  LR: 5.000e-05  Data: 0.005 (0.025)Time: 191.033s\n",
      "Train: 0 [1450/10009 ( 14%)]  Loss: 6.46 (6.60)  Time: 0.111s, 1152.20/s  (0.136s,  939.10/s)  LR: 5.000e-05  Data: 0.005 (0.025)Time: 197.772s\n",
      "Train: 0 [1500/10009 ( 15%)]  Loss: 6.28 (6.59)  Time: 0.110s, 1165.61/s  (0.136s,  939.30/s)  LR: 5.000e-05  Data: 0.006 (0.025)Time: 204.544s\n",
      "Train: 0 [1550/10009 ( 15%)]  Loss: 6.31 (6.58)  Time: 0.112s, 1147.51/s  (0.136s,  939.04/s)  LR: 5.000e-05  Data: 0.006 (0.025)Time: 211.416s\n",
      "Train: 0 [1600/10009 ( 16%)]  Loss: 6.25 (6.57)  Time: 0.230s,  556.78/s  (0.136s,  937.80/s)  LR: 5.000e-05  Data: 0.107 (0.025)Time: 218.519s\n",
      "Train: 0 [1650/10009 ( 16%)]  Loss: 6.29 (6.56)  Time: 0.125s, 1027.35/s  (0.136s,  937.79/s)  LR: 5.000e-05  Data: 0.005 (0.025)Time: 225.347s\n",
      "Train: 0 [1700/10009 ( 17%)]  Loss: 6.21 (6.55)  Time: 0.112s, 1145.69/s  (0.136s,  937.95/s)  LR: 5.000e-05  Data: 0.006 (0.025)Time: 232.131s\n",
      "Train: 0 [1750/10009 ( 17%)]  Loss: 6.00 (6.53)  Time: 0.113s, 1129.08/s  (0.136s,  937.75/s)  LR: 5.000e-05  Data: 0.006 (0.025)Time: 239.006s\n",
      "Train: 0 [1800/10009 ( 18%)]  Loss: 5.97 (6.53)  Time: 0.110s, 1161.48/s  (0.137s,  937.47/s)  LR: 5.000e-05  Data: 0.006 (0.025)Time: 245.905s\n",
      "Train: 0 [1850/10009 ( 18%)]  Loss: 6.25 (6.52)  Time: 0.109s, 1176.15/s  (0.137s,  935.18/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 253.351s\n",
      "Train: 0 [1900/10009 ( 19%)]  Loss: 6.05 (6.51)  Time: 0.147s,  873.04/s  (0.137s,  933.78/s)  LR: 5.000e-05  Data: 0.042 (0.026)Time: 260.583s\n",
      "Train: 0 [1950/10009 ( 19%)]  Loss: 6.08 (6.50)  Time: 0.112s, 1146.51/s  (0.137s,  931.47/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 268.099s\n",
      "Train: 0 [2000/10009 ( 20%)]  Loss: 6.23 (6.49)  Time: 0.156s,  819.71/s  (0.137s,  932.04/s)  LR: 5.000e-05  Data: 0.029 (0.026)Time: 274.804s\n",
      "Train: 0 [2050/10009 ( 20%)]  Loss: 6.12 (6.48)  Time: 0.113s, 1134.61/s  (0.137s,  932.71/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 281.468s\n",
      "Train: 0 [2100/10009 ( 21%)]  Loss: 5.98 (6.47)  Time: 0.112s, 1143.86/s  (0.137s,  933.12/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 288.202s\n",
      "Train: 0 [2150/10009 ( 21%)]  Loss: 5.92 (6.46)  Time: 0.220s,  581.45/s  (0.137s,  932.64/s)  LR: 5.000e-05  Data: 0.116 (0.026)Time: 295.212s\n",
      "Train: 0 [2200/10009 ( 22%)]  Loss: 6.10 (6.45)  Time: 0.118s, 1082.67/s  (0.137s,  932.54/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 302.109s\n",
      "Train: 0 [2250/10009 ( 22%)]  Loss: 6.22 (6.44)  Time: 0.109s, 1174.07/s  (0.137s,  933.35/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 308.701s\n",
      "Train: 0 [2300/10009 ( 23%)]  Loss: 5.89 (6.43)  Time: 0.126s, 1017.78/s  (0.137s,  933.14/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 315.630s\n",
      "Train: 0 [2350/10009 ( 23%)]  Loss: 6.00 (6.42)  Time: 0.175s,  729.77/s  (0.137s,  933.51/s)  LR: 5.000e-05  Data: 0.051 (0.026)Time: 322.362s\n",
      "Train: 0 [2400/10009 ( 24%)]  Loss: 5.83 (6.41)  Time: 0.113s, 1134.90/s  (0.137s,  932.19/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 329.684s\n",
      "Train: 0 [2450/10009 ( 24%)]  Loss: 5.92 (6.40)  Time: 0.109s, 1175.82/s  (0.137s,  931.34/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 336.854s\n",
      "Train: 0 [2500/10009 ( 25%)]  Loss: 5.86 (6.40)  Time: 0.127s, 1009.13/s  (0.137s,  931.98/s)  LR: 5.000e-05  Data: 0.020 (0.026)Time: 343.490s\n",
      "Train: 0 [2550/10009 ( 25%)]  Loss: 6.06 (6.39)  Time: 0.126s, 1014.81/s  (0.137s,  931.41/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 350.573s\n",
      "Train: 0 [2600/10009 ( 26%)]  Loss: 6.08 (6.38)  Time: 0.118s, 1088.81/s  (0.137s,  931.94/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 357.243s\n",
      "Train: 0 [2650/10009 ( 26%)]  Loss: 5.92 (6.37)  Time: 0.140s,  916.14/s  (0.137s,  932.51/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 363.885s\n",
      "Train: 0 [2700/10009 ( 27%)]  Loss: 6.08 (6.36)  Time: 0.141s,  906.93/s  (0.137s,  932.09/s)  LR: 5.000e-05  Data: 0.009 (0.026)Time: 370.914s\n",
      "Train: 0 [2750/10009 ( 27%)]  Loss: 5.95 (6.35)  Time: 0.110s, 1160.94/s  (0.137s,  931.51/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 378.019s\n",
      "Train: 0 [2800/10009 ( 28%)]  Loss: 5.89 (6.35)  Time: 0.109s, 1173.33/s  (0.137s,  931.96/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 384.703s\n",
      "Train: 0 [2850/10009 ( 28%)]  Loss: 5.86 (6.34)  Time: 0.269s,  476.62/s  (0.137s,  931.88/s)  LR: 5.000e-05  Data: 0.165 (0.026)Time: 391.601s\n",
      "Train: 0 [2900/10009 ( 29%)]  Loss: 5.84 (6.33)  Time: 0.108s, 1180.04/s  (0.137s,  932.28/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 398.298s\n",
      "Train: 0 [2950/10009 ( 29%)]  Loss: 5.92 (6.32)  Time: 0.112s, 1145.44/s  (0.137s,  932.43/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 405.099s\n",
      "Train: 0 [3000/10009 ( 30%)]  Loss: 5.96 (6.31)  Time: 0.283s,  452.66/s  (0.137s,  931.60/s)  LR: 5.000e-05  Data: 0.173 (0.026)Time: 412.330s\n",
      "Train: 0 [3050/10009 ( 30%)]  Loss: 5.94 (6.31)  Time: 0.110s, 1165.54/s  (0.137s,  932.02/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 419.011s\n",
      "Train: 0 [3100/10009 ( 31%)]  Loss: 5.73 (6.30)  Time: 0.125s, 1023.19/s  (0.137s,  931.84/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 425.962s\n",
      "Train: 0 [3150/10009 ( 31%)]  Loss: 5.96 (6.29)  Time: 0.108s, 1186.34/s  (0.138s,  930.06/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 433.659s\n",
      "Train: 0 [3200/10009 ( 32%)]  Loss: 5.72 (6.28)  Time: 0.112s, 1139.36/s  (0.138s,  929.42/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 440.840s\n",
      "Train: 0 [3250/10009 ( 32%)]  Loss: 5.68 (6.28)  Time: 0.109s, 1178.55/s  (0.138s,  928.83/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 448.010s\n",
      "Train: 0 [3300/10009 ( 33%)]  Loss: 5.94 (6.27)  Time: 0.110s, 1160.23/s  (0.138s,  928.22/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 455.202s\n",
      "Train: 0 [3350/10009 ( 33%)]  Loss: 5.77 (6.26)  Time: 0.139s,  923.11/s  (0.138s,  928.27/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 462.071s\n",
      "Train: 0 [3400/10009 ( 34%)]  Loss: 5.83 (6.25)  Time: 0.118s, 1086.15/s  (0.138s,  928.70/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 468.748s\n",
      "Train: 0 [3450/10009 ( 34%)]  Loss: 5.75 (6.25)  Time: 0.110s, 1167.90/s  (0.138s,  928.55/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 475.716s\n",
      "Train: 0 [3500/10009 ( 35%)]  Loss: 5.84 (6.24)  Time: 0.110s, 1163.67/s  (0.138s,  928.43/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 482.672s\n",
      "Train: 0 [3550/10009 ( 35%)]  Loss: 5.96 (6.23)  Time: 0.208s,  616.15/s  (0.138s,  928.71/s)  LR: 5.000e-05  Data: 0.075 (0.026)Time: 489.416s\n",
      "Train: 0 [3600/10009 ( 36%)]  Loss: 5.98 (6.23)  Time: 0.110s, 1165.54/s  (0.138s,  928.48/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 496.430s\n",
      "Train: 0 [3650/10009 ( 36%)]  Loss: 5.74 (6.22)  Time: 0.111s, 1155.22/s  (0.138s,  927.97/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 503.603s\n",
      "Train: 0 [3700/10009 ( 37%)]  Loss: 5.60 (6.21)  Time: 0.141s,  910.58/s  (0.138s,  927.51/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 510.750s\n",
      "Train: 0 [3750/10009 ( 37%)]  Loss: 5.57 (6.21)  Time: 0.148s,  864.49/s  (0.138s,  927.01/s)  LR: 5.000e-05  Data: 0.008 (0.026)Time: 517.932s\n",
      "Train: 0 [3800/10009 ( 38%)]  Loss: 5.76 (6.20)  Time: 0.112s, 1137.99/s  (0.138s,  925.98/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 525.418s\n",
      "Train: 0 [3850/10009 ( 38%)]  Loss: 5.66 (6.20)  Time: 0.111s, 1152.67/s  (0.138s,  925.77/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 532.451s\n",
      "Train: 0 [3900/10009 ( 39%)]  Loss: 5.73 (6.19)  Time: 0.436s,  293.87/s  (0.138s,  925.19/s)  LR: 5.000e-05  Data: 0.303 (0.026)Time: 539.700s\n",
      "Train: 0 [3950/10009 ( 39%)]  Loss: 5.67 (6.18)  Time: 0.108s, 1183.58/s  (0.139s,  924.18/s)  LR: 5.000e-05  Data: 0.004 (0.026)Time: 547.214s\n",
      "Train: 0 [4000/10009 ( 40%)]  Loss: 5.65 (6.18)  Time: 0.152s,  843.47/s  (0.138s,  924.46/s)  LR: 5.000e-05  Data: 0.048 (0.026)Time: 553.973s\n",
      "Train: 0 [4050/10009 ( 40%)]  Loss: 5.78 (6.17)  Time: 0.150s,  853.20/s  (0.139s,  923.05/s)  LR: 5.000e-05  Data: 0.013 (0.026)Time: 561.750s\n",
      "Train: 0 [4100/10009 ( 41%)]  Loss: 5.55 (6.16)  Time: 0.109s, 1172.29/s  (0.139s,  923.44/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 568.447s\n",
      "Train: 0 [4150/10009 ( 41%)]  Loss: 5.60 (6.16)  Time: 0.109s, 1177.77/s  (0.139s,  923.99/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 575.034s\n",
      "Train: 0 [4200/10009 ( 42%)]  Loss: 5.74 (6.15)  Time: 0.119s, 1075.64/s  (0.138s,  924.28/s)  LR: 5.000e-05  Data: 0.006 (0.026)Time: 581.780s\n",
      "Train: 0 [4250/10009 ( 42%)]  Loss: 5.76 (6.15)  Time: 0.146s,  875.81/s  (0.139s,  924.08/s)  LR: 5.000e-05  Data: 0.042 (0.026)Time: 588.832s\n",
      "Train: 0 [4300/10009 ( 43%)]  Loss: 5.51 (6.14)  Time: 0.180s,  710.97/s  (0.138s,  924.37/s)  LR: 5.000e-05  Data: 0.077 (0.026)Time: 595.570s\n",
      "Train: 0 [4350/10009 ( 43%)]  Loss: 5.51 (6.14)  Time: 0.252s,  508.36/s  (0.138s,  924.36/s)  LR: 5.000e-05  Data: 0.150 (0.026)Time: 602.503s\n",
      "Train: 0 [4400/10009 ( 44%)]  Loss: 5.66 (6.13)  Time: 0.134s,  955.37/s  (0.138s,  924.19/s)  LR: 5.000e-05  Data: 0.009 (0.026)Time: 609.535s\n",
      "Train: 0 [4450/10009 ( 44%)]  Loss: 5.77 (6.12)  Time: 0.155s,  823.63/s  (0.138s,  924.71/s)  LR: 5.000e-05  Data: 0.054 (0.026)Time: 616.112s\n",
      "Train: 0 [4500/10009 ( 45%)]  Loss: 5.71 (6.12)  Time: 0.130s,  981.67/s  (0.138s,  924.72/s)  LR: 5.000e-05  Data: 0.007 (0.026)Time: 623.029s\n",
      "Train: 0 [4550/10009 ( 45%)]  Loss: 5.60 (6.11)  Time: 0.135s,  947.30/s  (0.138s,  924.68/s)  LR: 5.000e-05  Data: 0.008 (0.026)Time: 629.974s\n",
      "Train: 0 [4600/10009 ( 46%)]  Loss: 5.70 (6.11)  Time: 0.111s, 1152.20/s  (0.138s,  924.51/s)  LR: 5.000e-05  Data: 0.009 (0.026)Time: 637.017s\n",
      "Train: 0 [4650/10009 ( 46%)]  Loss: 5.54 (6.10)  Time: 0.107s, 1191.68/s  (0.139s,  924.09/s)  LR: 5.000e-05  Data: 0.005 (0.026)Time: 644.230s\n",
      "Train: 0 [4700/10009 ( 47%)]  Loss: 5.67 (6.10)  Time: 0.181s,  707.32/s  (0.139s,  923.83/s)  LR: 5.000e-05  Data: 0.078 (0.026)Time: 651.338s\n",
      "Train: 0 [4750/10009 ( 47%)]  Loss: 5.51 (6.09)  Time: 0.172s,  746.21/s  (0.140s,  916.04/s)  LR: 5.000e-05  Data: 0.066 (0.028)Time: 663.863s\n",
      "Train: 0 [4800/10009 ( 48%)]  Loss: 5.59 (6.09)  Time: 0.174s,  736.85/s  (0.140s,  914.52/s)  LR: 5.000e-05  Data: 0.058 (0.028)Time: 671.968s\n",
      "Train: 0 [4850/10009 ( 48%)]  Loss: 5.35 (6.08)  Time: 0.108s, 1187.38/s  (0.140s,  914.51/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 678.969s\n",
      "Train: 0 [4900/10009 ( 49%)]  Loss: 5.62 (6.08)  Time: 0.109s, 1171.59/s  (0.140s,  914.30/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 686.126s\n",
      "Train: 0 [4950/10009 ( 49%)]  Loss: 5.56 (6.07)  Time: 0.131s,  978.16/s  (0.140s,  913.99/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 693.360s\n",
      "Train: 0 [5000/10009 ( 50%)]  Loss: 5.47 (6.07)  Time: 0.106s, 1203.86/s  (0.140s,  914.49/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 699.979s\n",
      "Train: 0 [5050/10009 ( 50%)]  Loss: 5.54 (6.06)  Time: 0.109s, 1178.47/s  (0.140s,  914.87/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 706.684s\n",
      "Train: 0 [5100/10009 ( 51%)]  Loss: 5.66 (6.06)  Time: 0.119s, 1072.01/s  (0.140s,  914.76/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 713.765s\n",
      "Train: 0 [5150/10009 ( 51%)]  Loss: 5.47 (6.05)  Time: 0.109s, 1169.64/s  (0.140s,  915.07/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 720.521s\n",
      "Train: 0 [5200/10009 ( 52%)]  Loss: 5.50 (6.05)  Time: 0.108s, 1189.25/s  (0.140s,  915.44/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 727.216s\n",
      "Train: 0 [5250/10009 ( 52%)]  Loss: 5.48 (6.04)  Time: 0.107s, 1197.17/s  (0.140s,  915.12/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 734.470s\n",
      "Train: 0 [5300/10009 ( 53%)]  Loss: 5.42 (6.04)  Time: 0.135s,  947.00/s  (0.140s,  915.24/s)  LR: 5.000e-05  Data: 0.008 (0.028)Time: 741.361s\n",
      "Train: 0 [5350/10009 ( 53%)]  Loss: 5.47 (6.03)  Time: 0.107s, 1195.71/s  (0.140s,  915.66/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 748.010s\n",
      "Train: 0 [5400/10009 ( 54%)]  Loss: 5.37 (6.03)  Time: 0.110s, 1165.73/s  (0.140s,  915.33/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 755.272s\n",
      "Train: 0 [5450/10009 ( 54%)]  Loss: 5.65 (6.02)  Time: 0.108s, 1188.65/s  (0.140s,  915.89/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 761.797s\n",
      "Train: 0 [5500/10009 ( 55%)]  Loss: 5.24 (6.02)  Time: 0.115s, 1115.35/s  (0.140s,  915.98/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 768.717s\n",
      "Train: 0 [5550/10009 ( 55%)]  Loss: 5.35 (6.01)  Time: 0.109s, 1170.46/s  (0.140s,  915.61/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 776.010s\n",
      "Train: 0 [5600/10009 ( 56%)]  Loss: 5.43 (6.01)  Time: 0.108s, 1189.21/s  (0.140s,  915.95/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 782.710s\n",
      "Train: 0 [5650/10009 ( 56%)]  Loss: 5.48 (6.00)  Time: 0.139s,  918.55/s  (0.140s,  916.10/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 789.569s\n",
      "Train: 0 [5700/10009 ( 57%)]  Loss: 5.48 (6.00)  Time: 0.125s, 1022.37/s  (0.140s,  916.60/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 796.120s\n",
      "Train: 0 [5750/10009 ( 57%)]  Loss: 5.31 (5.99)  Time: 0.134s,  955.92/s  (0.140s,  916.52/s)  LR: 5.000e-05  Data: 0.011 (0.028)Time: 803.176s\n",
      "Train: 0 [5800/10009 ( 58%)]  Loss: 5.51 (5.99)  Time: 0.109s, 1177.21/s  (0.140s,  917.02/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 809.720s\n",
      "Train: 0 [5850/10009 ( 58%)]  Loss: 5.78 (5.99)  Time: 0.185s,  693.05/s  (0.140s,  917.28/s)  LR: 5.000e-05  Data: 0.082 (0.028)Time: 816.462s\n",
      "Train: 0 [5900/10009 ( 59%)]  Loss: 5.53 (5.98)  Time: 0.114s, 1122.09/s  (0.140s,  916.91/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 823.774s\n",
      "Train: 0 [5950/10009 ( 59%)]  Loss: 5.58 (5.98)  Time: 0.149s,  856.22/s  (0.140s,  917.08/s)  LR: 5.000e-05  Data: 0.046 (0.028)Time: 830.601s\n",
      "Train: 0 [6000/10009 ( 60%)]  Loss: 5.64 (5.97)  Time: 0.106s, 1210.74/s  (0.140s,  916.94/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 837.709s\n",
      "Train: 0 [6050/10009 ( 60%)]  Loss: 5.31 (5.97)  Time: 0.263s,  486.22/s  (0.140s,  916.92/s)  LR: 5.000e-05  Data: 0.162 (0.028)Time: 844.702s\n",
      "Train: 0 [6100/10009 ( 61%)]  Loss: 5.45 (5.96)  Time: 0.132s,  968.70/s  (0.140s,  916.93/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 851.671s\n",
      "Train: 0 [6150/10009 ( 61%)]  Loss: 5.56 (5.96)  Time: 0.108s, 1185.07/s  (0.140s,  917.31/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 858.301s\n",
      "Train: 0 [6200/10009 ( 62%)]  Loss: 5.42 (5.96)  Time: 0.109s, 1178.04/s  (0.140s,  917.13/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 865.443s\n",
      "Train: 0 [6250/10009 ( 62%)]  Loss: 5.43 (5.95)  Time: 0.109s, 1176.44/s  (0.140s,  917.19/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 872.370s\n",
      "Train: 0 [6300/10009 ( 63%)]  Loss: 5.12 (5.95)  Time: 0.143s,  896.89/s  (0.140s,  917.28/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 879.260s\n",
      "Train: 0 [6350/10009 ( 63%)]  Loss: 5.62 (5.94)  Time: 0.109s, 1177.25/s  (0.139s,  917.61/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 885.921s\n",
      "Train: 0 [6400/10009 ( 64%)]  Loss: 5.44 (5.94)  Time: 0.115s, 1110.26/s  (0.139s,  918.02/s)  LR: 5.000e-05  Data: 0.008 (0.028)Time: 892.491s\n",
      "Train: 0 [6450/10009 ( 64%)]  Loss: 5.51 (5.93)  Time: 0.107s, 1196.68/s  (0.139s,  917.81/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 899.665s\n",
      "Train: 0 [6500/10009 ( 65%)]  Loss: 5.69 (5.93)  Time: 0.108s, 1180.18/s  (0.139s,  918.28/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 906.181s\n",
      "Train: 0 [6550/10009 ( 65%)]  Loss: 5.53 (5.93)  Time: 0.133s,  959.82/s  (0.139s,  917.59/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 913.837s\n",
      "Train: 0 [6600/10009 ( 66%)]  Loss: 5.36 (5.92)  Time: 0.107s, 1192.60/s  (0.139s,  918.01/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 920.388s\n",
      "Train: 0 [6650/10009 ( 66%)]  Loss: 5.38 (5.92)  Time: 0.108s, 1190.23/s  (0.139s,  917.81/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 927.559s\n",
      "Train: 0 [6700/10009 ( 67%)]  Loss: 5.39 (5.91)  Time: 0.108s, 1183.05/s  (0.139s,  918.31/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 934.029s\n",
      "Train: 0 [6750/10009 ( 67%)]  Loss: 5.58 (5.91)  Time: 0.107s, 1195.63/s  (0.139s,  918.54/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 940.756s\n",
      "Train: 0 [6800/10009 ( 68%)]  Loss: 5.41 (5.91)  Time: 0.107s, 1200.28/s  (0.139s,  918.47/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 947.800s\n",
      "Train: 0 [6850/10009 ( 68%)]  Loss: 5.13 (5.90)  Time: 0.108s, 1184.26/s  (0.139s,  918.60/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 954.637s\n",
      "Train: 0 [6900/10009 ( 69%)]  Loss: 5.34 (5.90)  Time: 0.118s, 1087.63/s  (0.139s,  918.49/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 961.710s\n",
      "Train: 0 [6950/10009 ( 69%)]  Loss: 5.31 (5.90)  Time: 0.113s, 1135.15/s  (0.139s,  918.57/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 968.594s\n",
      "Train: 0 [7000/10009 ( 70%)]  Loss: 5.24 (5.89)  Time: 0.128s,  997.01/s  (0.139s,  918.62/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 975.513s\n",
      "Train: 0 [7050/10009 ( 70%)]  Loss: 5.28 (5.89)  Time: 0.121s, 1055.45/s  (0.139s,  918.99/s)  LR: 5.000e-05  Data: 0.018 (0.028)Time: 982.089s\n",
      "Train: 0 [7100/10009 ( 71%)]  Loss: 5.46 (5.88)  Time: 0.109s, 1172.20/s  (0.139s,  919.09/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 988.942s\n",
      "Train: 0 [7150/10009 ( 71%)]  Loss: 5.27 (5.88)  Time: 0.109s, 1175.49/s  (0.139s,  919.48/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 995.484s\n",
      "Train: 0 [7200/10009 ( 72%)]  Loss: 5.51 (5.88)  Time: 0.297s,  430.83/s  (0.139s,  919.52/s)  LR: 5.000e-05  Data: 0.164 (0.028)Time: 1002.394s\n",
      "Train: 0 [7250/10009 ( 72%)]  Loss: 5.32 (5.87)  Time: 0.105s, 1213.51/s  (0.139s,  918.98/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1009.956s\n",
      "Train: 0 [7300/10009 ( 73%)]  Loss: 5.35 (5.87)  Time: 0.133s,  962.17/s  (0.139s,  919.02/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1016.874s\n",
      "Train: 0 [7350/10009 ( 73%)]  Loss: 5.21 (5.87)  Time: 0.108s, 1190.41/s  (0.139s,  919.22/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1023.614s\n",
      "Train: 0 [7400/10009 ( 74%)]  Loss: 5.53 (5.86)  Time: 0.109s, 1176.07/s  (0.139s,  919.74/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1029.994s\n",
      "Train: 0 [7450/10009 ( 74%)]  Loss: 5.42 (5.86)  Time: 0.109s, 1169.84/s  (0.139s,  919.72/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1036.978s\n",
      "Train: 0 [7500/10009 ( 75%)]  Loss: 5.55 (5.85)  Time: 0.117s, 1093.90/s  (0.139s,  919.89/s)  LR: 5.000e-05  Data: 0.014 (0.028)Time: 1043.742s\n",
      "Train: 0 [7550/10009 ( 75%)]  Loss: 5.50 (5.85)  Time: 0.107s, 1196.75/s  (0.139s,  920.08/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1050.476s\n",
      "Train: 0 [7600/10009 ( 76%)]  Loss: 5.31 (5.85)  Time: 0.389s,  329.36/s  (0.139s,  919.87/s)  LR: 5.000e-05  Data: 0.288 (0.028)Time: 1057.678s\n",
      "Train: 0 [7650/10009 ( 76%)]  Loss: 5.50 (5.84)  Time: 0.113s, 1129.50/s  (0.139s,  919.80/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1064.713s\n",
      "Train: 0 [7700/10009 ( 77%)]  Loss: 5.13 (5.84)  Time: 0.283s,  451.98/s  (0.139s,  919.38/s)  LR: 5.000e-05  Data: 0.177 (0.028)Time: 1072.168s\n",
      "Train: 0 [7750/10009 ( 77%)]  Loss: 5.54 (5.84)  Time: 0.137s,  935.93/s  (0.139s,  918.57/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1080.076s\n",
      "Train: 0 [7800/10009 ( 78%)]  Loss: 5.34 (5.83)  Time: 0.203s,  629.46/s  (0.139s,  918.71/s)  LR: 5.000e-05  Data: 0.090 (0.028)Time: 1086.882s\n",
      "Train: 0 [7850/10009 ( 78%)]  Loss: 5.31 (5.83)  Time: 0.113s, 1128.59/s  (0.139s,  919.02/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1093.473s\n",
      "Train: 0 [7900/10009 ( 79%)]  Loss: 5.28 (5.83)  Time: 0.109s, 1178.28/s  (0.139s,  919.43/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1099.952s\n",
      "Train: 0 [7950/10009 ( 79%)]  Loss: 5.37 (5.82)  Time: 0.107s, 1196.01/s  (0.139s,  919.64/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1106.657s\n",
      "Train: 0 [8000/10009 ( 80%)]  Loss: 5.19 (5.82)  Time: 0.107s, 1201.12/s  (0.139s,  919.77/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1113.453s\n",
      "Train: 0 [8050/10009 ( 80%)]  Loss: 5.30 (5.82)  Time: 0.108s, 1186.59/s  (0.139s,  919.63/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1120.589s\n",
      "Train: 0 [8100/10009 ( 81%)]  Loss: 5.07 (5.81)  Time: 0.106s, 1205.83/s  (0.139s,  919.76/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1127.387s\n",
      "Train: 0 [8150/10009 ( 81%)]  Loss: 5.26 (5.81)  Time: 0.109s, 1177.60/s  (0.139s,  919.83/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1134.254s\n",
      "Train: 0 [8200/10009 ( 82%)]  Loss: 5.11 (5.81)  Time: 0.113s, 1134.00/s  (0.139s,  919.84/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 1141.206s\n",
      "Train: 0 [8250/10009 ( 82%)]  Loss: 5.32 (5.80)  Time: 0.331s,  387.08/s  (0.139s,  919.57/s)  LR: 5.000e-05  Data: 0.230 (0.028)Time: 1148.502s\n",
      "Train: 0 [8300/10009 ( 83%)]  Loss: 5.18 (5.80)  Time: 0.107s, 1201.09/s  (0.139s,  919.72/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1155.267s\n",
      "Train: 0 [8350/10009 ( 83%)]  Loss: 5.17 (5.80)  Time: 0.109s, 1174.23/s  (0.139s,  920.06/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1161.803s\n",
      "Train: 0 [8400/10009 ( 84%)]  Loss: 5.20 (5.79)  Time: 0.111s, 1149.63/s  (0.139s,  920.21/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1168.559s\n",
      "Train: 0 [8450/10009 ( 84%)]  Loss: 5.24 (5.79)  Time: 0.108s, 1183.31/s  (0.139s,  920.28/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 1175.432s\n",
      "Train: 0 [8500/10009 ( 85%)]  Loss: 5.10 (5.79)  Time: 0.108s, 1184.79/s  (0.139s,  920.50/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1182.101s\n",
      "Train: 0 [8550/10009 ( 85%)]  Loss: 5.02 (5.78)  Time: 0.131s,  977.53/s  (0.139s,  920.43/s)  LR: 5.000e-05  Data: 0.007 (0.028)Time: 1189.142s\n",
      "Train: 0 [8600/10009 ( 86%)]  Loss: 5.07 (5.78)  Time: 0.107s, 1196.96/s  (0.139s,  920.73/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1195.704s\n",
      "Train: 0 [8650/10009 ( 86%)]  Loss: 5.08 (5.78)  Time: 0.131s,  978.43/s  (0.139s,  920.87/s)  LR: 5.000e-05  Data: 0.008 (0.028)Time: 1202.477s\n",
      "Train: 0 [8700/10009 ( 87%)]  Loss: 5.28 (5.77)  Time: 0.107s, 1192.80/s  (0.139s,  921.09/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1209.137s\n",
      "Train: 0 [8750/10009 ( 87%)]  Loss: 5.35 (5.77)  Time: 0.139s,  921.40/s  (0.139s,  921.15/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1216.002s\n",
      "Train: 0 [8800/10009 ( 88%)]  Loss: 5.18 (5.77)  Time: 0.108s, 1188.01/s  (0.139s,  921.31/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1222.737s\n",
      "Train: 0 [8850/10009 ( 88%)]  Loss: 5.19 (5.77)  Time: 0.108s, 1180.40/s  (0.139s,  921.23/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1229.793s\n",
      "Train: 0 [8900/10009 ( 89%)]  Loss: 5.07 (5.76)  Time: 0.108s, 1189.71/s  (0.139s,  921.51/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1236.363s\n",
      "Train: 0 [8950/10009 ( 89%)]  Loss: 5.34 (5.76)  Time: 0.115s, 1113.25/s  (0.139s,  921.71/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1243.040s\n",
      "Train: 0 [9000/10009 ( 90%)]  Loss: 5.20 (5.76)  Time: 0.107s, 1201.55/s  (0.139s,  921.82/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1249.837s\n",
      "Train: 0 [9050/10009 ( 90%)]  Loss: 5.08 (5.75)  Time: 0.111s, 1157.08/s  (0.139s,  922.07/s)  LR: 5.000e-05  Data: 0.008 (0.028)Time: 1256.432s\n",
      "Train: 0 [9100/10009 ( 91%)]  Loss: 5.22 (5.75)  Time: 0.107s, 1199.50/s  (0.139s,  921.89/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1263.621s\n",
      "Train: 0 [9150/10009 ( 91%)]  Loss: 5.39 (5.75)  Time: 0.146s,  876.48/s  (0.139s,  922.17/s)  LR: 5.000e-05  Data: 0.045 (0.028)Time: 1270.180s\n",
      "Train: 0 [9200/10009 ( 92%)]  Loss: 5.29 (5.74)  Time: 0.121s, 1058.46/s  (0.139s,  922.25/s)  LR: 5.000e-05  Data: 0.019 (0.028)Time: 1277.007s\n",
      "Train: 0 [9250/10009 ( 92%)]  Loss: 5.25 (5.74)  Time: 0.107s, 1195.58/s  (0.139s,  922.28/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1283.904s\n",
      "Train: 0 [9300/10009 ( 93%)]  Loss: 5.01 (5.74)  Time: 0.268s,  478.26/s  (0.139s,  922.47/s)  LR: 5.000e-05  Data: 0.132 (0.028)Time: 1290.577s\n",
      "Train: 0 [9350/10009 ( 93%)]  Loss: 5.12 (5.73)  Time: 0.107s, 1192.30/s  (0.139s,  922.51/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1297.467s\n",
      "Train: 0 [9400/10009 ( 94%)]  Loss: 5.07 (5.73)  Time: 0.364s,  351.21/s  (0.139s,  922.32/s)  LR: 5.000e-05  Data: 0.230 (0.028)Time: 1304.664s\n",
      "Train: 0 [9450/10009 ( 94%)]  Loss: 5.14 (5.73)  Time: 0.103s, 1238.50/s  (0.139s,  922.84/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1310.866s\n",
      "Train: 0 [9500/10009 ( 95%)]  Loss: 5.10 (5.72)  Time: 0.127s, 1007.32/s  (0.139s,  922.85/s)  LR: 5.000e-05  Data: 0.008 (0.028)Time: 1317.791s\n",
      "Train: 0 [9550/10009 ( 95%)]  Loss: 4.80 (5.72)  Time: 0.111s, 1152.36/s  (0.139s,  922.74/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1324.886s\n",
      "Train: 0 [9600/10009 ( 96%)]  Loss: 5.41 (5.72)  Time: 0.134s,  954.13/s  (0.139s,  922.92/s)  LR: 5.000e-05  Data: 0.008 (0.028)Time: 1331.567s\n",
      "Train: 0 [9650/10009 ( 96%)]  Loss: 5.17 (5.72)  Time: 0.170s,  752.42/s  (0.139s,  923.45/s)  LR: 5.000e-05  Data: 0.049 (0.028)Time: 1337.725s\n",
      "Train: 0 [9700/10009 ( 97%)]  Loss: 5.03 (5.71)  Time: 0.132s,  970.84/s  (0.139s,  923.49/s)  LR: 5.000e-05  Data: 0.011 (0.028)Time: 1344.595s\n",
      "Train: 0 [9750/10009 ( 97%)]  Loss: 5.25 (5.71)  Time: 0.112s, 1147.72/s  (0.139s,  923.74/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1351.169s\n",
      "Train: 0 [9800/10009 ( 98%)]  Loss: 5.21 (5.71)  Time: 0.103s, 1241.47/s  (0.139s,  923.46/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1358.496s\n",
      "Train: 0 [9850/10009 ( 98%)]  Loss: 5.13 (5.70)  Time: 0.105s, 1224.27/s  (0.139s,  923.77/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1364.982s\n",
      "Train: 0 [9900/10009 ( 99%)]  Loss: 5.11 (5.70)  Time: 0.103s, 1237.35/s  (0.139s,  923.86/s)  LR: 5.000e-05  Data: 0.006 (0.028)Time: 1371.764s\n",
      "Train: 0 [9950/10009 ( 99%)]  Loss: 5.02 (5.70)  Time: 0.102s, 1251.54/s  (0.139s,  923.97/s)  LR: 5.000e-05  Data: 0.005 (0.028)Time: 1378.536s\n",
      "Train: 0 [10000/10009 (100%)]  Loss: 5.00 (5.70)  Time: 0.158s,  809.24/s  (0.139s,  923.97/s)  LR: 5.000e-05  Data: 0.058 (0.028)Time: 1385.466s\n",
      "Test: [   0/390]  Time: 0.911 (0.911)  Loss:   3.472 ( 3.472)  Acc@1:  37.500 ( 37.500)  Acc@5:  57.031 ( 57.031)\n",
      "Test: [  50/390]  Time: 0.029 (0.156)  Loss:   4.439 ( 4.491)  Acc@1:  17.969 ( 16.008)  Acc@5:  39.062 ( 34.344)\n",
      "Test: [ 100/390]  Time: 0.047 (0.142)  Loss:   5.039 ( 4.855)  Acc@1:   1.562 ( 11.626)  Acc@5:  21.094 ( 27.413)\n",
      "Test: [ 150/390]  Time: 0.030 (0.142)  Loss:   4.976 ( 4.827)  Acc@1:   6.250 ( 11.455)  Acc@5:  26.562 ( 27.633)\n",
      "Test: [ 200/390]  Time: 0.029 (0.140)  Loss:   5.667 ( 4.936)  Acc@1:   0.781 ( 10.821)  Acc@5:   8.594 ( 26.154)\n",
      "Test: [ 250/390]  Time: 0.029 (0.141)  Loss:   4.774 ( 5.017)  Acc@1:  17.969 ( 10.327)  Acc@5:  33.594 ( 24.813)\n",
      "Test: [ 300/390]  Time: 0.033 (0.139)  Loss:   5.208 ( 5.094)  Acc@1:   7.812 (  9.648)  Acc@5:  27.344 ( 23.539)\n",
      "Test: [ 350/390]  Time: 0.029 (0.138)  Loss:   5.303 ( 5.147)  Acc@1:   2.344 (  9.255)  Acc@5:  12.500 ( 22.799)\n",
      "Test: [ 390/390]  Time: 0.141 (0.139)  Loss:   5.526 ( 5.152)  Acc@1:   1.250 (  9.416)  Acc@5:  12.500 ( 22.796)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-0.pth.tar', 9.416)\n",
      "\n",
      "Train: 1 [   0/10009 (  0%)]  Loss: 5.16 (5.16)  Time: 0.675s,  189.74/s  (0.675s,  189.74/s)  LR: 4.945e-05  Data: 0.568 (0.568)Time: 0.675s\n",
      "Train: 1 [  50/10009 (  0%)]  Loss: 5.10 (5.14)  Time: 0.111s, 1158.25/s  (0.145s,  882.41/s)  LR: 4.945e-05  Data: 0.005 (0.033)Time: 7.398s\n",
      "Train: 1 [ 100/10009 (  1%)]  Loss: 5.09 (5.11)  Time: 0.103s, 1237.67/s  (0.141s,  906.57/s)  LR: 4.945e-05  Data: 0.005 (0.032)Time: 14.261s\n",
      "Train: 1 [ 150/10009 (  1%)]  Loss: 5.18 (5.11)  Time: 0.103s, 1244.06/s  (0.142s,  899.66/s)  LR: 4.945e-05  Data: 0.005 (0.036)Time: 21.484s\n",
      "Train: 1 [ 200/10009 (  2%)]  Loss: 4.92 (5.11)  Time: 0.104s, 1232.49/s  (0.139s,  920.81/s)  LR: 4.945e-05  Data: 0.006 (0.034)Time: 27.941s\n",
      "Train: 1 [ 250/10009 (  2%)]  Loss: 5.17 (5.11)  Time: 0.104s, 1226.97/s  (0.138s,  927.29/s)  LR: 4.945e-05  Data: 0.005 (0.033)Time: 34.648s\n",
      "Train: 1 [ 300/10009 (  3%)]  Loss: 5.16 (5.10)  Time: 0.103s, 1241.89/s  (0.136s,  939.86/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 40.994s\n",
      "Train: 1 [ 350/10009 (  3%)]  Loss: 5.02 (5.10)  Time: 0.103s, 1246.46/s  (0.136s,  939.91/s)  LR: 4.945e-05  Data: 0.005 (0.031)Time: 47.801s\n",
      "Train: 1 [ 400/10009 (  4%)]  Loss: 4.82 (5.09)  Time: 0.103s, 1241.80/s  (0.136s,  944.57/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 54.340s\n",
      "Train: 1 [ 450/10009 (  4%)]  Loss: 5.11 (5.09)  Time: 0.250s,  511.16/s  (0.136s,  940.75/s)  LR: 4.945e-05  Data: 0.128 (0.031)Time: 61.364s\n",
      "Train: 1 [ 500/10009 (  5%)]  Loss: 5.24 (5.09)  Time: 0.103s, 1240.84/s  (0.136s,  943.68/s)  LR: 4.945e-05  Data: 0.006 (0.031)Time: 67.956s\n",
      "Train: 1 [ 550/10009 (  5%)]  Loss: 5.12 (5.09)  Time: 0.126s, 1019.44/s  (0.135s,  946.76/s)  LR: 4.945e-05  Data: 0.008 (0.030)Time: 74.494s\n",
      "Train: 1 [ 600/10009 (  6%)]  Loss: 5.22 (5.09)  Time: 0.105s, 1221.14/s  (0.135s,  945.87/s)  LR: 4.945e-05  Data: 0.006 (0.031)Time: 81.331s\n",
      "Train: 1 [ 650/10009 (  6%)]  Loss: 4.98 (5.09)  Time: 0.125s, 1024.01/s  (0.135s,  948.82/s)  LR: 4.945e-05  Data: 0.007 (0.030)Time: 87.823s\n",
      "Train: 1 [ 700/10009 (  7%)]  Loss: 5.32 (5.09)  Time: 0.103s, 1237.04/s  (0.134s,  952.25/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 94.228s\n",
      "Train: 1 [ 750/10009 (  7%)]  Loss: 4.86 (5.09)  Time: 0.104s, 1234.71/s  (0.134s,  956.71/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 100.478s\n",
      "Train: 1 [ 800/10009 (  8%)]  Loss: 4.89 (5.09)  Time: 0.104s, 1236.16/s  (0.134s,  954.59/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 107.405s\n",
      "Train: 1 [ 850/10009 (  8%)]  Loss: 4.89 (5.08)  Time: 0.105s, 1220.49/s  (0.134s,  955.00/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 114.061s\n",
      "Train: 1 [ 900/10009 (  9%)]  Loss: 4.87 (5.08)  Time: 0.106s, 1212.26/s  (0.134s,  955.70/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 120.674s\n",
      "Train: 1 [ 950/10009 (  9%)]  Loss: 4.70 (5.08)  Time: 0.102s, 1249.50/s  (0.134s,  952.04/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 127.860s\n",
      "Train: 1 [1000/10009 ( 10%)]  Loss: 5.00 (5.07)  Time: 0.103s, 1242.84/s  (0.135s,  949.50/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 134.942s\n",
      "Train: 1 [1050/10009 ( 10%)]  Loss: 5.20 (5.07)  Time: 0.190s,  675.10/s  (0.135s,  949.97/s)  LR: 4.945e-05  Data: 0.091 (0.031)Time: 141.612s\n",
      "Train: 1 [1100/10009 ( 11%)]  Loss: 4.95 (5.07)  Time: 0.328s,  390.31/s  (0.136s,  943.57/s)  LR: 4.945e-05  Data: 0.202 (0.032)Time: 149.357s\n",
      "Train: 1 [1150/10009 ( 11%)]  Loss: 5.04 (5.07)  Time: 0.104s, 1227.86/s  (0.135s,  946.40/s)  LR: 4.945e-05  Data: 0.005 (0.031)Time: 155.672s\n",
      "Train: 1 [1200/10009 ( 12%)]  Loss: 5.02 (5.07)  Time: 0.106s, 1212.50/s  (0.135s,  949.08/s)  LR: 4.945e-05  Data: 0.006 (0.031)Time: 161.976s\n",
      "Train: 1 [1250/10009 ( 12%)]  Loss: 4.96 (5.07)  Time: 0.117s, 1098.59/s  (0.135s,  950.55/s)  LR: 4.945e-05  Data: 0.007 (0.030)Time: 168.458s\n",
      "Train: 1 [1300/10009 ( 13%)]  Loss: 4.92 (5.07)  Time: 0.147s,  871.67/s  (0.135s,  950.01/s)  LR: 4.945e-05  Data: 0.007 (0.030)Time: 175.290s\n",
      "Train: 1 [1350/10009 ( 13%)]  Loss: 4.92 (5.07)  Time: 0.103s, 1241.65/s  (0.134s,  951.98/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 181.650s\n",
      "Train: 1 [1400/10009 ( 14%)]  Loss: 5.05 (5.06)  Time: 0.126s, 1015.06/s  (0.134s,  952.56/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 188.258s\n",
      "Train: 1 [1450/10009 ( 14%)]  Loss: 4.95 (5.06)  Time: 0.104s, 1225.68/s  (0.134s,  955.40/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 194.398s\n",
      "Train: 1 [1500/10009 ( 15%)]  Loss: 5.29 (5.06)  Time: 0.107s, 1199.85/s  (0.134s,  955.38/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 201.102s\n",
      "Train: 1 [1550/10009 ( 15%)]  Loss: 5.03 (5.06)  Time: 0.103s, 1238.55/s  (0.134s,  956.24/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 207.614s\n",
      "Train: 1 [1600/10009 ( 16%)]  Loss: 4.94 (5.06)  Time: 0.109s, 1175.24/s  (0.134s,  955.51/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 214.469s\n",
      "Train: 1 [1650/10009 ( 16%)]  Loss: 5.13 (5.06)  Time: 0.110s, 1164.57/s  (0.134s,  955.62/s)  LR: 4.945e-05  Data: 0.008 (0.029)Time: 221.143s\n",
      "Train: 1 [1700/10009 ( 17%)]  Loss: 4.61 (5.05)  Time: 0.103s, 1241.68/s  (0.134s,  955.99/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 227.752s\n",
      "Train: 1 [1750/10009 ( 17%)]  Loss: 4.92 (5.05)  Time: 0.104s, 1234.76/s  (0.134s,  954.15/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 234.898s\n",
      "Train: 1 [1800/10009 ( 18%)]  Loss: 4.94 (5.05)  Time: 0.105s, 1221.75/s  (0.134s,  954.47/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 241.525s\n",
      "Train: 1 [1850/10009 ( 18%)]  Loss: 4.74 (5.05)  Time: 0.105s, 1221.89/s  (0.134s,  955.69/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 247.914s\n",
      "Train: 1 [1900/10009 ( 19%)]  Loss: 4.78 (5.05)  Time: 0.104s, 1235.71/s  (0.134s,  955.39/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 254.688s\n",
      "Train: 1 [1950/10009 ( 19%)]  Loss: 4.74 (5.05)  Time: 0.163s,  787.30/s  (0.134s,  954.67/s)  LR: 4.945e-05  Data: 0.058 (0.029)Time: 261.585s\n",
      "Train: 1 [2000/10009 ( 20%)]  Loss: 4.93 (5.05)  Time: 0.103s, 1237.80/s  (0.134s,  955.33/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 268.104s\n",
      "Train: 1 [2050/10009 ( 20%)]  Loss: 5.03 (5.05)  Time: 0.128s,  996.77/s  (0.134s,  955.76/s)  LR: 4.945e-05  Data: 0.030 (0.029)Time: 274.679s\n",
      "Train: 1 [2100/10009 ( 21%)]  Loss: 5.09 (5.04)  Time: 0.135s,  948.27/s  (0.134s,  955.62/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 281.415s\n",
      "Train: 1 [2150/10009 ( 21%)]  Loss: 4.85 (5.04)  Time: 0.301s,  425.62/s  (0.134s,  955.46/s)  LR: 4.945e-05  Data: 0.201 (0.029)Time: 288.163s\n",
      "Train: 1 [2200/10009 ( 22%)]  Loss: 4.96 (5.04)  Time: 0.104s, 1232.82/s  (0.135s,  950.28/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 296.467s\n",
      "Train: 1 [2250/10009 ( 22%)]  Loss: 4.83 (5.04)  Time: 0.148s,  865.75/s  (0.135s,  949.75/s)  LR: 4.945e-05  Data: 0.033 (0.030)Time: 303.371s\n",
      "Train: 1 [2300/10009 ( 23%)]  Loss: 5.10 (5.04)  Time: 0.133s,  963.44/s  (0.135s,  950.52/s)  LR: 4.945e-05  Data: 0.010 (0.030)Time: 309.860s\n",
      "Train: 1 [2350/10009 ( 23%)]  Loss: 5.00 (5.04)  Time: 0.105s, 1213.96/s  (0.135s,  950.62/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 316.558s\n",
      "Train: 1 [2400/10009 ( 24%)]  Loss: 5.07 (5.04)  Time: 0.168s,  760.16/s  (0.135s,  951.23/s)  LR: 4.945e-05  Data: 0.069 (0.030)Time: 323.086s\n",
      "Train: 1 [2450/10009 ( 24%)]  Loss: 5.01 (5.04)  Time: 0.106s, 1211.24/s  (0.134s,  952.28/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 329.448s\n",
      "Train: 1 [2500/10009 ( 25%)]  Loss: 4.82 (5.04)  Time: 0.104s, 1229.96/s  (0.134s,  952.32/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 336.155s\n",
      "Train: 1 [2550/10009 ( 25%)]  Loss: 5.03 (5.03)  Time: 0.308s,  415.87/s  (0.134s,  951.80/s)  LR: 4.945e-05  Data: 0.175 (0.029)Time: 343.063s\n",
      "Train: 1 [2600/10009 ( 26%)]  Loss: 4.90 (5.03)  Time: 0.104s, 1230.59/s  (0.134s,  952.58/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 349.499s\n",
      "Train: 1 [2650/10009 ( 26%)]  Loss: 5.28 (5.03)  Time: 0.105s, 1223.64/s  (0.134s,  953.21/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 355.984s\n",
      "Train: 1 [2700/10009 ( 27%)]  Loss: 4.89 (5.03)  Time: 0.104s, 1227.40/s  (0.134s,  953.14/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 362.724s\n",
      "Train: 1 [2750/10009 ( 27%)]  Loss: 4.93 (5.03)  Time: 0.105s, 1220.93/s  (0.134s,  952.97/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 369.505s\n",
      "Train: 1 [2800/10009 ( 28%)]  Loss: 5.00 (5.03)  Time: 0.118s, 1085.88/s  (0.134s,  951.92/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 376.635s\n",
      "Train: 1 [2850/10009 ( 28%)]  Loss: 4.93 (5.03)  Time: 0.105s, 1214.80/s  (0.135s,  951.54/s)  LR: 4.945e-05  Data: 0.006 (0.030)Time: 383.512s\n",
      "Train: 1 [2900/10009 ( 29%)]  Loss: 4.82 (5.02)  Time: 0.104s, 1232.41/s  (0.134s,  952.33/s)  LR: 4.945e-05  Data: 0.006 (0.030)Time: 389.915s\n",
      "Train: 1 [2950/10009 ( 29%)]  Loss: 5.00 (5.02)  Time: 0.108s, 1189.64/s  (0.134s,  952.80/s)  LR: 4.945e-05  Data: 0.008 (0.030)Time: 396.437s\n",
      "Train: 1 [3000/10009 ( 30%)]  Loss: 5.14 (5.02)  Time: 0.123s, 1036.63/s  (0.134s,  952.47/s)  LR: 4.945e-05  Data: 0.008 (0.030)Time: 403.297s\n",
      "Train: 1 [3050/10009 ( 30%)]  Loss: 5.09 (5.02)  Time: 0.105s, 1217.33/s  (0.134s,  952.99/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 409.791s\n",
      "Train: 1 [3100/10009 ( 31%)]  Loss: 4.89 (5.02)  Time: 0.104s, 1230.84/s  (0.134s,  953.04/s)  LR: 4.945e-05  Data: 0.005 (0.030)Time: 416.486s\n",
      "Train: 1 [3150/10009 ( 31%)]  Loss: 4.78 (5.02)  Time: 0.133s,  960.16/s  (0.135s,  951.58/s)  LR: 4.945e-05  Data: 0.007 (0.030)Time: 423.851s\n",
      "Train: 1 [3200/10009 ( 32%)]  Loss: 4.64 (5.02)  Time: 0.105s, 1223.85/s  (0.134s,  952.20/s)  LR: 4.945e-05  Data: 0.007 (0.030)Time: 430.295s\n",
      "Train: 1 [3250/10009 ( 32%)]  Loss: 5.18 (5.01)  Time: 0.171s,  748.56/s  (0.134s,  951.72/s)  LR: 4.945e-05  Data: 0.073 (0.030)Time: 437.239s\n",
      "Train: 1 [3300/10009 ( 33%)]  Loss: 4.94 (5.01)  Time: 0.105s, 1222.51/s  (0.134s,  952.19/s)  LR: 4.945e-05  Data: 0.006 (0.030)Time: 443.740s\n",
      "Train: 1 [3350/10009 ( 33%)]  Loss: 4.60 (5.01)  Time: 0.125s, 1023.67/s  (0.134s,  953.28/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 449.948s\n",
      "Train: 1 [3400/10009 ( 34%)]  Loss: 4.91 (5.01)  Time: 0.163s,  785.10/s  (0.134s,  953.19/s)  LR: 4.945e-05  Data: 0.065 (0.029)Time: 456.704s\n",
      "Train: 1 [3450/10009 ( 34%)]  Loss: 4.85 (5.01)  Time: 0.105s, 1214.79/s  (0.134s,  953.67/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 463.185s\n",
      "Train: 1 [3500/10009 ( 35%)]  Loss: 5.04 (5.01)  Time: 0.106s, 1207.73/s  (0.134s,  954.74/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 469.370s\n",
      "Train: 1 [3550/10009 ( 35%)]  Loss: 4.60 (5.00)  Time: 0.105s, 1224.77/s  (0.134s,  954.42/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 476.233s\n",
      "Train: 1 [3600/10009 ( 36%)]  Loss: 4.62 (5.00)  Time: 0.102s, 1253.03/s  (0.134s,  953.84/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 483.234s\n",
      "Train: 1 [3650/10009 ( 36%)]  Loss: 5.00 (5.00)  Time: 0.122s, 1047.96/s  (0.134s,  954.14/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 489.789s\n",
      "Train: 1 [3700/10009 ( 37%)]  Loss: 4.75 (5.00)  Time: 0.163s,  784.41/s  (0.134s,  954.44/s)  LR: 4.945e-05  Data: 0.064 (0.029)Time: 496.339s\n",
      "Train: 1 [3750/10009 ( 37%)]  Loss: 5.12 (5.00)  Time: 0.103s, 1238.35/s  (0.134s,  954.26/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 503.140s\n",
      "Train: 1 [3800/10009 ( 38%)]  Loss: 4.99 (5.00)  Time: 0.110s, 1167.86/s  (0.134s,  954.51/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 509.713s\n",
      "Train: 1 [3850/10009 ( 38%)]  Loss: 4.99 (5.00)  Time: 0.120s, 1067.82/s  (0.134s,  954.82/s)  LR: 4.945e-05  Data: 0.020 (0.029)Time: 516.252s\n",
      "Train: 1 [3900/10009 ( 39%)]  Loss: 4.68 (5.00)  Time: 0.104s, 1233.11/s  (0.134s,  953.14/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 523.877s\n",
      "Train: 1 [3950/10009 ( 39%)]  Loss: 4.76 (4.99)  Time: 0.120s, 1066.80/s  (0.134s,  953.49/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 530.394s\n",
      "Train: 1 [4000/10009 ( 40%)]  Loss: 5.08 (4.99)  Time: 0.114s, 1120.04/s  (0.134s,  953.82/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 536.919s\n",
      "Train: 1 [4050/10009 ( 40%)]  Loss: 5.06 (4.99)  Time: 0.161s,  794.54/s  (0.134s,  953.59/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 543.763s\n",
      "Train: 1 [4100/10009 ( 41%)]  Loss: 4.99 (4.99)  Time: 0.131s,  980.84/s  (0.134s,  954.19/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 550.127s\n",
      "Train: 1 [4150/10009 ( 41%)]  Loss: 4.70 (4.99)  Time: 0.104s, 1233.02/s  (0.134s,  953.92/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 556.990s\n",
      "Train: 1 [4200/10009 ( 42%)]  Loss: 4.66 (4.99)  Time: 0.105s, 1223.17/s  (0.134s,  954.65/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 563.270s\n",
      "Train: 1 [4250/10009 ( 42%)]  Loss: 4.73 (4.99)  Time: 0.137s,  931.66/s  (0.134s,  953.76/s)  LR: 4.945e-05  Data: 0.010 (0.029)Time: 570.505s\n",
      "Train: 1 [4300/10009 ( 43%)]  Loss: 4.96 (4.99)  Time: 0.109s, 1177.91/s  (0.134s,  954.14/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 576.989s\n",
      "Train: 1 [4350/10009 ( 43%)]  Loss: 5.01 (4.98)  Time: 0.105s, 1216.32/s  (0.134s,  953.58/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 584.039s\n",
      "Train: 1 [4400/10009 ( 44%)]  Loss: 4.89 (4.98)  Time: 0.130s,  986.32/s  (0.134s,  953.99/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 590.497s\n",
      "Train: 1 [4450/10009 ( 44%)]  Loss: 4.76 (4.98)  Time: 0.131s,  978.72/s  (0.134s,  953.57/s)  LR: 4.945e-05  Data: 0.008 (0.029)Time: 597.464s\n",
      "Train: 1 [4500/10009 ( 45%)]  Loss: 4.85 (4.98)  Time: 0.123s, 1042.84/s  (0.134s,  953.67/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 604.115s\n",
      "Train: 1 [4550/10009 ( 45%)]  Loss: 4.88 (4.98)  Time: 0.104s, 1229.62/s  (0.134s,  953.85/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 610.710s\n",
      "Train: 1 [4600/10009 ( 46%)]  Loss: 4.95 (4.98)  Time: 0.114s, 1121.93/s  (0.134s,  954.51/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 616.995s\n",
      "Train: 1 [4650/10009 ( 46%)]  Loss: 4.77 (4.98)  Time: 0.202s,  632.50/s  (0.134s,  954.24/s)  LR: 4.945e-05  Data: 0.102 (0.029)Time: 623.877s\n",
      "Train: 1 [4700/10009 ( 47%)]  Loss: 4.84 (4.98)  Time: 0.103s, 1236.87/s  (0.134s,  954.26/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 630.566s\n",
      "Train: 1 [4750/10009 ( 47%)]  Loss: 4.82 (4.97)  Time: 0.129s,  995.79/s  (0.134s,  954.63/s)  LR: 4.945e-05  Data: 0.016 (0.029)Time: 637.031s\n",
      "Train: 1 [4800/10009 ( 48%)]  Loss: 4.93 (4.97)  Time: 0.126s, 1018.63/s  (0.134s,  954.21/s)  LR: 4.945e-05  Data: 0.008 (0.029)Time: 644.014s\n",
      "Train: 1 [4850/10009 ( 48%)]  Loss: 4.98 (4.97)  Time: 0.139s,  923.67/s  (0.134s,  954.09/s)  LR: 4.945e-05  Data: 0.008 (0.029)Time: 650.804s\n",
      "Train: 1 [4900/10009 ( 49%)]  Loss: 5.03 (4.97)  Time: 0.108s, 1188.72/s  (0.134s,  954.17/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 657.457s\n",
      "Train: 1 [4950/10009 ( 49%)]  Loss: 4.84 (4.97)  Time: 0.225s,  570.12/s  (0.134s,  953.69/s)  LR: 4.945e-05  Data: 0.111 (0.029)Time: 664.499s\n",
      "Train: 1 [5000/10009 ( 50%)]  Loss: 4.78 (4.97)  Time: 0.136s,  943.48/s  (0.134s,  953.56/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 671.301s\n",
      "Train: 1 [5050/10009 ( 50%)]  Loss: 4.56 (4.97)  Time: 0.107s, 1191.07/s  (0.134s,  953.50/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 678.053s\n",
      "Train: 1 [5100/10009 ( 51%)]  Loss: 4.97 (4.97)  Time: 0.130s,  981.99/s  (0.134s,  953.65/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 684.657s\n",
      "Train: 1 [5150/10009 ( 51%)]  Loss: 4.78 (4.96)  Time: 0.196s,  653.48/s  (0.134s,  953.11/s)  LR: 4.945e-05  Data: 0.093 (0.029)Time: 691.760s\n",
      "Train: 1 [5200/10009 ( 52%)]  Loss: 4.98 (4.96)  Time: 0.129s,  994.01/s  (0.134s,  953.04/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 698.530s\n",
      "Train: 1 [5250/10009 ( 52%)]  Loss: 4.90 (4.96)  Time: 0.221s,  579.80/s  (0.134s,  952.95/s)  LR: 4.945e-05  Data: 0.117 (0.029)Time: 705.308s\n",
      "Train: 1 [5300/10009 ( 53%)]  Loss: 4.79 (4.96)  Time: 0.107s, 1191.79/s  (0.134s,  953.13/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 711.889s\n",
      "Train: 1 [5350/10009 ( 53%)]  Loss: 4.70 (4.96)  Time: 0.148s,  866.51/s  (0.134s,  953.06/s)  LR: 4.945e-05  Data: 0.046 (0.029)Time: 718.661s\n",
      "Train: 1 [5400/10009 ( 54%)]  Loss: 4.82 (4.96)  Time: 0.109s, 1178.63/s  (0.134s,  953.15/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 725.309s\n",
      "Train: 1 [5450/10009 ( 54%)]  Loss: 4.71 (4.96)  Time: 0.241s,  530.37/s  (0.134s,  953.05/s)  LR: 4.945e-05  Data: 0.113 (0.029)Time: 732.098s\n",
      "Train: 1 [5500/10009 ( 55%)]  Loss: 4.70 (4.96)  Time: 0.108s, 1187.70/s  (0.134s,  953.26/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 738.650s\n",
      "Train: 1 [5550/10009 ( 55%)]  Loss: 4.65 (4.96)  Time: 0.157s,  815.78/s  (0.134s,  953.22/s)  LR: 4.945e-05  Data: 0.054 (0.028)Time: 745.392s\n",
      "Train: 1 [5600/10009 ( 56%)]  Loss: 4.59 (4.95)  Time: 0.115s, 1117.21/s  (0.134s,  953.26/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 752.075s\n",
      "Train: 1 [5650/10009 ( 56%)]  Loss: 4.52 (4.95)  Time: 0.109s, 1172.18/s  (0.134s,  953.08/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 758.932s\n",
      "Train: 1 [5700/10009 ( 57%)]  Loss: 4.84 (4.95)  Time: 0.106s, 1204.06/s  (0.134s,  952.55/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 766.076s\n",
      "Train: 1 [5750/10009 ( 57%)]  Loss: 4.82 (4.95)  Time: 0.122s, 1048.88/s  (0.134s,  952.72/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 772.656s\n",
      "Train: 1 [5800/10009 ( 58%)]  Loss: 4.77 (4.95)  Time: 0.134s,  956.13/s  (0.134s,  952.41/s)  LR: 4.945e-05  Data: 0.008 (0.028)Time: 779.625s\n",
      "Train: 1 [5850/10009 ( 58%)]  Loss: 4.76 (4.95)  Time: 0.108s, 1182.19/s  (0.134s,  952.45/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 786.313s\n",
      "Train: 1 [5900/10009 ( 59%)]  Loss: 4.75 (4.95)  Time: 0.107s, 1191.15/s  (0.134s,  952.38/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 793.094s\n",
      "Train: 1 [5950/10009 ( 59%)]  Loss: 4.81 (4.95)  Time: 0.121s, 1057.95/s  (0.134s,  952.37/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 799.822s\n",
      "Train: 1 [6000/10009 ( 60%)]  Loss: 4.78 (4.94)  Time: 0.108s, 1187.27/s  (0.135s,  951.42/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 807.350s\n",
      "Train: 1 [6050/10009 ( 60%)]  Loss: 4.95 (4.94)  Time: 0.117s, 1096.16/s  (0.135s,  951.24/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 814.225s\n",
      "Train: 1 [6100/10009 ( 61%)]  Loss: 4.81 (4.94)  Time: 0.109s, 1173.71/s  (0.135s,  951.29/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 820.913s\n",
      "Train: 1 [6150/10009 ( 61%)]  Loss: 4.70 (4.94)  Time: 0.110s, 1159.39/s  (0.135s,  951.22/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 827.698s\n",
      "Train: 1 [6200/10009 ( 62%)]  Loss: 4.94 (4.94)  Time: 0.114s, 1119.66/s  (0.135s,  951.10/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 834.531s\n",
      "Train: 1 [6250/10009 ( 62%)]  Loss: 4.61 (4.94)  Time: 0.110s, 1168.94/s  (0.135s,  951.22/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 841.156s\n",
      "Train: 1 [6300/10009 ( 63%)]  Loss: 4.49 (4.94)  Time: 0.137s,  933.49/s  (0.135s,  950.86/s)  LR: 4.945e-05  Data: 0.008 (0.028)Time: 848.202s\n",
      "Train: 1 [6350/10009 ( 63%)]  Loss: 4.87 (4.94)  Time: 0.109s, 1174.67/s  (0.135s,  950.69/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 855.093s\n",
      "Train: 1 [6400/10009 ( 64%)]  Loss: 4.64 (4.93)  Time: 0.106s, 1211.04/s  (0.135s,  949.50/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 862.905s\n",
      "Train: 1 [6450/10009 ( 64%)]  Loss: 4.63 (4.93)  Time: 0.107s, 1200.01/s  (0.135s,  949.59/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 869.561s\n",
      "Train: 1 [6500/10009 ( 65%)]  Loss: 4.85 (4.93)  Time: 0.137s,  930.99/s  (0.135s,  949.01/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 876.838s\n",
      "Train: 1 [6550/10009 ( 65%)]  Loss: 4.42 (4.93)  Time: 0.107s, 1191.89/s  (0.135s,  948.96/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 883.629s\n",
      "Train: 1 [6600/10009 ( 66%)]  Loss: 4.61 (4.93)  Time: 0.107s, 1195.76/s  (0.135s,  948.80/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 890.520s\n",
      "Train: 1 [6650/10009 ( 66%)]  Loss: 4.48 (4.93)  Time: 0.107s, 1197.71/s  (0.135s,  948.05/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 897.980s\n",
      "Train: 1 [6700/10009 ( 67%)]  Loss: 5.05 (4.93)  Time: 0.108s, 1187.92/s  (0.135s,  948.04/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 904.735s\n",
      "Train: 1 [6750/10009 ( 67%)]  Loss: 4.69 (4.93)  Time: 0.107s, 1192.16/s  (0.135s,  948.17/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 911.366s\n",
      "Train: 1 [6800/10009 ( 68%)]  Loss: 4.62 (4.93)  Time: 0.138s,  928.84/s  (0.135s,  948.08/s)  LR: 4.945e-05  Data: 0.008 (0.029)Time: 918.194s\n",
      "Train: 1 [6850/10009 ( 68%)]  Loss: 4.67 (4.92)  Time: 0.111s, 1152.84/s  (0.135s,  948.53/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 924.509s\n",
      "Train: 1 [6900/10009 ( 69%)]  Loss: 4.74 (4.92)  Time: 0.108s, 1182.42/s  (0.135s,  948.17/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 931.607s\n",
      "Train: 1 [6950/10009 ( 69%)]  Loss: 4.43 (4.92)  Time: 0.108s, 1187.08/s  (0.135s,  947.85/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 938.675s\n",
      "Train: 1 [7000/10009 ( 70%)]  Loss: 4.73 (4.92)  Time: 0.107s, 1195.61/s  (0.135s,  947.88/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 945.396s\n",
      "Train: 1 [7050/10009 ( 70%)]  Loss: 4.50 (4.92)  Time: 0.109s, 1178.12/s  (0.135s,  947.98/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 952.051s\n",
      "Train: 1 [7100/10009 ( 71%)]  Loss: 4.54 (4.92)  Time: 0.116s, 1102.50/s  (0.135s,  947.89/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 958.890s\n",
      "Train: 1 [7150/10009 ( 71%)]  Loss: 5.07 (4.92)  Time: 0.116s, 1099.76/s  (0.135s,  947.61/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 965.931s\n",
      "Train: 1 [7200/10009 ( 72%)]  Loss: 4.82 (4.92)  Time: 0.120s, 1069.44/s  (0.135s,  947.78/s)  LR: 4.945e-05  Data: 0.008 (0.028)Time: 972.515s\n",
      "Train: 1 [7250/10009 ( 72%)]  Loss: 4.57 (4.92)  Time: 0.106s, 1204.19/s  (0.135s,  947.75/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 979.291s\n",
      "Train: 1 [7300/10009 ( 73%)]  Loss: 4.46 (4.91)  Time: 0.108s, 1182.31/s  (0.135s,  947.62/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 986.183s\n",
      "Train: 1 [7350/10009 ( 73%)]  Loss: 5.14 (4.91)  Time: 0.251s,  510.19/s  (0.135s,  947.47/s)  LR: 4.945e-05  Data: 0.093 (0.028)Time: 993.088s\n",
      "Train: 1 [7400/10009 ( 74%)]  Loss: 4.72 (4.91)  Time: 0.106s, 1206.39/s  (0.135s,  947.18/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1000.153s\n",
      "Train: 1 [7450/10009 ( 74%)]  Loss: 4.80 (4.91)  Time: 0.281s,  456.20/s  (0.135s,  946.58/s)  LR: 4.945e-05  Data: 0.171 (0.028)Time: 1007.546s\n",
      "Train: 1 [7500/10009 ( 75%)]  Loss: 4.72 (4.91)  Time: 0.108s, 1182.94/s  (0.135s,  946.67/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 1014.215s\n",
      "Train: 1 [7550/10009 ( 75%)]  Loss: 4.79 (4.91)  Time: 0.108s, 1185.31/s  (0.135s,  946.55/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1021.098s\n",
      "Train: 1 [7600/10009 ( 76%)]  Loss: 4.79 (4.91)  Time: 0.108s, 1180.48/s  (0.135s,  946.56/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1027.857s\n",
      "Train: 1 [7650/10009 ( 76%)]  Loss: 4.68 (4.91)  Time: 0.132s,  972.61/s  (0.135s,  946.49/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 1034.696s\n",
      "Train: 1 [7700/10009 ( 77%)]  Loss: 4.87 (4.91)  Time: 0.108s, 1189.15/s  (0.135s,  946.61/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1041.320s\n",
      "Train: 1 [7750/10009 ( 77%)]  Loss: 4.78 (4.90)  Time: 0.110s, 1161.19/s  (0.135s,  945.80/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 1048.979s\n",
      "Train: 1 [7800/10009 ( 78%)]  Loss: 4.72 (4.90)  Time: 0.108s, 1179.88/s  (0.135s,  945.29/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 1056.318s\n",
      "Train: 1 [7850/10009 ( 78%)]  Loss: 4.81 (4.90)  Time: 0.109s, 1172.88/s  (0.135s,  944.99/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1063.427s\n",
      "Train: 1 [7900/10009 ( 79%)]  Loss: 4.67 (4.90)  Time: 0.107s, 1195.73/s  (0.135s,  945.14/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1070.026s\n",
      "Train: 1 [7950/10009 ( 79%)]  Loss: 4.75 (4.90)  Time: 0.106s, 1202.18/s  (0.135s,  944.82/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1077.161s\n",
      "Train: 1 [8000/10009 ( 80%)]  Loss: 4.65 (4.90)  Time: 0.108s, 1188.53/s  (0.135s,  944.71/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 1084.064s\n",
      "Train: 1 [8050/10009 ( 80%)]  Loss: 4.83 (4.90)  Time: 0.317s,  404.41/s  (0.136s,  944.27/s)  LR: 4.945e-05  Data: 0.214 (0.029)Time: 1091.341s\n",
      "Train: 1 [8100/10009 ( 81%)]  Loss: 4.73 (4.90)  Time: 0.128s,  998.95/s  (0.136s,  944.38/s)  LR: 4.945e-05  Data: 0.008 (0.028)Time: 1097.989s\n",
      "Train: 1 [8150/10009 ( 81%)]  Loss: 5.00 (4.90)  Time: 0.136s,  939.54/s  (0.136s,  944.38/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 1104.776s\n",
      "Train: 1 [8200/10009 ( 82%)]  Loss: 4.73 (4.90)  Time: 0.108s, 1186.86/s  (0.136s,  944.28/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 1111.661s\n",
      "Train: 1 [8250/10009 ( 82%)]  Loss: 4.86 (4.89)  Time: 0.107s, 1192.62/s  (0.136s,  944.50/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1118.187s\n",
      "Train: 1 [8300/10009 ( 83%)]  Loss: 4.60 (4.89)  Time: 0.109s, 1177.88/s  (0.136s,  944.55/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 1124.903s\n",
      "Train: 1 [8350/10009 ( 83%)]  Loss: 4.83 (4.89)  Time: 0.107s, 1201.03/s  (0.136s,  944.34/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1131.928s\n",
      "Train: 1 [8400/10009 ( 84%)]  Loss: 4.56 (4.89)  Time: 0.131s,  980.37/s  (0.136s,  944.48/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 1138.542s\n",
      "Train: 1 [8450/10009 ( 84%)]  Loss: 4.66 (4.89)  Time: 0.109s, 1170.36/s  (0.136s,  944.21/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1145.644s\n",
      "Train: 1 [8500/10009 ( 85%)]  Loss: 4.68 (4.89)  Time: 0.107s, 1193.04/s  (0.136s,  944.11/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1152.539s\n",
      "Train: 1 [8550/10009 ( 85%)]  Loss: 4.69 (4.89)  Time: 0.128s, 1002.71/s  (0.136s,  943.42/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 1160.161s\n",
      "Train: 1 [8600/10009 ( 86%)]  Loss: 4.56 (4.89)  Time: 0.139s,  923.23/s  (0.136s,  943.65/s)  LR: 4.945e-05  Data: 0.008 (0.028)Time: 1166.666s\n",
      "Train: 1 [8650/10009 ( 86%)]  Loss: 4.87 (4.89)  Time: 0.109s, 1178.92/s  (0.136s,  943.38/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 1173.778s\n",
      "Train: 1 [8700/10009 ( 87%)]  Loss: 4.68 (4.88)  Time: 0.115s, 1114.23/s  (0.136s,  943.08/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 1180.945s\n",
      "Train: 1 [8750/10009 ( 87%)]  Loss: 4.49 (4.88)  Time: 0.127s, 1009.28/s  (0.136s,  942.87/s)  LR: 4.945e-05  Data: 0.025 (0.029)Time: 1187.993s\n",
      "Train: 1 [8800/10009 ( 88%)]  Loss: 4.82 (4.88)  Time: 0.111s, 1151.09/s  (0.136s,  942.48/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 1195.279s\n",
      "Train: 1 [8850/10009 ( 88%)]  Loss: 4.52 (4.88)  Time: 0.182s,  704.23/s  (0.136s,  942.01/s)  LR: 4.945e-05  Data: 0.011 (0.029)Time: 1202.665s\n",
      "Train: 1 [8900/10009 ( 89%)]  Loss: 4.84 (4.88)  Time: 0.111s, 1151.30/s  (0.136s,  941.67/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 1209.895s\n",
      "Train: 1 [8950/10009 ( 89%)]  Loss: 4.70 (4.88)  Time: 0.112s, 1147.52/s  (0.136s,  941.29/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 1217.191s\n",
      "Train: 1 [9000/10009 ( 90%)]  Loss: 4.52 (4.88)  Time: 0.204s,  628.71/s  (0.136s,  941.01/s)  LR: 4.945e-05  Data: 0.100 (0.029)Time: 1224.347s\n",
      "Train: 1 [9050/10009 ( 90%)]  Loss: 4.73 (4.88)  Time: 0.144s,  886.84/s  (0.136s,  940.74/s)  LR: 4.945e-05  Data: 0.007 (0.029)Time: 1231.498s\n",
      "Train: 1 [9100/10009 ( 91%)]  Loss: 4.42 (4.88)  Time: 0.123s, 1040.71/s  (0.136s,  940.59/s)  LR: 4.945e-05  Data: 0.015 (0.029)Time: 1238.497s\n",
      "Train: 1 [9150/10009 ( 91%)]  Loss: 4.61 (4.88)  Time: 0.114s, 1126.69/s  (0.136s,  940.57/s)  LR: 4.945e-05  Data: 0.008 (0.029)Time: 1245.338s\n",
      "Train: 1 [9200/10009 ( 92%)]  Loss: 4.88 (4.87)  Time: 0.139s,  920.52/s  (0.136s,  940.38/s)  LR: 4.945e-05  Data: 0.012 (0.029)Time: 1252.397s\n",
      "Train: 1 [9250/10009 ( 92%)]  Loss: 4.98 (4.87)  Time: 0.107s, 1194.54/s  (0.136s,  940.11/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 1259.554s\n",
      "Train: 1 [9300/10009 ( 93%)]  Loss: 4.57 (4.87)  Time: 0.148s,  863.10/s  (0.136s,  939.93/s)  LR: 4.945e-05  Data: 0.031 (0.029)Time: 1266.609s\n",
      "Train: 1 [9350/10009 ( 93%)]  Loss: 4.65 (4.87)  Time: 0.108s, 1185.32/s  (0.136s,  939.78/s)  LR: 4.945e-05  Data: 0.006 (0.029)Time: 1273.620s\n",
      "Train: 1 [9400/10009 ( 94%)]  Loss: 4.65 (4.87)  Time: 0.115s, 1115.12/s  (0.136s,  939.30/s)  LR: 4.945e-05  Data: 0.005 (0.029)Time: 1281.092s\n",
      "Train: 1 [9450/10009 ( 94%)]  Loss: 4.30 (4.87)  Time: 0.110s, 1162.32/s  (0.136s,  939.20/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1288.035s\n",
      "Train: 1 [9500/10009 ( 95%)]  Loss: 4.55 (4.87)  Time: 0.123s, 1039.50/s  (0.136s,  938.98/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 1295.153s\n",
      "Train: 1 [9550/10009 ( 95%)]  Loss: 4.90 (4.87)  Time: 0.118s, 1085.48/s  (0.136s,  938.97/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1301.985s\n",
      "Train: 1 [9600/10009 ( 96%)]  Loss: 4.68 (4.87)  Time: 0.262s,  488.89/s  (0.136s,  938.81/s)  LR: 4.945e-05  Data: 0.155 (0.028)Time: 1309.024s\n",
      "Train: 1 [9650/10009 ( 96%)]  Loss: 4.74 (4.87)  Time: 0.119s, 1074.47/s  (0.136s,  938.72/s)  LR: 4.945e-05  Data: 0.007 (0.028)Time: 1315.959s\n",
      "Train: 1 [9700/10009 ( 97%)]  Loss: 4.79 (4.86)  Time: 0.108s, 1185.41/s  (0.136s,  938.66/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1322.869s\n",
      "Train: 1 [9750/10009 ( 97%)]  Loss: 4.60 (4.86)  Time: 0.115s, 1108.53/s  (0.136s,  938.49/s)  LR: 4.945e-05  Data: 0.006 (0.028)Time: 1329.927s\n",
      "Train: 1 [9800/10009 ( 98%)]  Loss: 4.60 (4.86)  Time: 0.106s, 1203.75/s  (0.136s,  938.63/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1336.552s\n",
      "Train: 1 [9850/10009 ( 98%)]  Loss: 4.72 (4.86)  Time: 0.106s, 1205.01/s  (0.136s,  938.66/s)  LR: 4.945e-05  Data: 0.005 (0.028)Time: 1343.319s\n",
      "Train: 1 [9900/10009 ( 99%)]  Loss: 4.19 (4.86)  Time: 0.136s,  940.97/s  (0.136s,  938.63/s)  LR: 4.945e-05  Data: 0.008 (0.028)Time: 1350.184s\n",
      "Train: 1 [9950/10009 ( 99%)]  Loss: 4.71 (4.86)  Time: 0.149s,  858.20/s  (0.136s,  938.90/s)  LR: 4.945e-05  Data: 0.020 (0.028)Time: 1356.608s\n",
      "Train: 1 [10000/10009 (100%)]  Loss: 4.48 (4.86)  Time: 0.224s,  572.28/s  (0.136s,  938.97/s)  LR: 4.945e-05  Data: 0.107 (0.028)Time: 1363.333s\n",
      "Test: [   0/390]  Time: 0.654 (0.654)  Loss:   2.773 ( 2.773)  Acc@1:  44.531 ( 44.531)  Acc@5:  74.219 ( 74.219)\n",
      "Test: [  50/390]  Time: 0.061 (0.149)  Loss:   3.393 ( 3.873)  Acc@1:  30.469 ( 25.383)  Acc@5:  57.812 ( 47.319)\n",
      "Test: [ 100/390]  Time: 0.247 (0.145)  Loss:   4.225 ( 4.062)  Acc@1:   9.375 ( 21.009)  Acc@5:  42.969 ( 42.976)\n",
      "Test: [ 150/390]  Time: 0.033 (0.147)  Loss:   3.994 ( 4.025)  Acc@1:  11.719 ( 21.182)  Acc@5:  55.469 ( 43.776)\n",
      "Test: [ 200/390]  Time: 0.031 (0.148)  Loss:   5.338 ( 4.172)  Acc@1:   2.344 ( 19.776)  Acc@5:  21.094 ( 41.387)\n",
      "Test: [ 250/390]  Time: 0.167 (0.146)  Loss:   4.812 ( 4.284)  Acc@1:  14.844 ( 18.579)  Acc@5:  31.250 ( 39.287)\n",
      "Test: [ 300/390]  Time: 0.033 (0.143)  Loss:   4.464 ( 4.366)  Acc@1:  17.969 ( 17.727)  Acc@5:  42.969 ( 37.765)\n",
      "Test: [ 350/390]  Time: 0.032 (0.141)  Loss:   4.884 ( 4.429)  Acc@1:   5.469 ( 17.143)  Acc@5:  18.750 ( 36.625)\n",
      "Test: [ 390/390]  Time: 0.018 (0.142)  Loss:   5.277 ( 4.412)  Acc@1:  11.250 ( 17.546)  Acc@5:  23.750 ( 37.066)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-1.pth.tar', 17.546)\n",
      " ('./output/budgeted/checkpoint-0.pth.tar', 9.416)\n",
      "\n",
      "Train: 2 [   0/10009 (  0%)]  Loss: 4.72 (4.72)  Time: 0.749s,  170.87/s  (0.749s,  170.87/s)  LR: 4.784e-05  Data: 0.620 (0.620)Time: 0.750s\n",
      "Train: 2 [  50/10009 (  0%)]  Loss: 4.57 (4.62)  Time: 0.109s, 1175.29/s  (0.162s,  792.22/s)  LR: 4.784e-05  Data: 0.005 (0.055)Time: 8.241s\n",
      "Train: 2 [ 100/10009 (  1%)]  Loss: 4.45 (4.60)  Time: 0.131s,  977.56/s  (0.148s,  867.50/s)  LR: 4.784e-05  Data: 0.007 (0.041)Time: 14.903s\n",
      "Train: 2 [ 150/10009 (  1%)]  Loss: 4.70 (4.61)  Time: 0.108s, 1185.62/s  (0.145s,  883.74/s)  LR: 4.784e-05  Data: 0.005 (0.039)Time: 21.871s\n",
      "Train: 2 [ 200/10009 (  2%)]  Loss: 4.62 (4.61)  Time: 0.105s, 1220.48/s  (0.143s,  892.92/s)  LR: 4.784e-05  Data: 0.005 (0.037)Time: 28.814s\n",
      "Train: 2 [ 250/10009 (  2%)]  Loss: 4.66 (4.61)  Time: 0.110s, 1162.91/s  (0.142s,  899.62/s)  LR: 4.784e-05  Data: 0.006 (0.036)Time: 35.713s\n",
      "Train: 2 [ 300/10009 (  3%)]  Loss: 4.68 (4.61)  Time: 0.107s, 1194.82/s  (0.141s,  907.16/s)  LR: 4.784e-05  Data: 0.005 (0.034)Time: 42.472s\n",
      "Train: 2 [ 350/10009 (  3%)]  Loss: 4.34 (4.60)  Time: 0.106s, 1207.05/s  (0.140s,  916.87/s)  LR: 4.784e-05  Data: 0.005 (0.032)Time: 49.002s\n",
      "Train: 2 [ 400/10009 (  4%)]  Loss: 4.50 (4.61)  Time: 0.137s,  936.60/s  (0.138s,  924.19/s)  LR: 4.784e-05  Data: 0.006 (0.030)Time: 55.539s\n",
      "Train: 2 [ 450/10009 (  4%)]  Loss: 4.81 (4.61)  Time: 0.107s, 1193.58/s  (0.138s,  928.79/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 62.154s\n",
      "Train: 2 [ 500/10009 (  5%)]  Loss: 4.57 (4.61)  Time: 0.109s, 1174.53/s  (0.138s,  925.75/s)  LR: 4.784e-05  Data: 0.005 (0.029)Time: 69.272s\n",
      "Train: 2 [ 550/10009 (  5%)]  Loss: 4.76 (4.61)  Time: 0.136s,  944.28/s  (0.138s,  926.14/s)  LR: 4.784e-05  Data: 0.010 (0.029)Time: 76.153s\n",
      "Train: 2 [ 600/10009 (  6%)]  Loss: 4.74 (4.61)  Time: 0.109s, 1177.87/s  (0.137s,  931.35/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 82.599s\n",
      "Train: 2 [ 650/10009 (  6%)]  Loss: 4.60 (4.61)  Time: 0.107s, 1199.18/s  (0.137s,  934.86/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 89.134s\n",
      "Train: 2 [ 700/10009 (  7%)]  Loss: 4.41 (4.61)  Time: 0.113s, 1135.00/s  (0.136s,  937.91/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 95.669s\n",
      "Train: 2 [ 750/10009 (  7%)]  Loss: 4.33 (4.60)  Time: 0.107s, 1196.51/s  (0.137s,  937.32/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 102.556s\n",
      "Train: 2 [ 800/10009 (  8%)]  Loss: 4.21 (4.60)  Time: 0.117s, 1093.62/s  (0.138s,  930.61/s)  LR: 4.784e-05  Data: 0.006 (0.029)Time: 110.173s\n",
      "Train: 2 [ 850/10009 (  8%)]  Loss: 4.38 (4.60)  Time: 0.119s, 1079.09/s  (0.137s,  932.50/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 116.813s\n",
      "Train: 2 [ 900/10009 (  9%)]  Loss: 4.47 (4.60)  Time: 0.113s, 1133.72/s  (0.137s,  933.54/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 123.539s\n",
      "Train: 2 [ 950/10009 (  9%)]  Loss: 4.69 (4.60)  Time: 0.207s,  619.47/s  (0.137s,  937.19/s)  LR: 4.784e-05  Data: 0.083 (0.027)Time: 129.886s\n",
      "Train: 2 [1000/10009 ( 10%)]  Loss: 4.61 (4.60)  Time: 0.114s, 1127.06/s  (0.137s,  931.62/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 137.532s\n",
      "Train: 2 [1050/10009 ( 10%)]  Loss: 4.67 (4.60)  Time: 0.357s,  358.75/s  (0.137s,  930.93/s)  LR: 4.784e-05  Data: 0.225 (0.028)Time: 144.510s\n",
      "Train: 2 [1100/10009 ( 11%)]  Loss: 4.47 (4.59)  Time: 0.109s, 1176.67/s  (0.137s,  932.35/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 151.154s\n",
      "Train: 2 [1150/10009 ( 11%)]  Loss: 4.76 (4.59)  Time: 0.122s, 1048.08/s  (0.138s,  929.42/s)  LR: 4.784e-05  Data: 0.009 (0.028)Time: 158.516s\n",
      "Train: 2 [1200/10009 ( 12%)]  Loss: 4.54 (4.59)  Time: 0.108s, 1189.63/s  (0.137s,  932.69/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 164.821s\n",
      "Train: 2 [1250/10009 ( 12%)]  Loss: 4.48 (4.59)  Time: 0.108s, 1183.93/s  (0.137s,  931.30/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 171.940s\n",
      "Train: 2 [1300/10009 ( 13%)]  Loss: 4.58 (4.59)  Time: 0.109s, 1170.61/s  (0.137s,  934.39/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 178.220s\n",
      "Train: 2 [1350/10009 ( 13%)]  Loss: 4.40 (4.59)  Time: 0.142s,  900.32/s  (0.137s,  934.67/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 185.014s\n",
      "Train: 2 [1400/10009 ( 14%)]  Loss: 4.34 (4.59)  Time: 0.107s, 1191.97/s  (0.137s,  935.06/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 191.782s\n",
      "Train: 2 [1450/10009 ( 14%)]  Loss: 4.61 (4.59)  Time: 0.108s, 1183.25/s  (0.137s,  934.96/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 198.647s\n",
      "Train: 2 [1500/10009 ( 15%)]  Loss: 4.59 (4.59)  Time: 0.114s, 1124.54/s  (0.137s,  936.22/s)  LR: 4.784e-05  Data: 0.011 (0.028)Time: 205.216s\n",
      "Train: 2 [1550/10009 ( 15%)]  Loss: 4.36 (4.59)  Time: 0.162s,  788.22/s  (0.137s,  936.25/s)  LR: 4.784e-05  Data: 0.060 (0.028)Time: 212.046s\n",
      "Train: 2 [1600/10009 ( 16%)]  Loss: 4.43 (4.59)  Time: 0.106s, 1202.33/s  (0.137s,  934.02/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 219.403s\n",
      "Train: 2 [1650/10009 ( 16%)]  Loss: 4.45 (4.59)  Time: 0.148s,  864.47/s  (0.137s,  934.60/s)  LR: 4.784e-05  Data: 0.046 (0.028)Time: 226.116s\n",
      "Train: 2 [1700/10009 ( 17%)]  Loss: 4.16 (4.59)  Time: 0.108s, 1189.01/s  (0.137s,  934.54/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 232.979s\n",
      "Train: 2 [1750/10009 ( 17%)]  Loss: 4.52 (4.59)  Time: 0.129s,  989.54/s  (0.137s,  933.63/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 240.060s\n",
      "Train: 2 [1800/10009 ( 18%)]  Loss: 4.74 (4.59)  Time: 0.106s, 1210.32/s  (0.137s,  934.44/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 246.702s\n",
      "Train: 2 [1850/10009 ( 18%)]  Loss: 4.76 (4.59)  Time: 0.268s,  477.69/s  (0.137s,  934.74/s)  LR: 4.784e-05  Data: 0.152 (0.028)Time: 253.469s\n",
      "Train: 2 [1900/10009 ( 19%)]  Loss: 4.33 (4.59)  Time: 0.107s, 1200.17/s  (0.137s,  934.96/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 260.253s\n",
      "Train: 2 [1950/10009 ( 19%)]  Loss: 4.63 (4.59)  Time: 0.132s,  973.32/s  (0.137s,  935.39/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 266.978s\n",
      "Train: 2 [2000/10009 ( 20%)]  Loss: 4.56 (4.59)  Time: 0.113s, 1128.80/s  (0.137s,  936.32/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 273.548s\n",
      "Train: 2 [2050/10009 ( 20%)]  Loss: 4.35 (4.59)  Time: 0.157s,  816.27/s  (0.137s,  936.14/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 280.436s\n",
      "Train: 2 [2100/10009 ( 21%)]  Loss: 4.68 (4.59)  Time: 0.191s,  670.64/s  (0.136s,  938.22/s)  LR: 4.784e-05  Data: 0.060 (0.027)Time: 286.636s\n",
      "Train: 2 [2150/10009 ( 21%)]  Loss: 4.69 (4.59)  Time: 0.108s, 1182.61/s  (0.136s,  938.74/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 293.296s\n",
      "Train: 2 [2200/10009 ( 22%)]  Loss: 4.70 (4.59)  Time: 0.129s,  992.27/s  (0.136s,  938.68/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 300.131s\n",
      "Train: 2 [2250/10009 ( 22%)]  Loss: 4.44 (4.59)  Time: 0.108s, 1186.41/s  (0.136s,  938.78/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 306.918s\n",
      "Train: 2 [2300/10009 ( 23%)]  Loss: 4.58 (4.59)  Time: 0.108s, 1183.25/s  (0.136s,  939.29/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 313.564s\n",
      "Train: 2 [2350/10009 ( 23%)]  Loss: 4.79 (4.59)  Time: 0.185s,  691.42/s  (0.137s,  937.68/s)  LR: 4.784e-05  Data: 0.084 (0.027)Time: 320.929s\n",
      "Train: 2 [2400/10009 ( 24%)]  Loss: 4.64 (4.59)  Time: 0.107s, 1196.27/s  (0.136s,  938.81/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 327.360s\n",
      "Train: 2 [2450/10009 ( 24%)]  Loss: 4.66 (4.59)  Time: 0.372s,  344.52/s  (0.137s,  937.51/s)  LR: 4.784e-05  Data: 0.273 (0.027)Time: 334.638s\n",
      "Train: 2 [2500/10009 ( 25%)]  Loss: 4.89 (4.58)  Time: 0.134s,  953.80/s  (0.137s,  936.90/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 341.689s\n",
      "Train: 2 [2550/10009 ( 25%)]  Loss: 4.56 (4.58)  Time: 0.315s,  406.74/s  (0.137s,  935.76/s)  LR: 4.784e-05  Data: 0.175 (0.028)Time: 348.944s\n",
      "Train: 2 [2600/10009 ( 26%)]  Loss: 4.29 (4.58)  Time: 0.104s, 1225.63/s  (0.137s,  936.02/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 355.685s\n",
      "Train: 2 [2650/10009 ( 26%)]  Loss: 4.68 (4.58)  Time: 0.107s, 1193.47/s  (0.137s,  935.70/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 362.645s\n",
      "Train: 2 [2700/10009 ( 27%)]  Loss: 4.41 (4.58)  Time: 0.108s, 1184.22/s  (0.137s,  935.56/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 369.540s\n",
      "Train: 2 [2750/10009 ( 27%)]  Loss: 4.36 (4.58)  Time: 0.108s, 1186.06/s  (0.137s,  935.40/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 376.446s\n",
      "Train: 2 [2800/10009 ( 28%)]  Loss: 4.34 (4.58)  Time: 0.142s,  901.53/s  (0.137s,  935.66/s)  LR: 4.784e-05  Data: 0.041 (0.028)Time: 383.181s\n",
      "Train: 2 [2850/10009 ( 28%)]  Loss: 4.38 (4.58)  Time: 0.150s,  855.00/s  (0.137s,  935.55/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 390.069s\n",
      "Train: 2 [2900/10009 ( 29%)]  Loss: 4.44 (4.58)  Time: 0.142s,  899.26/s  (0.137s,  936.78/s)  LR: 4.784e-05  Data: 0.041 (0.028)Time: 396.386s\n",
      "Train: 2 [2950/10009 ( 29%)]  Loss: 4.65 (4.58)  Time: 0.107s, 1197.78/s  (0.137s,  935.72/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 403.676s\n",
      "Train: 2 [3000/10009 ( 30%)]  Loss: 4.67 (4.58)  Time: 0.108s, 1183.28/s  (0.137s,  936.02/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 410.382s\n",
      "Train: 2 [3050/10009 ( 30%)]  Loss: 4.37 (4.58)  Time: 0.175s,  733.13/s  (0.137s,  936.42/s)  LR: 4.784e-05  Data: 0.059 (0.028)Time: 417.043s\n",
      "Train: 2 [3100/10009 ( 31%)]  Loss: 4.88 (4.58)  Time: 0.137s,  934.50/s  (0.137s,  935.51/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 424.291s\n",
      "Train: 2 [3150/10009 ( 31%)]  Loss: 4.45 (4.58)  Time: 0.132s,  966.52/s  (0.137s,  935.60/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 431.091s\n",
      "Train: 2 [3200/10009 ( 32%)]  Loss: 4.69 (4.58)  Time: 0.109s, 1179.34/s  (0.137s,  935.37/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 438.036s\n",
      "Train: 2 [3250/10009 ( 32%)]  Loss: 4.64 (4.58)  Time: 0.153s,  837.63/s  (0.137s,  936.17/s)  LR: 4.784e-05  Data: 0.048 (0.028)Time: 444.500s\n",
      "Train: 2 [3300/10009 ( 33%)]  Loss: 4.19 (4.57)  Time: 0.108s, 1184.17/s  (0.137s,  936.87/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 451.001s\n",
      "Train: 2 [3350/10009 ( 33%)]  Loss: 4.67 (4.57)  Time: 0.141s,  908.47/s  (0.137s,  937.17/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 457.685s\n",
      "Train: 2 [3400/10009 ( 34%)]  Loss: 4.33 (4.57)  Time: 0.111s, 1155.70/s  (0.137s,  937.53/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 464.333s\n",
      "Train: 2 [3450/10009 ( 34%)]  Loss: 4.70 (4.57)  Time: 0.136s,  939.37/s  (0.136s,  937.89/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 470.979s\n",
      "Train: 2 [3500/10009 ( 35%)]  Loss: 4.33 (4.57)  Time: 0.110s, 1168.88/s  (0.136s,  938.20/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 477.648s\n",
      "Train: 2 [3550/10009 ( 35%)]  Loss: 4.39 (4.57)  Time: 0.127s, 1008.77/s  (0.136s,  938.21/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 484.463s\n",
      "Train: 2 [3600/10009 ( 36%)]  Loss: 4.41 (4.57)  Time: 0.133s,  960.42/s  (0.136s,  938.95/s)  LR: 4.784e-05  Data: 0.008 (0.027)Time: 490.896s\n",
      "Train: 2 [3650/10009 ( 36%)]  Loss: 4.73 (4.57)  Time: 0.124s, 1033.85/s  (0.136s,  939.70/s)  LR: 4.784e-05  Data: 0.008 (0.027)Time: 497.314s\n",
      "Train: 2 [3700/10009 ( 37%)]  Loss: 4.38 (4.57)  Time: 0.136s,  943.14/s  (0.136s,  940.33/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 503.790s\n",
      "Train: 2 [3750/10009 ( 37%)]  Loss: 4.66 (4.57)  Time: 0.225s,  568.84/s  (0.136s,  940.57/s)  LR: 4.784e-05  Data: 0.124 (0.027)Time: 510.464s\n",
      "Train: 2 [3800/10009 ( 38%)]  Loss: 4.81 (4.57)  Time: 0.110s, 1168.47/s  (0.136s,  939.93/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 517.619s\n",
      "Train: 2 [3850/10009 ( 38%)]  Loss: 4.67 (4.57)  Time: 0.233s,  550.11/s  (0.136s,  938.81/s)  LR: 4.784e-05  Data: 0.129 (0.027)Time: 525.053s\n",
      "Train: 2 [3900/10009 ( 39%)]  Loss: 4.39 (4.57)  Time: 0.109s, 1177.46/s  (0.136s,  939.01/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 531.756s\n",
      "Train: 2 [3950/10009 ( 39%)]  Loss: 4.43 (4.57)  Time: 0.109s, 1173.29/s  (0.136s,  938.56/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 538.831s\n",
      "Train: 2 [4000/10009 ( 40%)]  Loss: 4.65 (4.57)  Time: 0.113s, 1128.04/s  (0.136s,  938.29/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 545.806s\n",
      "Train: 2 [4050/10009 ( 40%)]  Loss: 4.54 (4.57)  Time: 0.117s, 1090.44/s  (0.137s,  937.37/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 553.169s\n",
      "Train: 2 [4100/10009 ( 41%)]  Loss: 4.69 (4.57)  Time: 0.108s, 1182.71/s  (0.137s,  936.09/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 560.763s\n",
      "Train: 2 [4150/10009 ( 41%)]  Loss: 4.62 (4.56)  Time: 0.108s, 1182.12/s  (0.137s,  936.10/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 567.596s\n",
      "Train: 2 [4200/10009 ( 42%)]  Loss: 4.33 (4.56)  Time: 0.112s, 1139.47/s  (0.137s,  936.05/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 574.463s\n",
      "Train: 2 [4250/10009 ( 42%)]  Loss: 4.69 (4.56)  Time: 0.111s, 1155.26/s  (0.137s,  936.51/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 581.017s\n",
      "Train: 2 [4300/10009 ( 43%)]  Loss: 4.56 (4.56)  Time: 0.107s, 1194.67/s  (0.137s,  935.55/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 588.450s\n",
      "Train: 2 [4350/10009 ( 43%)]  Loss: 4.55 (4.56)  Time: 0.122s, 1045.75/s  (0.137s,  935.79/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 595.138s\n",
      "Train: 2 [4400/10009 ( 44%)]  Loss: 4.35 (4.56)  Time: 0.157s,  817.67/s  (0.137s,  934.26/s)  LR: 4.784e-05  Data: 0.011 (0.027)Time: 602.967s\n",
      "Train: 2 [4450/10009 ( 44%)]  Loss: 4.39 (4.56)  Time: 0.121s, 1059.62/s  (0.137s,  933.59/s)  LR: 4.784e-05  Data: 0.009 (0.027)Time: 610.252s\n",
      "Train: 2 [4500/10009 ( 45%)]  Loss: 4.15 (4.56)  Time: 0.106s, 1204.20/s  (0.137s,  932.92/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 617.552s\n",
      "Train: 2 [4550/10009 ( 45%)]  Loss: 4.54 (4.56)  Time: 0.115s, 1112.42/s  (0.137s,  932.99/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 624.364s\n",
      "Train: 2 [4600/10009 ( 46%)]  Loss: 4.57 (4.56)  Time: 0.106s, 1202.06/s  (0.137s,  932.63/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 631.465s\n",
      "Train: 2 [4650/10009 ( 46%)]  Loss: 4.39 (4.56)  Time: 0.108s, 1185.02/s  (0.137s,  932.13/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 638.672s\n",
      "Train: 2 [4700/10009 ( 47%)]  Loss: 4.49 (4.56)  Time: 0.212s,  603.13/s  (0.137s,  931.54/s)  LR: 4.784e-05  Data: 0.085 (0.028)Time: 645.948s\n",
      "Train: 2 [4750/10009 ( 47%)]  Loss: 4.50 (4.56)  Time: 0.107s, 1197.46/s  (0.137s,  931.85/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 652.599s\n",
      "Train: 2 [4800/10009 ( 48%)]  Loss: 4.51 (4.56)  Time: 0.112s, 1142.36/s  (0.137s,  932.03/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 659.340s\n",
      "Train: 2 [4850/10009 ( 48%)]  Loss: 4.39 (4.56)  Time: 0.123s, 1037.37/s  (0.137s,  932.23/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 666.064s\n",
      "Train: 2 [4900/10009 ( 49%)]  Loss: 4.64 (4.56)  Time: 0.109s, 1173.90/s  (0.137s,  932.24/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 672.925s\n",
      "Train: 2 [4950/10009 ( 49%)]  Loss: 4.54 (4.56)  Time: 0.107s, 1198.86/s  (0.137s,  932.69/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 679.464s\n",
      "Train: 2 [5000/10009 ( 50%)]  Loss: 4.49 (4.56)  Time: 0.129s,  993.93/s  (0.137s,  932.52/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 686.447s\n",
      "Train: 2 [5050/10009 ( 50%)]  Loss: 4.62 (4.55)  Time: 0.136s,  944.61/s  (0.137s,  932.51/s)  LR: 4.784e-05  Data: 0.009 (0.028)Time: 693.318s\n",
      "Train: 2 [5100/10009 ( 51%)]  Loss: 4.75 (4.55)  Time: 0.126s, 1015.91/s  (0.137s,  932.82/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 699.945s\n",
      "Train: 2 [5150/10009 ( 51%)]  Loss: 4.30 (4.55)  Time: 0.107s, 1196.68/s  (0.137s,  931.92/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 707.489s\n",
      "Train: 2 [5200/10009 ( 52%)]  Loss: 4.56 (4.55)  Time: 0.127s, 1011.73/s  (0.137s,  931.89/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 714.382s\n",
      "Train: 2 [5250/10009 ( 52%)]  Loss: 4.55 (4.55)  Time: 0.112s, 1141.11/s  (0.137s,  932.34/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 720.901s\n",
      "Train: 2 [5300/10009 ( 53%)]  Loss: 4.68 (4.55)  Time: 0.108s, 1185.67/s  (0.137s,  932.65/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 727.522s\n",
      "Train: 2 [5350/10009 ( 53%)]  Loss: 4.56 (4.55)  Time: 0.123s, 1036.56/s  (0.137s,  932.56/s)  LR: 4.784e-05  Data: 0.021 (0.028)Time: 734.457s\n",
      "Train: 2 [5400/10009 ( 54%)]  Loss: 4.59 (4.55)  Time: 0.112s, 1139.77/s  (0.137s,  932.85/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 741.090s\n",
      "Train: 2 [5450/10009 ( 54%)]  Loss: 4.42 (4.55)  Time: 0.106s, 1203.34/s  (0.137s,  932.39/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 748.318s\n",
      "Train: 2 [5500/10009 ( 55%)]  Loss: 4.38 (4.55)  Time: 0.128s,  999.88/s  (0.137s,  932.71/s)  LR: 4.784e-05  Data: 0.007 (0.028)Time: 754.925s\n",
      "Train: 2 [5550/10009 ( 55%)]  Loss: 4.56 (4.55)  Time: 0.110s, 1164.91/s  (0.137s,  933.00/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 761.547s\n",
      "Train: 2 [5600/10009 ( 56%)]  Loss: 4.63 (4.55)  Time: 0.279s,  458.01/s  (0.137s,  933.30/s)  LR: 4.784e-05  Data: 0.176 (0.028)Time: 768.160s\n",
      "Train: 2 [5650/10009 ( 56%)]  Loss: 4.36 (4.55)  Time: 0.107s, 1198.84/s  (0.137s,  933.35/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 774.982s\n",
      "Train: 2 [5700/10009 ( 57%)]  Loss: 4.19 (4.55)  Time: 0.108s, 1190.32/s  (0.137s,  933.29/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 781.882s\n",
      "Train: 2 [5750/10009 ( 57%)]  Loss: 4.42 (4.55)  Time: 0.109s, 1178.15/s  (0.137s,  933.25/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 788.780s\n",
      "Train: 2 [5800/10009 ( 58%)]  Loss: 4.27 (4.55)  Time: 0.106s, 1205.60/s  (0.137s,  933.23/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 795.654s\n",
      "Train: 2 [5850/10009 ( 58%)]  Loss: 4.22 (4.55)  Time: 0.108s, 1180.75/s  (0.137s,  933.21/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 802.529s\n",
      "Train: 2 [5900/10009 ( 59%)]  Loss: 4.45 (4.55)  Time: 0.195s,  657.30/s  (0.137s,  933.10/s)  LR: 4.784e-05  Data: 0.091 (0.028)Time: 809.477s\n",
      "Train: 2 [5950/10009 ( 59%)]  Loss: 4.58 (4.54)  Time: 0.124s, 1028.37/s  (0.137s,  933.50/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 815.986s\n",
      "Train: 2 [6000/10009 ( 60%)]  Loss: 4.36 (4.54)  Time: 0.214s,  598.49/s  (0.137s,  933.59/s)  LR: 4.784e-05  Data: 0.112 (0.028)Time: 822.766s\n",
      "Train: 2 [6050/10009 ( 60%)]  Loss: 4.48 (4.54)  Time: 0.106s, 1207.36/s  (0.137s,  933.63/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 829.589s\n",
      "Train: 2 [6100/10009 ( 61%)]  Loss: 4.41 (4.54)  Time: 0.284s,  451.36/s  (0.137s,  933.61/s)  LR: 4.784e-05  Data: 0.166 (0.028)Time: 836.461s\n",
      "Train: 2 [6150/10009 ( 61%)]  Loss: 4.30 (4.54)  Time: 0.182s,  702.01/s  (0.137s,  934.04/s)  LR: 4.784e-05  Data: 0.081 (0.028)Time: 842.924s\n",
      "Train: 2 [6200/10009 ( 62%)]  Loss: 4.54 (4.54)  Time: 0.107s, 1192.32/s  (0.137s,  933.82/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 849.974s\n",
      "Train: 2 [6250/10009 ( 62%)]  Loss: 4.53 (4.54)  Time: 0.108s, 1188.93/s  (0.137s,  933.93/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 856.731s\n",
      "Train: 2 [6300/10009 ( 63%)]  Loss: 4.68 (4.54)  Time: 0.109s, 1178.72/s  (0.137s,  934.13/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 863.393s\n",
      "Train: 2 [6350/10009 ( 63%)]  Loss: 4.51 (4.54)  Time: 0.131s,  979.55/s  (0.137s,  934.47/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 869.933s\n",
      "Train: 2 [6400/10009 ( 64%)]  Loss: 4.79 (4.54)  Time: 0.223s,  573.90/s  (0.137s,  934.43/s)  LR: 4.784e-05  Data: 0.093 (0.028)Time: 876.822s\n",
      "Train: 2 [6450/10009 ( 64%)]  Loss: 4.63 (4.54)  Time: 0.108s, 1190.55/s  (0.137s,  934.54/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 883.567s\n",
      "Train: 2 [6500/10009 ( 65%)]  Loss: 4.65 (4.54)  Time: 0.110s, 1162.03/s  (0.137s,  934.85/s)  LR: 4.784e-05  Data: 0.008 (0.028)Time: 890.121s\n",
      "Train: 2 [6550/10009 ( 65%)]  Loss: 4.61 (4.54)  Time: 0.150s,  853.51/s  (0.137s,  934.63/s)  LR: 4.784e-05  Data: 0.011 (0.028)Time: 897.175s\n",
      "Train: 2 [6600/10009 ( 66%)]  Loss: 4.36 (4.54)  Time: 0.116s, 1108.15/s  (0.137s,  935.24/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 903.430s\n",
      "Train: 2 [6650/10009 ( 66%)]  Loss: 4.51 (4.54)  Time: 0.106s, 1204.68/s  (0.137s,  935.10/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 910.412s\n",
      "Train: 2 [6700/10009 ( 67%)]  Loss: 4.62 (4.54)  Time: 0.241s,  531.42/s  (0.137s,  934.89/s)  LR: 4.784e-05  Data: 0.112 (0.028)Time: 917.462s\n",
      "Train: 2 [6750/10009 ( 67%)]  Loss: 4.50 (4.54)  Time: 0.108s, 1188.75/s  (0.137s,  935.04/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 924.162s\n",
      "Train: 2 [6800/10009 ( 68%)]  Loss: 4.53 (4.54)  Time: 0.140s,  911.63/s  (0.137s,  935.17/s)  LR: 4.784e-05  Data: 0.038 (0.028)Time: 930.878s\n",
      "Train: 2 [6850/10009 ( 68%)]  Loss: 4.44 (4.53)  Time: 0.107s, 1200.84/s  (0.137s,  934.57/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 938.320s\n",
      "Train: 2 [6900/10009 ( 69%)]  Loss: 4.47 (4.53)  Time: 0.109s, 1175.45/s  (0.137s,  935.00/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 944.734s\n",
      "Train: 2 [6950/10009 ( 69%)]  Loss: 4.60 (4.53)  Time: 0.108s, 1183.85/s  (0.137s,  935.22/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 951.350s\n",
      "Train: 2 [7000/10009 ( 70%)]  Loss: 4.52 (4.53)  Time: 0.107s, 1196.26/s  (0.137s,  935.37/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 958.045s\n",
      "Train: 2 [7050/10009 ( 70%)]  Loss: 4.63 (4.53)  Time: 0.273s,  469.00/s  (0.137s,  935.22/s)  LR: 4.784e-05  Data: 0.163 (0.028)Time: 965.046s\n",
      "Train: 2 [7100/10009 ( 71%)]  Loss: 4.50 (4.53)  Time: 0.108s, 1182.65/s  (0.137s,  935.72/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 971.361s\n",
      "Train: 2 [7150/10009 ( 71%)]  Loss: 4.53 (4.53)  Time: 0.187s,  684.76/s  (0.137s,  935.79/s)  LR: 4.784e-05  Data: 0.068 (0.028)Time: 978.134s\n",
      "Train: 2 [7200/10009 ( 72%)]  Loss: 4.26 (4.53)  Time: 0.208s,  615.90/s  (0.137s,  936.13/s)  LR: 4.784e-05  Data: 0.105 (0.028)Time: 984.610s\n",
      "Train: 2 [7250/10009 ( 72%)]  Loss: 4.68 (4.53)  Time: 0.106s, 1209.06/s  (0.137s,  936.04/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 991.539s\n",
      "Train: 2 [7300/10009 ( 73%)]  Loss: 4.41 (4.53)  Time: 0.160s,  799.80/s  (0.137s,  935.99/s)  LR: 4.784e-05  Data: 0.058 (0.028)Time: 998.432s\n",
      "Train: 2 [7350/10009 ( 73%)]  Loss: 4.52 (4.53)  Time: 0.107s, 1198.69/s  (0.137s,  936.45/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 1004.774s\n",
      "Train: 2 [7400/10009 ( 74%)]  Loss: 4.63 (4.53)  Time: 0.127s, 1010.47/s  (0.137s,  936.36/s)  LR: 4.784e-05  Data: 0.006 (0.028)Time: 1011.709s\n",
      "Train: 2 [7450/10009 ( 74%)]  Loss: 4.38 (4.53)  Time: 0.108s, 1182.03/s  (0.137s,  936.59/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 1018.292s\n",
      "Train: 2 [7500/10009 ( 75%)]  Loss: 4.02 (4.53)  Time: 0.107s, 1193.70/s  (0.137s,  937.02/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1024.655s\n",
      "Train: 2 [7550/10009 ( 75%)]  Loss: 4.44 (4.53)  Time: 0.104s, 1226.81/s  (0.137s,  936.82/s)  LR: 4.784e-05  Data: 0.005 (0.028)Time: 1031.704s\n",
      "Train: 2 [7600/10009 ( 76%)]  Loss: 4.39 (4.53)  Time: 0.108s, 1181.35/s  (0.137s,  937.53/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1037.753s\n",
      "Train: 2 [7650/10009 ( 76%)]  Loss: 4.36 (4.53)  Time: 0.133s,  960.15/s  (0.137s,  937.62/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 1044.478s\n",
      "Train: 2 [7700/10009 ( 77%)]  Loss: 4.46 (4.52)  Time: 0.111s, 1157.65/s  (0.137s,  937.72/s)  LR: 4.784e-05  Data: 0.008 (0.027)Time: 1051.198s\n",
      "Train: 2 [7750/10009 ( 77%)]  Loss: 4.44 (4.52)  Time: 0.108s, 1183.14/s  (0.136s,  937.77/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1057.965s\n",
      "Train: 2 [7800/10009 ( 78%)]  Loss: 4.41 (4.52)  Time: 0.110s, 1163.59/s  (0.136s,  937.77/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1064.785s\n",
      "Train: 2 [7850/10009 ( 78%)]  Loss: 4.66 (4.52)  Time: 0.106s, 1210.41/s  (0.136s,  937.86/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1071.503s\n",
      "Train: 2 [7900/10009 ( 79%)]  Loss: 4.75 (4.52)  Time: 0.245s,  523.46/s  (0.136s,  937.91/s)  LR: 4.784e-05  Data: 0.130 (0.027)Time: 1078.280s\n",
      "Train: 2 [7950/10009 ( 79%)]  Loss: 4.34 (4.52)  Time: 0.111s, 1152.82/s  (0.136s,  937.86/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1085.152s\n",
      "Train: 2 [8000/10009 ( 80%)]  Loss: 4.76 (4.52)  Time: 0.206s,  621.29/s  (0.136s,  938.13/s)  LR: 4.784e-05  Data: 0.105 (0.027)Time: 1091.660s\n",
      "Train: 2 [8050/10009 ( 80%)]  Loss: 4.46 (4.52)  Time: 0.108s, 1180.81/s  (0.136s,  938.44/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1098.124s\n",
      "Train: 2 [8100/10009 ( 81%)]  Loss: 4.45 (4.52)  Time: 0.232s,  550.99/s  (0.136s,  938.42/s)  LR: 4.784e-05  Data: 0.132 (0.027)Time: 1104.972s\n",
      "Train: 2 [8150/10009 ( 81%)]  Loss: 4.11 (4.52)  Time: 0.117s, 1091.69/s  (0.136s,  938.50/s)  LR: 4.784e-05  Data: 0.014 (0.027)Time: 1111.699s\n",
      "Train: 2 [8200/10009 ( 82%)]  Loss: 4.32 (4.52)  Time: 0.139s,  921.15/s  (0.136s,  938.57/s)  LR: 4.784e-05  Data: 0.011 (0.027)Time: 1118.429s\n",
      "Train: 2 [8250/10009 ( 82%)]  Loss: 4.45 (4.52)  Time: 0.107s, 1193.36/s  (0.136s,  938.92/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1124.829s\n",
      "Train: 2 [8300/10009 ( 83%)]  Loss: 4.55 (4.52)  Time: 0.186s,  687.42/s  (0.136s,  939.16/s)  LR: 4.784e-05  Data: 0.085 (0.027)Time: 1131.355s\n",
      "Train: 2 [8350/10009 ( 83%)]  Loss: 4.40 (4.52)  Time: 0.109s, 1174.68/s  (0.136s,  939.18/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1138.151s\n",
      "Train: 2 [8400/10009 ( 84%)]  Loss: 4.46 (4.52)  Time: 0.107s, 1196.43/s  (0.136s,  938.56/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1145.722s\n",
      "Train: 2 [8450/10009 ( 84%)]  Loss: 4.47 (4.52)  Time: 0.107s, 1191.15/s  (0.136s,  938.74/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1152.321s\n",
      "Train: 2 [8500/10009 ( 85%)]  Loss: 4.47 (4.52)  Time: 0.315s,  406.25/s  (0.136s,  938.38/s)  LR: 4.784e-05  Data: 0.191 (0.027)Time: 1159.576s\n",
      "Train: 2 [8550/10009 ( 85%)]  Loss: 4.29 (4.52)  Time: 0.107s, 1196.31/s  (0.136s,  938.66/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1166.046s\n",
      "Train: 2 [8600/10009 ( 86%)]  Loss: 4.30 (4.52)  Time: 0.114s, 1127.35/s  (0.136s,  938.64/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 1172.893s\n",
      "Train: 2 [8650/10009 ( 86%)]  Loss: 4.48 (4.51)  Time: 0.109s, 1175.32/s  (0.136s,  938.67/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1179.676s\n",
      "Train: 2 [8700/10009 ( 87%)]  Loss: 4.60 (4.51)  Time: 0.181s,  708.24/s  (0.136s,  938.72/s)  LR: 4.784e-05  Data: 0.079 (0.027)Time: 1186.430s\n",
      "Train: 2 [8750/10009 ( 87%)]  Loss: 4.46 (4.51)  Time: 0.120s, 1062.56/s  (0.136s,  938.71/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1193.256s\n",
      "Train: 2 [8800/10009 ( 88%)]  Loss: 4.31 (4.51)  Time: 0.136s,  940.04/s  (0.136s,  938.81/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1199.946s\n",
      "Train: 2 [8850/10009 ( 88%)]  Loss: 4.32 (4.51)  Time: 0.122s, 1052.04/s  (0.136s,  938.51/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1207.157s\n",
      "Train: 2 [8900/10009 ( 89%)]  Loss: 4.35 (4.51)  Time: 0.139s,  920.97/s  (0.136s,  938.47/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1214.017s\n",
      "Train: 2 [8950/10009 ( 89%)]  Loss: 4.44 (4.51)  Time: 0.103s, 1237.02/s  (0.136s,  937.98/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1221.480s\n",
      "Train: 2 [9000/10009 ( 90%)]  Loss: 4.50 (4.51)  Time: 0.254s,  504.88/s  (0.137s,  937.72/s)  LR: 4.784e-05  Data: 0.156 (0.027)Time: 1228.641s\n",
      "Train: 2 [9050/10009 ( 90%)]  Loss: 4.47 (4.51)  Time: 0.200s,  641.45/s  (0.136s,  938.03/s)  LR: 4.784e-05  Data: 0.101 (0.027)Time: 1235.056s\n",
      "Train: 2 [9100/10009 ( 91%)]  Loss: 4.40 (4.51)  Time: 0.103s, 1240.18/s  (0.136s,  938.54/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1241.206s\n",
      "Train: 2 [9150/10009 ( 91%)]  Loss: 4.48 (4.51)  Time: 0.104s, 1235.69/s  (0.136s,  938.59/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1247.959s\n",
      "Train: 2 [9200/10009 ( 92%)]  Loss: 4.47 (4.51)  Time: 0.122s, 1045.73/s  (0.136s,  938.87/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1254.413s\n",
      "Train: 2 [9250/10009 ( 92%)]  Loss: 4.35 (4.51)  Time: 0.105s, 1224.61/s  (0.136s,  939.26/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1260.702s\n",
      "Train: 2 [9300/10009 ( 93%)]  Loss: 4.36 (4.51)  Time: 0.105s, 1221.24/s  (0.136s,  939.16/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1267.648s\n",
      "Train: 2 [9350/10009 ( 93%)]  Loss: 4.35 (4.51)  Time: 0.104s, 1229.45/s  (0.136s,  939.49/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1274.008s\n",
      "Train: 2 [9400/10009 ( 94%)]  Loss: 4.20 (4.51)  Time: 0.108s, 1181.09/s  (0.136s,  939.79/s)  LR: 4.784e-05  Data: 0.011 (0.027)Time: 1280.421s\n",
      "Train: 2 [9450/10009 ( 94%)]  Loss: 4.29 (4.51)  Time: 0.133s,  958.99/s  (0.136s,  939.92/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 1287.055s\n",
      "Train: 2 [9500/10009 ( 95%)]  Loss: 4.30 (4.51)  Time: 0.105s, 1214.88/s  (0.136s,  940.29/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1293.352s\n",
      "Train: 2 [9550/10009 ( 95%)]  Loss: 4.41 (4.50)  Time: 0.103s, 1239.06/s  (0.136s,  940.84/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1299.403s\n",
      "Train: 2 [9600/10009 ( 96%)]  Loss: 4.30 (4.50)  Time: 0.105s, 1224.47/s  (0.136s,  941.15/s)  LR: 4.784e-05  Data: 0.005 (0.027)Time: 1305.766s\n",
      "Train: 2 [9650/10009 ( 96%)]  Loss: 4.37 (4.50)  Time: 0.105s, 1217.89/s  (0.136s,  940.92/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1312.883s\n",
      "Train: 2 [9700/10009 ( 97%)]  Loss: 4.21 (4.50)  Time: 0.105s, 1218.68/s  (0.136s,  941.15/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1319.365s\n",
      "Train: 2 [9750/10009 ( 97%)]  Loss: 4.53 (4.50)  Time: 0.120s, 1065.22/s  (0.136s,  941.38/s)  LR: 4.784e-05  Data: 0.021 (0.027)Time: 1325.846s\n",
      "Train: 2 [9800/10009 ( 98%)]  Loss: 4.62 (4.50)  Time: 0.104s, 1226.98/s  (0.136s,  941.69/s)  LR: 4.784e-05  Data: 0.006 (0.027)Time: 1332.204s\n",
      "Train: 2 [9850/10009 ( 98%)]  Loss: 4.26 (4.50)  Time: 0.118s, 1082.59/s  (0.136s,  942.07/s)  LR: 4.784e-05  Data: 0.007 (0.027)Time: 1338.466s\n",
      "Train: 2 [9900/10009 ( 99%)]  Loss: 4.62 (4.50)  Time: 0.130s,  984.39/s  (0.136s,  942.36/s)  LR: 4.784e-05  Data: 0.008 (0.027)Time: 1344.836s\n",
      "Train: 2 [9950/10009 ( 99%)]  Loss: 4.31 (4.50)  Time: 0.131s,  975.16/s  (0.136s,  942.46/s)  LR: 4.784e-05  Data: 0.008 (0.027)Time: 1351.487s\n",
      "Train: 2 [10000/10009 (100%)]  Loss: 4.30 (4.50)  Time: 0.202s,  632.53/s  (0.136s,  942.07/s)  LR: 4.784e-05  Data: 0.104 (0.027)Time: 1358.837s\n",
      "Test: [   0/390]  Time: 0.686 (0.686)  Loss:   2.478 ( 2.478)  Acc@1:  50.000 ( 50.000)  Acc@5:  75.781 ( 75.781)\n",
      "Test: [  50/390]  Time: 0.554 (0.151)  Loss:   2.600 ( 3.696)  Acc@1:  43.750 ( 28.232)  Acc@5:  67.188 ( 50.567)\n",
      "Test: [ 100/390]  Time: 0.031 (0.140)  Loss:   3.806 ( 3.800)  Acc@1:  14.062 ( 24.745)  Acc@5:  51.562 ( 47.811)\n",
      "Test: [ 150/390]  Time: 0.461 (0.141)  Loss:   3.172 ( 3.709)  Acc@1:  28.906 ( 25.880)  Acc@5:  64.844 ( 49.472)\n",
      "Test: [ 200/390]  Time: 0.029 (0.139)  Loss:   4.557 ( 3.854)  Acc@1:   8.594 ( 24.098)  Acc@5:  31.250 ( 47.062)\n",
      "Test: [ 250/390]  Time: 0.354 (0.137)  Loss:   4.781 ( 3.964)  Acc@1:  17.969 ( 22.809)  Acc@5:  32.812 ( 45.045)\n",
      "Test: [ 300/390]  Time: 0.029 (0.138)  Loss:   4.238 ( 4.041)  Acc@1:  25.000 ( 21.784)  Acc@5:  42.969 ( 43.670)\n",
      "Test: [ 350/390]  Time: 0.377 (0.137)  Loss:   4.023 ( 4.104)  Acc@1:  17.188 ( 21.018)  Acc@5:  46.875 ( 42.541)\n",
      "Test: [ 390/390]  Time: 0.021 (0.136)  Loss:   5.177 ( 4.069)  Acc@1:   8.750 ( 21.734)  Acc@5:  20.000 ( 43.228)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-2.pth.tar', 21.734)\n",
      " ('./output/budgeted/checkpoint-1.pth.tar', 17.546)\n",
      " ('./output/budgeted/checkpoint-0.pth.tar', 9.416)\n",
      "\n",
      "Train: 3 [   0/10009 (  0%)]  Loss: 4.28 (4.28)  Time: 0.669s,  191.31/s  (0.669s,  191.31/s)  LR: 4.523e-05  Data: 0.566 (0.566)Time: 0.670s\n",
      "Train: 3 [  50/10009 (  0%)]  Loss: 4.37 (4.29)  Time: 0.103s, 1248.08/s  (0.146s,  875.81/s)  LR: 4.523e-05  Data: 0.005 (0.045)Time: 7.454s\n",
      "Train: 3 [ 100/10009 (  1%)]  Loss: 4.40 (4.30)  Time: 0.104s, 1234.26/s  (0.139s,  923.02/s)  LR: 4.523e-05  Data: 0.005 (0.038)Time: 14.007s\n",
      "Train: 3 [ 150/10009 (  1%)]  Loss: 4.33 (4.30)  Time: 0.104s, 1230.97/s  (0.136s,  942.12/s)  LR: 4.523e-05  Data: 0.005 (0.035)Time: 20.516s\n",
      "Train: 3 [ 200/10009 (  2%)]  Loss: 4.46 (4.32)  Time: 0.274s,  467.76/s  (0.135s,  946.09/s)  LR: 4.523e-05  Data: 0.175 (0.035)Time: 27.194s\n",
      "Train: 3 [ 250/10009 (  2%)]  Loss: 4.09 (4.31)  Time: 0.103s, 1244.22/s  (0.135s,  947.26/s)  LR: 4.523e-05  Data: 0.005 (0.034)Time: 33.917s\n",
      "Train: 3 [ 300/10009 (  3%)]  Loss: 4.53 (4.31)  Time: 0.108s, 1185.07/s  (0.136s,  943.03/s)  LR: 4.523e-05  Data: 0.007 (0.035)Time: 40.856s\n",
      "Train: 3 [ 350/10009 (  3%)]  Loss: 4.41 (4.31)  Time: 0.131s,  977.47/s  (0.136s,  942.92/s)  LR: 4.523e-05  Data: 0.009 (0.033)Time: 47.648s\n",
      "Train: 3 [ 400/10009 (  4%)]  Loss: 4.21 (4.31)  Time: 0.122s, 1049.20/s  (0.135s,  946.20/s)  LR: 4.523e-05  Data: 0.005 (0.032)Time: 54.247s\n",
      "Train: 3 [ 450/10009 (  4%)]  Loss: 4.40 (4.31)  Time: 0.126s, 1019.36/s  (0.134s,  954.92/s)  LR: 4.523e-05  Data: 0.008 (0.030)Time: 60.453s\n",
      "Train: 3 [ 500/10009 (  5%)]  Loss: 4.41 (4.31)  Time: 0.103s, 1240.84/s  (0.134s,  958.49/s)  LR: 4.523e-05  Data: 0.005 (0.029)Time: 66.906s\n",
      "Train: 3 [ 550/10009 (  5%)]  Loss: 4.41 (4.31)  Time: 0.102s, 1255.43/s  (0.136s,  940.00/s)  LR: 4.523e-05  Data: 0.005 (0.032)Time: 75.030s\n",
      "Train: 3 [ 600/10009 (  6%)]  Loss: 4.24 (4.31)  Time: 0.104s, 1232.11/s  (0.136s,  939.56/s)  LR: 4.523e-05  Data: 0.005 (0.033)Time: 81.877s\n",
      "Train: 3 [ 650/10009 (  6%)]  Loss: 4.53 (4.31)  Time: 0.137s,  934.19/s  (0.137s,  935.46/s)  LR: 4.523e-05  Data: 0.008 (0.033)Time: 89.077s\n",
      "Train: 3 [ 700/10009 (  7%)]  Loss: 4.36 (4.31)  Time: 0.105s, 1218.10/s  (0.136s,  939.48/s)  LR: 4.523e-05  Data: 0.006 (0.032)Time: 95.508s\n",
      "Train: 3 [ 750/10009 (  7%)]  Loss: 4.14 (4.31)  Time: 0.302s,  423.14/s  (0.136s,  943.40/s)  LR: 4.523e-05  Data: 0.194 (0.032)Time: 101.896s\n",
      "Train: 3 [ 800/10009 (  8%)]  Loss: 4.42 (4.31)  Time: 0.342s,  374.10/s  (0.135s,  945.29/s)  LR: 4.523e-05  Data: 0.243 (0.031)Time: 108.462s\n",
      "Train: 3 [ 850/10009 (  8%)]  Loss: 4.38 (4.31)  Time: 0.102s, 1250.82/s  (0.135s,  948.38/s)  LR: 4.523e-05  Data: 0.005 (0.031)Time: 114.857s\n",
      "Train: 3 [ 900/10009 (  9%)]  Loss: 4.15 (4.31)  Time: 0.157s,  814.38/s  (0.135s,  948.22/s)  LR: 4.523e-05  Data: 0.059 (0.031)Time: 121.626s\n",
      "Train: 3 [ 950/10009 (  9%)]  Loss: 4.44 (4.31)  Time: 0.178s,  720.91/s  (0.135s,  950.61/s)  LR: 4.523e-05  Data: 0.079 (0.031)Time: 128.052s\n",
      "Train: 3 [1000/10009 ( 10%)]  Loss: 4.21 (4.31)  Time: 0.169s,  755.61/s  (0.134s,  953.09/s)  LR: 4.523e-05  Data: 0.070 (0.030)Time: 134.434s\n",
      "Train: 3 [1050/10009 ( 10%)]  Loss: 4.50 (4.31)  Time: 0.104s, 1233.30/s  (0.134s,  956.33/s)  LR: 4.523e-05  Data: 0.005 (0.030)Time: 140.671s\n",
      "Train: 3 [1100/10009 ( 11%)]  Loss: 4.27 (4.31)  Time: 0.103s, 1242.62/s  (0.134s,  955.46/s)  LR: 4.523e-05  Data: 0.005 (0.030)Time: 147.497s\n",
      "Train: 3 [1150/10009 ( 11%)]  Loss: 4.11 (4.31)  Time: 0.103s, 1241.28/s  (0.134s,  955.94/s)  LR: 4.523e-05  Data: 0.005 (0.030)Time: 154.118s\n",
      "Train: 3 [1200/10009 ( 12%)]  Loss: 4.54 (4.31)  Time: 0.106s, 1202.50/s  (0.134s,  955.59/s)  LR: 4.523e-05  Data: 0.005 (0.030)Time: 160.873s\n",
      "Train: 3 [1250/10009 ( 12%)]  Loss: 4.12 (4.31)  Time: 0.142s,  901.29/s  (0.134s,  955.50/s)  LR: 4.523e-05  Data: 0.045 (0.030)Time: 167.586s\n",
      "Train: 3 [1300/10009 ( 13%)]  Loss: 3.97 (4.31)  Time: 0.105s, 1215.36/s  (0.134s,  957.53/s)  LR: 4.523e-05  Data: 0.007 (0.029)Time: 173.914s\n",
      "Train: 3 [1350/10009 ( 13%)]  Loss: 4.34 (4.31)  Time: 0.133s,  963.53/s  (0.133s,  960.45/s)  LR: 4.523e-05  Data: 0.007 (0.029)Time: 180.049s\n",
      "Train: 3 [1400/10009 ( 14%)]  Loss: 4.46 (4.31)  Time: 0.104s, 1230.68/s  (0.133s,  961.54/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 186.500s\n",
      "Train: 3 [1450/10009 ( 14%)]  Loss: 4.29 (4.31)  Time: 0.131s,  980.26/s  (0.133s,  962.49/s)  LR: 4.523e-05  Data: 0.009 (0.027)Time: 192.966s\n",
      "Train: 3 [1500/10009 ( 15%)]  Loss: 4.52 (4.31)  Time: 0.104s, 1225.37/s  (0.133s,  962.11/s)  LR: 4.523e-05  Data: 0.006 (0.028)Time: 199.694s\n",
      "Train: 3 [1550/10009 ( 15%)]  Loss: 4.29 (4.31)  Time: 0.106s, 1210.65/s  (0.133s,  962.59/s)  LR: 4.523e-05  Data: 0.006 (0.028)Time: 206.243s\n",
      "Train: 3 [1600/10009 ( 16%)]  Loss: 4.17 (4.31)  Time: 0.101s, 1263.15/s  (0.133s,  961.05/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 213.233s\n",
      "Train: 3 [1650/10009 ( 16%)]  Loss: 4.35 (4.31)  Time: 0.180s,  713.00/s  (0.133s,  958.89/s)  LR: 4.523e-05  Data: 0.081 (0.028)Time: 220.388s\n",
      "Train: 3 [1700/10009 ( 17%)]  Loss: 4.33 (4.31)  Time: 0.125s, 1021.91/s  (0.134s,  957.15/s)  LR: 4.523e-05  Data: 0.008 (0.028)Time: 227.474s\n",
      "Train: 3 [1750/10009 ( 17%)]  Loss: 4.16 (4.31)  Time: 0.102s, 1257.97/s  (0.134s,  957.99/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 233.956s\n",
      "Train: 3 [1800/10009 ( 18%)]  Loss: 4.34 (4.31)  Time: 0.229s,  557.85/s  (0.134s,  958.08/s)  LR: 4.523e-05  Data: 0.131 (0.028)Time: 240.615s\n",
      "Train: 3 [1850/10009 ( 18%)]  Loss: 4.36 (4.31)  Time: 0.103s, 1240.92/s  (0.134s,  958.78/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 247.113s\n",
      "Train: 3 [1900/10009 ( 19%)]  Loss: 4.16 (4.31)  Time: 0.164s,  781.73/s  (0.134s,  957.02/s)  LR: 4.523e-05  Data: 0.047 (0.029)Time: 254.257s\n",
      "Train: 3 [1950/10009 ( 19%)]  Loss: 4.24 (4.31)  Time: 0.105s, 1222.76/s  (0.133s,  959.09/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 260.381s\n",
      "Train: 3 [2000/10009 ( 20%)]  Loss: 4.33 (4.31)  Time: 0.102s, 1251.11/s  (0.134s,  958.45/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 267.230s\n",
      "Train: 3 [2050/10009 ( 20%)]  Loss: 4.59 (4.31)  Time: 0.106s, 1206.25/s  (0.133s,  958.95/s)  LR: 4.523e-05  Data: 0.007 (0.028)Time: 273.766s\n",
      "Train: 3 [2100/10009 ( 21%)]  Loss: 4.29 (4.31)  Time: 0.129s,  993.84/s  (0.133s,  959.41/s)  LR: 4.523e-05  Data: 0.007 (0.028)Time: 280.306s\n",
      "Train: 3 [2150/10009 ( 21%)]  Loss: 4.49 (4.31)  Time: 0.110s, 1168.27/s  (0.133s,  958.86/s)  LR: 4.523e-05  Data: 0.007 (0.028)Time: 287.142s\n",
      "Train: 3 [2200/10009 ( 22%)]  Loss: 4.34 (4.31)  Time: 0.137s,  935.53/s  (0.133s,  959.65/s)  LR: 4.523e-05  Data: 0.037 (0.028)Time: 293.574s\n",
      "Train: 3 [2250/10009 ( 22%)]  Loss: 4.38 (4.31)  Time: 0.241s,  532.15/s  (0.133s,  959.29/s)  LR: 4.523e-05  Data: 0.143 (0.029)Time: 300.354s\n",
      "Train: 3 [2300/10009 ( 23%)]  Loss: 4.42 (4.31)  Time: 0.103s, 1243.95/s  (0.133s,  959.91/s)  LR: 4.523e-05  Data: 0.005 (0.029)Time: 306.828s\n",
      "Train: 3 [2350/10009 ( 23%)]  Loss: 4.07 (4.31)  Time: 0.105s, 1220.46/s  (0.133s,  960.58/s)  LR: 4.523e-05  Data: 0.006 (0.028)Time: 313.278s\n",
      "Train: 3 [2400/10009 ( 24%)]  Loss: 4.22 (4.30)  Time: 0.157s,  817.15/s  (0.133s,  960.80/s)  LR: 4.523e-05  Data: 0.015 (0.028)Time: 319.865s\n",
      "Train: 3 [2450/10009 ( 24%)]  Loss: 4.19 (4.30)  Time: 0.133s,  963.26/s  (0.133s,  962.32/s)  LR: 4.523e-05  Data: 0.009 (0.028)Time: 326.013s\n",
      "Train: 3 [2500/10009 ( 25%)]  Loss: 4.69 (4.30)  Time: 0.104s, 1232.37/s  (0.133s,  964.15/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 332.029s\n",
      "Train: 3 [2550/10009 ( 25%)]  Loss: 4.39 (4.30)  Time: 0.114s, 1121.32/s  (0.133s,  961.46/s)  LR: 4.523e-05  Data: 0.007 (0.028)Time: 339.616s\n",
      "Train: 3 [2600/10009 ( 26%)]  Loss: 4.04 (4.30)  Time: 0.114s, 1124.26/s  (0.133s,  961.77/s)  LR: 4.523e-05  Data: 0.006 (0.028)Time: 346.161s\n",
      "Train: 3 [2650/10009 ( 26%)]  Loss: 4.34 (4.30)  Time: 0.120s, 1069.21/s  (0.133s,  961.62/s)  LR: 4.523e-05  Data: 0.006 (0.028)Time: 352.872s\n",
      "Train: 3 [2700/10009 ( 27%)]  Loss: 4.33 (4.30)  Time: 0.103s, 1244.14/s  (0.133s,  962.20/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 359.308s\n",
      "Train: 3 [2750/10009 ( 27%)]  Loss: 4.76 (4.30)  Time: 0.103s, 1241.28/s  (0.133s,  963.57/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 365.439s\n",
      "Train: 3 [2800/10009 ( 28%)]  Loss: 4.33 (4.30)  Time: 0.104s, 1227.50/s  (0.133s,  963.63/s)  LR: 4.523e-05  Data: 0.005 (0.028)Time: 372.059s\n",
      "Train: 3 [2850/10009 ( 28%)]  Loss: 4.25 (4.30)  Time: 0.106s, 1211.98/s  (0.133s,  964.58/s)  LR: 4.523e-05  Data: 0.006 (0.027)Time: 378.326s\n",
      "Train: 3 [2900/10009 ( 29%)]  Loss: 4.20 (4.30)  Time: 0.111s, 1152.86/s  (0.133s,  964.48/s)  LR: 4.523e-05  Data: 0.006 (0.027)Time: 385.004s\n",
      "Train: 3 [2950/10009 ( 29%)]  Loss: 4.52 (4.30)  Time: 0.243s,  526.22/s  (0.133s,  964.85/s)  LR: 4.523e-05  Data: 0.146 (0.027)Time: 391.489s\n",
      "Train: 3 [3000/10009 ( 30%)]  Loss: 4.27 (4.30)  Time: 0.103s, 1237.62/s  (0.133s,  964.81/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 398.137s\n",
      "Train: 3 [3050/10009 ( 30%)]  Loss: 4.08 (4.30)  Time: 0.104s, 1235.00/s  (0.133s,  964.60/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 404.860s\n",
      "Train: 3 [3100/10009 ( 31%)]  Loss: 4.79 (4.30)  Time: 0.104s, 1235.04/s  (0.133s,  965.21/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 411.236s\n",
      "Train: 3 [3150/10009 ( 31%)]  Loss: 4.54 (4.30)  Time: 0.104s, 1234.28/s  (0.132s,  966.72/s)  LR: 4.523e-05  Data: 0.006 (0.027)Time: 417.211s\n",
      "Train: 3 [3200/10009 ( 32%)]  Loss: 3.98 (4.30)  Time: 0.104s, 1232.50/s  (0.132s,  967.53/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 423.477s\n",
      "Train: 3 [3250/10009 ( 32%)]  Loss: 4.59 (4.30)  Time: 0.103s, 1245.17/s  (0.132s,  968.89/s)  LR: 4.523e-05  Data: 0.006 (0.027)Time: 429.487s\n",
      "Train: 3 [3300/10009 ( 33%)]  Loss: 4.16 (4.30)  Time: 0.104s, 1233.63/s  (0.132s,  970.06/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 435.568s\n",
      "Train: 3 [3350/10009 ( 33%)]  Loss: 4.24 (4.30)  Time: 0.105s, 1221.48/s  (0.132s,  970.46/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 441.981s\n",
      "Train: 3 [3400/10009 ( 34%)]  Loss: 4.37 (4.30)  Time: 0.106s, 1204.08/s  (0.132s,  971.62/s)  LR: 4.523e-05  Data: 0.006 (0.027)Time: 448.042s\n",
      "Train: 3 [3450/10009 ( 34%)]  Loss: 4.13 (4.30)  Time: 0.103s, 1241.81/s  (0.132s,  972.37/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 454.277s\n",
      "Train: 3 [3500/10009 ( 35%)]  Loss: 3.90 (4.30)  Time: 0.180s,  712.62/s  (0.131s,  973.41/s)  LR: 4.523e-05  Data: 0.074 (0.027)Time: 460.367s\n",
      "Train: 3 [3550/10009 ( 35%)]  Loss: 3.91 (4.30)  Time: 0.106s, 1202.23/s  (0.131s,  974.52/s)  LR: 4.523e-05  Data: 0.006 (0.027)Time: 466.412s\n",
      "Train: 3 [3600/10009 ( 36%)]  Loss: 4.01 (4.30)  Time: 0.190s,  675.34/s  (0.131s,  975.05/s)  LR: 4.523e-05  Data: 0.092 (0.027)Time: 472.723s\n",
      "Train: 3 [3650/10009 ( 36%)]  Loss: 4.32 (4.30)  Time: 0.112s, 1144.37/s  (0.131s,  975.40/s)  LR: 4.523e-05  Data: 0.007 (0.027)Time: 479.112s\n",
      "Train: 3 [3700/10009 ( 37%)]  Loss: 4.31 (4.30)  Time: 0.137s,  935.61/s  (0.131s,  976.36/s)  LR: 4.523e-05  Data: 0.039 (0.027)Time: 485.197s\n",
      "Train: 3 [3750/10009 ( 37%)]  Loss: 4.23 (4.30)  Time: 0.104s, 1235.90/s  (0.131s,  977.11/s)  LR: 4.523e-05  Data: 0.005 (0.027)Time: 491.376s\n",
      "Train: 3 [3800/10009 ( 38%)]  Loss: 4.43 (4.30)  Time: 0.132s,  969.03/s  (0.131s,  978.20/s)  LR: 4.523e-05  Data: 0.033 (0.026)Time: 497.370s\n",
      "Train: 3 [3850/10009 ( 38%)]  Loss: 4.44 (4.30)  Time: 0.104s, 1228.79/s  (0.131s,  978.83/s)  LR: 4.523e-05  Data: 0.006 (0.026)Time: 503.586s\n",
      "Train: 3 [3900/10009 ( 39%)]  Loss: 4.22 (4.30)  Time: 0.207s,  619.47/s  (0.131s,  979.32/s)  LR: 4.523e-05  Data: 0.101 (0.026)Time: 509.871s\n",
      "Train: 3 [3950/10009 ( 39%)]  Loss: 4.47 (4.30)  Time: 0.104s, 1233.58/s  (0.131s,  980.16/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 515.964s\n",
      "Train: 3 [4000/10009 ( 40%)]  Loss: 4.56 (4.30)  Time: 0.174s,  735.35/s  (0.130s,  980.86/s)  LR: 4.523e-05  Data: 0.076 (0.026)Time: 522.122s\n",
      "Train: 3 [4050/10009 ( 40%)]  Loss: 4.26 (4.29)  Time: 0.104s, 1230.40/s  (0.130s,  981.43/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 528.339s\n",
      "Train: 3 [4100/10009 ( 41%)]  Loss: 4.05 (4.29)  Time: 0.177s,  722.58/s  (0.130s,  981.42/s)  LR: 4.523e-05  Data: 0.079 (0.026)Time: 534.864s\n",
      "Train: 3 [4150/10009 ( 41%)]  Loss: 4.57 (4.29)  Time: 0.104s, 1226.60/s  (0.130s,  982.37/s)  LR: 4.523e-05  Data: 0.006 (0.026)Time: 540.863s\n",
      "Train: 3 [4200/10009 ( 42%)]  Loss: 4.28 (4.29)  Time: 0.146s,  877.37/s  (0.130s,  982.95/s)  LR: 4.523e-05  Data: 0.047 (0.026)Time: 547.054s\n",
      "Train: 3 [4250/10009 ( 42%)]  Loss: 4.30 (4.29)  Time: 0.106s, 1208.35/s  (0.130s,  983.96/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 552.999s\n",
      "Train: 3 [4300/10009 ( 43%)]  Loss: 4.29 (4.29)  Time: 0.105s, 1223.62/s  (0.130s,  984.64/s)  LR: 4.523e-05  Data: 0.007 (0.026)Time: 559.115s\n",
      "Train: 3 [4350/10009 ( 43%)]  Loss: 4.52 (4.29)  Time: 0.105s, 1219.11/s  (0.130s,  984.72/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 565.571s\n",
      "Train: 3 [4400/10009 ( 44%)]  Loss: 4.39 (4.29)  Time: 0.107s, 1201.09/s  (0.130s,  985.40/s)  LR: 4.523e-05  Data: 0.006 (0.026)Time: 571.671s\n",
      "Train: 3 [4450/10009 ( 44%)]  Loss: 4.29 (4.29)  Time: 0.103s, 1236.72/s  (0.130s,  985.99/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 577.820s\n",
      "Train: 3 [4500/10009 ( 45%)]  Loss: 3.93 (4.29)  Time: 0.103s, 1237.23/s  (0.130s,  986.77/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 583.851s\n",
      "Train: 3 [4550/10009 ( 45%)]  Loss: 4.52 (4.29)  Time: 0.105s, 1222.17/s  (0.130s,  987.28/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 590.034s\n",
      "Train: 3 [4600/10009 ( 46%)]  Loss: 4.26 (4.29)  Time: 0.104s, 1232.03/s  (0.130s,  987.75/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 596.230s\n",
      "Train: 3 [4650/10009 ( 46%)]  Loss: 4.18 (4.29)  Time: 0.191s,  670.93/s  (0.130s,  987.75/s)  LR: 4.523e-05  Data: 0.093 (0.026)Time: 602.708s\n",
      "Train: 3 [4700/10009 ( 47%)]  Loss: 4.56 (4.29)  Time: 0.102s, 1251.03/s  (0.130s,  988.17/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 608.929s\n",
      "Train: 3 [4750/10009 ( 47%)]  Loss: 4.16 (4.29)  Time: 0.171s,  748.60/s  (0.129s,  988.92/s)  LR: 4.523e-05  Data: 0.070 (0.026)Time: 614.941s\n",
      "Train: 3 [4800/10009 ( 48%)]  Loss: 4.38 (4.29)  Time: 0.104s, 1235.01/s  (0.129s,  989.48/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 621.060s\n",
      "Train: 3 [4850/10009 ( 48%)]  Loss: 4.38 (4.29)  Time: 0.187s,  685.16/s  (0.129s,  989.91/s)  LR: 4.523e-05  Data: 0.088 (0.026)Time: 627.254s\n",
      "Train: 3 [4900/10009 ( 49%)]  Loss: 4.48 (4.29)  Time: 0.105s, 1222.19/s  (0.129s,  990.28/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 633.485s\n",
      "Train: 3 [4950/10009 ( 49%)]  Loss: 4.46 (4.29)  Time: 0.198s,  646.54/s  (0.129s,  990.53/s)  LR: 4.523e-05  Data: 0.100 (0.026)Time: 639.783s\n",
      "Train: 3 [5000/10009 ( 50%)]  Loss: 4.61 (4.29)  Time: 0.105s, 1217.95/s  (0.129s,  991.13/s)  LR: 4.523e-05  Data: 0.006 (0.026)Time: 645.855s\n",
      "Train: 3 [5050/10009 ( 50%)]  Loss: 4.27 (4.29)  Time: 0.175s,  732.74/s  (0.129s,  991.74/s)  LR: 4.523e-05  Data: 0.077 (0.026)Time: 651.911s\n",
      "Train: 3 [5100/10009 ( 51%)]  Loss: 4.11 (4.29)  Time: 0.103s, 1238.86/s  (0.129s,  992.24/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 658.033s\n",
      "Train: 3 [5150/10009 ( 51%)]  Loss: 4.20 (4.29)  Time: 0.244s,  525.34/s  (0.129s,  992.49/s)  LR: 4.523e-05  Data: 0.146 (0.026)Time: 664.318s\n",
      "Train: 3 [5200/10009 ( 52%)]  Loss: 3.84 (4.29)  Time: 0.104s, 1230.88/s  (0.129s,  992.77/s)  LR: 4.523e-05  Data: 0.006 (0.026)Time: 670.573s\n",
      "Train: 3 [5250/10009 ( 52%)]  Loss: 4.16 (4.29)  Time: 0.113s, 1133.93/s  (0.129s,  992.63/s)  LR: 4.523e-05  Data: 0.007 (0.026)Time: 677.114s\n",
      "Train: 3 [5300/10009 ( 53%)]  Loss: 4.24 (4.29)  Time: 0.124s, 1028.19/s  (0.129s,  991.98/s)  LR: 4.523e-05  Data: 0.007 (0.026)Time: 684.013s\n",
      "Train: 3 [5350/10009 ( 53%)]  Loss: 4.20 (4.29)  Time: 0.110s, 1167.73/s  (0.129s,  991.47/s)  LR: 4.523e-05  Data: 0.005 (0.026)Time: 690.818s\n",
      "Train: 3 [5400/10009 ( 54%)]  Loss: 4.19 (4.29)  Time: 0.191s,  670.01/s  (0.129s,  990.88/s)  LR: 4.523e-05  Data: 0.081 (0.026)Time: 697.686s\n",
      "Train: 3 [5450/10009 ( 54%)]  Loss: 4.19 (4.29)  Time: 0.121s, 1061.65/s  (0.129s,  990.18/s)  LR: 4.523e-05  Data: 0.006 (0.026)Time: 704.646s\n",
      "Train: 3 [5500/10009 ( 55%)]  Loss: 4.17 (4.29)  Time: 0.183s,  700.57/s  (0.129s,  989.97/s)  LR: 4.523e-05  Data: 0.078 (0.025)Time: 711.257s\n",
      "Train: 3 [5550/10009 ( 55%)]  Loss: 4.16 (4.29)  Time: 0.109s, 1175.40/s  (0.129s,  990.13/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 717.608s\n",
      "Train: 3 [5600/10009 ( 56%)]  Loss: 3.92 (4.29)  Time: 0.125s, 1021.73/s  (0.129s,  990.25/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 723.986s\n",
      "Train: 3 [5650/10009 ( 56%)]  Loss: 4.51 (4.29)  Time: 0.107s, 1197.10/s  (0.129s,  990.05/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 730.596s\n",
      "Train: 3 [5700/10009 ( 57%)]  Loss: 4.12 (4.29)  Time: 0.108s, 1184.87/s  (0.129s,  990.27/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 736.898s\n",
      "Train: 3 [5750/10009 ( 57%)]  Loss: 4.39 (4.29)  Time: 0.108s, 1181.35/s  (0.129s,  990.71/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 743.032s\n",
      "Train: 3 [5800/10009 ( 58%)]  Loss: 4.35 (4.29)  Time: 0.173s,  739.75/s  (0.129s,  990.53/s)  LR: 4.523e-05  Data: 0.061 (0.025)Time: 749.622s\n",
      "Train: 3 [5850/10009 ( 58%)]  Loss: 4.57 (4.29)  Time: 0.113s, 1133.69/s  (0.129s,  990.69/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 755.963s\n",
      "Train: 3 [5900/10009 ( 59%)]  Loss: 4.21 (4.29)  Time: 0.108s, 1180.23/s  (0.129s,  990.98/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 762.202s\n",
      "Train: 3 [5950/10009 ( 59%)]  Loss: 4.26 (4.28)  Time: 0.108s, 1185.55/s  (0.129s,  991.24/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 768.454s\n",
      "Train: 3 [6000/10009 ( 60%)]  Loss: 4.16 (4.28)  Time: 0.108s, 1185.94/s  (0.129s,  991.31/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 774.859s\n",
      "Train: 3 [6050/10009 ( 60%)]  Loss: 4.39 (4.28)  Time: 0.183s,  700.71/s  (0.129s,  991.45/s)  LR: 4.523e-05  Data: 0.019 (0.025)Time: 781.207s\n",
      "Train: 3 [6100/10009 ( 61%)]  Loss: 4.39 (4.28)  Time: 0.109s, 1169.65/s  (0.129s,  991.64/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 787.506s\n",
      "Train: 3 [6150/10009 ( 61%)]  Loss: 4.30 (4.28)  Time: 0.125s, 1023.89/s  (0.129s,  991.96/s)  LR: 4.523e-05  Data: 0.023 (0.025)Time: 793.706s\n",
      "Train: 3 [6200/10009 ( 62%)]  Loss: 4.16 (4.28)  Time: 0.107s, 1195.85/s  (0.129s,  992.30/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 799.883s\n",
      "Train: 3 [6250/10009 ( 62%)]  Loss: 4.08 (4.28)  Time: 0.159s,  806.80/s  (0.129s,  992.46/s)  LR: 4.523e-05  Data: 0.054 (0.025)Time: 806.202s\n",
      "Train: 3 [6300/10009 ( 63%)]  Loss: 4.26 (4.28)  Time: 0.105s, 1213.74/s  (0.129s,  992.69/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 812.467s\n",
      "Train: 3 [6350/10009 ( 63%)]  Loss: 4.29 (4.28)  Time: 0.149s,  856.40/s  (0.129s,  992.79/s)  LR: 4.523e-05  Data: 0.048 (0.025)Time: 818.830s\n",
      "Train: 3 [6400/10009 ( 64%)]  Loss: 3.97 (4.28)  Time: 0.109s, 1176.03/s  (0.129s,  993.16/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 824.966s\n",
      "Train: 3 [6450/10009 ( 64%)]  Loss: 4.24 (4.28)  Time: 0.121s, 1056.16/s  (0.129s,  993.38/s)  LR: 4.523e-05  Data: 0.017 (0.025)Time: 831.230s\n",
      "Train: 3 [6500/10009 ( 65%)]  Loss: 4.28 (4.28)  Time: 0.109s, 1177.33/s  (0.129s,  993.65/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 837.447s\n",
      "Train: 3 [6550/10009 ( 65%)]  Loss: 4.23 (4.28)  Time: 0.112s, 1139.59/s  (0.129s,  993.92/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 843.652s\n",
      "Train: 3 [6600/10009 ( 66%)]  Loss: 4.21 (4.28)  Time: 0.168s,  761.51/s  (0.129s,  994.05/s)  LR: 4.523e-05  Data: 0.060 (0.025)Time: 849.986s\n",
      "Train: 3 [6650/10009 ( 66%)]  Loss: 4.00 (4.28)  Time: 0.151s,  846.81/s  (0.129s,  994.27/s)  LR: 4.523e-05  Data: 0.047 (0.025)Time: 856.234s\n",
      "Train: 3 [6700/10009 ( 67%)]  Loss: 4.37 (4.28)  Time: 0.108s, 1180.95/s  (0.129s,  994.56/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 862.416s\n",
      "Train: 3 [6750/10009 ( 67%)]  Loss: 4.08 (4.28)  Time: 0.374s,  342.38/s  (0.129s,  993.78/s)  LR: 4.523e-05  Data: 0.247 (0.025)Time: 869.535s\n",
      "Train: 3 [6800/10009 ( 68%)]  Loss: 4.13 (4.28)  Time: 0.308s,  415.02/s  (0.129s,  988.57/s)  LR: 4.523e-05  Data: 0.206 (0.025)Time: 880.595s\n",
      "Train: 3 [6850/10009 ( 68%)]  Loss: 4.46 (4.28)  Time: 0.213s,  601.17/s  (0.130s,  987.60/s)  LR: 4.523e-05  Data: 0.111 (0.025)Time: 887.932s\n",
      "Train: 3 [6900/10009 ( 69%)]  Loss: 4.14 (4.28)  Time: 0.113s, 1133.36/s  (0.130s,  988.04/s)  LR: 4.523e-05  Data: 0.011 (0.025)Time: 894.019s\n",
      "Train: 3 [6950/10009 ( 69%)]  Loss: 4.33 (4.28)  Time: 0.201s,  636.39/s  (0.130s,  988.24/s)  LR: 4.523e-05  Data: 0.099 (0.025)Time: 900.315s\n",
      "Train: 3 [7000/10009 ( 70%)]  Loss: 4.25 (4.28)  Time: 0.109s, 1175.13/s  (0.130s,  988.33/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 906.705s\n",
      "Train: 3 [7050/10009 ( 70%)]  Loss: 4.31 (4.28)  Time: 0.109s, 1169.25/s  (0.129s,  988.55/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 912.974s\n",
      "Train: 3 [7100/10009 ( 71%)]  Loss: 4.17 (4.28)  Time: 0.107s, 1193.79/s  (0.129s,  988.81/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 919.208s\n",
      "Train: 3 [7150/10009 ( 71%)]  Loss: 4.08 (4.28)  Time: 0.108s, 1181.33/s  (0.129s,  988.84/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 925.653s\n",
      "Train: 3 [7200/10009 ( 72%)]  Loss: 4.27 (4.28)  Time: 0.109s, 1178.76/s  (0.129s,  989.05/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 931.927s\n",
      "Train: 3 [7250/10009 ( 72%)]  Loss: 4.24 (4.28)  Time: 0.108s, 1185.82/s  (0.129s,  989.35/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 938.115s\n",
      "Train: 3 [7300/10009 ( 73%)]  Loss: 4.05 (4.28)  Time: 0.123s, 1040.48/s  (0.129s,  989.57/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 944.374s\n",
      "Train: 3 [7350/10009 ( 73%)]  Loss: 4.07 (4.28)  Time: 0.109s, 1176.89/s  (0.129s,  989.62/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 950.791s\n",
      "Train: 3 [7400/10009 ( 74%)]  Loss: 4.45 (4.28)  Time: 0.124s, 1032.81/s  (0.129s,  989.70/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 957.186s\n",
      "Train: 3 [7450/10009 ( 74%)]  Loss: 4.32 (4.28)  Time: 0.108s, 1184.00/s  (0.129s,  989.50/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 963.841s\n",
      "Train: 3 [7500/10009 ( 75%)]  Loss: 4.02 (4.28)  Time: 0.107s, 1201.07/s  (0.129s,  989.68/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 970.135s\n",
      "Train: 3 [7550/10009 ( 75%)]  Loss: 3.90 (4.28)  Time: 0.109s, 1177.99/s  (0.129s,  989.81/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 976.474s\n",
      "Train: 3 [7600/10009 ( 76%)]  Loss: 4.33 (4.27)  Time: 0.108s, 1187.67/s  (0.129s,  989.98/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 982.767s\n",
      "Train: 3 [7650/10009 ( 76%)]  Loss: 4.21 (4.27)  Time: 0.110s, 1168.20/s  (0.129s,  989.93/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 989.290s\n",
      "Train: 3 [7700/10009 ( 77%)]  Loss: 4.10 (4.27)  Time: 0.113s, 1134.76/s  (0.129s,  989.90/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 995.781s\n",
      "Train: 3 [7750/10009 ( 77%)]  Loss: 4.14 (4.27)  Time: 0.108s, 1183.90/s  (0.129s,  990.05/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 1002.100s\n",
      "Train: 3 [7800/10009 ( 78%)]  Loss: 4.31 (4.27)  Time: 0.111s, 1154.92/s  (0.129s,  990.09/s)  LR: 4.523e-05  Data: 0.008 (0.025)Time: 1008.524s\n",
      "Train: 3 [7850/10009 ( 78%)]  Loss: 4.40 (4.27)  Time: 0.104s, 1225.25/s  (0.129s,  990.11/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1014.960s\n",
      "Train: 3 [7900/10009 ( 79%)]  Loss: 4.19 (4.27)  Time: 0.110s, 1162.79/s  (0.129s,  990.18/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1021.358s\n",
      "Train: 3 [7950/10009 ( 79%)]  Loss: 4.41 (4.27)  Time: 0.108s, 1184.73/s  (0.129s,  990.32/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 1027.670s\n",
      "Train: 3 [8000/10009 ( 80%)]  Loss: 4.15 (4.27)  Time: 0.108s, 1181.31/s  (0.129s,  990.21/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1034.245s\n",
      "Train: 3 [8050/10009 ( 80%)]  Loss: 4.15 (4.27)  Time: 0.108s, 1183.57/s  (0.129s,  990.22/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 1040.705s\n",
      "Train: 3 [8100/10009 ( 81%)]  Loss: 3.96 (4.27)  Time: 0.107s, 1199.35/s  (0.129s,  990.16/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1047.227s\n",
      "Train: 3 [8150/10009 ( 81%)]  Loss: 4.26 (4.27)  Time: 0.111s, 1148.80/s  (0.129s,  990.12/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1053.736s\n",
      "Train: 3 [8200/10009 ( 82%)]  Loss: 4.01 (4.27)  Time: 0.106s, 1205.30/s  (0.129s,  990.09/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1060.227s\n",
      "Train: 3 [8250/10009 ( 82%)]  Loss: 4.14 (4.27)  Time: 0.107s, 1200.63/s  (0.129s,  990.16/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1066.619s\n",
      "Train: 3 [8300/10009 ( 83%)]  Loss: 4.28 (4.27)  Time: 0.169s,  757.71/s  (0.129s,  990.16/s)  LR: 4.523e-05  Data: 0.035 (0.025)Time: 1073.078s\n",
      "Train: 3 [8350/10009 ( 83%)]  Loss: 4.29 (4.27)  Time: 0.140s,  917.44/s  (0.129s,  990.28/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 1079.412s\n",
      "Train: 3 [8400/10009 ( 84%)]  Loss: 4.22 (4.27)  Time: 0.108s, 1189.13/s  (0.129s,  990.24/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1085.923s\n",
      "Train: 3 [8450/10009 ( 84%)]  Loss: 4.33 (4.27)  Time: 0.108s, 1180.17/s  (0.129s,  990.31/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1092.309s\n",
      "Train: 3 [8500/10009 ( 85%)]  Loss: 4.17 (4.27)  Time: 0.107s, 1195.64/s  (0.129s,  990.31/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 1098.775s\n",
      "Train: 3 [8550/10009 ( 85%)]  Loss: 4.22 (4.27)  Time: 0.112s, 1142.72/s  (0.129s,  990.18/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1105.375s\n",
      "Train: 3 [8600/10009 ( 86%)]  Loss: 4.44 (4.27)  Time: 0.108s, 1189.73/s  (0.129s,  990.58/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1111.395s\n",
      "Train: 3 [8650/10009 ( 86%)]  Loss: 4.01 (4.27)  Time: 0.161s,  795.49/s  (0.129s,  990.55/s)  LR: 4.523e-05  Data: 0.056 (0.025)Time: 1117.893s\n",
      "Train: 3 [8700/10009 ( 87%)]  Loss: 4.17 (4.27)  Time: 0.107s, 1196.23/s  (0.129s,  990.81/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1124.053s\n",
      "Train: 3 [8750/10009 ( 87%)]  Loss: 4.09 (4.27)  Time: 0.130s,  985.53/s  (0.129s,  990.97/s)  LR: 4.523e-05  Data: 0.028 (0.025)Time: 1130.327s\n",
      "Train: 3 [8800/10009 ( 88%)]  Loss: 4.26 (4.27)  Time: 0.107s, 1194.22/s  (0.129s,  991.28/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 1136.431s\n",
      "Train: 3 [8850/10009 ( 88%)]  Loss: 4.18 (4.27)  Time: 0.109s, 1171.79/s  (0.129s,  991.24/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 1142.935s\n",
      "Train: 3 [8900/10009 ( 89%)]  Loss: 4.01 (4.27)  Time: 0.107s, 1197.15/s  (0.129s,  991.19/s)  LR: 4.523e-05  Data: 0.007 (0.025)Time: 1149.453s\n",
      "Train: 3 [8950/10009 ( 89%)]  Loss: 4.25 (4.27)  Time: 0.107s, 1197.13/s  (0.129s,  991.26/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1155.829s\n",
      "Train: 3 [9000/10009 ( 90%)]  Loss: 4.12 (4.27)  Time: 0.108s, 1186.40/s  (0.129s,  991.49/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1162.013s\n",
      "Train: 3 [9050/10009 ( 90%)]  Loss: 4.49 (4.27)  Time: 0.108s, 1182.09/s  (0.129s,  991.64/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1168.288s\n",
      "Train: 3 [9100/10009 ( 91%)]  Loss: 4.05 (4.26)  Time: 0.204s,  628.39/s  (0.129s,  991.61/s)  LR: 4.523e-05  Data: 0.102 (0.025)Time: 1174.781s\n",
      "Train: 3 [9150/10009 ( 91%)]  Loss: 4.22 (4.26)  Time: 0.106s, 1202.49/s  (0.129s,  991.66/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1181.178s\n",
      "Train: 3 [9200/10009 ( 92%)]  Loss: 4.28 (4.26)  Time: 0.107s, 1195.62/s  (0.129s,  991.76/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 1187.505s\n",
      "Train: 3 [9250/10009 ( 92%)]  Loss: 4.24 (4.26)  Time: 0.109s, 1178.61/s  (0.129s,  991.85/s)  LR: 4.523e-05  Data: 0.005 (0.025)Time: 1193.849s\n",
      "Train: 3 [9300/10009 ( 93%)]  Loss: 4.21 (4.26)  Time: 0.108s, 1182.92/s  (0.129s,  991.86/s)  LR: 4.523e-05  Data: 0.006 (0.025)Time: 1200.289s\n",
      "Train: 3 [9350/10009 ( 93%)]  Loss: 4.02 (4.26)  Time: 0.110s, 1159.43/s  (0.129s,  991.97/s)  LR: 4.523e-05  Data: 0.007 (0.024)Time: 1206.608s\n",
      "Train: 3 [9400/10009 ( 94%)]  Loss: 4.10 (4.26)  Time: 0.108s, 1181.96/s  (0.129s,  992.13/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1212.866s\n",
      "Train: 3 [9450/10009 ( 94%)]  Loss: 4.10 (4.26)  Time: 0.112s, 1143.67/s  (0.129s,  992.23/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1219.201s\n",
      "Train: 3 [9500/10009 ( 95%)]  Loss: 4.20 (4.26)  Time: 0.107s, 1193.69/s  (0.129s,  992.32/s)  LR: 4.523e-05  Data: 0.005 (0.024)Time: 1225.532s\n",
      "Train: 3 [9550/10009 ( 95%)]  Loss: 4.24 (4.26)  Time: 0.109s, 1177.92/s  (0.129s,  992.46/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1231.809s\n",
      "Train: 3 [9600/10009 ( 96%)]  Loss: 4.32 (4.26)  Time: 0.131s,  979.05/s  (0.129s,  992.40/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1238.331s\n",
      "Train: 3 [9650/10009 ( 96%)]  Loss: 4.42 (4.26)  Time: 0.110s, 1166.90/s  (0.129s,  992.38/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1244.812s\n",
      "Train: 3 [9700/10009 ( 97%)]  Loss: 4.32 (4.26)  Time: 0.108s, 1183.13/s  (0.129s,  992.32/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1251.334s\n",
      "Train: 3 [9750/10009 ( 97%)]  Loss: 4.22 (4.26)  Time: 0.109s, 1171.22/s  (0.129s,  992.53/s)  LR: 4.523e-05  Data: 0.007 (0.024)Time: 1257.522s\n",
      "Train: 3 [9800/10009 ( 98%)]  Loss: 4.34 (4.26)  Time: 0.146s,  877.05/s  (0.129s,  992.59/s)  LR: 4.523e-05  Data: 0.017 (0.024)Time: 1263.890s\n",
      "Train: 3 [9850/10009 ( 98%)]  Loss: 3.83 (4.26)  Time: 0.108s, 1185.57/s  (0.129s,  992.79/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1270.084s\n",
      "Train: 3 [9900/10009 ( 99%)]  Loss: 4.10 (4.26)  Time: 0.108s, 1182.00/s  (0.129s,  992.94/s)  LR: 4.523e-05  Data: 0.007 (0.024)Time: 1276.341s\n",
      "Train: 3 [9950/10009 ( 99%)]  Loss: 4.65 (4.26)  Time: 0.108s, 1180.15/s  (0.129s,  993.03/s)  LR: 4.523e-05  Data: 0.006 (0.024)Time: 1282.669s\n",
      "Train: 3 [10000/10009 (100%)]  Loss: 4.26 (4.26)  Time: 0.204s,  627.21/s  (0.129s,  993.09/s)  LR: 4.523e-05  Data: 0.095 (0.024)Time: 1289.031s\n",
      "Test: [   0/390]  Time: 0.697 (0.697)  Loss:   2.458 ( 2.458)  Acc@1:  52.344 ( 52.344)  Acc@5:  73.438 ( 73.438)\n",
      "Test: [  50/390]  Time: 0.032 (0.140)  Loss:   1.967 ( 3.487)  Acc@1:  58.594 ( 32.721)  Acc@5:  83.594 ( 55.193)\n",
      "Test: [ 100/390]  Time: 0.039 (0.132)  Loss:   3.761 ( 3.598)  Acc@1:  15.625 ( 28.226)  Acc@5:  50.781 ( 52.452)\n",
      "Test: [ 150/390]  Time: 0.031 (0.137)  Loss:   3.187 ( 3.536)  Acc@1:  25.000 ( 28.730)  Acc@5:  66.406 ( 53.487)\n",
      "Test: [ 200/390]  Time: 0.029 (0.133)  Loss:   4.616 ( 3.670)  Acc@1:  12.500 ( 26.978)  Acc@5:  28.125 ( 50.824)\n",
      "Test: [ 250/390]  Time: 0.038 (0.132)  Loss:   4.651 ( 3.782)  Acc@1:  17.188 ( 25.454)  Acc@5:  32.031 ( 48.503)\n",
      "Test: [ 300/390]  Time: 0.159 (0.131)  Loss:   3.891 ( 3.849)  Acc@1:  24.219 ( 24.660)  Acc@5:  45.312 ( 47.241)\n",
      "Test: [ 350/390]  Time: 0.033 (0.130)  Loss:   3.681 ( 3.895)  Acc@1:  24.219 ( 24.010)  Acc@5:  53.125 ( 46.483)\n",
      "Test: [ 390/390]  Time: 0.021 (0.130)  Loss:   4.863 ( 3.859)  Acc@1:   7.500 ( 24.724)  Acc@5:  23.750 ( 47.232)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-3.pth.tar', 24.724)\n",
      " ('./output/budgeted/checkpoint-2.pth.tar', 21.734)\n",
      " ('./output/budgeted/checkpoint-1.pth.tar', 17.546)\n",
      " ('./output/budgeted/checkpoint-0.pth.tar', 9.416)\n",
      "\n",
      "Train: 4 [   0/10009 (  0%)]  Loss: 4.29 (4.29)  Time: 0.678s,  188.81/s  (0.678s,  188.81/s)  LR: 4.173e-05  Data: 0.577 (0.577)Time: 0.678s\n",
      "Train: 4 [  50/10009 (  0%)]  Loss: 4.06 (4.13)  Time: 0.104s, 1233.29/s  (0.137s,  936.00/s)  LR: 4.173e-05  Data: 0.006 (0.037)Time: 6.975s\n",
      "Train: 4 [ 100/10009 (  1%)]  Loss: 3.90 (4.13)  Time: 0.106s, 1208.37/s  (0.131s,  979.60/s)  LR: 4.173e-05  Data: 0.006 (0.031)Time: 13.198s\n",
      "Train: 4 [ 150/10009 (  1%)]  Loss: 4.00 (4.13)  Time: 0.102s, 1249.00/s  (0.128s,  996.29/s)  LR: 4.173e-05  Data: 0.005 (0.029)Time: 19.400s\n",
      "Train: 4 [ 200/10009 (  2%)]  Loss: 4.43 (4.14)  Time: 0.152s,  843.11/s  (0.127s, 1006.78/s)  LR: 4.173e-05  Data: 0.051 (0.027)Time: 25.555s\n",
      "Train: 4 [ 250/10009 (  2%)]  Loss: 4.42 (4.14)  Time: 0.106s, 1212.92/s  (0.127s, 1011.58/s)  LR: 4.173e-05  Data: 0.006 (0.027)Time: 31.760s\n",
      "Train: 4 [ 300/10009 (  3%)]  Loss: 4.12 (4.13)  Time: 0.142s,  904.15/s  (0.127s, 1008.37/s)  LR: 4.173e-05  Data: 0.044 (0.027)Time: 38.208s\n",
      "Train: 4 [ 350/10009 (  3%)]  Loss: 4.16 (4.13)  Time: 0.113s, 1134.21/s  (0.128s, 1002.82/s)  LR: 4.173e-05  Data: 0.007 (0.026)Time: 44.802s\n",
      "Train: 4 [ 400/10009 (  4%)]  Loss: 3.98 (4.12)  Time: 0.174s,  736.95/s  (0.127s, 1005.83/s)  LR: 4.173e-05  Data: 0.072 (0.025)Time: 51.031s\n",
      "Train: 4 [ 450/10009 (  4%)]  Loss: 4.01 (4.12)  Time: 0.107s, 1198.19/s  (0.127s, 1004.88/s)  LR: 4.173e-05  Data: 0.005 (0.025)Time: 57.448s\n",
      "Train: 4 [ 500/10009 (  5%)]  Loss: 4.14 (4.12)  Time: 0.163s,  786.25/s  (0.127s, 1007.76/s)  LR: 4.173e-05  Data: 0.060 (0.024)Time: 63.634s\n",
      "Train: 4 [ 550/10009 (  5%)]  Loss: 4.19 (4.12)  Time: 0.109s, 1178.81/s  (0.127s, 1009.71/s)  LR: 4.173e-05  Data: 0.006 (0.024)Time: 69.850s\n",
      "Train: 4 [ 600/10009 (  6%)]  Loss: 3.97 (4.12)  Time: 0.113s, 1131.88/s  (0.127s, 1007.86/s)  LR: 4.173e-05  Data: 0.006 (0.024)Time: 76.329s\n",
      "Train: 4 [ 650/10009 (  6%)]  Loss: 3.83 (4.12)  Time: 0.108s, 1189.10/s  (0.127s, 1006.97/s)  LR: 4.173e-05  Data: 0.005 (0.024)Time: 82.751s\n",
      "Train: 4 [ 700/10009 (  7%)]  Loss: 4.39 (4.12)  Time: 0.111s, 1150.53/s  (0.127s, 1008.48/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 88.974s\n",
      "Train: 4 [ 750/10009 (  7%)]  Loss: 4.16 (4.12)  Time: 0.108s, 1190.24/s  (0.127s, 1008.83/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 95.287s\n",
      "Train: 4 [ 800/10009 (  8%)]  Loss: 4.07 (4.12)  Time: 0.107s, 1190.93/s  (0.127s, 1009.51/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 101.562s\n",
      "Train: 4 [ 850/10009 (  8%)]  Loss: 4.05 (4.12)  Time: 0.109s, 1169.69/s  (0.127s, 1010.36/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 107.812s\n",
      "Train: 4 [ 900/10009 (  9%)]  Loss: 4.26 (4.12)  Time: 0.109s, 1176.58/s  (0.127s, 1006.47/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 114.586s\n",
      "Train: 4 [ 950/10009 (  9%)]  Loss: 4.08 (4.12)  Time: 0.109s, 1174.12/s  (0.127s, 1007.26/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 120.851s\n",
      "Train: 4 [1000/10009 ( 10%)]  Loss: 4.39 (4.12)  Time: 0.109s, 1176.79/s  (0.127s, 1007.99/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 127.112s\n",
      "Train: 4 [1050/10009 ( 10%)]  Loss: 4.04 (4.12)  Time: 0.109s, 1177.85/s  (0.127s, 1006.27/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 133.690s\n",
      "Train: 4 [1100/10009 ( 11%)]  Loss: 4.02 (4.12)  Time: 0.106s, 1203.80/s  (0.127s, 1004.93/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 140.236s\n",
      "Train: 4 [1150/10009 ( 11%)]  Loss: 4.08 (4.12)  Time: 0.109s, 1174.76/s  (0.127s, 1005.23/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 146.562s\n",
      "Train: 4 [1200/10009 ( 12%)]  Loss: 3.96 (4.12)  Time: 0.106s, 1211.28/s  (0.127s, 1005.36/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 152.908s\n",
      "Train: 4 [1250/10009 ( 12%)]  Loss: 4.36 (4.12)  Time: 0.107s, 1192.68/s  (0.127s, 1006.17/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 159.145s\n",
      "Train: 4 [1300/10009 ( 13%)]  Loss: 3.88 (4.12)  Time: 0.108s, 1183.20/s  (0.127s, 1005.65/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 165.592s\n",
      "Train: 4 [1350/10009 ( 13%)]  Loss: 3.85 (4.12)  Time: 0.112s, 1141.63/s  (0.127s, 1005.65/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 171.956s\n",
      "Train: 4 [1400/10009 ( 14%)]  Loss: 3.85 (4.12)  Time: 0.140s,  912.24/s  (0.127s, 1005.83/s)  LR: 4.173e-05  Data: 0.038 (0.023)Time: 178.288s\n",
      "Train: 4 [1450/10009 ( 14%)]  Loss: 4.16 (4.12)  Time: 0.110s, 1158.75/s  (0.127s, 1006.78/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 184.476s\n",
      "Train: 4 [1500/10009 ( 15%)]  Loss: 4.18 (4.12)  Time: 0.148s,  862.35/s  (0.127s, 1006.92/s)  LR: 4.173e-05  Data: 0.045 (0.022)Time: 190.806s\n",
      "Train: 4 [1550/10009 ( 15%)]  Loss: 4.31 (4.12)  Time: 0.109s, 1170.73/s  (0.127s, 1007.66/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 197.018s\n",
      "Train: 4 [1600/10009 ( 16%)]  Loss: 4.17 (4.12)  Time: 0.141s,  907.18/s  (0.127s, 1007.46/s)  LR: 4.173e-05  Data: 0.039 (0.022)Time: 203.411s\n",
      "Train: 4 [1650/10009 ( 16%)]  Loss: 4.11 (4.12)  Time: 0.108s, 1180.88/s  (0.127s, 1008.14/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 209.622s\n",
      "Train: 4 [1700/10009 ( 17%)]  Loss: 4.05 (4.12)  Time: 0.161s,  794.86/s  (0.127s, 1007.51/s)  LR: 4.173e-05  Data: 0.060 (0.022)Time: 216.106s\n",
      "Train: 4 [1750/10009 ( 17%)]  Loss: 4.30 (4.12)  Time: 0.108s, 1182.99/s  (0.127s, 1007.17/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 222.532s\n",
      "Train: 4 [1800/10009 ( 18%)]  Loss: 4.34 (4.12)  Time: 0.157s,  814.63/s  (0.127s, 1007.97/s)  LR: 4.173e-05  Data: 0.054 (0.022)Time: 228.706s\n",
      "Train: 4 [1850/10009 ( 18%)]  Loss: 4.27 (4.12)  Time: 0.108s, 1180.71/s  (0.127s, 1007.55/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 235.151s\n",
      "Train: 4 [1900/10009 ( 19%)]  Loss: 3.90 (4.12)  Time: 0.214s,  598.23/s  (0.127s, 1006.22/s)  LR: 4.173e-05  Data: 0.084 (0.022)Time: 241.824s\n",
      "Train: 4 [1950/10009 ( 19%)]  Loss: 4.12 (4.12)  Time: 0.124s, 1032.67/s  (0.127s, 1005.49/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 248.363s\n",
      "Train: 4 [2000/10009 ( 20%)]  Loss: 4.31 (4.12)  Time: 0.224s,  572.05/s  (0.127s, 1004.66/s)  LR: 4.173e-05  Data: 0.093 (0.023)Time: 254.941s\n",
      "Train: 4 [2050/10009 ( 20%)]  Loss: 4.15 (4.11)  Time: 0.109s, 1173.22/s  (0.127s, 1004.72/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 261.295s\n",
      "Train: 4 [2100/10009 ( 21%)]  Loss: 4.49 (4.11)  Time: 0.224s,  571.55/s  (0.127s, 1005.01/s)  LR: 4.173e-05  Data: 0.119 (0.022)Time: 267.588s\n",
      "Train: 4 [2150/10009 ( 21%)]  Loss: 4.24 (4.11)  Time: 0.108s, 1183.05/s  (0.127s, 1005.13/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 273.921s\n",
      "Train: 4 [2200/10009 ( 22%)]  Loss: 4.57 (4.11)  Time: 0.135s,  947.83/s  (0.127s, 1005.25/s)  LR: 4.173e-05  Data: 0.032 (0.022)Time: 280.257s\n",
      "Train: 4 [2250/10009 ( 22%)]  Loss: 3.94 (4.11)  Time: 0.109s, 1174.08/s  (0.127s, 1005.73/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 286.486s\n",
      "Train: 4 [2300/10009 ( 23%)]  Loss: 4.25 (4.11)  Time: 0.108s, 1186.51/s  (0.127s, 1006.04/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 292.759s\n",
      "Train: 4 [2350/10009 ( 23%)]  Loss: 4.16 (4.11)  Time: 0.108s, 1190.16/s  (0.127s, 1006.26/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 299.054s\n",
      "Train: 4 [2400/10009 ( 24%)]  Loss: 4.24 (4.11)  Time: 0.143s,  894.93/s  (0.127s, 1005.79/s)  LR: 4.173e-05  Data: 0.040 (0.022)Time: 305.558s\n",
      "Train: 4 [2450/10009 ( 24%)]  Loss: 4.18 (4.11)  Time: 0.107s, 1191.91/s  (0.127s, 1005.80/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 311.917s\n",
      "Train: 4 [2500/10009 ( 25%)]  Loss: 4.32 (4.11)  Time: 0.142s,  903.40/s  (0.127s, 1005.90/s)  LR: 4.173e-05  Data: 0.039 (0.022)Time: 318.251s\n",
      "Train: 4 [2550/10009 ( 25%)]  Loss: 4.49 (4.11)  Time: 0.108s, 1186.71/s  (0.127s, 1006.00/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 324.580s\n",
      "Train: 4 [2600/10009 ( 26%)]  Loss: 3.77 (4.11)  Time: 0.131s,  974.21/s  (0.127s, 1005.97/s)  LR: 4.173e-05  Data: 0.030 (0.022)Time: 330.951s\n",
      "Train: 4 [2650/10009 ( 26%)]  Loss: 4.08 (4.11)  Time: 0.107s, 1191.82/s  (0.127s, 1005.26/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 337.552s\n",
      "Train: 4 [2700/10009 ( 27%)]  Loss: 3.83 (4.11)  Time: 0.150s,  851.94/s  (0.127s, 1006.09/s)  LR: 4.173e-05  Data: 0.049 (0.022)Time: 343.633s\n",
      "Train: 4 [2750/10009 ( 27%)]  Loss: 4.21 (4.11)  Time: 0.113s, 1130.68/s  (0.127s, 1005.84/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 350.082s\n",
      "Train: 4 [2800/10009 ( 28%)]  Loss: 4.00 (4.11)  Time: 0.169s,  755.33/s  (0.127s, 1005.40/s)  LR: 4.173e-05  Data: 0.068 (0.022)Time: 356.600s\n",
      "Train: 4 [2850/10009 ( 28%)]  Loss: 4.08 (4.11)  Time: 0.107s, 1191.82/s  (0.127s, 1005.52/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 362.922s\n",
      "Train: 4 [2900/10009 ( 29%)]  Loss: 4.20 (4.11)  Time: 0.127s, 1010.08/s  (0.127s, 1005.94/s)  LR: 4.173e-05  Data: 0.015 (0.022)Time: 369.134s\n",
      "Train: 4 [2950/10009 ( 29%)]  Loss: 4.24 (4.11)  Time: 0.107s, 1198.38/s  (0.127s, 1006.04/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 375.458s\n",
      "Train: 4 [3000/10009 ( 30%)]  Loss: 3.88 (4.11)  Time: 0.109s, 1177.43/s  (0.127s, 1006.06/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 381.812s\n",
      "Train: 4 [3050/10009 ( 30%)]  Loss: 4.07 (4.11)  Time: 0.108s, 1180.05/s  (0.127s, 1005.98/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 388.206s\n",
      "Train: 4 [3100/10009 ( 31%)]  Loss: 4.10 (4.11)  Time: 0.107s, 1194.86/s  (0.127s, 1005.63/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 394.704s\n",
      "Train: 4 [3150/10009 ( 31%)]  Loss: 4.10 (4.11)  Time: 0.122s, 1047.10/s  (0.127s, 1005.56/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 401.095s\n",
      "Train: 4 [3200/10009 ( 32%)]  Loss: 4.05 (4.11)  Time: 0.123s, 1040.35/s  (0.127s, 1005.09/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 407.651s\n",
      "Train: 4 [3250/10009 ( 32%)]  Loss: 3.96 (4.11)  Time: 0.107s, 1193.76/s  (0.127s, 1005.01/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 414.054s\n",
      "Train: 4 [3300/10009 ( 33%)]  Loss: 4.02 (4.11)  Time: 0.106s, 1202.13/s  (0.127s, 1004.72/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 420.541s\n",
      "Train: 4 [3350/10009 ( 33%)]  Loss: 3.98 (4.11)  Time: 0.107s, 1201.05/s  (0.127s, 1004.84/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 426.861s\n",
      "Train: 4 [3400/10009 ( 34%)]  Loss: 3.99 (4.11)  Time: 0.106s, 1211.83/s  (0.127s, 1005.00/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 433.162s\n",
      "Train: 4 [3450/10009 ( 34%)]  Loss: 3.93 (4.11)  Time: 0.107s, 1193.22/s  (0.127s, 1005.13/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 439.472s\n",
      "Train: 4 [3500/10009 ( 35%)]  Loss: 4.13 (4.11)  Time: 0.109s, 1176.67/s  (0.127s, 1005.34/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 445.747s\n",
      "Train: 4 [3550/10009 ( 35%)]  Loss: 3.95 (4.11)  Time: 0.107s, 1191.79/s  (0.127s, 1005.53/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 452.028s\n",
      "Train: 4 [3600/10009 ( 36%)]  Loss: 3.88 (4.11)  Time: 0.108s, 1185.60/s  (0.127s, 1004.73/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 458.757s\n",
      "Train: 4 [3650/10009 ( 36%)]  Loss: 3.94 (4.11)  Time: 0.111s, 1151.21/s  (0.127s, 1004.77/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 465.107s\n",
      "Train: 4 [3700/10009 ( 37%)]  Loss: 4.07 (4.11)  Time: 0.109s, 1173.08/s  (0.127s, 1004.72/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 471.502s\n",
      "Train: 4 [3750/10009 ( 37%)]  Loss: 3.95 (4.11)  Time: 0.108s, 1180.39/s  (0.127s, 1004.69/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 477.888s\n",
      "Train: 4 [3800/10009 ( 38%)]  Loss: 4.24 (4.11)  Time: 0.108s, 1188.36/s  (0.127s, 1004.61/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 484.296s\n",
      "Train: 4 [3850/10009 ( 38%)]  Loss: 4.07 (4.11)  Time: 0.109s, 1174.34/s  (0.127s, 1004.79/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 490.576s\n",
      "Train: 4 [3900/10009 ( 39%)]  Loss: 4.55 (4.11)  Time: 0.107s, 1196.25/s  (0.127s, 1004.69/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 496.994s\n",
      "Train: 4 [3950/10009 ( 39%)]  Loss: 4.11 (4.11)  Time: 0.110s, 1168.49/s  (0.127s, 1005.02/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 503.201s\n",
      "Train: 4 [4000/10009 ( 40%)]  Loss: 4.10 (4.11)  Time: 0.108s, 1182.35/s  (0.127s, 1005.04/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 509.557s\n",
      "Train: 4 [4050/10009 ( 40%)]  Loss: 4.19 (4.11)  Time: 0.116s, 1104.42/s  (0.127s, 1005.45/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 515.716s\n",
      "Train: 4 [4100/10009 ( 41%)]  Loss: 4.12 (4.11)  Time: 0.108s, 1187.10/s  (0.127s, 1005.59/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 522.007s\n",
      "Train: 4 [4150/10009 ( 41%)]  Loss: 4.04 (4.11)  Time: 0.178s,  719.15/s  (0.127s, 1005.71/s)  LR: 4.173e-05  Data: 0.077 (0.022)Time: 528.308s\n",
      "Train: 4 [4200/10009 ( 42%)]  Loss: 4.58 (4.11)  Time: 0.110s, 1164.93/s  (0.127s, 1005.28/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 534.902s\n",
      "Train: 4 [4250/10009 ( 42%)]  Loss: 4.10 (4.11)  Time: 0.109s, 1173.33/s  (0.127s, 1005.45/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 541.176s\n",
      "Train: 4 [4300/10009 ( 43%)]  Loss: 4.15 (4.11)  Time: 0.124s, 1030.70/s  (0.127s, 1005.60/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 547.463s\n",
      "Train: 4 [4350/10009 ( 43%)]  Loss: 3.91 (4.11)  Time: 0.107s, 1192.70/s  (0.127s, 1005.39/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 553.939s\n",
      "Train: 4 [4400/10009 ( 44%)]  Loss: 3.70 (4.10)  Time: 0.107s, 1200.28/s  (0.127s, 1005.34/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 560.332s\n",
      "Train: 4 [4450/10009 ( 44%)]  Loss: 4.07 (4.10)  Time: 0.108s, 1185.98/s  (0.127s, 1005.06/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 566.856s\n",
      "Train: 4 [4500/10009 ( 45%)]  Loss: 3.92 (4.10)  Time: 0.115s, 1113.27/s  (0.127s, 1004.45/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 573.571s\n",
      "Train: 4 [4550/10009 ( 45%)]  Loss: 4.11 (4.10)  Time: 0.109s, 1175.25/s  (0.127s, 1004.71/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 579.794s\n",
      "Train: 4 [4600/10009 ( 46%)]  Loss: 4.04 (4.10)  Time: 0.114s, 1123.64/s  (0.127s, 1004.73/s)  LR: 4.173e-05  Data: 0.009 (0.022)Time: 586.152s\n",
      "Train: 4 [4650/10009 ( 46%)]  Loss: 4.05 (4.10)  Time: 0.109s, 1171.47/s  (0.127s, 1004.83/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 592.467s\n",
      "Train: 4 [4700/10009 ( 47%)]  Loss: 3.77 (4.10)  Time: 0.110s, 1161.07/s  (0.127s, 1004.93/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 598.775s\n",
      "Train: 4 [4750/10009 ( 47%)]  Loss: 4.00 (4.10)  Time: 0.110s, 1159.52/s  (0.127s, 1004.56/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 605.365s\n",
      "Train: 4 [4800/10009 ( 48%)]  Loss: 4.10 (4.10)  Time: 0.183s,  700.79/s  (0.127s, 1004.70/s)  LR: 4.173e-05  Data: 0.082 (0.022)Time: 611.651s\n",
      "Train: 4 [4850/10009 ( 48%)]  Loss: 4.14 (4.10)  Time: 0.107s, 1192.43/s  (0.127s, 1004.68/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 618.031s\n",
      "Train: 4 [4900/10009 ( 49%)]  Loss: 3.93 (4.10)  Time: 0.107s, 1193.65/s  (0.127s, 1004.88/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 624.280s\n",
      "Train: 4 [4950/10009 ( 49%)]  Loss: 4.21 (4.10)  Time: 0.111s, 1150.89/s  (0.127s, 1004.88/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 630.648s\n",
      "Train: 4 [5000/10009 ( 50%)]  Loss: 4.11 (4.10)  Time: 0.111s, 1149.57/s  (0.127s, 1005.06/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 636.904s\n",
      "Train: 4 [5050/10009 ( 50%)]  Loss: 4.08 (4.10)  Time: 0.104s, 1231.30/s  (0.127s, 1005.08/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 643.259s\n",
      "Train: 4 [5100/10009 ( 51%)]  Loss: 3.98 (4.10)  Time: 0.104s, 1228.94/s  (0.127s, 1005.53/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 649.338s\n",
      "Train: 4 [5150/10009 ( 51%)]  Loss: 4.10 (4.10)  Time: 0.104s, 1226.40/s  (0.127s, 1005.39/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 655.788s\n",
      "Train: 4 [5200/10009 ( 52%)]  Loss: 4.24 (4.10)  Time: 0.105s, 1222.69/s  (0.127s, 1005.76/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 661.910s\n",
      "Train: 4 [5250/10009 ( 52%)]  Loss: 4.44 (4.10)  Time: 0.104s, 1228.25/s  (0.127s, 1006.08/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 668.066s\n",
      "Train: 4 [5300/10009 ( 53%)]  Loss: 4.38 (4.10)  Time: 0.104s, 1235.91/s  (0.127s, 1006.45/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 674.180s\n",
      "Train: 4 [5350/10009 ( 53%)]  Loss: 4.22 (4.10)  Time: 0.104s, 1226.95/s  (0.127s, 1006.40/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 680.572s\n",
      "Train: 4 [5400/10009 ( 54%)]  Loss: 3.97 (4.10)  Time: 0.105s, 1221.02/s  (0.127s, 1006.90/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 686.591s\n",
      "Train: 4 [5450/10009 ( 54%)]  Loss: 4.08 (4.10)  Time: 0.105s, 1214.13/s  (0.127s, 1007.26/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 692.694s\n",
      "Train: 4 [5500/10009 ( 55%)]  Loss: 4.22 (4.10)  Time: 0.103s, 1246.42/s  (0.127s, 1007.39/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 698.959s\n",
      "Train: 4 [5550/10009 ( 55%)]  Loss: 4.04 (4.10)  Time: 0.105s, 1221.82/s  (0.127s, 1007.63/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 705.149s\n",
      "Train: 4 [5600/10009 ( 56%)]  Loss: 3.92 (4.10)  Time: 0.208s,  614.28/s  (0.127s, 1007.76/s)  LR: 4.173e-05  Data: 0.094 (0.022)Time: 711.403s\n",
      "Train: 4 [5650/10009 ( 56%)]  Loss: 3.87 (4.10)  Time: 0.105s, 1221.93/s  (0.127s, 1008.12/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 717.498s\n",
      "Train: 4 [5700/10009 ( 57%)]  Loss: 3.58 (4.10)  Time: 0.137s,  932.79/s  (0.127s, 1008.23/s)  LR: 4.173e-05  Data: 0.038 (0.022)Time: 723.766s\n",
      "Train: 4 [5750/10009 ( 57%)]  Loss: 4.03 (4.10)  Time: 0.204s,  626.26/s  (0.127s, 1008.63/s)  LR: 4.173e-05  Data: 0.105 (0.022)Time: 729.824s\n",
      "Train: 4 [5800/10009 ( 58%)]  Loss: 3.85 (4.10)  Time: 0.103s, 1237.65/s  (0.127s, 1008.78/s)  LR: 4.173e-05  Data: 0.004 (0.022)Time: 736.062s\n",
      "Train: 4 [5850/10009 ( 58%)]  Loss: 3.99 (4.10)  Time: 0.115s, 1109.54/s  (0.127s, 1008.81/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 742.387s\n",
      "Train: 4 [5900/10009 ( 59%)]  Loss: 4.23 (4.10)  Time: 0.114s, 1124.41/s  (0.127s, 1009.03/s)  LR: 4.173e-05  Data: 0.011 (0.022)Time: 748.566s\n",
      "Train: 4 [5950/10009 ( 59%)]  Loss: 4.02 (4.10)  Time: 0.103s, 1242.24/s  (0.127s, 1009.17/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 754.801s\n",
      "Train: 4 [6000/10009 ( 60%)]  Loss: 4.32 (4.10)  Time: 0.104s, 1229.68/s  (0.127s, 1009.52/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 760.885s\n",
      "Train: 4 [6050/10009 ( 60%)]  Loss: 4.26 (4.10)  Time: 0.105s, 1214.18/s  (0.127s, 1009.84/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 766.975s\n",
      "Train: 4 [6100/10009 ( 61%)]  Loss: 4.06 (4.10)  Time: 0.103s, 1241.47/s  (0.127s, 1009.97/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 773.216s\n",
      "Train: 4 [6150/10009 ( 61%)]  Loss: 4.35 (4.10)  Time: 0.105s, 1222.55/s  (0.127s, 1010.26/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 779.333s\n",
      "Train: 4 [6200/10009 ( 62%)]  Loss: 4.15 (4.10)  Time: 0.104s, 1229.75/s  (0.127s, 1010.46/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 785.506s\n",
      "Train: 4 [6250/10009 ( 62%)]  Loss: 4.13 (4.10)  Time: 0.104s, 1231.48/s  (0.127s, 1010.52/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 791.798s\n",
      "Train: 4 [6300/10009 ( 63%)]  Loss: 4.10 (4.10)  Time: 0.104s, 1230.73/s  (0.127s, 1010.91/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 797.823s\n",
      "Train: 4 [6350/10009 ( 63%)]  Loss: 3.74 (4.10)  Time: 0.164s,  778.85/s  (0.127s, 1010.99/s)  LR: 4.173e-05  Data: 0.065 (0.022)Time: 804.091s\n",
      "Train: 4 [6400/10009 ( 64%)]  Loss: 3.95 (4.10)  Time: 0.103s, 1236.87/s  (0.127s, 1011.29/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 810.180s\n",
      "Train: 4 [6450/10009 ( 64%)]  Loss: 3.98 (4.10)  Time: 0.104s, 1232.29/s  (0.127s, 1011.35/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 816.458s\n",
      "Train: 4 [6500/10009 ( 65%)]  Loss: 4.11 (4.10)  Time: 0.104s, 1231.63/s  (0.127s, 1011.30/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 822.829s\n",
      "Train: 4 [6550/10009 ( 65%)]  Loss: 4.31 (4.09)  Time: 0.104s, 1226.83/s  (0.127s, 1011.53/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 828.966s\n",
      "Train: 4 [6600/10009 ( 66%)]  Loss: 4.00 (4.10)  Time: 0.107s, 1201.78/s  (0.126s, 1011.97/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 834.934s\n",
      "Train: 4 [6650/10009 ( 66%)]  Loss: 3.75 (4.10)  Time: 0.103s, 1236.88/s  (0.126s, 1012.07/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 841.176s\n",
      "Train: 4 [6700/10009 ( 67%)]  Loss: 4.13 (4.09)  Time: 0.103s, 1244.64/s  (0.126s, 1012.08/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 847.489s\n",
      "Train: 4 [6750/10009 ( 67%)]  Loss: 3.97 (4.09)  Time: 0.104s, 1232.96/s  (0.126s, 1012.47/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 853.484s\n",
      "Train: 4 [6800/10009 ( 68%)]  Loss: 4.42 (4.09)  Time: 0.106s, 1212.58/s  (0.126s, 1012.63/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 859.665s\n",
      "Train: 4 [6850/10009 ( 68%)]  Loss: 4.23 (4.09)  Time: 0.104s, 1231.47/s  (0.126s, 1012.84/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 865.812s\n",
      "Train: 4 [6900/10009 ( 69%)]  Loss: 4.09 (4.09)  Time: 0.148s,  865.78/s  (0.126s, 1012.80/s)  LR: 4.173e-05  Data: 0.049 (0.022)Time: 872.164s\n",
      "Train: 4 [6950/10009 ( 69%)]  Loss: 4.01 (4.09)  Time: 0.105s, 1218.29/s  (0.126s, 1012.79/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 878.493s\n",
      "Train: 4 [7000/10009 ( 70%)]  Loss: 4.33 (4.09)  Time: 0.133s,  959.80/s  (0.126s, 1012.98/s)  LR: 4.173e-05  Data: 0.036 (0.022)Time: 884.638s\n",
      "Train: 4 [7050/10009 ( 70%)]  Loss: 4.11 (4.09)  Time: 0.104s, 1232.97/s  (0.126s, 1013.19/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 890.771s\n",
      "Train: 4 [7100/10009 ( 71%)]  Loss: 3.83 (4.09)  Time: 0.214s,  597.74/s  (0.126s, 1013.45/s)  LR: 4.173e-05  Data: 0.091 (0.022)Time: 896.860s\n",
      "Train: 4 [7150/10009 ( 71%)]  Loss: 3.94 (4.09)  Time: 0.104s, 1235.71/s  (0.126s, 1013.59/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 903.055s\n",
      "Train: 4 [7200/10009 ( 72%)]  Loss: 3.67 (4.09)  Time: 0.180s,  710.76/s  (0.126s, 1013.40/s)  LR: 4.173e-05  Data: 0.082 (0.022)Time: 909.539s\n",
      "Train: 4 [7250/10009 ( 72%)]  Loss: 4.07 (4.09)  Time: 0.107s, 1195.75/s  (0.126s, 1013.57/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 915.698s\n",
      "Train: 4 [7300/10009 ( 73%)]  Loss: 4.06 (4.09)  Time: 0.155s,  828.11/s  (0.126s, 1013.82/s)  LR: 4.173e-05  Data: 0.055 (0.022)Time: 921.790s\n",
      "Train: 4 [7350/10009 ( 73%)]  Loss: 4.01 (4.09)  Time: 0.104s, 1228.96/s  (0.126s, 1013.85/s)  LR: 4.173e-05  Data: 0.005 (0.022)Time: 928.071s\n",
      "Train: 4 [7400/10009 ( 74%)]  Loss: 4.47 (4.09)  Time: 0.106s, 1207.36/s  (0.126s, 1014.09/s)  LR: 4.173e-05  Data: 0.007 (0.022)Time: 934.167s\n",
      "Train: 4 [7450/10009 ( 74%)]  Loss: 4.05 (4.09)  Time: 0.108s, 1190.27/s  (0.126s, 1014.30/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 940.274s\n",
      "Train: 4 [7500/10009 ( 75%)]  Loss: 4.17 (4.09)  Time: 0.236s,  542.25/s  (0.126s, 1014.28/s)  LR: 4.173e-05  Data: 0.137 (0.022)Time: 946.610s\n",
      "Train: 4 [7550/10009 ( 75%)]  Loss: 3.94 (4.09)  Time: 0.104s, 1230.05/s  (0.126s, 1014.16/s)  LR: 4.173e-05  Data: 0.006 (0.022)Time: 953.031s\n",
      "Train: 4 [7600/10009 ( 76%)]  Loss: 4.43 (4.09)  Time: 0.232s,  551.38/s  (0.126s, 1014.26/s)  LR: 4.173e-05  Data: 0.134 (0.022)Time: 959.242s\n",
      "Train: 4 [7650/10009 ( 76%)]  Loss: 4.27 (4.09)  Time: 0.103s, 1237.12/s  (0.126s, 1014.25/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 965.562s\n",
      "Train: 4 [7700/10009 ( 77%)]  Loss: 4.12 (4.09)  Time: 0.158s,  807.98/s  (0.126s, 1014.23/s)  LR: 4.173e-05  Data: 0.061 (0.023)Time: 971.890s\n",
      "Train: 4 [7750/10009 ( 77%)]  Loss: 3.63 (4.09)  Time: 0.103s, 1237.23/s  (0.126s, 1014.24/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 978.194s\n",
      "Train: 4 [7800/10009 ( 78%)]  Loss: 4.47 (4.09)  Time: 0.213s,  601.85/s  (0.126s, 1014.10/s)  LR: 4.173e-05  Data: 0.115 (0.023)Time: 984.645s\n",
      "Train: 4 [7850/10009 ( 78%)]  Loss: 3.74 (4.09)  Time: 0.106s, 1205.04/s  (0.126s, 1014.32/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 990.734s\n",
      "Train: 4 [7900/10009 ( 79%)]  Loss: 4.12 (4.09)  Time: 0.222s,  575.99/s  (0.126s, 1014.40/s)  LR: 4.173e-05  Data: 0.124 (0.023)Time: 996.971s\n",
      "Train: 4 [7950/10009 ( 79%)]  Loss: 4.08 (4.09)  Time: 0.105s, 1223.09/s  (0.126s, 1014.44/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1003.238s\n",
      "Train: 4 [8000/10009 ( 80%)]  Loss: 4.22 (4.09)  Time: 0.134s,  958.79/s  (0.126s, 1014.50/s)  LR: 4.173e-05  Data: 0.036 (0.023)Time: 1009.485s\n",
      "Train: 4 [8050/10009 ( 80%)]  Loss: 3.82 (4.09)  Time: 0.104s, 1230.36/s  (0.126s, 1014.64/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1015.655s\n",
      "Train: 4 [8100/10009 ( 81%)]  Loss: 3.62 (4.09)  Time: 0.144s,  886.39/s  (0.126s, 1014.59/s)  LR: 4.173e-05  Data: 0.046 (0.023)Time: 1022.010s\n",
      "Train: 4 [8150/10009 ( 81%)]  Loss: 4.07 (4.09)  Time: 0.154s,  831.44/s  (0.126s, 1014.85/s)  LR: 4.173e-05  Data: 0.056 (0.023)Time: 1028.058s\n",
      "Train: 4 [8200/10009 ( 82%)]  Loss: 4.05 (4.09)  Time: 0.107s, 1193.48/s  (0.126s, 1014.77/s)  LR: 4.173e-05  Data: 0.008 (0.023)Time: 1034.448s\n",
      "Train: 4 [8250/10009 ( 82%)]  Loss: 3.91 (4.09)  Time: 0.163s,  787.36/s  (0.126s, 1014.64/s)  LR: 4.173e-05  Data: 0.049 (0.023)Time: 1040.886s\n",
      "Train: 4 [8300/10009 ( 83%)]  Loss: 3.92 (4.09)  Time: 0.105s, 1213.75/s  (0.126s, 1014.83/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 1047.002s\n",
      "Train: 4 [8350/10009 ( 83%)]  Loss: 4.02 (4.09)  Time: 0.104s, 1225.48/s  (0.126s, 1014.78/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1053.352s\n",
      "Train: 4 [8400/10009 ( 84%)]  Loss: 3.99 (4.09)  Time: 0.155s,  824.12/s  (0.126s, 1014.53/s)  LR: 4.173e-05  Data: 0.054 (0.023)Time: 1059.924s\n",
      "Train: 4 [8450/10009 ( 84%)]  Loss: 4.01 (4.09)  Time: 0.105s, 1223.45/s  (0.126s, 1014.53/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1066.232s\n",
      "Train: 4 [8500/10009 ( 85%)]  Loss: 4.02 (4.09)  Time: 0.202s,  633.15/s  (0.126s, 1014.53/s)  LR: 4.173e-05  Data: 0.104 (0.023)Time: 1072.538s\n",
      "Train: 4 [8550/10009 ( 85%)]  Loss: 3.89 (4.09)  Time: 0.131s,  973.74/s  (0.126s, 1014.65/s)  LR: 4.173e-05  Data: 0.012 (0.023)Time: 1078.719s\n",
      "Train: 4 [8600/10009 ( 86%)]  Loss: 3.97 (4.09)  Time: 0.127s, 1011.20/s  (0.126s, 1014.77/s)  LR: 4.173e-05  Data: 0.029 (0.023)Time: 1084.900s\n",
      "Train: 4 [8650/10009 ( 86%)]  Loss: 3.96 (4.09)  Time: 0.103s, 1237.54/s  (0.126s, 1014.91/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1091.052s\n",
      "Train: 4 [8700/10009 ( 87%)]  Loss: 4.13 (4.09)  Time: 0.207s,  619.46/s  (0.126s, 1014.88/s)  LR: 4.173e-05  Data: 0.104 (0.023)Time: 1097.399s\n",
      "Train: 4 [8750/10009 ( 87%)]  Loss: 4.11 (4.09)  Time: 0.125s, 1026.23/s  (0.126s, 1015.08/s)  LR: 4.173e-05  Data: 0.027 (0.023)Time: 1103.479s\n",
      "Train: 4 [8800/10009 ( 88%)]  Loss: 4.19 (4.09)  Time: 0.105s, 1224.21/s  (0.126s, 1015.09/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1109.777s\n",
      "Train: 4 [8850/10009 ( 88%)]  Loss: 4.21 (4.09)  Time: 0.152s,  840.29/s  (0.126s, 1015.28/s)  LR: 4.173e-05  Data: 0.051 (0.023)Time: 1115.874s\n",
      "Train: 4 [8900/10009 ( 89%)]  Loss: 4.07 (4.09)  Time: 0.104s, 1232.42/s  (0.126s, 1015.47/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1121.969s\n",
      "Train: 4 [8950/10009 ( 89%)]  Loss: 4.11 (4.09)  Time: 0.152s,  843.93/s  (0.126s, 1015.63/s)  LR: 4.173e-05  Data: 0.054 (0.023)Time: 1128.089s\n",
      "Train: 4 [9000/10009 ( 90%)]  Loss: 4.32 (4.09)  Time: 0.104s, 1236.11/s  (0.126s, 1015.63/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1134.391s\n",
      "Train: 4 [9050/10009 ( 90%)]  Loss: 3.92 (4.09)  Time: 0.104s, 1226.82/s  (0.126s, 1015.42/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1140.932s\n",
      "Train: 4 [9100/10009 ( 91%)]  Loss: 4.30 (4.09)  Time: 0.103s, 1242.31/s  (0.126s, 1015.25/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1147.431s\n",
      "Train: 4 [9150/10009 ( 91%)]  Loss: 3.81 (4.09)  Time: 0.109s, 1179.56/s  (0.126s, 1015.37/s)  LR: 4.173e-05  Data: 0.008 (0.023)Time: 1153.597s\n",
      "Train: 4 [9200/10009 ( 92%)]  Loss: 4.29 (4.09)  Time: 0.106s, 1205.59/s  (0.126s, 1015.46/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1159.797s\n",
      "Train: 4 [9250/10009 ( 92%)]  Loss: 4.15 (4.09)  Time: 0.104s, 1231.43/s  (0.126s, 1015.50/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1166.045s\n",
      "Train: 4 [9300/10009 ( 93%)]  Loss: 3.79 (4.09)  Time: 0.104s, 1229.42/s  (0.126s, 1015.68/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1172.149s\n",
      "Train: 4 [9350/10009 ( 93%)]  Loss: 4.29 (4.09)  Time: 0.143s,  892.40/s  (0.126s, 1015.73/s)  LR: 4.173e-05  Data: 0.047 (0.023)Time: 1178.390s\n",
      "Train: 4 [9400/10009 ( 94%)]  Loss: 4.20 (4.09)  Time: 0.102s, 1251.88/s  (0.126s, 1016.00/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1184.378s\n",
      "Train: 4 [9450/10009 ( 94%)]  Loss: 4.45 (4.09)  Time: 0.105s, 1221.00/s  (0.126s, 1015.72/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 1191.006s\n",
      "Train: 4 [9500/10009 ( 95%)]  Loss: 4.19 (4.09)  Time: 0.103s, 1238.43/s  (0.126s, 1015.77/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1197.244s\n",
      "Train: 4 [9550/10009 ( 95%)]  Loss: 3.88 (4.08)  Time: 0.105s, 1223.06/s  (0.126s, 1015.76/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1203.551s\n",
      "Train: 4 [9600/10009 ( 96%)]  Loss: 4.16 (4.08)  Time: 0.104s, 1228.79/s  (0.126s, 1015.83/s)  LR: 4.173e-05  Data: 0.005 (0.023)Time: 1209.770s\n",
      "Train: 4 [9650/10009 ( 96%)]  Loss: 4.12 (4.08)  Time: 0.125s, 1020.33/s  (0.126s, 1015.59/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 1216.364s\n",
      "Train: 4 [9700/10009 ( 97%)]  Loss: 3.94 (4.08)  Time: 0.104s, 1225.62/s  (0.126s, 1015.93/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1222.255s\n",
      "Train: 4 [9750/10009 ( 97%)]  Loss: 4.08 (4.08)  Time: 0.105s, 1221.52/s  (0.126s, 1015.80/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 1228.704s\n",
      "Train: 4 [9800/10009 ( 98%)]  Loss: 4.12 (4.08)  Time: 0.123s, 1044.49/s  (0.126s, 1015.81/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1235.004s\n",
      "Train: 4 [9850/10009 ( 98%)]  Loss: 3.94 (4.08)  Time: 0.105s, 1217.12/s  (0.126s, 1015.86/s)  LR: 4.173e-05  Data: 0.006 (0.023)Time: 1241.241s\n",
      "Train: 4 [9900/10009 ( 99%)]  Loss: 4.22 (4.08)  Time: 0.105s, 1219.90/s  (0.126s, 1015.98/s)  LR: 4.173e-05  Data: 0.007 (0.023)Time: 1247.387s\n",
      "Train: 4 [9950/10009 ( 99%)]  Loss: 3.87 (4.08)  Time: 0.169s,  759.20/s  (0.126s, 1016.13/s)  LR: 4.173e-05  Data: 0.070 (0.023)Time: 1253.509s\n",
      "Train: 4 [10000/10009 (100%)]  Loss: 3.68 (4.08)  Time: 0.153s,  838.70/s  (0.126s, 1016.12/s)  LR: 4.173e-05  Data: 0.056 (0.023)Time: 1259.819s\n",
      "Test: [   0/390]  Time: 0.655 (0.655)  Loss:   2.443 ( 2.443)  Acc@1:  51.562 ( 51.562)  Acc@5:  76.562 ( 76.562)\n",
      "Test: [  50/390]  Time: 0.030 (0.139)  Loss:   2.064 ( 3.260)  Acc@1:  49.219 ( 35.646)  Acc@5:  82.031 ( 58.578)\n",
      "Test: [ 100/390]  Time: 0.305 (0.133)  Loss:   3.190 ( 3.355)  Acc@1:  25.781 ( 31.730)  Acc@5:  64.844 ( 56.691)\n",
      "Test: [ 150/390]  Time: 0.028 (0.138)  Loss:   2.740 ( 3.266)  Acc@1:  35.156 ( 33.169)  Acc@5:  69.531 ( 58.620)\n",
      "Test: [ 200/390]  Time: 0.031 (0.134)  Loss:   4.475 ( 3.422)  Acc@1:   8.594 ( 30.970)  Acc@5:  32.812 ( 55.581)\n",
      "Test: [ 250/390]  Time: 0.029 (0.134)  Loss:   4.030 ( 3.517)  Acc@1:  25.000 ( 29.616)  Acc@5:  44.531 ( 53.744)\n",
      "Test: [ 300/390]  Time: 0.106 (0.132)  Loss:   3.780 ( 3.587)  Acc@1:  29.688 ( 28.745)  Acc@5:  46.094 ( 52.320)\n",
      "Test: [ 350/390]  Time: 0.033 (0.132)  Loss:   3.699 ( 3.643)  Acc@1:  25.781 ( 27.920)  Acc@5:  54.688 ( 51.238)\n",
      "Test: [ 390/390]  Time: 0.019 (0.132)  Loss:   4.585 ( 3.607)  Acc@1:  12.500 ( 28.652)  Acc@5:  37.500 ( 51.896)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-4.pth.tar', 28.652)\n",
      " ('./output/budgeted/checkpoint-3.pth.tar', 24.724)\n",
      " ('./output/budgeted/checkpoint-2.pth.tar', 21.734)\n",
      " ('./output/budgeted/checkpoint-1.pth.tar', 17.546)\n",
      " ('./output/budgeted/checkpoint-0.pth.tar', 9.416)\n",
      "\n",
      "*** Best metric: 28.652 (epoch 4)\n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        for epoch in range(start_epoch, 5):\n",
    "            if hasattr(dataset_train, 'set_epoch'):\n",
    "                dataset_train.set_epoch(epoch)\n",
    "            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n",
    "                loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                train_loss_fn,\n",
    "                args,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                saver=saver,\n",
    "                output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast,\n",
    "                loss_scaler=loss_scaler,\n",
    "                model_ema=model_ema,\n",
    "                mixup_fn=mixup_fn,\n",
    "                # fish: add preconditioner\n",
    "                preconditioner=preconditioner,\n",
    "            )\n",
    "\n",
    "            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                if utils.is_primary(args):\n",
    "                    _logger.info(\"Distributing BatchNorm running means and vars\")\n",
    "                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "            eval_metrics = validate(\n",
    "                model,\n",
    "                loader_eval,\n",
    "                validate_loss_fn,\n",
    "                args,\n",
    "                amp_autocast=amp_autocast,\n",
    "            )\n",
    "\n",
    "            if model_ema is not None and not args.model_ema_force_cpu:\n",
    "                if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                    utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "                ema_eval_metrics = validate(\n",
    "                    model_ema.module,\n",
    "                    loader_eval,\n",
    "                    validate_loss_fn,\n",
    "                    args,\n",
    "                    amp_autocast=amp_autocast,\n",
    "                    log_suffix=' (EMA)',\n",
    "                )\n",
    "                eval_metrics = ema_eval_metrics\n",
    "\n",
    "            if output_dir is not None:\n",
    "                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "                utils.update_summary(\n",
    "                    epoch,\n",
    "                    train_metrics,\n",
    "                    eval_metrics,\n",
    "                    filename=os.path.join(output_dir, 'summary.csv'),\n",
    "                    lr=sum(lrs) / len(lrs),\n",
    "                    write_header=best_metric is None,\n",
    "                    log_wandb=args.log_wandb and has_wandb,\n",
    "                )\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model bud_small_patch16_224_child created, param count:11417704\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n",
      "Using native Torch AMP. Training in mixed precision.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ltuo5m6f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43ecd538460435f9bf4188a0bb747d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>eval_loss</td><td>█▅▃▂▁</td></tr><tr><td>eval_top1</td><td>▁▄▅▇█</td></tr><tr><td>eval_top5</td><td>▁▄▆▇█</td></tr><tr><td>lr</td><td>██▆▄▁</td></tr><tr><td>train_loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>eval_loss</td><td>3.60661</td></tr><tr><td>eval_top1</td><td>28.652</td></tr><tr><td>eval_top5</td><td>51.896</td></tr><tr><td>lr</td><td>4e-05</td></tr><tr><td>train_loss</td><td>4.08336</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">budgeted</strong> at: <a href='https://wandb.ai/fabfish/timm/runs/ltuo5m6f' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/ltuo5m6f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230827_153352-ltuo5m6f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ltuo5m6f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b180ae3061491aad73b59588ef705b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016667971833764266, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fish/Documents/GitHub/pytorch-image-models/wandb/run-20230827_175115-euj4dvhf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabfish/timm/runs/euj4dvhf' target=\"_blank\">budgeted</a></strong> to <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">https://wandb.ai/fabfish/timm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabfish/timm/runs/euj4dvhf' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/euj4dvhf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scheduled epochs: 15. LR stepped per epoch.\n"
     ]
    }
   ],
   "source": [
    "    old_model = model\n",
    "    old_optimizer = optimizer\n",
    "    old_loss_scaler = loss_scaler\n",
    "    old_lr_scheduler = lr_scheduler\n",
    "    if args.use_eva:\n",
    "        old_preconditioner = preconditioner\n",
    "\n",
    "    args.model = args.model.replace('baby', 'child')\n",
    "    # args.model = args.model.replace('child', 'man')\n",
    "    # args.model = args.model.replace('man', 'baby')\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm',name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 5\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0315, -0.0346,  0.0174,  ...,  0.0112,  0.0255,  0.0137],\n",
      "        [ 0.0115,  0.0173,  0.0114,  ...,  0.0149,  0.0192, -0.0242],\n",
      "        [ 0.0148,  0.0067, -0.0030,  ...,  0.0030, -0.0055, -0.0211],\n",
      "        ...,\n",
      "        [-0.0092,  0.0030, -0.0129,  ..., -0.0232,  0.0294, -0.0003],\n",
      "        [-0.0041,  0.0124, -0.0091,  ..., -0.0248, -0.0320, -0.0066],\n",
      "        [-0.0025, -0.0027,  0.0012,  ..., -0.0146,  0.0144,  0.0328]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0359, -0.0161, -0.0002,  ...,  0.0428, -0.0494, -0.0629],\n",
      "        [-0.0269,  0.0833,  0.0226,  ...,  0.0066, -0.0962, -0.1213],\n",
      "        [-0.0033,  0.0057, -0.1324,  ...,  0.0825, -0.0265, -0.0474],\n",
      "        ...,\n",
      "        [-0.0475,  0.0690,  0.0118,  ...,  0.0751, -0.0250, -0.0040],\n",
      "        [ 0.0761,  0.0436,  0.0120,  ...,  0.0375, -0.0297,  0.0170],\n",
      "        [ 0.0940,  0.0508, -0.0287,  ...,  0.0445, -0.0137,  0.0418]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0359, -0.0161, -0.0002,  ...,  0.0428, -0.0494, -0.0629],\n",
      "        [-0.0269,  0.0833,  0.0226,  ...,  0.0066, -0.0962, -0.1213],\n",
      "        [-0.0033,  0.0057, -0.1324,  ...,  0.0825, -0.0265, -0.0474],\n",
      "        ...,\n",
      "        [-0.0475,  0.0690,  0.0118,  ...,  0.0751, -0.0250, -0.0040],\n",
      "        [ 0.0761,  0.0436,  0.0120,  ...,  0.0375, -0.0297,  0.0170],\n",
      "        [ 0.0940,  0.0508, -0.0287,  ...,  0.0445, -0.0137,  0.0418]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "print(old_model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    tmp = old_model.state_dict()\n",
    "    for param_tensor in old_model.state_dict(): # 字典的遍历默认是遍历 key，所以param_tensor实际上是键值\n",
    "        # print(param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "        # 如果不是 attn 或者 mlp 层\n",
    "        if 'attn' not in param_tensor and 'mlp' not in param_tensor:\n",
    "            continue\n",
    "        old_size = old_model.state_dict()[param_tensor].size()\n",
    "        new_size = model.state_dict()[param_tensor].size()\n",
    "        flag = False\n",
    "        if len(old_size) != len(new_size):\n",
    "            continue\n",
    "        elif len(old_size) == 1:\n",
    "            # 如果整除\n",
    "            if new_size[0] % old_size[0] == 0:\n",
    "                times = new_size[0] // old_size[0]\n",
    "                tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "            # if old_size[0]*2 == new_size[0]:\n",
    "            #     tmp[param_tensor] = torch.cat((tmp[param_tensor], tmp[param_tensor]), 0)\n",
    "                flag = True\n",
    "            # 如果是 2:3\n",
    "            elif old_size[0]*3 == new_size[0]*2:\n",
    "                # 拼接tensor和它自己的一半\n",
    "                if old_size[0] % 2 == 0:\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                    # tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][]), 0)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    # 未实现\n",
    "                    print('error')\n",
    "        elif len(old_size) == 2:\n",
    "            if old_size[0] == new_size[0]:\n",
    "                # if old_size[1]*2 == new_size[1]:\n",
    "                if new_size[1] % old_size[1] == 0:\n",
    "                    times = new_size[1] // old_size[1]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 1)\n",
    "                    flag = True\n",
    "                elif old_size[1]*3 == new_size[1]*2:\n",
    "                    if old_size[1] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:,old_size[1]//2]), 1)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "            elif old_size[1] == new_size[1]:\n",
    "                # if old_size[0]*2 == new_size[0]:\n",
    "                if new_size[0] % old_size[0] == 0:\n",
    "                    times = new_size[0] // old_size[0]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "                    flag = True\n",
    "                elif old_size[0]*3 == new_size[0]*2:    \n",
    "                    if old_size[0] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "        if not flag:\n",
    "            if param_tensor in model.state_dict():\n",
    "                tmp[param_tensor] = model.state_dict()[param_tensor]\n",
    "            else:\n",
    "                continue\n",
    "    model.load_state_dict(tmp, strict=False)\n",
    "    print(model.state_dict()['blocks.0.attn.qkv.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 384]) torch.Size([1, 1, 384])\n",
      "torch.Size([1, 197, 384]) torch.Size([1, 197, 384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([288]) torch.Size([576])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([768])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([1000]) torch.Size([1000])\n",
      "torch.Size([384, 3, 16, 16]) torch.Size([384, 3, 16, 16])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([288, 384]) torch.Size([576, 384])\n",
      "torch.Size([384, 96]) torch.Size([384, 192])\n",
      "torch.Size([384, 384]) torch.Size([768, 384])\n",
      "torch.Size([384, 384]) torch.Size([384, 768])\n",
      "torch.Size([1000, 384]) torch.Size([1000, 384])\n"
     ]
    }
   ],
   "source": [
    "len(old_optimizer.param_groups[0]['params'])\n",
    "# 接下来，遍历原始模型的参数，把他们加入新的优化器中\n",
    "for param_group in range(len(old_optimizer.param_groups)):\n",
    "    for i, (old_p, new_p) in enumerate(zip(old_optimizer.param_groups[param_group]['params'], optimizer.param_groups[param_group]['params'])):\n",
    "        # 查看 old_p 和 new_p 的 shape\n",
    "        print(old_p.shape, new_p.shape)\n",
    "        with torch.no_grad():\n",
    "            if old_p.shape == new_p.shape:\n",
    "                new_p = old_p\n",
    "            else:\n",
    "                # 否则，用 0 填补 new_p 的后半部分\n",
    "                # 如果 new_p 是 1 维\n",
    "                if len(new_p.shape) == 1:\n",
    "                    new_p[:old_p.shape[0]] = old_p\n",
    "                # 如果 new_p 是 2 维\n",
    "                elif len(new_p.shape) == 2:\n",
    "                    new_p[:old_p.shape[0], :old_p.shape[1]] = old_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 5 [   0/10009 (  0%)]  Loss: 7.20 (7.20)  Time: 0.784s,  163.30/s  (0.784s,  163.30/s)  LR: 3.750e-05  Data: 0.609 (0.609)Time: 0.784s\n",
      "Train: 5 [  50/10009 (  0%)]  Loss: 5.49 (6.04)  Time: 0.131s,  974.83/s  (0.152s,  844.65/s)  LR: 3.750e-05  Data: 0.006 (0.023)Time: 7.729s\n",
      "Train: 5 [ 100/10009 (  1%)]  Loss: 4.96 (5.53)  Time: 0.141s,  907.29/s  (0.144s,  889.68/s)  LR: 3.750e-05  Data: 0.008 (0.015)Time: 14.531s\n",
      "Train: 5 [ 150/10009 (  1%)]  Loss: 4.55 (5.23)  Time: 0.131s,  979.00/s  (0.141s,  910.39/s)  LR: 3.750e-05  Data: 0.006 (0.012)Time: 21.231s\n",
      "Train: 5 [ 200/10009 (  2%)]  Loss: 4.31 (5.03)  Time: 0.141s,  908.29/s  (0.140s,  916.91/s)  LR: 3.750e-05  Data: 0.007 (0.011)Time: 28.060s\n",
      "Train: 5 [ 250/10009 (  2%)]  Loss: 4.08 (4.90)  Time: 0.131s,  973.47/s  (0.140s,  914.46/s)  LR: 3.750e-05  Data: 0.006 (0.010)Time: 35.134s\n",
      "Train: 5 [ 300/10009 (  3%)]  Loss: 4.62 (4.81)  Time: 0.158s,  810.58/s  (0.141s,  910.68/s)  LR: 3.750e-05  Data: 0.009 (0.009)Time: 42.307s\n",
      "Train: 5 [ 350/10009 (  3%)]  Loss: 4.20 (4.72)  Time: 0.139s,  919.63/s  (0.140s,  914.26/s)  LR: 3.750e-05  Data: 0.007 (0.009)Time: 49.142s\n",
      "Train: 5 [ 400/10009 (  4%)]  Loss: 4.15 (4.66)  Time: 0.134s,  957.68/s  (0.140s,  913.67/s)  LR: 3.750e-05  Data: 0.007 (0.009)Time: 56.178s\n",
      "Train: 5 [ 450/10009 (  4%)]  Loss: 4.38 (4.61)  Time: 0.133s,  963.04/s  (0.140s,  916.01/s)  LR: 3.750e-05  Data: 0.007 (0.009)Time: 63.021s\n",
      "Train: 5 [ 500/10009 (  5%)]  Loss: 4.26 (4.57)  Time: 0.133s,  965.23/s  (0.140s,  917.48/s)  LR: 3.750e-05  Data: 0.006 (0.008)Time: 69.896s\n",
      "Train: 5 [ 550/10009 (  5%)]  Loss: 4.07 (4.54)  Time: 0.134s,  958.51/s  (0.139s,  920.00/s)  LR: 3.750e-05  Data: 0.005 (0.008)Time: 76.661s\n",
      "Train: 5 [ 600/10009 (  6%)]  Loss: 4.00 (4.50)  Time: 0.133s,  961.90/s  (0.139s,  922.16/s)  LR: 3.750e-05  Data: 0.007 (0.008)Time: 83.422s\n",
      "Train: 5 [ 650/10009 (  6%)]  Loss: 4.17 (4.47)  Time: 0.136s,  943.61/s  (0.139s,  923.20/s)  LR: 3.750e-05  Data: 0.008 (0.008)Time: 90.260s\n",
      "Train: 5 [ 700/10009 (  7%)]  Loss: 4.13 (4.45)  Time: 0.132s,  967.92/s  (0.138s,  924.45/s)  LR: 3.750e-05  Data: 0.006 (0.008)Time: 97.061s\n",
      "Train: 5 [ 750/10009 (  7%)]  Loss: 4.11 (4.43)  Time: 0.136s,  943.32/s  (0.138s,  925.37/s)  LR: 3.750e-05  Data: 0.007 (0.008)Time: 103.880s\n",
      "Train: 5 [ 800/10009 (  8%)]  Loss: 3.94 (4.41)  Time: 0.131s,  976.24/s  (0.138s,  926.12/s)  LR: 3.750e-05  Data: 0.005 (0.008)Time: 110.707s\n",
      "Train: 5 [ 850/10009 (  8%)]  Loss: 4.09 (4.40)  Time: 0.132s,  969.38/s  (0.138s,  926.32/s)  LR: 3.750e-05  Data: 0.006 (0.008)Time: 117.593s\n",
      "Train: 5 [ 900/10009 (  9%)]  Loss: 4.13 (4.38)  Time: 0.133s,  965.46/s  (0.138s,  926.97/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 124.414s\n",
      "Train: 5 [ 950/10009 (  9%)]  Loss: 3.99 (4.37)  Time: 0.132s,  973.27/s  (0.138s,  927.48/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 131.246s\n",
      "Train: 5 [1000/10009 ( 10%)]  Loss: 4.16 (4.36)  Time: 0.137s,  935.43/s  (0.138s,  927.54/s)  LR: 3.750e-05  Data: 0.010 (0.007)Time: 138.137s\n",
      "Train: 5 [1050/10009 ( 10%)]  Loss: 4.06 (4.35)  Time: 0.153s,  839.02/s  (0.138s,  927.77/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 145.002s\n",
      "Train: 5 [1100/10009 ( 11%)]  Loss: 4.37 (4.33)  Time: 0.131s,  980.04/s  (0.138s,  927.73/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 151.907s\n",
      "Train: 5 [1150/10009 ( 11%)]  Loss: 4.28 (4.32)  Time: 0.135s,  945.46/s  (0.138s,  928.59/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 158.658s\n",
      "Train: 5 [1200/10009 ( 12%)]  Loss: 4.06 (4.32)  Time: 0.131s,  978.89/s  (0.138s,  929.48/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 165.391s\n",
      "Train: 5 [1250/10009 ( 12%)]  Loss: 4.03 (4.31)  Time: 0.163s,  786.29/s  (0.138s,  929.53/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 172.268s\n",
      "Train: 5 [1300/10009 ( 13%)]  Loss: 4.08 (4.30)  Time: 0.133s,  962.69/s  (0.138s,  929.91/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 179.079s\n",
      "Train: 5 [1350/10009 ( 13%)]  Loss: 4.17 (4.29)  Time: 0.153s,  834.62/s  (0.138s,  929.68/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 186.007s\n",
      "Train: 5 [1400/10009 ( 14%)]  Loss: 3.98 (4.28)  Time: 0.134s,  956.40/s  (0.138s,  930.11/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 192.803s\n",
      "Train: 5 [1450/10009 ( 14%)]  Loss: 4.39 (4.28)  Time: 0.131s,  975.50/s  (0.138s,  928.94/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 199.935s\n",
      "Train: 5 [1500/10009 ( 15%)]  Loss: 3.86 (4.27)  Time: 0.185s,  693.18/s  (0.138s,  929.25/s)  LR: 3.750e-05  Data: 0.011 (0.007)Time: 206.755s\n",
      "Train: 5 [1550/10009 ( 15%)]  Loss: 3.91 (4.27)  Time: 0.134s,  957.28/s  (0.138s,  929.12/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 213.673s\n",
      "Train: 5 [1600/10009 ( 16%)]  Loss: 3.85 (4.26)  Time: 0.133s,  964.81/s  (0.138s,  929.35/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 220.507s\n",
      "Train: 5 [1650/10009 ( 16%)]  Loss: 4.07 (4.25)  Time: 0.132s,  970.36/s  (0.138s,  929.77/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 227.290s\n",
      "Train: 5 [1700/10009 ( 17%)]  Loss: 4.21 (4.25)  Time: 0.135s,  948.79/s  (0.138s,  930.16/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 234.075s\n",
      "Train: 5 [1750/10009 ( 17%)]  Loss: 4.08 (4.24)  Time: 0.133s,  962.32/s  (0.138s,  930.24/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 240.934s\n",
      "Train: 5 [1800/10009 ( 18%)]  Loss: 4.17 (4.24)  Time: 0.133s,  963.60/s  (0.138s,  930.22/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 247.821s\n",
      "Train: 5 [1850/10009 ( 18%)]  Loss: 4.11 (4.24)  Time: 0.130s,  981.82/s  (0.138s,  930.53/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 254.616s\n",
      "Train: 5 [1900/10009 ( 19%)]  Loss: 3.80 (4.23)  Time: 0.132s,  971.56/s  (0.138s,  930.82/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 261.412s\n",
      "Train: 5 [1950/10009 ( 19%)]  Loss: 3.90 (4.23)  Time: 0.130s,  985.36/s  (0.137s,  930.91/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 268.261s\n",
      "Train: 5 [2000/10009 ( 20%)]  Loss: 4.05 (4.22)  Time: 0.131s,  974.18/s  (0.137s,  931.10/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 275.082s\n",
      "Train: 5 [2050/10009 ( 20%)]  Loss: 3.79 (4.22)  Time: 0.134s,  957.29/s  (0.137s,  931.46/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 281.846s\n",
      "Train: 5 [2100/10009 ( 21%)]  Loss: 3.96 (4.22)  Time: 0.134s,  958.76/s  (0.137s,  931.88/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 288.585s\n",
      "Train: 5 [2150/10009 ( 21%)]  Loss: 3.97 (4.21)  Time: 0.132s,  972.55/s  (0.137s,  932.16/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 295.364s\n",
      "Train: 5 [2200/10009 ( 22%)]  Loss: 4.30 (4.21)  Time: 0.136s,  944.38/s  (0.137s,  932.16/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 302.229s\n",
      "Train: 5 [2250/10009 ( 22%)]  Loss: 3.99 (4.21)  Time: 0.131s,  973.55/s  (0.137s,  932.39/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 309.020s\n",
      "Train: 5 [2300/10009 ( 23%)]  Loss: 3.92 (4.20)  Time: 0.132s,  969.74/s  (0.137s,  932.59/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 315.815s\n",
      "Train: 5 [2350/10009 ( 23%)]  Loss: 3.93 (4.20)  Time: 0.131s,  977.18/s  (0.137s,  932.77/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 322.616s\n",
      "Train: 5 [2400/10009 ( 24%)]  Loss: 3.96 (4.20)  Time: 0.134s,  953.36/s  (0.137s,  933.44/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 329.241s\n",
      "Train: 5 [2450/10009 ( 24%)]  Loss: 4.01 (4.19)  Time: 0.155s,  826.59/s  (0.137s,  933.37/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 336.123s\n",
      "Train: 5 [2500/10009 ( 25%)]  Loss: 4.34 (4.19)  Time: 0.133s,  960.36/s  (0.137s,  933.24/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 343.029s\n",
      "Train: 5 [2550/10009 ( 25%)]  Loss: 4.01 (4.19)  Time: 0.136s,  937.82/s  (0.137s,  933.18/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 349.908s\n",
      "Train: 5 [2600/10009 ( 26%)]  Loss: 4.00 (4.19)  Time: 0.161s,  796.78/s  (0.137s,  932.66/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 356.966s\n",
      "Train: 5 [2650/10009 ( 26%)]  Loss: 3.93 (4.18)  Time: 0.133s,  960.79/s  (0.137s,  932.78/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 363.780s\n",
      "Train: 5 [2700/10009 ( 27%)]  Loss: 3.97 (4.18)  Time: 0.133s,  963.26/s  (0.137s,  933.17/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 370.486s\n",
      "Train: 5 [2750/10009 ( 27%)]  Loss: 4.19 (4.18)  Time: 0.131s,  979.69/s  (0.137s,  933.24/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 377.315s\n",
      "Train: 5 [2800/10009 ( 28%)]  Loss: 4.11 (4.17)  Time: 0.135s,  947.42/s  (0.137s,  933.62/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 384.019s\n",
      "Train: 5 [2850/10009 ( 28%)]  Loss: 4.12 (4.17)  Time: 0.136s,  939.31/s  (0.137s,  934.02/s)  LR: 3.750e-05  Data: 0.009 (0.007)Time: 390.708s\n",
      "Train: 5 [2900/10009 ( 29%)]  Loss: 4.26 (4.17)  Time: 0.131s,  976.73/s  (0.137s,  934.22/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 397.474s\n",
      "Train: 5 [2950/10009 ( 29%)]  Loss: 3.98 (4.17)  Time: 0.131s,  978.24/s  (0.137s,  933.96/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 404.438s\n",
      "Train: 5 [3000/10009 ( 30%)]  Loss: 3.94 (4.17)  Time: 0.130s,  984.29/s  (0.137s,  933.95/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 411.294s\n",
      "Train: 5 [3050/10009 ( 30%)]  Loss: 3.96 (4.16)  Time: 0.133s,  964.08/s  (0.137s,  933.95/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 418.146s\n",
      "Train: 5 [3100/10009 ( 31%)]  Loss: 4.09 (4.16)  Time: 0.133s,  965.90/s  (0.137s,  933.76/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 425.086s\n",
      "Train: 5 [3150/10009 ( 31%)]  Loss: 3.92 (4.16)  Time: 0.134s,  953.68/s  (0.137s,  933.77/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 431.933s\n",
      "Train: 5 [3200/10009 ( 32%)]  Loss: 3.91 (4.16)  Time: 0.133s,  963.44/s  (0.137s,  933.72/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 438.811s\n",
      "Train: 5 [3250/10009 ( 32%)]  Loss: 4.01 (4.16)  Time: 0.131s,  977.02/s  (0.137s,  933.60/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 445.724s\n",
      "Train: 5 [3300/10009 ( 33%)]  Loss: 3.60 (4.15)  Time: 0.146s,  877.72/s  (0.137s,  933.96/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 452.403s\n",
      "Train: 5 [3350/10009 ( 33%)]  Loss: 4.23 (4.15)  Time: 0.132s,  966.97/s  (0.137s,  934.16/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 459.158s\n",
      "Train: 5 [3400/10009 ( 34%)]  Loss: 4.08 (4.15)  Time: 0.165s,  775.42/s  (0.137s,  934.22/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 465.979s\n",
      "Train: 5 [3450/10009 ( 34%)]  Loss: 3.99 (4.15)  Time: 0.133s,  962.55/s  (0.137s,  934.34/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 472.771s\n",
      "Train: 5 [3500/10009 ( 35%)]  Loss: 3.90 (4.15)  Time: 0.137s,  935.89/s  (0.137s,  934.41/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 479.583s\n",
      "Train: 5 [3550/10009 ( 35%)]  Loss: 4.19 (4.14)  Time: 0.148s,  863.89/s  (0.137s,  934.57/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 486.347s\n",
      "Train: 5 [3600/10009 ( 36%)]  Loss: 3.91 (4.14)  Time: 0.132s,  967.96/s  (0.137s,  934.77/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 493.092s\n",
      "Train: 5 [3650/10009 ( 36%)]  Loss: 4.11 (4.14)  Time: 0.132s,  967.15/s  (0.137s,  934.66/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 499.998s\n",
      "Train: 5 [3700/10009 ( 37%)]  Loss: 4.03 (4.14)  Time: 0.131s,  975.96/s  (0.137s,  934.58/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 506.889s\n",
      "Train: 5 [3750/10009 ( 37%)]  Loss: 3.85 (4.14)  Time: 0.133s,  961.26/s  (0.137s,  934.79/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 513.619s\n",
      "Train: 5 [3800/10009 ( 38%)]  Loss: 3.92 (4.13)  Time: 0.133s,  960.04/s  (0.137s,  934.95/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 520.379s\n",
      "Train: 5 [3850/10009 ( 38%)]  Loss: 4.00 (4.13)  Time: 0.133s,  964.95/s  (0.137s,  935.05/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 527.165s\n",
      "Train: 5 [3900/10009 ( 39%)]  Loss: 3.84 (4.13)  Time: 0.134s,  952.08/s  (0.137s,  935.20/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 533.923s\n",
      "Train: 5 [3950/10009 ( 39%)]  Loss: 4.33 (4.13)  Time: 0.133s,  960.42/s  (0.137s,  935.32/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 540.701s\n",
      "Train: 5 [4000/10009 ( 40%)]  Loss: 4.27 (4.13)  Time: 0.133s,  965.45/s  (0.137s,  935.37/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 547.515s\n",
      "Train: 5 [4050/10009 ( 40%)]  Loss: 4.14 (4.13)  Time: 0.135s,  949.40/s  (0.137s,  935.55/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 554.247s\n",
      "Train: 5 [4100/10009 ( 41%)]  Loss: 3.77 (4.13)  Time: 0.145s,  881.08/s  (0.137s,  935.62/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 561.045s\n",
      "Train: 5 [4150/10009 ( 41%)]  Loss: 3.91 (4.12)  Time: 0.133s,  965.38/s  (0.137s,  935.79/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 567.781s\n",
      "Train: 5 [4200/10009 ( 42%)]  Loss: 3.96 (4.12)  Time: 0.132s,  970.30/s  (0.137s,  935.98/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 574.506s\n",
      "Train: 5 [4250/10009 ( 42%)]  Loss: 3.87 (4.12)  Time: 0.134s,  957.21/s  (0.137s,  936.20/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 581.209s\n",
      "Train: 5 [4300/10009 ( 43%)]  Loss: 4.15 (4.12)  Time: 0.135s,  949.41/s  (0.137s,  936.35/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 587.952s\n",
      "Train: 5 [4350/10009 ( 43%)]  Loss: 4.12 (4.12)  Time: 0.136s,  939.21/s  (0.137s,  936.40/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 594.754s\n",
      "Train: 5 [4400/10009 ( 44%)]  Loss: 3.96 (4.12)  Time: 0.133s,  965.07/s  (0.137s,  936.40/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 601.585s\n",
      "Train: 5 [4450/10009 ( 44%)]  Loss: 4.03 (4.12)  Time: 0.134s,  955.87/s  (0.137s,  936.27/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 608.509s\n",
      "Train: 5 [4500/10009 ( 45%)]  Loss: 4.07 (4.12)  Time: 0.132s,  969.42/s  (0.137s,  936.11/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 615.449s\n",
      "Train: 5 [4550/10009 ( 45%)]  Loss: 4.01 (4.11)  Time: 0.148s,  867.25/s  (0.137s,  936.09/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 622.296s\n",
      "Train: 5 [4600/10009 ( 46%)]  Loss: 3.93 (4.11)  Time: 0.133s,  963.04/s  (0.137s,  936.09/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 629.131s\n",
      "Train: 5 [4650/10009 ( 46%)]  Loss: 3.75 (4.11)  Time: 0.135s,  949.25/s  (0.137s,  936.14/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 635.939s\n",
      "Train: 5 [4700/10009 ( 47%)]  Loss: 3.90 (4.11)  Time: 0.135s,  946.44/s  (0.137s,  936.33/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 642.643s\n",
      "Train: 5 [4750/10009 ( 47%)]  Loss: 3.59 (4.11)  Time: 0.134s,  953.00/s  (0.137s,  936.44/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 649.401s\n",
      "Train: 5 [4800/10009 ( 48%)]  Loss: 3.68 (4.11)  Time: 0.155s,  823.65/s  (0.137s,  936.38/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 656.279s\n",
      "Train: 5 [4850/10009 ( 48%)]  Loss: 3.88 (4.11)  Time: 0.135s,  950.38/s  (0.137s,  936.26/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 663.201s\n",
      "Train: 5 [4900/10009 ( 49%)]  Loss: 3.90 (4.10)  Time: 0.134s,  956.50/s  (0.137s,  936.39/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 669.941s\n",
      "Train: 5 [4950/10009 ( 49%)]  Loss: 4.14 (4.10)  Time: 0.134s,  958.52/s  (0.137s,  936.35/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 676.806s\n",
      "Train: 5 [5000/10009 ( 50%)]  Loss: 4.15 (4.10)  Time: 0.136s,  944.28/s  (0.137s,  936.34/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 683.644s\n",
      "Train: 5 [5050/10009 ( 50%)]  Loss: 3.98 (4.10)  Time: 0.133s,  959.85/s  (0.137s,  936.48/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 690.382s\n",
      "Train: 5 [5100/10009 ( 51%)]  Loss: 3.93 (4.10)  Time: 0.133s,  959.72/s  (0.137s,  936.49/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 697.206s\n",
      "Train: 5 [5150/10009 ( 51%)]  Loss: 3.99 (4.10)  Time: 0.132s,  971.70/s  (0.137s,  936.51/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 704.023s\n",
      "Train: 5 [5200/10009 ( 52%)]  Loss: 3.88 (4.10)  Time: 0.139s,  922.52/s  (0.137s,  936.40/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 710.941s\n",
      "Train: 5 [5250/10009 ( 52%)]  Loss: 4.09 (4.10)  Time: 0.134s,  957.34/s  (0.137s,  936.40/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 717.776s\n",
      "Train: 5 [5300/10009 ( 53%)]  Loss: 3.91 (4.10)  Time: 0.133s,  960.56/s  (0.137s,  936.45/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 724.570s\n",
      "Train: 5 [5350/10009 ( 53%)]  Loss: 4.04 (4.09)  Time: 0.132s,  966.41/s  (0.137s,  936.38/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 731.458s\n",
      "Train: 5 [5400/10009 ( 54%)]  Loss: 4.06 (4.09)  Time: 0.135s,  951.57/s  (0.137s,  936.45/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 738.241s\n",
      "Train: 5 [5450/10009 ( 54%)]  Loss: 3.69 (4.09)  Time: 0.133s,  963.30/s  (0.137s,  936.40/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 745.113s\n",
      "Train: 5 [5500/10009 ( 55%)]  Loss: 4.06 (4.09)  Time: 0.142s,  903.79/s  (0.137s,  936.47/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 751.897s\n",
      "Train: 5 [5550/10009 ( 55%)]  Loss: 3.67 (4.09)  Time: 0.135s,  945.68/s  (0.137s,  936.43/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 758.762s\n",
      "Train: 5 [5600/10009 ( 56%)]  Loss: 4.05 (4.09)  Time: 0.136s,  943.22/s  (0.137s,  936.40/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 765.620s\n",
      "Train: 5 [5650/10009 ( 56%)]  Loss: 3.98 (4.09)  Time: 0.133s,  961.93/s  (0.137s,  936.32/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 772.517s\n",
      "Train: 5 [5700/10009 ( 57%)]  Loss: 3.96 (4.09)  Time: 0.142s,  900.60/s  (0.137s,  936.30/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 779.372s\n",
      "Train: 5 [5750/10009 ( 57%)]  Loss: 4.31 (4.09)  Time: 0.133s,  961.42/s  (0.137s,  936.35/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 786.162s\n",
      "Train: 5 [5800/10009 ( 58%)]  Loss: 3.81 (4.09)  Time: 0.134s,  953.49/s  (0.137s,  936.35/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 793.004s\n",
      "Train: 5 [5850/10009 ( 58%)]  Loss: 4.03 (4.09)  Time: 0.135s,  948.67/s  (0.137s,  936.34/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 799.844s\n",
      "Train: 5 [5900/10009 ( 59%)]  Loss: 3.97 (4.08)  Time: 0.136s,  938.47/s  (0.137s,  936.32/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 806.695s\n",
      "Train: 5 [5950/10009 ( 59%)]  Loss: 4.18 (4.08)  Time: 0.133s,  959.82/s  (0.137s,  936.31/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 813.544s\n",
      "Train: 5 [6000/10009 ( 60%)]  Loss: 4.05 (4.08)  Time: 0.139s,  920.14/s  (0.137s,  936.25/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 820.424s\n",
      "Train: 5 [6050/10009 ( 60%)]  Loss: 3.77 (4.08)  Time: 0.133s,  964.78/s  (0.137s,  936.23/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 827.280s\n",
      "Train: 5 [6100/10009 ( 61%)]  Loss: 3.91 (4.08)  Time: 0.132s,  970.46/s  (0.137s,  936.37/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 833.993s\n",
      "Train: 5 [6150/10009 ( 61%)]  Loss: 4.11 (4.08)  Time: 0.134s,  958.24/s  (0.137s,  936.41/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 840.788s\n",
      "Train: 5 [6200/10009 ( 62%)]  Loss: 4.17 (4.08)  Time: 0.140s,  917.11/s  (0.137s,  936.39/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 847.640s\n",
      "Train: 5 [6250/10009 ( 62%)]  Loss: 3.96 (4.08)  Time: 0.184s,  694.03/s  (0.137s,  936.42/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 854.453s\n",
      "Train: 5 [6300/10009 ( 63%)]  Loss: 3.82 (4.08)  Time: 0.133s,  959.46/s  (0.137s,  936.54/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 861.173s\n",
      "Train: 5 [6350/10009 ( 63%)]  Loss: 4.48 (4.08)  Time: 0.134s,  955.00/s  (0.137s,  936.64/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 867.912s\n",
      "Train: 5 [6400/10009 ( 64%)]  Loss: 4.06 (4.07)  Time: 0.132s,  970.32/s  (0.137s,  936.60/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 874.785s\n",
      "Train: 5 [6450/10009 ( 64%)]  Loss: 4.11 (4.07)  Time: 0.132s,  966.21/s  (0.137s,  936.70/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 881.523s\n",
      "Train: 5 [6500/10009 ( 65%)]  Loss: 4.15 (4.07)  Time: 0.131s,  976.83/s  (0.137s,  936.82/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 888.243s\n",
      "Train: 5 [6550/10009 ( 65%)]  Loss: 3.71 (4.07)  Time: 0.134s,  954.20/s  (0.137s,  936.88/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 895.018s\n",
      "Train: 5 [6600/10009 ( 66%)]  Loss: 4.24 (4.07)  Time: 0.133s,  958.89/s  (0.137s,  936.98/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 901.757s\n",
      "Train: 5 [6650/10009 ( 66%)]  Loss: 4.09 (4.07)  Time: 0.132s,  972.57/s  (0.137s,  937.06/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 908.507s\n",
      "Train: 5 [6700/10009 ( 67%)]  Loss: 3.92 (4.07)  Time: 0.134s,  956.23/s  (0.137s,  937.13/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 915.264s\n",
      "Train: 5 [6750/10009 ( 67%)]  Loss: 4.00 (4.07)  Time: 0.132s,  972.57/s  (0.137s,  937.13/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 922.097s\n",
      "Train: 5 [6800/10009 ( 68%)]  Loss: 3.91 (4.07)  Time: 0.135s,  950.82/s  (0.137s,  937.16/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 928.897s\n",
      "Train: 5 [6850/10009 ( 68%)]  Loss: 4.14 (4.07)  Time: 0.133s,  965.64/s  (0.137s,  937.19/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 935.701s\n",
      "Train: 5 [6900/10009 ( 69%)]  Loss: 4.28 (4.07)  Time: 0.135s,  946.70/s  (0.137s,  937.30/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 942.411s\n",
      "Train: 5 [6950/10009 ( 69%)]  Loss: 4.08 (4.07)  Time: 0.134s,  957.73/s  (0.137s,  937.36/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 949.181s\n",
      "Train: 5 [7000/10009 ( 70%)]  Loss: 3.65 (4.06)  Time: 0.133s,  965.19/s  (0.137s,  937.32/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 956.053s\n",
      "Train: 5 [7050/10009 ( 70%)]  Loss: 4.02 (4.06)  Time: 0.137s,  931.46/s  (0.137s,  937.38/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 962.818s\n",
      "Train: 5 [7100/10009 ( 71%)]  Loss: 4.04 (4.06)  Time: 0.133s,  961.78/s  (0.137s,  937.31/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 969.720s\n",
      "Train: 5 [7150/10009 ( 71%)]  Loss: 3.92 (4.06)  Time: 0.133s,  960.10/s  (0.137s,  937.30/s)  LR: 3.750e-05  Data: 0.005 (0.007)Time: 976.556s\n",
      "Train: 5 [7200/10009 ( 72%)]  Loss: 3.98 (4.06)  Time: 0.133s,  962.21/s  (0.137s,  937.21/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 983.474s\n",
      "Train: 5 [7250/10009 ( 72%)]  Loss: 3.76 (4.06)  Time: 0.151s,  848.72/s  (0.137s,  937.24/s)  LR: 3.750e-05  Data: 0.009 (0.007)Time: 990.273s\n",
      "Train: 5 [7300/10009 ( 73%)]  Loss: 4.10 (4.06)  Time: 0.134s,  956.50/s  (0.137s,  937.24/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 997.103s\n",
      "Train: 5 [7350/10009 ( 73%)]  Loss: 4.12 (4.06)  Time: 0.135s,  948.95/s  (0.137s,  937.29/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1003.878s\n",
      "Train: 5 [7400/10009 ( 74%)]  Loss: 3.85 (4.06)  Time: 0.134s,  957.11/s  (0.137s,  937.20/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1010.802s\n",
      "Train: 5 [7450/10009 ( 74%)]  Loss: 3.94 (4.06)  Time: 0.132s,  969.60/s  (0.137s,  937.19/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1017.645s\n",
      "Train: 5 [7500/10009 ( 75%)]  Loss: 4.10 (4.06)  Time: 0.134s,  957.62/s  (0.137s,  937.22/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1024.440s\n",
      "Train: 5 [7550/10009 ( 75%)]  Loss: 3.65 (4.06)  Time: 0.133s,  965.90/s  (0.137s,  937.18/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1031.310s\n",
      "Train: 5 [7600/10009 ( 76%)]  Loss: 3.84 (4.06)  Time: 0.131s,  980.68/s  (0.137s,  937.08/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1038.256s\n",
      "Train: 5 [7650/10009 ( 76%)]  Loss: 4.07 (4.05)  Time: 0.132s,  967.52/s  (0.137s,  937.08/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1045.076s\n",
      "Train: 5 [7700/10009 ( 77%)]  Loss: 3.91 (4.05)  Time: 0.132s,  972.82/s  (0.137s,  936.97/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1052.032s\n",
      "Train: 5 [7750/10009 ( 77%)]  Loss: 4.03 (4.05)  Time: 0.133s,  963.62/s  (0.137s,  936.98/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1058.853s\n",
      "Train: 5 [7800/10009 ( 78%)]  Loss: 3.91 (4.05)  Time: 0.155s,  826.65/s  (0.137s,  936.91/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1065.759s\n",
      "Train: 5 [7850/10009 ( 78%)]  Loss: 3.88 (4.05)  Time: 0.132s,  970.24/s  (0.137s,  937.01/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1072.482s\n",
      "Train: 5 [7900/10009 ( 79%)]  Loss: 4.09 (4.05)  Time: 0.132s,  969.32/s  (0.137s,  937.00/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1079.325s\n",
      "Train: 5 [7950/10009 ( 79%)]  Loss: 3.84 (4.05)  Time: 0.133s,  965.93/s  (0.137s,  937.06/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1086.084s\n",
      "Train: 5 [8000/10009 ( 80%)]  Loss: 4.02 (4.05)  Time: 0.142s,  899.96/s  (0.137s,  937.12/s)  LR: 3.750e-05  Data: 0.009 (0.007)Time: 1092.841s\n",
      "Train: 5 [8050/10009 ( 80%)]  Loss: 3.77 (4.05)  Time: 0.132s,  967.25/s  (0.137s,  937.18/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1099.601s\n",
      "Train: 5 [8100/10009 ( 81%)]  Loss: 3.74 (4.05)  Time: 0.134s,  957.99/s  (0.137s,  937.13/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1106.492s\n",
      "Train: 5 [8150/10009 ( 81%)]  Loss: 3.98 (4.05)  Time: 0.133s,  965.21/s  (0.137s,  937.17/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1113.271s\n",
      "Train: 5 [8200/10009 ( 82%)]  Loss: 4.10 (4.05)  Time: 0.132s,  970.47/s  (0.137s,  937.17/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1120.105s\n",
      "Train: 5 [8250/10009 ( 82%)]  Loss: 4.21 (4.05)  Time: 0.131s,  974.34/s  (0.137s,  937.20/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1126.892s\n",
      "Train: 5 [8300/10009 ( 83%)]  Loss: 3.57 (4.05)  Time: 0.144s,  888.12/s  (0.137s,  937.25/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 1133.665s\n",
      "Train: 5 [8350/10009 ( 83%)]  Loss: 3.76 (4.05)  Time: 0.133s,  965.40/s  (0.137s,  937.24/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1140.504s\n",
      "Train: 5 [8400/10009 ( 84%)]  Loss: 3.88 (4.04)  Time: 0.132s,  969.27/s  (0.137s,  937.20/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1147.376s\n",
      "Train: 5 [8450/10009 ( 84%)]  Loss: 3.89 (4.04)  Time: 0.134s,  958.22/s  (0.137s,  937.22/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1154.189s\n",
      "Train: 5 [8500/10009 ( 85%)]  Loss: 3.82 (4.04)  Time: 0.136s,  942.82/s  (0.137s,  937.23/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1160.997s\n",
      "Train: 5 [8550/10009 ( 85%)]  Loss: 4.06 (4.04)  Time: 0.132s,  972.93/s  (0.137s,  937.30/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1167.743s\n",
      "Train: 5 [8600/10009 ( 86%)]  Loss: 3.93 (4.04)  Time: 0.131s,  975.71/s  (0.137s,  937.37/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1174.483s\n",
      "Train: 5 [8650/10009 ( 86%)]  Loss: 4.05 (4.04)  Time: 0.132s,  966.05/s  (0.137s,  937.45/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1181.206s\n",
      "Train: 5 [8700/10009 ( 87%)]  Loss: 3.88 (4.04)  Time: 0.132s,  969.77/s  (0.137s,  937.50/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1187.967s\n",
      "Train: 5 [8750/10009 ( 87%)]  Loss: 3.93 (4.04)  Time: 0.135s,  951.30/s  (0.137s,  937.51/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1194.788s\n",
      "Train: 5 [8800/10009 ( 88%)]  Loss: 3.83 (4.04)  Time: 0.129s,  989.49/s  (0.137s,  937.56/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1201.545s\n",
      "Train: 5 [8850/10009 ( 88%)]  Loss: 4.32 (4.04)  Time: 0.133s,  964.31/s  (0.137s,  937.55/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1208.394s\n",
      "Train: 5 [8900/10009 ( 89%)]  Loss: 3.88 (4.04)  Time: 0.132s,  971.25/s  (0.137s,  937.59/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1215.157s\n",
      "Train: 5 [8950/10009 ( 89%)]  Loss: 3.89 (4.04)  Time: 0.136s,  938.85/s  (0.137s,  937.66/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1221.904s\n",
      "Train: 5 [9000/10009 ( 90%)]  Loss: 3.80 (4.04)  Time: 0.133s,  961.92/s  (0.137s,  937.68/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1228.699s\n",
      "Train: 5 [9050/10009 ( 90%)]  Loss: 3.98 (4.04)  Time: 0.136s,  944.43/s  (0.137s,  937.58/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1235.658s\n",
      "Train: 5 [9100/10009 ( 91%)]  Loss: 3.72 (4.04)  Time: 0.163s,  782.99/s  (0.137s,  937.62/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 1242.428s\n",
      "Train: 5 [9150/10009 ( 91%)]  Loss: 3.90 (4.04)  Time: 0.135s,  948.84/s  (0.137s,  937.48/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1249.435s\n",
      "Train: 5 [9200/10009 ( 92%)]  Loss: 3.93 (4.04)  Time: 0.157s,  813.91/s  (0.137s,  937.48/s)  LR: 3.750e-05  Data: 0.008 (0.007)Time: 1256.264s\n",
      "Train: 5 [9250/10009 ( 92%)]  Loss: 3.97 (4.03)  Time: 0.133s,  959.94/s  (0.137s,  937.38/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1263.223s\n",
      "Train: 5 [9300/10009 ( 93%)]  Loss: 4.00 (4.03)  Time: 0.137s,  932.92/s  (0.137s,  937.39/s)  LR: 3.750e-05  Data: 0.009 (0.007)Time: 1270.045s\n",
      "Train: 5 [9350/10009 ( 93%)]  Loss: 3.64 (4.03)  Time: 0.127s, 1009.69/s  (0.137s,  937.42/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1276.824s\n",
      "Train: 5 [9400/10009 ( 94%)]  Loss: 4.03 (4.03)  Time: 0.124s, 1031.79/s  (0.136s,  937.80/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1283.141s\n",
      "Train: 5 [9450/10009 ( 94%)]  Loss: 3.77 (4.03)  Time: 0.126s, 1013.38/s  (0.136s,  938.15/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1289.474s\n",
      "Train: 5 [9500/10009 ( 95%)]  Loss: 3.79 (4.03)  Time: 0.124s, 1029.07/s  (0.136s,  938.52/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1295.787s\n",
      "Train: 5 [9550/10009 ( 95%)]  Loss: 4.00 (4.03)  Time: 0.124s, 1033.44/s  (0.136s,  938.90/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1302.083s\n",
      "Train: 5 [9600/10009 ( 96%)]  Loss: 4.00 (4.03)  Time: 0.124s, 1034.40/s  (0.136s,  939.24/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1308.425s\n",
      "Train: 5 [9650/10009 ( 96%)]  Loss: 3.94 (4.03)  Time: 0.125s, 1021.63/s  (0.136s,  939.63/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1314.689s\n",
      "Train: 5 [9700/10009 ( 97%)]  Loss: 3.97 (4.03)  Time: 0.125s, 1020.42/s  (0.136s,  940.01/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1320.972s\n",
      "Train: 5 [9750/10009 ( 97%)]  Loss: 4.27 (4.03)  Time: 0.125s, 1026.79/s  (0.136s,  940.34/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1327.312s\n",
      "Train: 5 [9800/10009 ( 98%)]  Loss: 3.74 (4.03)  Time: 0.123s, 1039.54/s  (0.136s,  940.62/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1333.716s\n",
      "Train: 5 [9850/10009 ( 98%)]  Loss: 4.33 (4.03)  Time: 0.125s, 1020.62/s  (0.136s,  940.95/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1340.055s\n",
      "Train: 5 [9900/10009 ( 99%)]  Loss: 4.19 (4.03)  Time: 0.129s,  994.48/s  (0.136s,  941.22/s)  LR: 3.750e-05  Data: 0.006 (0.007)Time: 1346.472s\n",
      "Train: 5 [9950/10009 ( 99%)]  Loss: 3.99 (4.03)  Time: 0.131s,  978.65/s  (0.136s,  941.55/s)  LR: 3.750e-05  Data: 0.007 (0.007)Time: 1352.799s\n",
      "Train: 5 [10000/10009 (100%)]  Loss: 3.96 (4.03)  Time: 0.169s,  756.52/s  (0.136s,  941.78/s)  LR: 3.750e-05  Data: 0.053 (0.007)Time: 1359.261s\n",
      "Test: [   0/390]  Time: 0.674 (0.674)  Loss:   2.138 ( 2.138)  Acc@1:  57.031 ( 57.031)  Acc@5:  77.344 ( 77.344)\n",
      "Test: [  50/390]  Time: 0.038 (0.138)  Loss:   1.760 ( 2.790)  Acc@1:  55.469 ( 41.774)  Acc@5:  86.719 ( 66.238)\n",
      "Test: [ 100/390]  Time: 0.253 (0.132)  Loss:   3.437 ( 2.829)  Acc@1:  16.406 ( 38.405)  Acc@5:  53.906 ( 65.664)\n",
      "Test: [ 150/390]  Time: 0.041 (0.137)  Loss:   2.224 ( 2.783)  Acc@1:  46.094 ( 39.373)  Acc@5:  78.125 ( 66.603)\n",
      "Test: [ 200/390]  Time: 0.042 (0.135)  Loss:   3.927 ( 2.960)  Acc@1:  15.625 ( 36.983)  Acc@5:  42.969 ( 63.483)\n",
      "Test: [ 250/390]  Time: 0.043 (0.135)  Loss:   3.155 ( 3.072)  Acc@1:  41.406 ( 35.636)  Acc@5:  60.156 ( 61.330)\n",
      "Test: [ 300/390]  Time: 0.404 (0.135)  Loss:   3.101 ( 3.152)  Acc@1:  41.406 ( 34.518)  Acc@5:  60.938 ( 59.738)\n",
      "Test: [ 350/390]  Time: 0.041 (0.133)  Loss:   3.414 ( 3.224)  Acc@1:  26.562 ( 33.578)  Acc@5:  60.938 ( 58.549)\n",
      "Test: [ 390/390]  Time: 0.026 (0.133)  Loss:   4.316 ( 3.189)  Acc@1:  12.500 ( 34.288)  Acc@5:  35.000 ( 59.210)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-5.pth.tar', 34.288)\n",
      "\n",
      "Train: 6 [   0/10009 (  0%)]  Loss: 3.87 (3.87)  Time: 0.683s,  187.34/s  (0.683s,  187.34/s)  LR: 3.273e-05  Data: 0.557 (0.557)Time: 0.684s\n",
      "Train: 6 [  50/10009 (  0%)]  Loss: 3.65 (3.86)  Time: 0.131s,  979.14/s  (0.150s,  852.30/s)  LR: 3.273e-05  Data: 0.006 (0.022)Time: 7.660s\n",
      "Train: 6 [ 100/10009 (  1%)]  Loss: 4.14 (3.86)  Time: 0.135s,  949.51/s  (0.143s,  893.16/s)  LR: 3.273e-05  Data: 0.006 (0.014)Time: 14.475s\n",
      "Train: 6 [ 150/10009 (  1%)]  Loss: 4.00 (3.85)  Time: 0.143s,  894.72/s  (0.141s,  907.34/s)  LR: 3.273e-05  Data: 0.007 (0.012)Time: 21.302s\n",
      "Train: 6 [ 200/10009 (  2%)]  Loss: 3.58 (3.86)  Time: 0.133s,  959.00/s  (0.140s,  916.91/s)  LR: 3.273e-05  Data: 0.006 (0.010)Time: 28.060s\n",
      "Train: 6 [ 250/10009 (  2%)]  Loss: 3.75 (3.85)  Time: 0.132s,  969.84/s  (0.139s,  920.23/s)  LR: 3.273e-05  Data: 0.006 (0.010)Time: 34.913s\n",
      "Train: 6 [ 300/10009 (  3%)]  Loss: 3.65 (3.84)  Time: 0.131s,  978.94/s  (0.138s,  924.29/s)  LR: 3.273e-05  Data: 0.006 (0.009)Time: 41.684s\n",
      "Train: 6 [ 350/10009 (  3%)]  Loss: 3.75 (3.84)  Time: 0.134s,  956.69/s  (0.138s,  928.07/s)  LR: 3.273e-05  Data: 0.007 (0.009)Time: 48.410s\n",
      "Train: 6 [ 400/10009 (  4%)]  Loss: 3.85 (3.83)  Time: 0.132s,  968.31/s  (0.138s,  929.68/s)  LR: 3.273e-05  Data: 0.006 (0.008)Time: 55.211s\n",
      "Train: 6 [ 450/10009 (  4%)]  Loss: 3.89 (3.83)  Time: 0.164s,  780.71/s  (0.138s,  929.91/s)  LR: 3.273e-05  Data: 0.017 (0.008)Time: 62.079s\n",
      "Train: 6 [ 500/10009 (  5%)]  Loss: 3.91 (3.83)  Time: 0.135s,  948.31/s  (0.137s,  931.50/s)  LR: 3.273e-05  Data: 0.007 (0.008)Time: 68.844s\n",
      "Train: 6 [ 550/10009 (  5%)]  Loss: 3.56 (3.83)  Time: 0.135s,  950.57/s  (0.137s,  932.70/s)  LR: 3.273e-05  Data: 0.006 (0.008)Time: 75.617s\n",
      "Train: 6 [ 600/10009 (  6%)]  Loss: 3.69 (3.83)  Time: 0.132s,  966.94/s  (0.137s,  933.72/s)  LR: 3.273e-05  Data: 0.007 (0.008)Time: 82.388s\n",
      "Train: 6 [ 650/10009 (  6%)]  Loss: 3.91 (3.83)  Time: 0.133s,  959.33/s  (0.137s,  934.22/s)  LR: 3.273e-05  Data: 0.006 (0.008)Time: 89.195s\n",
      "Train: 6 [ 700/10009 (  7%)]  Loss: 3.66 (3.83)  Time: 0.134s,  954.24/s  (0.137s,  935.31/s)  LR: 3.273e-05  Data: 0.008 (0.008)Time: 95.934s\n",
      "Train: 6 [ 750/10009 (  7%)]  Loss: 3.86 (3.83)  Time: 0.131s,  976.22/s  (0.137s,  936.50/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 102.646s\n",
      "Train: 6 [ 800/10009 (  8%)]  Loss: 3.91 (3.83)  Time: 0.133s,  965.85/s  (0.137s,  935.14/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 109.640s\n",
      "Train: 6 [ 850/10009 (  8%)]  Loss: 3.98 (3.83)  Time: 0.133s,  965.58/s  (0.137s,  935.33/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 116.460s\n",
      "Train: 6 [ 900/10009 (  9%)]  Loss: 3.39 (3.83)  Time: 0.132s,  967.33/s  (0.137s,  936.05/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 123.208s\n",
      "Train: 6 [ 950/10009 (  9%)]  Loss: 3.45 (3.83)  Time: 0.133s,  963.58/s  (0.137s,  935.43/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 130.131s\n",
      "Train: 6 [1000/10009 ( 10%)]  Loss: 3.36 (3.83)  Time: 0.134s,  953.99/s  (0.137s,  935.23/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 137.002s\n",
      "Train: 6 [1050/10009 ( 10%)]  Loss: 3.79 (3.83)  Time: 0.133s,  964.30/s  (0.137s,  935.59/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 143.790s\n",
      "Train: 6 [1100/10009 ( 11%)]  Loss: 3.74 (3.83)  Time: 0.132s,  968.04/s  (0.137s,  935.89/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 150.581s\n",
      "Train: 6 [1150/10009 ( 11%)]  Loss: 3.99 (3.83)  Time: 0.132s,  968.83/s  (0.137s,  936.21/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 157.366s\n",
      "Train: 6 [1200/10009 ( 12%)]  Loss: 3.60 (3.83)  Time: 0.149s,  860.76/s  (0.137s,  935.28/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 164.366s\n",
      "Train: 6 [1250/10009 ( 12%)]  Loss: 3.87 (3.83)  Time: 0.162s,  789.37/s  (0.137s,  935.07/s)  LR: 3.273e-05  Data: 0.008 (0.007)Time: 171.247s\n",
      "Train: 6 [1300/10009 ( 13%)]  Loss: 3.87 (3.83)  Time: 0.133s,  965.33/s  (0.137s,  934.13/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 178.271s\n",
      "Train: 6 [1350/10009 ( 13%)]  Loss: 3.50 (3.83)  Time: 0.131s,  980.16/s  (0.137s,  934.40/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 185.068s\n",
      "Train: 6 [1400/10009 ( 14%)]  Loss: 3.71 (3.83)  Time: 0.133s,  962.98/s  (0.137s,  934.10/s)  LR: 3.273e-05  Data: 0.005 (0.007)Time: 191.978s\n",
      "Train: 6 [1450/10009 ( 14%)]  Loss: 4.08 (3.83)  Time: 0.132s,  971.15/s  (0.137s,  934.47/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 198.753s\n",
      "Train: 6 [1500/10009 ( 15%)]  Loss: 3.59 (3.83)  Time: 0.134s,  956.13/s  (0.137s,  934.33/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 205.632s\n",
      "Train: 6 [1550/10009 ( 15%)]  Loss: 3.66 (3.83)  Time: 0.137s,  936.56/s  (0.137s,  934.60/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 212.420s\n",
      "Train: 6 [1600/10009 ( 16%)]  Loss: 3.67 (3.83)  Time: 0.140s,  916.59/s  (0.137s,  934.54/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 219.281s\n",
      "Train: 6 [1650/10009 ( 16%)]  Loss: 3.62 (3.83)  Time: 0.131s,  976.19/s  (0.137s,  933.59/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 226.359s\n",
      "Train: 6 [1700/10009 ( 17%)]  Loss: 3.94 (3.83)  Time: 0.133s,  964.76/s  (0.137s,  933.66/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 233.198s\n",
      "Train: 6 [1750/10009 ( 17%)]  Loss: 3.83 (3.83)  Time: 0.135s,  946.66/s  (0.137s,  933.29/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 240.148s\n",
      "Train: 6 [1800/10009 ( 18%)]  Loss: 4.09 (3.83)  Time: 0.135s,  947.26/s  (0.137s,  933.54/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 246.939s\n",
      "Train: 6 [1850/10009 ( 18%)]  Loss: 3.99 (3.83)  Time: 0.134s,  957.67/s  (0.137s,  933.66/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 253.763s\n",
      "Train: 6 [1900/10009 ( 19%)]  Loss: 3.76 (3.83)  Time: 0.134s,  952.60/s  (0.137s,  933.79/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 260.581s\n",
      "Train: 6 [1950/10009 ( 19%)]  Loss: 3.66 (3.83)  Time: 0.160s,  799.69/s  (0.137s,  933.78/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 267.437s\n",
      "Train: 6 [2000/10009 ( 20%)]  Loss: 3.77 (3.83)  Time: 0.133s,  963.97/s  (0.137s,  933.70/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 274.314s\n",
      "Train: 6 [2050/10009 ( 20%)]  Loss: 3.69 (3.83)  Time: 0.139s,  918.26/s  (0.137s,  933.63/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 281.189s\n",
      "Train: 6 [2100/10009 ( 21%)]  Loss: 4.00 (3.83)  Time: 0.134s,  952.26/s  (0.137s,  933.58/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 288.059s\n",
      "Train: 6 [2150/10009 ( 21%)]  Loss: 3.71 (3.83)  Time: 0.132s,  969.45/s  (0.137s,  933.43/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 294.963s\n",
      "Train: 6 [2200/10009 ( 22%)]  Loss: 4.09 (3.83)  Time: 0.132s,  966.33/s  (0.137s,  933.48/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 301.803s\n",
      "Train: 6 [2250/10009 ( 22%)]  Loss: 4.08 (3.83)  Time: 0.133s,  965.30/s  (0.137s,  933.51/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 308.650s\n",
      "Train: 6 [2300/10009 ( 23%)]  Loss: 3.78 (3.83)  Time: 0.134s,  953.08/s  (0.137s,  933.61/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 315.471s\n",
      "Train: 6 [2350/10009 ( 23%)]  Loss: 3.81 (3.83)  Time: 0.133s,  960.81/s  (0.137s,  933.90/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 322.227s\n",
      "Train: 6 [2400/10009 ( 24%)]  Loss: 3.95 (3.83)  Time: 0.137s,  934.14/s  (0.137s,  934.28/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 328.946s\n",
      "Train: 6 [2450/10009 ( 24%)]  Loss: 3.67 (3.83)  Time: 0.136s,  942.08/s  (0.137s,  934.30/s)  LR: 3.273e-05  Data: 0.009 (0.007)Time: 335.787s\n",
      "Train: 6 [2500/10009 ( 25%)]  Loss: 3.75 (3.83)  Time: 0.144s,  887.21/s  (0.137s,  934.45/s)  LR: 3.273e-05  Data: 0.009 (0.007)Time: 342.584s\n",
      "Train: 6 [2550/10009 ( 25%)]  Loss: 3.61 (3.83)  Time: 0.135s,  951.59/s  (0.137s,  934.62/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 349.368s\n",
      "Train: 6 [2600/10009 ( 26%)]  Loss: 3.73 (3.83)  Time: 0.132s,  969.06/s  (0.137s,  934.67/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 356.199s\n",
      "Train: 6 [2650/10009 ( 26%)]  Loss: 4.07 (3.83)  Time: 0.134s,  954.10/s  (0.137s,  934.78/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 363.003s\n",
      "Train: 6 [2700/10009 ( 27%)]  Loss: 4.09 (3.83)  Time: 0.132s,  968.58/s  (0.137s,  934.68/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 369.889s\n",
      "Train: 6 [2750/10009 ( 27%)]  Loss: 3.80 (3.83)  Time: 0.133s,  961.70/s  (0.137s,  934.58/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 376.775s\n",
      "Train: 6 [2800/10009 ( 28%)]  Loss: 4.07 (3.83)  Time: 0.134s,  953.06/s  (0.137s,  934.62/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 383.608s\n",
      "Train: 6 [2850/10009 ( 28%)]  Loss: 3.75 (3.83)  Time: 0.134s,  953.80/s  (0.137s,  934.69/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 390.426s\n",
      "Train: 6 [2900/10009 ( 29%)]  Loss: 3.96 (3.83)  Time: 0.132s,  968.61/s  (0.137s,  934.95/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 397.161s\n",
      "Train: 6 [2950/10009 ( 29%)]  Loss: 3.93 (3.83)  Time: 0.134s,  957.74/s  (0.137s,  935.08/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 403.953s\n",
      "Train: 6 [3000/10009 ( 30%)]  Loss: 3.72 (3.83)  Time: 0.132s,  968.22/s  (0.137s,  935.04/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 410.811s\n",
      "Train: 6 [3050/10009 ( 30%)]  Loss: 3.60 (3.83)  Time: 0.136s,  942.58/s  (0.137s,  935.21/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 417.580s\n",
      "Train: 6 [3100/10009 ( 31%)]  Loss: 3.89 (3.83)  Time: 0.133s,  964.53/s  (0.137s,  935.34/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 424.368s\n",
      "Train: 6 [3150/10009 ( 31%)]  Loss: 3.96 (3.83)  Time: 0.134s,  952.03/s  (0.137s,  935.43/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 431.166s\n",
      "Train: 6 [3200/10009 ( 32%)]  Loss: 3.52 (3.83)  Time: 0.131s,  979.80/s  (0.137s,  935.57/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 437.942s\n",
      "Train: 6 [3250/10009 ( 32%)]  Loss: 3.78 (3.83)  Time: 0.135s,  951.49/s  (0.137s,  935.37/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 444.878s\n",
      "Train: 6 [3300/10009 ( 33%)]  Loss: 3.77 (3.83)  Time: 0.133s,  961.52/s  (0.137s,  935.44/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 451.687s\n",
      "Train: 6 [3350/10009 ( 33%)]  Loss: 3.73 (3.83)  Time: 0.132s,  968.73/s  (0.137s,  935.43/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 458.536s\n",
      "Train: 6 [3400/10009 ( 34%)]  Loss: 3.51 (3.83)  Time: 0.153s,  835.76/s  (0.137s,  935.34/s)  LR: 3.273e-05  Data: 0.008 (0.007)Time: 465.421s\n",
      "Train: 6 [3450/10009 ( 34%)]  Loss: 4.11 (3.83)  Time: 0.133s,  965.73/s  (0.137s,  935.41/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 472.226s\n",
      "Train: 6 [3500/10009 ( 35%)]  Loss: 3.88 (3.83)  Time: 0.134s,  954.85/s  (0.137s,  935.55/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 478.997s\n",
      "Train: 6 [3550/10009 ( 35%)]  Loss: 3.63 (3.83)  Time: 0.134s,  952.65/s  (0.137s,  935.58/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 485.821s\n",
      "Train: 6 [3600/10009 ( 36%)]  Loss: 3.99 (3.83)  Time: 0.133s,  965.54/s  (0.137s,  935.75/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 492.574s\n",
      "Train: 6 [3650/10009 ( 36%)]  Loss: 3.85 (3.83)  Time: 0.131s,  975.80/s  (0.137s,  935.77/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 499.406s\n",
      "Train: 6 [3700/10009 ( 37%)]  Loss: 3.79 (3.82)  Time: 0.143s,  896.97/s  (0.137s,  935.80/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 506.229s\n",
      "Train: 6 [3750/10009 ( 37%)]  Loss: 3.53 (3.82)  Time: 0.149s,  856.76/s  (0.137s,  936.06/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 512.925s\n",
      "Train: 6 [3800/10009 ( 38%)]  Loss: 4.11 (3.82)  Time: 0.132s,  966.51/s  (0.137s,  936.08/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 519.751s\n",
      "Train: 6 [3850/10009 ( 38%)]  Loss: 3.57 (3.82)  Time: 0.132s,  970.02/s  (0.137s,  936.17/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 526.537s\n",
      "Train: 6 [3900/10009 ( 39%)]  Loss: 3.58 (3.82)  Time: 0.134s,  952.51/s  (0.137s,  936.27/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 533.314s\n",
      "Train: 6 [3950/10009 ( 39%)]  Loss: 3.90 (3.82)  Time: 0.132s,  966.46/s  (0.137s,  936.27/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 540.149s\n",
      "Train: 6 [4000/10009 ( 40%)]  Loss: 3.64 (3.82)  Time: 0.145s,  881.30/s  (0.137s,  936.27/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 546.985s\n",
      "Train: 6 [4050/10009 ( 40%)]  Loss: 3.71 (3.82)  Time: 0.130s,  982.04/s  (0.137s,  936.48/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 553.696s\n",
      "Train: 6 [4100/10009 ( 41%)]  Loss: 3.87 (3.82)  Time: 0.144s,  891.37/s  (0.137s,  936.65/s)  LR: 3.273e-05  Data: 0.010 (0.007)Time: 560.429s\n",
      "Train: 6 [4150/10009 ( 41%)]  Loss: 3.82 (3.82)  Time: 0.143s,  894.72/s  (0.137s,  936.80/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 567.172s\n",
      "Train: 6 [4200/10009 ( 42%)]  Loss: 4.08 (3.82)  Time: 0.134s,  956.43/s  (0.137s,  936.94/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 573.919s\n",
      "Train: 6 [4250/10009 ( 42%)]  Loss: 3.82 (3.82)  Time: 0.161s,  793.32/s  (0.137s,  936.86/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 580.796s\n",
      "Train: 6 [4300/10009 ( 43%)]  Loss: 3.84 (3.82)  Time: 0.139s,  920.83/s  (0.137s,  936.97/s)  LR: 3.273e-05  Data: 0.010 (0.007)Time: 587.559s\n",
      "Train: 6 [4350/10009 ( 43%)]  Loss: 3.85 (3.82)  Time: 0.134s,  958.24/s  (0.137s,  937.17/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 594.261s\n",
      "Train: 6 [4400/10009 ( 44%)]  Loss: 3.97 (3.82)  Time: 0.132s,  970.37/s  (0.137s,  937.30/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 601.012s\n",
      "Train: 6 [4450/10009 ( 44%)]  Loss: 3.73 (3.82)  Time: 0.132s,  967.19/s  (0.137s,  937.43/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 607.754s\n",
      "Train: 6 [4500/10009 ( 45%)]  Loss: 4.04 (3.82)  Time: 0.135s,  948.33/s  (0.137s,  937.53/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 614.517s\n",
      "Train: 6 [4550/10009 ( 45%)]  Loss: 3.83 (3.82)  Time: 0.134s,  953.21/s  (0.137s,  937.34/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 621.466s\n",
      "Train: 6 [4600/10009 ( 46%)]  Loss: 3.83 (3.82)  Time: 0.162s,  788.83/s  (0.137s,  937.17/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 628.408s\n",
      "Train: 6 [4650/10009 ( 46%)]  Loss: 4.04 (3.82)  Time: 0.135s,  948.89/s  (0.137s,  937.24/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 635.194s\n",
      "Train: 6 [4700/10009 ( 47%)]  Loss: 3.90 (3.82)  Time: 0.165s,  774.39/s  (0.137s,  937.22/s)  LR: 3.273e-05  Data: 0.011 (0.007)Time: 642.033s\n",
      "Train: 6 [4750/10009 ( 47%)]  Loss: 4.19 (3.82)  Time: 0.133s,  960.36/s  (0.137s,  937.38/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 648.753s\n",
      "Train: 6 [4800/10009 ( 48%)]  Loss: 3.97 (3.82)  Time: 0.133s,  959.08/s  (0.137s,  937.62/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 655.409s\n",
      "Train: 6 [4850/10009 ( 48%)]  Loss: 3.81 (3.82)  Time: 0.134s,  953.21/s  (0.136s,  937.84/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 662.083s\n",
      "Train: 6 [4900/10009 ( 49%)]  Loss: 3.97 (3.82)  Time: 0.133s,  962.47/s  (0.136s,  937.84/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 668.902s\n",
      "Train: 6 [4950/10009 ( 49%)]  Loss: 3.65 (3.82)  Time: 0.136s,  938.92/s  (0.136s,  938.04/s)  LR: 3.273e-05  Data: 0.009 (0.007)Time: 675.583s\n",
      "Train: 6 [5000/10009 ( 50%)]  Loss: 3.59 (3.82)  Time: 0.137s,  934.83/s  (0.136s,  938.16/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 682.319s\n",
      "Train: 6 [5050/10009 ( 50%)]  Loss: 3.93 (3.82)  Time: 0.133s,  961.45/s  (0.136s,  938.14/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 689.157s\n",
      "Train: 6 [5100/10009 ( 51%)]  Loss: 3.82 (3.82)  Time: 0.142s,  899.78/s  (0.136s,  938.08/s)  LR: 3.273e-05  Data: 0.008 (0.007)Time: 696.021s\n",
      "Train: 6 [5150/10009 ( 51%)]  Loss: 3.94 (3.82)  Time: 0.135s,  947.06/s  (0.136s,  938.15/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 702.791s\n",
      "Train: 6 [5200/10009 ( 52%)]  Loss: 3.69 (3.82)  Time: 0.136s,  939.65/s  (0.136s,  938.28/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 709.515s\n",
      "Train: 6 [5250/10009 ( 52%)]  Loss: 4.06 (3.82)  Time: 0.133s,  961.00/s  (0.136s,  938.38/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 716.261s\n",
      "Train: 6 [5300/10009 ( 53%)]  Loss: 3.72 (3.82)  Time: 0.134s,  958.40/s  (0.136s,  938.42/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 723.048s\n",
      "Train: 6 [5350/10009 ( 53%)]  Loss: 3.59 (3.82)  Time: 0.135s,  948.37/s  (0.136s,  938.50/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 729.805s\n",
      "Train: 6 [5400/10009 ( 54%)]  Loss: 3.62 (3.82)  Time: 0.134s,  954.28/s  (0.136s,  938.56/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 736.582s\n",
      "Train: 6 [5450/10009 ( 54%)]  Loss: 3.91 (3.82)  Time: 0.132s,  971.65/s  (0.136s,  938.67/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 743.311s\n",
      "Train: 6 [5500/10009 ( 55%)]  Loss: 3.78 (3.82)  Time: 0.134s,  957.85/s  (0.136s,  938.79/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 750.039s\n",
      "Train: 6 [5550/10009 ( 55%)]  Loss: 4.00 (3.82)  Time: 0.133s,  964.87/s  (0.136s,  938.90/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 756.761s\n",
      "Train: 6 [5600/10009 ( 56%)]  Loss: 3.77 (3.82)  Time: 0.132s,  969.95/s  (0.136s,  939.02/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 763.484s\n",
      "Train: 6 [5650/10009 ( 56%)]  Loss: 3.46 (3.82)  Time: 0.140s,  916.22/s  (0.136s,  938.94/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 770.366s\n",
      "Train: 6 [5700/10009 ( 57%)]  Loss: 3.65 (3.82)  Time: 0.131s,  980.09/s  (0.136s,  939.00/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 777.127s\n",
      "Train: 6 [5750/10009 ( 57%)]  Loss: 3.39 (3.82)  Time: 0.124s, 1029.70/s  (0.136s,  939.15/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 783.819s\n",
      "Train: 6 [5800/10009 ( 58%)]  Loss: 3.96 (3.82)  Time: 0.126s, 1018.91/s  (0.136s,  939.71/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 790.161s\n",
      "Train: 6 [5850/10009 ( 58%)]  Loss: 3.58 (3.82)  Time: 0.125s, 1024.00/s  (0.136s,  940.21/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 796.552s\n",
      "Train: 6 [5900/10009 ( 59%)]  Loss: 3.44 (3.82)  Time: 0.124s, 1029.41/s  (0.136s,  940.84/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 802.816s\n",
      "Train: 6 [5950/10009 ( 59%)]  Loss: 3.60 (3.82)  Time: 0.142s,  900.41/s  (0.136s,  941.35/s)  LR: 3.273e-05  Data: 0.010 (0.007)Time: 809.184s\n",
      "Train: 6 [6000/10009 ( 60%)]  Loss: 3.86 (3.82)  Time: 0.123s, 1041.89/s  (0.136s,  941.89/s)  LR: 3.273e-05  Data: 0.005 (0.007)Time: 815.513s\n",
      "Train: 6 [6050/10009 ( 60%)]  Loss: 3.57 (3.82)  Time: 0.157s,  813.81/s  (0.136s,  942.38/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 821.884s\n",
      "Train: 6 [6100/10009 ( 61%)]  Loss: 3.90 (3.82)  Time: 0.132s,  970.73/s  (0.136s,  942.11/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 828.909s\n",
      "Train: 6 [6150/10009 ( 61%)]  Loss: 3.68 (3.82)  Time: 0.130s,  983.57/s  (0.136s,  942.02/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 835.782s\n",
      "Train: 6 [6200/10009 ( 62%)]  Loss: 3.72 (3.82)  Time: 0.133s,  961.80/s  (0.136s,  941.97/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 842.621s\n",
      "Train: 6 [6250/10009 ( 62%)]  Loss: 3.67 (3.82)  Time: 0.136s,  943.70/s  (0.136s,  941.92/s)  LR: 3.273e-05  Data: 0.009 (0.007)Time: 849.461s\n",
      "Train: 6 [6300/10009 ( 63%)]  Loss: 3.75 (3.82)  Time: 0.132s,  968.40/s  (0.136s,  941.92/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 856.258s\n",
      "Train: 6 [6350/10009 ( 63%)]  Loss: 3.76 (3.82)  Time: 0.144s,  889.66/s  (0.136s,  941.86/s)  LR: 3.273e-05  Data: 0.009 (0.007)Time: 863.105s\n",
      "Train: 6 [6400/10009 ( 64%)]  Loss: 3.84 (3.82)  Time: 0.133s,  960.73/s  (0.136s,  941.89/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 869.878s\n",
      "Train: 6 [6450/10009 ( 64%)]  Loss: 3.64 (3.82)  Time: 0.133s,  962.21/s  (0.136s,  941.93/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 876.631s\n",
      "Train: 6 [6500/10009 ( 65%)]  Loss: 3.78 (3.82)  Time: 0.132s,  968.04/s  (0.136s,  941.95/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 883.405s\n",
      "Train: 6 [6550/10009 ( 65%)]  Loss: 4.15 (3.82)  Time: 0.147s,  868.47/s  (0.136s,  941.67/s)  LR: 3.273e-05  Data: 0.008 (0.007)Time: 890.467s\n",
      "Train: 6 [6600/10009 ( 66%)]  Loss: 3.71 (3.82)  Time: 0.133s,  962.59/s  (0.136s,  941.47/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 897.451s\n",
      "Train: 6 [6650/10009 ( 66%)]  Loss: 3.74 (3.82)  Time: 0.132s,  966.55/s  (0.136s,  940.79/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 904.902s\n",
      "Train: 6 [6700/10009 ( 67%)]  Loss: 3.82 (3.82)  Time: 0.131s,  977.44/s  (0.136s,  940.82/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 911.679s\n",
      "Train: 6 [6750/10009 ( 67%)]  Loss: 3.61 (3.82)  Time: 0.131s,  973.70/s  (0.136s,  940.82/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 918.480s\n",
      "Train: 6 [6800/10009 ( 68%)]  Loss: 3.95 (3.82)  Time: 0.142s,  903.51/s  (0.136s,  940.80/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 925.305s\n",
      "Train: 6 [6850/10009 ( 68%)]  Loss: 3.66 (3.82)  Time: 0.130s,  984.14/s  (0.136s,  940.87/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 932.041s\n",
      "Train: 6 [6900/10009 ( 69%)]  Loss: 3.99 (3.82)  Time: 0.135s,  950.52/s  (0.136s,  940.88/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 938.826s\n",
      "Train: 6 [6950/10009 ( 69%)]  Loss: 3.85 (3.82)  Time: 0.132s,  967.13/s  (0.136s,  940.99/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 945.517s\n",
      "Train: 6 [7000/10009 ( 70%)]  Loss: 3.73 (3.82)  Time: 0.133s,  958.81/s  (0.136s,  940.90/s)  LR: 3.273e-05  Data: 0.005 (0.007)Time: 952.410s\n",
      "Train: 6 [7050/10009 ( 70%)]  Loss: 3.84 (3.82)  Time: 0.129s,  994.36/s  (0.136s,  940.97/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 959.144s\n",
      "Train: 6 [7100/10009 ( 71%)]  Loss: 3.56 (3.82)  Time: 0.127s, 1006.13/s  (0.136s,  940.96/s)  LR: 3.273e-05  Data: 0.005 (0.007)Time: 965.955s\n",
      "Train: 6 [7150/10009 ( 71%)]  Loss: 3.83 (3.82)  Time: 0.159s,  806.08/s  (0.136s,  940.86/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 972.862s\n",
      "Train: 6 [7200/10009 ( 72%)]  Loss: 3.85 (3.82)  Time: 0.135s,  947.97/s  (0.136s,  941.05/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 979.463s\n",
      "Train: 6 [7250/10009 ( 72%)]  Loss: 3.60 (3.81)  Time: 0.132s,  966.13/s  (0.136s,  941.10/s)  LR: 3.273e-05  Data: 0.009 (0.007)Time: 986.208s\n",
      "Train: 6 [7300/10009 ( 73%)]  Loss: 3.75 (3.81)  Time: 0.134s,  955.29/s  (0.136s,  941.02/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 993.094s\n",
      "Train: 6 [7350/10009 ( 73%)]  Loss: 3.81 (3.81)  Time: 0.134s,  956.58/s  (0.136s,  940.97/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 999.952s\n",
      "Train: 6 [7400/10009 ( 74%)]  Loss: 4.06 (3.82)  Time: 0.132s,  967.19/s  (0.136s,  940.96/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1006.764s\n",
      "Train: 6 [7450/10009 ( 74%)]  Loss: 3.76 (3.82)  Time: 0.133s,  965.89/s  (0.136s,  940.96/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1013.561s\n",
      "Train: 6 [7500/10009 ( 75%)]  Loss: 3.71 (3.82)  Time: 0.132s,  967.36/s  (0.136s,  940.95/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1020.379s\n",
      "Train: 6 [7550/10009 ( 75%)]  Loss: 3.68 (3.81)  Time: 0.132s,  969.82/s  (0.136s,  941.03/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1027.094s\n",
      "Train: 6 [7600/10009 ( 76%)]  Loss: 3.76 (3.81)  Time: 0.134s,  952.75/s  (0.136s,  940.95/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1033.984s\n",
      "Train: 6 [7650/10009 ( 76%)]  Loss: 3.87 (3.81)  Time: 0.132s,  967.01/s  (0.136s,  940.89/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1040.855s\n",
      "Train: 6 [7700/10009 ( 77%)]  Loss: 3.72 (3.81)  Time: 0.137s,  935.91/s  (0.136s,  940.85/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1047.696s\n",
      "Train: 6 [7750/10009 ( 77%)]  Loss: 3.74 (3.81)  Time: 0.146s,  877.17/s  (0.136s,  940.82/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1054.530s\n",
      "Train: 6 [7800/10009 ( 78%)]  Loss: 3.72 (3.81)  Time: 0.132s,  968.14/s  (0.136s,  940.83/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1061.325s\n",
      "Train: 6 [7850/10009 ( 78%)]  Loss: 3.89 (3.81)  Time: 0.135s,  950.55/s  (0.136s,  940.73/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1068.243s\n",
      "Train: 6 [7900/10009 ( 79%)]  Loss: 3.70 (3.81)  Time: 0.135s,  946.16/s  (0.136s,  940.77/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1074.993s\n",
      "Train: 6 [7950/10009 ( 79%)]  Loss: 3.86 (3.81)  Time: 0.133s,  965.44/s  (0.136s,  940.75/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1081.823s\n",
      "Train: 6 [8000/10009 ( 80%)]  Loss: 3.57 (3.81)  Time: 0.134s,  954.01/s  (0.136s,  940.71/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1088.674s\n",
      "Train: 6 [8050/10009 ( 80%)]  Loss: 3.81 (3.81)  Time: 0.147s,  873.15/s  (0.136s,  940.69/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1095.501s\n",
      "Train: 6 [8100/10009 ( 81%)]  Loss: 3.75 (3.81)  Time: 0.133s,  960.58/s  (0.136s,  940.63/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1102.369s\n",
      "Train: 6 [8150/10009 ( 81%)]  Loss: 3.78 (3.81)  Time: 0.135s,  944.87/s  (0.136s,  940.59/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1109.227s\n",
      "Train: 6 [8200/10009 ( 82%)]  Loss: 3.52 (3.81)  Time: 0.134s,  952.37/s  (0.136s,  940.50/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1116.136s\n",
      "Train: 6 [8250/10009 ( 82%)]  Loss: 3.93 (3.81)  Time: 0.144s,  886.83/s  (0.136s,  940.48/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1122.966s\n",
      "Train: 6 [8300/10009 ( 83%)]  Loss: 4.13 (3.81)  Time: 0.132s,  967.09/s  (0.136s,  940.52/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1129.724s\n",
      "Train: 6 [8350/10009 ( 83%)]  Loss: 4.08 (3.81)  Time: 0.132s,  969.05/s  (0.136s,  940.49/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1136.565s\n",
      "Train: 6 [8400/10009 ( 84%)]  Loss: 3.80 (3.81)  Time: 0.133s,  960.04/s  (0.136s,  940.42/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1143.456s\n",
      "Train: 6 [8450/10009 ( 84%)]  Loss: 3.90 (3.81)  Time: 0.152s,  844.03/s  (0.136s,  940.43/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1150.239s\n",
      "Train: 6 [8500/10009 ( 85%)]  Loss: 3.93 (3.81)  Time: 0.134s,  958.77/s  (0.136s,  940.45/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1157.028s\n",
      "Train: 6 [8550/10009 ( 85%)]  Loss: 3.71 (3.81)  Time: 0.145s,  879.98/s  (0.136s,  940.48/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1163.791s\n",
      "Train: 6 [8600/10009 ( 86%)]  Loss: 3.69 (3.81)  Time: 0.151s,  846.01/s  (0.136s,  940.46/s)  LR: 3.273e-05  Data: 0.008 (0.007)Time: 1170.629s\n",
      "Train: 6 [8650/10009 ( 86%)]  Loss: 3.64 (3.81)  Time: 0.133s,  961.91/s  (0.136s,  940.50/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1177.379s\n",
      "Train: 6 [8700/10009 ( 87%)]  Loss: 4.09 (3.81)  Time: 0.135s,  950.02/s  (0.136s,  940.45/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1184.250s\n",
      "Train: 6 [8750/10009 ( 87%)]  Loss: 3.62 (3.81)  Time: 0.136s,  942.76/s  (0.136s,  940.50/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1190.982s\n",
      "Train: 6 [8800/10009 ( 88%)]  Loss: 3.98 (3.81)  Time: 0.133s,  962.84/s  (0.136s,  940.54/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1197.748s\n",
      "Train: 6 [8850/10009 ( 88%)]  Loss: 3.88 (3.81)  Time: 0.133s,  965.45/s  (0.136s,  940.55/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1204.536s\n",
      "Train: 6 [8900/10009 ( 89%)]  Loss: 3.64 (3.81)  Time: 0.141s,  908.29/s  (0.136s,  940.58/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1211.305s\n",
      "Train: 6 [8950/10009 ( 89%)]  Loss: 3.83 (3.81)  Time: 0.133s,  964.86/s  (0.136s,  940.52/s)  LR: 3.273e-05  Data: 0.005 (0.007)Time: 1218.186s\n",
      "Train: 6 [9000/10009 ( 90%)]  Loss: 3.91 (3.81)  Time: 0.134s,  955.99/s  (0.136s,  940.57/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1224.924s\n",
      "Train: 6 [9050/10009 ( 90%)]  Loss: 3.81 (3.81)  Time: 0.133s,  960.42/s  (0.136s,  940.52/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1231.794s\n",
      "Train: 6 [9100/10009 ( 91%)]  Loss: 3.99 (3.81)  Time: 0.132s,  967.85/s  (0.136s,  940.45/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1238.691s\n",
      "Train: 6 [9150/10009 ( 91%)]  Loss: 3.88 (3.81)  Time: 0.133s,  960.14/s  (0.136s,  940.43/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1245.519s\n",
      "Train: 6 [9200/10009 ( 92%)]  Loss: 3.63 (3.81)  Time: 0.135s,  948.80/s  (0.136s,  940.37/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1252.411s\n",
      "Train: 6 [9250/10009 ( 92%)]  Loss: 4.29 (3.81)  Time: 0.132s,  967.68/s  (0.136s,  940.40/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1259.165s\n",
      "Train: 6 [9300/10009 ( 93%)]  Loss: 3.71 (3.81)  Time: 0.134s,  954.87/s  (0.136s,  940.40/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1265.970s\n",
      "Train: 6 [9350/10009 ( 93%)]  Loss: 3.82 (3.81)  Time: 0.133s,  961.64/s  (0.136s,  940.43/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1272.742s\n",
      "Train: 6 [9400/10009 ( 94%)]  Loss: 3.65 (3.81)  Time: 0.132s,  967.69/s  (0.136s,  940.39/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1279.597s\n",
      "Train: 6 [9450/10009 ( 94%)]  Loss: 3.60 (3.81)  Time: 0.131s,  974.49/s  (0.136s,  940.50/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1286.262s\n",
      "Train: 6 [9500/10009 ( 95%)]  Loss: 3.77 (3.81)  Time: 0.133s,  962.64/s  (0.136s,  940.55/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1292.990s\n",
      "Train: 6 [9550/10009 ( 95%)]  Loss: 3.61 (3.81)  Time: 0.133s,  959.00/s  (0.136s,  940.59/s)  LR: 3.273e-05  Data: 0.005 (0.007)Time: 1299.745s\n",
      "Train: 6 [9600/10009 ( 96%)]  Loss: 3.66 (3.81)  Time: 0.132s,  970.94/s  (0.136s,  940.58/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1306.556s\n",
      "Train: 6 [9650/10009 ( 96%)]  Loss: 3.76 (3.81)  Time: 0.132s,  969.92/s  (0.136s,  940.60/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1313.342s\n",
      "Train: 6 [9700/10009 ( 97%)]  Loss: 3.74 (3.81)  Time: 0.131s,  974.28/s  (0.136s,  940.60/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1320.147s\n",
      "Train: 6 [9750/10009 ( 97%)]  Loss: 3.56 (3.81)  Time: 0.130s,  984.62/s  (0.136s,  940.59/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1326.959s\n",
      "Train: 6 [9800/10009 ( 98%)]  Loss: 4.08 (3.81)  Time: 0.131s,  978.55/s  (0.136s,  940.65/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1333.679s\n",
      "Train: 6 [9850/10009 ( 98%)]  Loss: 3.58 (3.81)  Time: 0.133s,  960.12/s  (0.136s,  940.59/s)  LR: 3.273e-05  Data: 0.008 (0.007)Time: 1340.570s\n",
      "Train: 6 [9900/10009 ( 99%)]  Loss: 3.97 (3.81)  Time: 0.134s,  955.33/s  (0.136s,  940.56/s)  LR: 3.273e-05  Data: 0.007 (0.007)Time: 1347.410s\n",
      "Train: 6 [9950/10009 ( 99%)]  Loss: 3.74 (3.81)  Time: 0.131s,  973.65/s  (0.136s,  940.54/s)  LR: 3.273e-05  Data: 0.006 (0.007)Time: 1354.250s\n",
      "Train: 6 [10000/10009 (100%)]  Loss: 3.60 (3.81)  Time: 0.184s,  695.08/s  (0.136s,  940.41/s)  LR: 3.273e-05  Data: 0.058 (0.007)Time: 1361.236s\n",
      "Test: [   0/390]  Time: 0.722 (0.722)  Loss:   2.101 ( 2.101)  Acc@1:  56.250 ( 56.250)  Acc@5:  78.906 ( 78.906)\n",
      "Test: [  50/390]  Time: 0.041 (0.151)  Loss:   1.591 ( 2.621)  Acc@1:  63.281 ( 45.496)  Acc@5:  86.719 ( 68.888)\n",
      "Test: [ 100/390]  Time: 0.179 (0.143)  Loss:   2.941 ( 2.692)  Acc@1:  25.781 ( 41.747)  Acc@5:  65.625 ( 68.154)\n",
      "Test: [ 150/390]  Time: 0.041 (0.145)  Loss:   2.615 ( 2.645)  Acc@1:  34.375 ( 42.312)  Acc@5:  69.531 ( 68.978)\n",
      "Test: [ 200/390]  Time: 0.040 (0.141)  Loss:   3.913 ( 2.809)  Acc@1:  17.188 ( 39.840)  Acc@5:  39.062 ( 66.091)\n",
      "Test: [ 250/390]  Time: 0.043 (0.139)  Loss:   2.719 ( 2.914)  Acc@1:  48.438 ( 38.527)  Acc@5:  64.062 ( 64.196)\n",
      "Test: [ 300/390]  Time: 0.332 (0.138)  Loss:   3.024 ( 3.006)  Acc@1:  47.656 ( 37.124)  Acc@5:  64.844 ( 62.469)\n",
      "Test: [ 350/390]  Time: 0.268 (0.136)  Loss:   2.919 ( 3.079)  Acc@1:  39.844 ( 36.064)  Acc@5:  67.969 ( 61.238)\n",
      "Test: [ 390/390]  Time: 0.026 (0.136)  Loss:   4.177 ( 3.048)  Acc@1:  12.500 ( 36.718)  Acc@5:  43.750 ( 61.796)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-6.pth.tar', 36.718)\n",
      " ('./output/budgeted/checkpoint-5.pth.tar', 34.288)\n",
      "\n",
      "Train: 7 [   0/10009 (  0%)]  Loss: 3.49 (3.49)  Time: 0.721s,  177.62/s  (0.721s,  177.62/s)  LR: 2.761e-05  Data: 0.578 (0.578)Time: 0.721s\n",
      "Train: 7 [  50/10009 (  0%)]  Loss: 4.03 (3.69)  Time: 0.138s,  925.68/s  (0.150s,  851.63/s)  LR: 2.761e-05  Data: 0.006 (0.021)Time: 7.666s\n",
      "Train: 7 [ 100/10009 (  1%)]  Loss: 4.04 (3.70)  Time: 0.160s,  800.62/s  (0.143s,  894.16/s)  LR: 2.761e-05  Data: 0.006 (0.014)Time: 14.459s\n",
      "Train: 7 [ 150/10009 (  1%)]  Loss: 3.83 (3.71)  Time: 0.133s,  964.39/s  (0.141s,  909.23/s)  LR: 2.761e-05  Data: 0.008 (0.011)Time: 21.258s\n",
      "Train: 7 [ 200/10009 (  2%)]  Loss: 3.41 (3.70)  Time: 0.135s,  948.09/s  (0.139s,  919.03/s)  LR: 2.761e-05  Data: 0.007 (0.010)Time: 27.995s\n",
      "Train: 7 [ 250/10009 (  2%)]  Loss: 3.57 (3.69)  Time: 0.130s,  986.02/s  (0.139s,  922.78/s)  LR: 2.761e-05  Data: 0.006 (0.009)Time: 34.817s\n",
      "Train: 7 [ 300/10009 (  3%)]  Loss: 3.71 (3.69)  Time: 0.133s,  965.25/s  (0.138s,  926.38/s)  LR: 2.761e-05  Data: 0.006 (0.009)Time: 41.590s\n",
      "Train: 7 [ 350/10009 (  3%)]  Loss: 3.58 (3.69)  Time: 0.132s,  968.47/s  (0.138s,  928.40/s)  LR: 2.761e-05  Data: 0.006 (0.009)Time: 48.393s\n",
      "Train: 7 [ 400/10009 (  4%)]  Loss: 3.60 (3.70)  Time: 0.130s,  987.17/s  (0.137s,  931.57/s)  LR: 2.761e-05  Data: 0.006 (0.008)Time: 55.099s\n",
      "Train: 7 [ 450/10009 (  4%)]  Loss: 3.64 (3.69)  Time: 0.158s,  809.09/s  (0.137s,  932.94/s)  LR: 2.761e-05  Data: 0.006 (0.008)Time: 61.878s\n",
      "Train: 7 [ 500/10009 (  5%)]  Loss: 3.82 (3.69)  Time: 0.146s,  878.78/s  (0.137s,  932.85/s)  LR: 2.761e-05  Data: 0.007 (0.008)Time: 68.744s\n",
      "Train: 7 [ 550/10009 (  5%)]  Loss: 3.84 (3.69)  Time: 0.131s,  974.18/s  (0.137s,  934.97/s)  LR: 2.761e-05  Data: 0.005 (0.008)Time: 75.434s\n",
      "Train: 7 [ 600/10009 (  6%)]  Loss: 3.77 (3.69)  Time: 0.137s,  935.77/s  (0.137s,  936.47/s)  LR: 2.761e-05  Data: 0.010 (0.008)Time: 82.147s\n",
      "Train: 7 [ 650/10009 (  6%)]  Loss: 3.73 (3.69)  Time: 0.133s,  961.80/s  (0.137s,  936.18/s)  LR: 2.761e-05  Data: 0.006 (0.008)Time: 89.009s\n",
      "Train: 7 [ 700/10009 (  7%)]  Loss: 3.92 (3.69)  Time: 0.159s,  804.33/s  (0.137s,  935.64/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 95.901s\n",
      "Train: 7 [ 750/10009 (  7%)]  Loss: 3.75 (3.69)  Time: 0.132s,  972.56/s  (0.137s,  937.14/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 102.577s\n",
      "Train: 7 [ 800/10009 (  8%)]  Loss: 3.50 (3.69)  Time: 0.131s,  976.38/s  (0.137s,  937.65/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 109.345s\n",
      "Train: 7 [ 850/10009 (  8%)]  Loss: 4.08 (3.68)  Time: 0.132s,  967.54/s  (0.137s,  936.99/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 116.253s\n",
      "Train: 7 [ 900/10009 (  9%)]  Loss: 3.87 (3.68)  Time: 0.140s,  912.78/s  (0.137s,  935.72/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 123.251s\n",
      "Train: 7 [ 950/10009 (  9%)]  Loss: 3.66 (3.68)  Time: 0.133s,  962.90/s  (0.137s,  935.81/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 130.078s\n",
      "Train: 7 [1000/10009 ( 10%)]  Loss: 3.53 (3.68)  Time: 0.136s,  940.35/s  (0.137s,  936.64/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 136.796s\n",
      "Train: 7 [1050/10009 ( 10%)]  Loss: 3.56 (3.68)  Time: 0.132s,  970.04/s  (0.136s,  937.83/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 143.446s\n",
      "Train: 7 [1100/10009 ( 11%)]  Loss: 3.72 (3.69)  Time: 0.131s,  980.29/s  (0.136s,  938.14/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 150.221s\n",
      "Train: 7 [1150/10009 ( 11%)]  Loss: 3.80 (3.68)  Time: 0.133s,  963.36/s  (0.136s,  937.94/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 157.075s\n",
      "Train: 7 [1200/10009 ( 12%)]  Loss: 3.66 (3.68)  Time: 0.133s,  960.43/s  (0.137s,  937.58/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 163.962s\n",
      "Train: 7 [1250/10009 ( 12%)]  Loss: 3.82 (3.68)  Time: 0.134s,  956.82/s  (0.137s,  937.56/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 170.793s\n",
      "Train: 7 [1300/10009 ( 13%)]  Loss: 3.95 (3.69)  Time: 0.132s,  970.11/s  (0.136s,  937.78/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 177.578s\n",
      "Train: 7 [1350/10009 ( 13%)]  Loss: 3.58 (3.68)  Time: 0.132s,  968.74/s  (0.137s,  937.45/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 184.466s\n",
      "Train: 7 [1400/10009 ( 14%)]  Loss: 3.51 (3.68)  Time: 0.149s,  861.45/s  (0.137s,  936.93/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 191.400s\n",
      "Train: 7 [1450/10009 ( 14%)]  Loss: 3.65 (3.68)  Time: 0.132s,  969.24/s  (0.137s,  937.13/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 198.187s\n",
      "Train: 7 [1500/10009 ( 15%)]  Loss: 3.72 (3.68)  Time: 0.135s,  949.97/s  (0.137s,  936.90/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 205.067s\n",
      "Train: 7 [1550/10009 ( 15%)]  Loss: 3.77 (3.68)  Time: 0.138s,  927.30/s  (0.137s,  936.94/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 211.890s\n",
      "Train: 7 [1600/10009 ( 16%)]  Loss: 3.62 (3.68)  Time: 0.133s,  959.84/s  (0.137s,  936.85/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 218.740s\n",
      "Train: 7 [1650/10009 ( 16%)]  Loss: 4.02 (3.68)  Time: 0.133s,  960.00/s  (0.137s,  936.88/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 225.567s\n",
      "Train: 7 [1700/10009 ( 17%)]  Loss: 3.42 (3.68)  Time: 0.132s,  968.22/s  (0.137s,  937.04/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 232.357s\n",
      "Train: 7 [1750/10009 ( 17%)]  Loss: 3.65 (3.68)  Time: 0.149s,  861.86/s  (0.137s,  937.19/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 239.150s\n",
      "Train: 7 [1800/10009 ( 18%)]  Loss: 3.63 (3.68)  Time: 0.134s,  956.82/s  (0.137s,  937.17/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 245.983s\n",
      "Train: 7 [1850/10009 ( 18%)]  Loss: 3.83 (3.68)  Time: 0.133s,  964.26/s  (0.137s,  937.29/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 252.780s\n",
      "Train: 7 [1900/10009 ( 19%)]  Loss: 3.43 (3.68)  Time: 0.134s,  958.22/s  (0.137s,  937.56/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 259.534s\n",
      "Train: 7 [1950/10009 ( 19%)]  Loss: 3.31 (3.69)  Time: 0.135s,  945.05/s  (0.136s,  937.80/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 266.292s\n",
      "Train: 7 [2000/10009 ( 20%)]  Loss: 3.69 (3.69)  Time: 0.134s,  956.30/s  (0.136s,  938.21/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 272.997s\n",
      "Train: 7 [2050/10009 ( 20%)]  Loss: 3.90 (3.69)  Time: 0.133s,  962.32/s  (0.136s,  938.60/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 279.701s\n",
      "Train: 7 [2100/10009 ( 21%)]  Loss: 3.63 (3.68)  Time: 0.135s,  951.31/s  (0.136s,  938.75/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 286.475s\n",
      "Train: 7 [2150/10009 ( 21%)]  Loss: 3.67 (3.68)  Time: 0.133s,  959.75/s  (0.136s,  939.21/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 293.147s\n",
      "Train: 7 [2200/10009 ( 22%)]  Loss: 3.38 (3.68)  Time: 0.134s,  956.27/s  (0.136s,  939.27/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 299.942s\n",
      "Train: 7 [2250/10009 ( 22%)]  Loss: 3.55 (3.68)  Time: 0.134s,  955.44/s  (0.136s,  939.27/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 306.758s\n",
      "Train: 7 [2300/10009 ( 23%)]  Loss: 3.89 (3.68)  Time: 0.134s,  951.72/s  (0.136s,  939.14/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 313.614s\n",
      "Train: 7 [2350/10009 ( 23%)]  Loss: 3.75 (3.68)  Time: 0.133s,  960.33/s  (0.136s,  939.36/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 320.353s\n",
      "Train: 7 [2400/10009 ( 24%)]  Loss: 3.79 (3.68)  Time: 0.141s,  908.60/s  (0.136s,  939.20/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 327.221s\n",
      "Train: 7 [2450/10009 ( 24%)]  Loss: 3.85 (3.68)  Time: 0.134s,  954.20/s  (0.136s,  938.79/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 334.182s\n",
      "Train: 7 [2500/10009 ( 25%)]  Loss: 3.76 (3.68)  Time: 0.134s,  956.03/s  (0.136s,  938.94/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 340.945s\n",
      "Train: 7 [2550/10009 ( 25%)]  Loss: 3.56 (3.69)  Time: 0.131s,  974.80/s  (0.136s,  939.06/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 347.716s\n",
      "Train: 7 [2600/10009 ( 26%)]  Loss: 3.93 (3.69)  Time: 0.136s,  944.34/s  (0.136s,  939.24/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 354.463s\n",
      "Train: 7 [2650/10009 ( 26%)]  Loss: 3.66 (3.69)  Time: 0.132s,  967.62/s  (0.136s,  939.18/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 361.302s\n",
      "Train: 7 [2700/10009 ( 27%)]  Loss: 3.80 (3.68)  Time: 0.163s,  787.01/s  (0.136s,  938.86/s)  LR: 2.761e-05  Data: 0.009 (0.007)Time: 368.241s\n",
      "Train: 7 [2750/10009 ( 27%)]  Loss: 3.77 (3.68)  Time: 0.133s,  962.64/s  (0.136s,  938.92/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 375.036s\n",
      "Train: 7 [2800/10009 ( 28%)]  Loss: 3.66 (3.68)  Time: 0.133s,  965.35/s  (0.136s,  939.09/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 381.782s\n",
      "Train: 7 [2850/10009 ( 28%)]  Loss: 3.91 (3.69)  Time: 0.135s,  949.11/s  (0.136s,  939.24/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 388.533s\n",
      "Train: 7 [2900/10009 ( 29%)]  Loss: 3.84 (3.68)  Time: 0.133s,  962.13/s  (0.136s,  939.24/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 395.350s\n",
      "Train: 7 [2950/10009 ( 29%)]  Loss: 4.14 (3.69)  Time: 0.135s,  950.29/s  (0.136s,  939.33/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 402.122s\n",
      "Train: 7 [3000/10009 ( 30%)]  Loss: 3.52 (3.69)  Time: 0.136s,  944.06/s  (0.136s,  939.25/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 408.974s\n",
      "Train: 7 [3050/10009 ( 30%)]  Loss: 3.75 (3.69)  Time: 0.135s,  948.00/s  (0.136s,  939.25/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 415.786s\n",
      "Train: 7 [3100/10009 ( 31%)]  Loss: 3.77 (3.69)  Time: 0.133s,  961.05/s  (0.136s,  939.37/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 422.547s\n",
      "Train: 7 [3150/10009 ( 31%)]  Loss: 3.57 (3.69)  Time: 0.132s,  968.47/s  (0.136s,  939.48/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 429.308s\n",
      "Train: 7 [3200/10009 ( 32%)]  Loss: 3.74 (3.69)  Time: 0.133s,  962.25/s  (0.136s,  939.78/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 435.981s\n",
      "Train: 7 [3250/10009 ( 32%)]  Loss: 3.63 (3.69)  Time: 0.132s,  966.65/s  (0.136s,  940.05/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 442.664s\n",
      "Train: 7 [3300/10009 ( 33%)]  Loss: 3.52 (3.69)  Time: 0.152s,  843.93/s  (0.136s,  940.19/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 449.408s\n",
      "Train: 7 [3350/10009 ( 33%)]  Loss: 3.74 (3.68)  Time: 0.131s,  975.55/s  (0.136s,  940.18/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 456.218s\n",
      "Train: 7 [3400/10009 ( 34%)]  Loss: 3.47 (3.68)  Time: 0.131s,  980.66/s  (0.136s,  940.08/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 463.075s\n",
      "Train: 7 [3450/10009 ( 34%)]  Loss: 3.52 (3.68)  Time: 0.138s,  928.58/s  (0.136s,  940.17/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 469.835s\n",
      "Train: 7 [3500/10009 ( 35%)]  Loss: 3.86 (3.68)  Time: 0.132s,  968.50/s  (0.136s,  940.21/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 476.625s\n",
      "Train: 7 [3550/10009 ( 35%)]  Loss: 3.79 (3.68)  Time: 0.131s,  976.43/s  (0.136s,  940.21/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 483.429s\n",
      "Train: 7 [3600/10009 ( 36%)]  Loss: 3.54 (3.68)  Time: 0.130s,  981.58/s  (0.136s,  940.44/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 490.120s\n",
      "Train: 7 [3650/10009 ( 36%)]  Loss: 3.72 (3.68)  Time: 0.135s,  950.53/s  (0.136s,  940.54/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 496.869s\n",
      "Train: 7 [3700/10009 ( 37%)]  Loss: 3.76 (3.68)  Time: 0.131s,  980.52/s  (0.136s,  940.42/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 503.738s\n",
      "Train: 7 [3750/10009 ( 37%)]  Loss: 3.82 (3.68)  Time: 0.134s,  952.19/s  (0.136s,  940.33/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 510.592s\n",
      "Train: 7 [3800/10009 ( 38%)]  Loss: 3.78 (3.68)  Time: 0.160s,  799.79/s  (0.136s,  940.32/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 517.408s\n",
      "Train: 7 [3850/10009 ( 38%)]  Loss: 3.67 (3.68)  Time: 0.135s,  949.52/s  (0.136s,  940.40/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 524.166s\n",
      "Train: 7 [3900/10009 ( 39%)]  Loss: 3.56 (3.68)  Time: 0.143s,  897.84/s  (0.136s,  940.29/s)  LR: 2.761e-05  Data: 0.009 (0.007)Time: 531.035s\n",
      "Train: 7 [3950/10009 ( 39%)]  Loss: 3.72 (3.68)  Time: 0.134s,  958.19/s  (0.136s,  940.44/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 537.755s\n",
      "Train: 7 [4000/10009 ( 40%)]  Loss: 3.61 (3.68)  Time: 0.134s,  953.06/s  (0.136s,  940.48/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 544.540s\n",
      "Train: 7 [4050/10009 ( 40%)]  Loss: 3.48 (3.68)  Time: 0.132s,  969.34/s  (0.136s,  940.34/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 551.426s\n",
      "Train: 7 [4100/10009 ( 41%)]  Loss: 3.53 (3.68)  Time: 0.131s,  976.96/s  (0.136s,  940.44/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 558.171s\n",
      "Train: 7 [4150/10009 ( 41%)]  Loss: 3.80 (3.68)  Time: 0.130s,  985.94/s  (0.136s,  940.56/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 564.902s\n",
      "Train: 7 [4200/10009 ( 42%)]  Loss: 3.41 (3.68)  Time: 0.133s,  964.35/s  (0.136s,  940.51/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 571.742s\n",
      "Train: 7 [4250/10009 ( 42%)]  Loss: 3.55 (3.68)  Time: 0.136s,  944.53/s  (0.136s,  940.47/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 578.567s\n",
      "Train: 7 [4300/10009 ( 43%)]  Loss: 3.61 (3.68)  Time: 0.137s,  931.97/s  (0.136s,  940.47/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 585.375s\n",
      "Train: 7 [4350/10009 ( 43%)]  Loss: 3.60 (3.68)  Time: 0.135s,  951.12/s  (0.136s,  940.34/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 592.259s\n",
      "Train: 7 [4400/10009 ( 44%)]  Loss: 3.59 (3.68)  Time: 0.133s,  965.89/s  (0.136s,  940.36/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 599.053s\n",
      "Train: 7 [4450/10009 ( 44%)]  Loss: 4.01 (3.68)  Time: 0.125s, 1025.84/s  (0.136s,  940.32/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 605.888s\n",
      "Train: 7 [4500/10009 ( 45%)]  Loss: 3.98 (3.68)  Time: 0.124s, 1028.79/s  (0.136s,  941.05/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 612.215s\n",
      "Train: 7 [4550/10009 ( 45%)]  Loss: 3.78 (3.68)  Time: 0.125s, 1027.53/s  (0.136s,  941.84/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 618.499s\n",
      "Train: 7 [4600/10009 ( 46%)]  Loss: 3.65 (3.68)  Time: 0.127s, 1007.04/s  (0.136s,  942.59/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 624.798s\n",
      "Train: 7 [4650/10009 ( 46%)]  Loss: 3.82 (3.68)  Time: 0.126s, 1014.01/s  (0.136s,  943.35/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 631.074s\n",
      "Train: 7 [4700/10009 ( 47%)]  Loss: 3.54 (3.68)  Time: 0.128s, 1002.46/s  (0.136s,  943.91/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 637.484s\n",
      "Train: 7 [4750/10009 ( 47%)]  Loss: 3.46 (3.68)  Time: 0.124s, 1029.50/s  (0.135s,  944.69/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 643.728s\n",
      "Train: 7 [4800/10009 ( 48%)]  Loss: 3.68 (3.68)  Time: 0.125s, 1024.83/s  (0.135s,  945.31/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 650.076s\n",
      "Train: 7 [4850/10009 ( 48%)]  Loss: 3.30 (3.68)  Time: 0.127s, 1008.48/s  (0.135s,  945.77/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 656.529s\n",
      "Train: 7 [4900/10009 ( 49%)]  Loss: 3.64 (3.68)  Time: 0.124s, 1028.25/s  (0.135s,  946.17/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 663.019s\n",
      "Train: 7 [4950/10009 ( 49%)]  Loss: 3.56 (3.68)  Time: 0.125s, 1024.38/s  (0.135s,  946.74/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 669.377s\n",
      "Train: 7 [5000/10009 ( 50%)]  Loss: 3.81 (3.68)  Time: 0.127s, 1009.29/s  (0.135s,  947.33/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 675.719s\n",
      "Train: 7 [5050/10009 ( 50%)]  Loss: 3.71 (3.68)  Time: 0.124s, 1030.30/s  (0.135s,  947.88/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 682.076s\n",
      "Train: 7 [5100/10009 ( 51%)]  Loss: 3.71 (3.68)  Time: 0.126s, 1012.58/s  (0.135s,  948.52/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 688.365s\n",
      "Train: 7 [5150/10009 ( 51%)]  Loss: 3.57 (3.68)  Time: 0.126s, 1012.76/s  (0.135s,  949.11/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 694.680s\n",
      "Train: 7 [5200/10009 ( 52%)]  Loss: 3.47 (3.68)  Time: 0.125s, 1023.96/s  (0.135s,  949.64/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 701.030s\n",
      "Train: 7 [5250/10009 ( 52%)]  Loss: 3.68 (3.68)  Time: 0.125s, 1023.40/s  (0.135s,  950.18/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 707.370s\n",
      "Train: 7 [5300/10009 ( 53%)]  Loss: 3.70 (3.68)  Time: 0.125s, 1021.55/s  (0.135s,  950.67/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 713.735s\n",
      "Train: 7 [5350/10009 ( 53%)]  Loss: 3.56 (3.68)  Time: 0.123s, 1043.64/s  (0.135s,  951.20/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 720.066s\n",
      "Train: 7 [5400/10009 ( 54%)]  Loss: 3.79 (3.68)  Time: 0.123s, 1036.68/s  (0.135s,  951.55/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 726.525s\n",
      "Train: 7 [5450/10009 ( 54%)]  Loss: 3.65 (3.68)  Time: 0.124s, 1030.94/s  (0.134s,  952.05/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 732.867s\n",
      "Train: 7 [5500/10009 ( 55%)]  Loss: 3.46 (3.68)  Time: 0.126s, 1013.60/s  (0.134s,  952.56/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 739.195s\n",
      "Train: 7 [5550/10009 ( 55%)]  Loss: 3.35 (3.68)  Time: 0.126s, 1013.63/s  (0.134s,  952.92/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 745.629s\n",
      "Train: 7 [5600/10009 ( 56%)]  Loss: 4.02 (3.68)  Time: 0.126s, 1019.35/s  (0.134s,  953.48/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 751.903s\n",
      "Train: 7 [5650/10009 ( 56%)]  Loss: 3.95 (3.68)  Time: 0.124s, 1032.50/s  (0.134s,  954.00/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 758.200s\n",
      "Train: 7 [5700/10009 ( 57%)]  Loss: 3.48 (3.68)  Time: 0.127s, 1010.32/s  (0.134s,  954.52/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 764.493s\n",
      "Train: 7 [5750/10009 ( 57%)]  Loss: 3.57 (3.68)  Time: 0.132s,  972.66/s  (0.134s,  954.98/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 770.825s\n",
      "Train: 7 [5800/10009 ( 58%)]  Loss: 3.82 (3.68)  Time: 0.129s,  993.18/s  (0.134s,  955.49/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 777.118s\n",
      "Train: 7 [5850/10009 ( 58%)]  Loss: 3.80 (3.68)  Time: 0.125s, 1027.01/s  (0.134s,  955.94/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 783.445s\n",
      "Train: 7 [5900/10009 ( 59%)]  Loss: 3.62 (3.68)  Time: 0.125s, 1024.38/s  (0.134s,  956.38/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 789.772s\n",
      "Train: 7 [5950/10009 ( 59%)]  Loss: 3.87 (3.68)  Time: 0.124s, 1034.83/s  (0.134s,  956.78/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 796.138s\n",
      "Train: 7 [6000/10009 ( 60%)]  Loss: 3.78 (3.68)  Time: 0.123s, 1039.08/s  (0.134s,  957.26/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 802.425s\n",
      "Train: 7 [6050/10009 ( 60%)]  Loss: 3.79 (3.68)  Time: 0.125s, 1023.13/s  (0.134s,  957.65/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 808.781s\n",
      "Train: 7 [6100/10009 ( 61%)]  Loss: 4.02 (3.68)  Time: 0.124s, 1034.97/s  (0.134s,  957.95/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 815.202s\n",
      "Train: 7 [6150/10009 ( 61%)]  Loss: 3.44 (3.68)  Time: 0.133s,  964.93/s  (0.134s,  958.31/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 821.577s\n",
      "Train: 7 [6200/10009 ( 62%)]  Loss: 3.88 (3.68)  Time: 0.125s, 1026.37/s  (0.134s,  958.71/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 827.913s\n",
      "Train: 7 [6250/10009 ( 62%)]  Loss: 3.65 (3.68)  Time: 0.124s, 1032.90/s  (0.133s,  959.14/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 834.208s\n",
      "Train: 7 [6300/10009 ( 63%)]  Loss: 3.53 (3.68)  Time: 0.124s, 1028.48/s  (0.133s,  959.58/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 840.502s\n",
      "Train: 7 [6350/10009 ( 63%)]  Loss: 3.64 (3.68)  Time: 0.124s, 1033.99/s  (0.133s,  959.82/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 846.953s\n",
      "Train: 7 [6400/10009 ( 64%)]  Loss: 3.82 (3.68)  Time: 0.124s, 1033.64/s  (0.133s,  960.04/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 853.428s\n",
      "Train: 7 [6450/10009 ( 64%)]  Loss: 3.78 (3.68)  Time: 0.124s, 1029.14/s  (0.133s,  960.38/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 859.793s\n",
      "Train: 7 [6500/10009 ( 65%)]  Loss: 3.81 (3.68)  Time: 0.123s, 1040.63/s  (0.133s,  960.75/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 866.116s\n",
      "Train: 7 [6550/10009 ( 65%)]  Loss: 3.71 (3.68)  Time: 0.123s, 1036.59/s  (0.133s,  961.13/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 872.440s\n",
      "Train: 7 [6600/10009 ( 66%)]  Loss: 3.67 (3.68)  Time: 0.124s, 1028.83/s  (0.133s,  961.49/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 878.768s\n",
      "Train: 7 [6650/10009 ( 66%)]  Loss: 3.59 (3.68)  Time: 0.131s,  976.82/s  (0.133s,  961.87/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 885.076s\n",
      "Train: 7 [6700/10009 ( 67%)]  Loss: 3.82 (3.68)  Time: 0.125s, 1021.10/s  (0.133s,  962.19/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 891.432s\n",
      "Train: 7 [6750/10009 ( 67%)]  Loss: 3.90 (3.68)  Time: 0.125s, 1025.40/s  (0.133s,  962.47/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 897.822s\n",
      "Train: 7 [6800/10009 ( 68%)]  Loss: 3.48 (3.68)  Time: 0.125s, 1025.60/s  (0.133s,  962.77/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 904.188s\n",
      "Train: 7 [6850/10009 ( 68%)]  Loss: 4.11 (3.68)  Time: 0.127s, 1004.55/s  (0.133s,  963.10/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 910.524s\n",
      "Train: 7 [6900/10009 ( 69%)]  Loss: 3.42 (3.68)  Time: 0.124s, 1030.70/s  (0.133s,  963.32/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 916.964s\n",
      "Train: 7 [6950/10009 ( 69%)]  Loss: 3.74 (3.68)  Time: 0.123s, 1040.44/s  (0.133s,  963.70/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 923.243s\n",
      "Train: 7 [7000/10009 ( 70%)]  Loss: 3.70 (3.68)  Time: 0.124s, 1028.84/s  (0.133s,  964.02/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 929.575s\n",
      "Train: 7 [7050/10009 ( 70%)]  Loss: 3.94 (3.68)  Time: 0.124s, 1032.01/s  (0.133s,  964.32/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 935.915s\n",
      "Train: 7 [7100/10009 ( 71%)]  Loss: 3.89 (3.68)  Time: 0.127s, 1005.23/s  (0.133s,  964.56/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 942.321s\n",
      "Train: 7 [7150/10009 ( 71%)]  Loss: 3.57 (3.68)  Time: 0.125s, 1026.07/s  (0.133s,  964.82/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 948.700s\n",
      "Train: 7 [7200/10009 ( 72%)]  Loss: 3.52 (3.68)  Time: 0.123s, 1041.24/s  (0.133s,  965.16/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 954.998s\n",
      "Train: 7 [7250/10009 ( 72%)]  Loss: 3.53 (3.68)  Time: 0.133s,  964.74/s  (0.133s,  965.43/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 961.364s\n",
      "Train: 7 [7300/10009 ( 73%)]  Loss: 3.48 (3.68)  Time: 0.127s, 1009.13/s  (0.133s,  965.70/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 967.717s\n",
      "Train: 7 [7350/10009 ( 73%)]  Loss: 3.85 (3.68)  Time: 0.125s, 1027.27/s  (0.133s,  965.90/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 974.142s\n",
      "Train: 7 [7400/10009 ( 74%)]  Loss: 3.67 (3.68)  Time: 0.125s, 1025.21/s  (0.132s,  966.26/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 980.403s\n",
      "Train: 7 [7450/10009 ( 74%)]  Loss: 3.66 (3.68)  Time: 0.125s, 1026.76/s  (0.132s,  966.34/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 986.941s\n",
      "Train: 7 [7500/10009 ( 75%)]  Loss: 3.76 (3.68)  Time: 0.124s, 1028.75/s  (0.132s,  966.62/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 993.281s\n",
      "Train: 7 [7550/10009 ( 75%)]  Loss: 3.56 (3.68)  Time: 0.127s, 1004.20/s  (0.132s,  966.85/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 999.664s\n",
      "Train: 7 [7600/10009 ( 76%)]  Loss: 3.36 (3.68)  Time: 0.123s, 1038.71/s  (0.132s,  966.90/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 1006.228s\n",
      "Train: 7 [7650/10009 ( 76%)]  Loss: 4.33 (3.68)  Time: 0.148s,  864.60/s  (0.132s,  967.13/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1012.608s\n",
      "Train: 7 [7700/10009 ( 77%)]  Loss: 3.57 (3.68)  Time: 0.124s, 1034.29/s  (0.132s,  967.39/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1018.954s\n",
      "Train: 7 [7750/10009 ( 77%)]  Loss: 3.53 (3.68)  Time: 0.125s, 1020.68/s  (0.132s,  967.67/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1025.273s\n",
      "Train: 7 [7800/10009 ( 78%)]  Loss: 3.55 (3.68)  Time: 0.124s, 1032.16/s  (0.132s,  967.94/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1031.594s\n",
      "Train: 7 [7850/10009 ( 78%)]  Loss: 3.64 (3.68)  Time: 0.127s, 1009.02/s  (0.132s,  968.20/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1037.934s\n",
      "Train: 7 [7900/10009 ( 79%)]  Loss: 3.39 (3.68)  Time: 0.126s, 1018.46/s  (0.132s,  968.38/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 1044.347s\n",
      "Train: 7 [7950/10009 ( 79%)]  Loss: 3.70 (3.68)  Time: 0.125s, 1026.92/s  (0.132s,  968.64/s)  LR: 2.761e-05  Data: 0.005 (0.007)Time: 1050.673s\n",
      "Train: 7 [8000/10009 ( 80%)]  Loss: 3.58 (3.68)  Time: 0.124s, 1030.64/s  (0.132s,  968.94/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1056.956s\n",
      "Train: 7 [8050/10009 ( 80%)]  Loss: 3.61 (3.68)  Time: 0.124s, 1030.66/s  (0.132s,  969.22/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1063.247s\n",
      "Train: 7 [8100/10009 ( 81%)]  Loss: 3.57 (3.68)  Time: 0.124s, 1028.70/s  (0.132s,  969.47/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1069.583s\n",
      "Train: 7 [8150/10009 ( 81%)]  Loss: 3.91 (3.68)  Time: 0.135s,  950.12/s  (0.132s,  969.68/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 1075.944s\n",
      "Train: 7 [8200/10009 ( 82%)]  Loss: 3.54 (3.68)  Time: 0.128s,  996.88/s  (0.132s,  969.92/s)  LR: 2.761e-05  Data: 0.009 (0.007)Time: 1082.280s\n",
      "Train: 7 [8250/10009 ( 82%)]  Loss: 3.59 (3.68)  Time: 0.123s, 1037.41/s  (0.132s,  970.07/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1088.706s\n",
      "Train: 7 [8300/10009 ( 83%)]  Loss: 3.57 (3.68)  Time: 0.125s, 1025.59/s  (0.132s,  970.32/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1095.025s\n",
      "Train: 7 [8350/10009 ( 83%)]  Loss: 3.30 (3.68)  Time: 0.125s, 1023.16/s  (0.132s,  970.57/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1101.339s\n",
      "Train: 7 [8400/10009 ( 84%)]  Loss: 3.58 (3.68)  Time: 0.125s, 1020.28/s  (0.132s,  970.75/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1107.722s\n",
      "Train: 7 [8450/10009 ( 84%)]  Loss: 3.50 (3.68)  Time: 0.123s, 1037.38/s  (0.132s,  970.95/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1114.089s\n",
      "Train: 7 [8500/10009 ( 85%)]  Loss: 3.78 (3.68)  Time: 0.125s, 1025.57/s  (0.132s,  971.06/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1120.558s\n",
      "Train: 7 [8550/10009 ( 85%)]  Loss: 3.50 (3.68)  Time: 0.138s,  925.93/s  (0.132s,  971.22/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 1126.954s\n",
      "Train: 7 [8600/10009 ( 86%)]  Loss: 3.32 (3.68)  Time: 0.125s, 1024.06/s  (0.132s,  971.41/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1133.323s\n",
      "Train: 7 [8650/10009 ( 86%)]  Loss: 3.65 (3.68)  Time: 0.124s, 1033.60/s  (0.132s,  971.67/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1139.607s\n",
      "Train: 7 [8700/10009 ( 87%)]  Loss: 3.74 (3.68)  Time: 0.125s, 1026.06/s  (0.132s,  971.90/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1145.920s\n",
      "Train: 7 [8750/10009 ( 87%)]  Loss: 3.45 (3.68)  Time: 0.124s, 1032.47/s  (0.132s,  972.14/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1152.227s\n",
      "Train: 7 [8800/10009 ( 88%)]  Loss: 3.62 (3.68)  Time: 0.125s, 1026.20/s  (0.132s,  972.40/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1158.502s\n",
      "Train: 7 [8850/10009 ( 88%)]  Loss: 3.88 (3.68)  Time: 0.124s, 1036.03/s  (0.132s,  972.55/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1164.900s\n",
      "Train: 7 [8900/10009 ( 89%)]  Loss: 3.59 (3.68)  Time: 0.125s, 1027.42/s  (0.132s,  972.80/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1171.182s\n",
      "Train: 7 [8950/10009 ( 89%)]  Loss: 3.69 (3.68)  Time: 0.124s, 1033.46/s  (0.132s,  973.03/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1177.486s\n",
      "Train: 7 [9000/10009 ( 90%)]  Loss: 3.49 (3.68)  Time: 0.124s, 1034.53/s  (0.132s,  973.25/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1183.790s\n",
      "Train: 7 [9050/10009 ( 90%)]  Loss: 3.61 (3.68)  Time: 0.125s, 1027.86/s  (0.131s,  973.52/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1190.041s\n",
      "Train: 7 [9100/10009 ( 91%)]  Loss: 3.29 (3.68)  Time: 0.125s, 1025.62/s  (0.131s,  973.73/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1196.347s\n",
      "Train: 7 [9150/10009 ( 91%)]  Loss: 3.58 (3.67)  Time: 0.127s, 1011.83/s  (0.131s,  973.94/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1202.666s\n",
      "Train: 7 [9200/10009 ( 92%)]  Loss: 3.52 (3.67)  Time: 0.126s, 1017.26/s  (0.131s,  974.17/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1208.954s\n",
      "Train: 7 [9250/10009 ( 92%)]  Loss: 3.52 (3.67)  Time: 0.126s, 1016.81/s  (0.131s,  974.40/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 1215.230s\n",
      "Train: 7 [9300/10009 ( 93%)]  Loss: 3.76 (3.67)  Time: 0.124s, 1030.88/s  (0.131s,  974.59/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1221.564s\n",
      "Train: 7 [9350/10009 ( 93%)]  Loss: 4.08 (3.67)  Time: 0.125s, 1022.49/s  (0.131s,  974.74/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1227.946s\n",
      "Train: 7 [9400/10009 ( 94%)]  Loss: 3.77 (3.67)  Time: 0.124s, 1032.91/s  (0.131s,  974.94/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1234.252s\n",
      "Train: 7 [9450/10009 ( 94%)]  Loss: 3.72 (3.67)  Time: 0.124s, 1030.77/s  (0.131s,  975.16/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1240.540s\n",
      "Train: 7 [9500/10009 ( 95%)]  Loss: 3.62 (3.67)  Time: 0.124s, 1029.06/s  (0.131s,  975.37/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1246.831s\n",
      "Train: 7 [9550/10009 ( 95%)]  Loss: 3.80 (3.67)  Time: 0.123s, 1036.73/s  (0.131s,  975.57/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1253.134s\n",
      "Train: 7 [9600/10009 ( 96%)]  Loss: 3.62 (3.67)  Time: 0.124s, 1028.51/s  (0.131s,  975.78/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1259.424s\n",
      "Train: 7 [9650/10009 ( 96%)]  Loss: 3.95 (3.67)  Time: 0.125s, 1021.00/s  (0.131s,  975.99/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1265.715s\n",
      "Train: 7 [9700/10009 ( 97%)]  Loss: 3.50 (3.67)  Time: 0.125s, 1023.33/s  (0.131s,  976.16/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1272.047s\n",
      "Train: 7 [9750/10009 ( 97%)]  Loss: 3.56 (3.67)  Time: 0.125s, 1027.74/s  (0.131s,  976.37/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1278.329s\n",
      "Train: 7 [9800/10009 ( 98%)]  Loss: 3.73 (3.67)  Time: 0.125s, 1023.65/s  (0.131s,  976.50/s)  LR: 2.761e-05  Data: 0.006 (0.007)Time: 1284.718s\n",
      "Train: 7 [9850/10009 ( 98%)]  Loss: 3.56 (3.67)  Time: 0.125s, 1026.24/s  (0.131s,  976.68/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1291.027s\n",
      "Train: 7 [9900/10009 ( 99%)]  Loss: 3.29 (3.67)  Time: 0.125s, 1027.62/s  (0.131s,  976.88/s)  LR: 2.761e-05  Data: 0.007 (0.007)Time: 1297.319s\n",
      "Train: 7 [9950/10009 ( 99%)]  Loss: 4.15 (3.67)  Time: 0.138s,  928.72/s  (0.131s,  976.99/s)  LR: 2.761e-05  Data: 0.008 (0.007)Time: 1303.727s\n",
      "Train: 7 [10000/10009 (100%)]  Loss: 3.77 (3.67)  Time: 0.169s,  755.26/s  (0.131s,  977.17/s)  LR: 2.761e-05  Data: 0.052 (0.007)Time: 1310.026s\n",
      "Test: [   0/390]  Time: 0.651 (0.651)  Loss:   2.054 ( 2.054)  Acc@1:  60.938 ( 60.938)  Acc@5:  78.125 ( 78.125)\n",
      "Test: [  50/390]  Time: 0.040 (0.138)  Loss:   1.163 ( 2.492)  Acc@1:  76.562 ( 48.407)  Acc@5:  90.625 ( 70.925)\n",
      "Test: [ 100/390]  Time: 0.094 (0.131)  Loss:   2.443 ( 2.512)  Acc@1:  44.531 ( 45.096)  Acc@5:  75.000 ( 71.310)\n",
      "Test: [ 150/390]  Time: 0.039 (0.136)  Loss:   2.071 ( 2.444)  Acc@1:  47.656 ( 46.218)  Acc@5:  78.906 ( 72.460)\n",
      "Test: [ 200/390]  Time: 0.040 (0.133)  Loss:   3.372 ( 2.625)  Acc@1:  32.031 ( 43.517)  Acc@5:  56.250 ( 69.236)\n",
      "Test: [ 250/390]  Time: 0.043 (0.132)  Loss:   2.700 ( 2.746)  Acc@1:  43.750 ( 41.798)  Acc@5:  66.406 ( 67.079)\n",
      "Test: [ 300/390]  Time: 0.271 (0.131)  Loss:   2.790 ( 2.839)  Acc@1:  44.531 ( 40.381)  Acc@5:  67.188 ( 65.360)\n",
      "Test: [ 350/390]  Time: 0.038 (0.130)  Loss:   3.328 ( 2.918)  Acc@1:  32.812 ( 39.049)  Acc@5:  57.812 ( 63.991)\n",
      "Test: [ 390/390]  Time: 0.025 (0.130)  Loss:   4.075 ( 2.885)  Acc@1:  15.000 ( 39.734)  Acc@5:  50.000 ( 64.512)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-7.pth.tar', 39.734)\n",
      " ('./output/budgeted/checkpoint-6.pth.tar', 36.718)\n",
      " ('./output/budgeted/checkpoint-5.pth.tar', 34.288)\n",
      "\n",
      "Train: 8 [   0/10009 (  0%)]  Loss: 3.58 (3.58)  Time: 0.639s,  200.16/s  (0.639s,  200.16/s)  LR: 2.239e-05  Data: 0.522 (0.522)Time: 0.640s\n",
      "Train: 8 [  50/10009 (  0%)]  Loss: 3.63 (3.58)  Time: 0.122s, 1046.52/s  (0.143s,  895.98/s)  LR: 2.239e-05  Data: 0.004 (0.022)Time: 7.286s\n",
      "Train: 8 [ 100/10009 (  1%)]  Loss: 3.72 (3.60)  Time: 0.122s, 1046.95/s  (0.134s,  952.27/s)  LR: 2.239e-05  Data: 0.004 (0.014)Time: 13.576s\n",
      "Train: 8 [ 150/10009 (  1%)]  Loss: 3.44 (3.59)  Time: 0.124s, 1032.92/s  (0.132s,  969.77/s)  LR: 2.239e-05  Data: 0.006 (0.012)Time: 19.931s\n",
      "Train: 8 [ 200/10009 (  2%)]  Loss: 3.92 (3.59)  Time: 0.125s, 1021.32/s  (0.130s,  982.98/s)  LR: 2.239e-05  Data: 0.006 (0.011)Time: 26.174s\n",
      "Train: 8 [ 250/10009 (  2%)]  Loss: 3.75 (3.58)  Time: 0.125s, 1024.89/s  (0.129s,  990.70/s)  LR: 2.239e-05  Data: 0.006 (0.010)Time: 32.430s\n",
      "Train: 8 [ 300/10009 (  3%)]  Loss: 3.64 (3.58)  Time: 0.124s, 1034.50/s  (0.129s,  993.36/s)  LR: 2.239e-05  Data: 0.006 (0.009)Time: 38.786s\n",
      "Train: 8 [ 350/10009 (  3%)]  Loss: 3.46 (3.58)  Time: 0.125s, 1026.68/s  (0.129s,  995.20/s)  LR: 2.239e-05  Data: 0.005 (0.009)Time: 45.145s\n",
      "Train: 8 [ 400/10009 (  4%)]  Loss: 3.50 (3.57)  Time: 0.126s, 1015.91/s  (0.129s,  996.05/s)  LR: 2.239e-05  Data: 0.007 (0.009)Time: 51.532s\n",
      "Train: 8 [ 450/10009 (  4%)]  Loss: 3.48 (3.57)  Time: 0.125s, 1022.22/s  (0.128s,  998.04/s)  LR: 2.239e-05  Data: 0.006 (0.008)Time: 57.841s\n",
      "Train: 8 [ 500/10009 (  5%)]  Loss: 3.51 (3.57)  Time: 0.127s, 1008.48/s  (0.128s,  998.49/s)  LR: 2.239e-05  Data: 0.005 (0.008)Time: 64.225s\n",
      "Train: 8 [ 550/10009 (  5%)]  Loss: 3.65 (3.57)  Time: 0.123s, 1039.53/s  (0.128s, 1000.57/s)  LR: 2.239e-05  Data: 0.006 (0.008)Time: 70.488s\n",
      "Train: 8 [ 600/10009 (  6%)]  Loss: 3.58 (3.57)  Time: 0.125s, 1025.74/s  (0.128s, 1002.36/s)  LR: 2.239e-05  Data: 0.006 (0.008)Time: 76.747s\n",
      "Train: 8 [ 650/10009 (  6%)]  Loss: 3.41 (3.57)  Time: 0.124s, 1029.10/s  (0.128s, 1002.99/s)  LR: 2.239e-05  Data: 0.007 (0.008)Time: 83.079s\n",
      "Train: 8 [ 700/10009 (  7%)]  Loss: 3.51 (3.57)  Time: 0.125s, 1024.57/s  (0.127s, 1004.26/s)  LR: 2.239e-05  Data: 0.006 (0.008)Time: 89.347s\n",
      "Train: 8 [ 750/10009 (  7%)]  Loss: 3.65 (3.57)  Time: 0.124s, 1034.73/s  (0.127s, 1005.28/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 95.623s\n",
      "Train: 8 [ 800/10009 (  8%)]  Loss: 3.40 (3.57)  Time: 0.127s, 1010.59/s  (0.127s, 1006.14/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 101.902s\n",
      "Train: 8 [ 850/10009 (  8%)]  Loss: 3.66 (3.57)  Time: 0.151s,  849.96/s  (0.127s, 1006.78/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 108.194s\n",
      "Train: 8 [ 900/10009 (  9%)]  Loss: 3.58 (3.57)  Time: 0.125s, 1027.64/s  (0.127s, 1006.42/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 114.592s\n",
      "Train: 8 [ 950/10009 (  9%)]  Loss: 3.50 (3.57)  Time: 0.124s, 1035.08/s  (0.127s, 1006.60/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 120.929s\n",
      "Train: 8 [1000/10009 ( 10%)]  Loss: 3.41 (3.57)  Time: 0.122s, 1045.69/s  (0.127s, 1006.83/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 127.259s\n",
      "Train: 8 [1050/10009 ( 10%)]  Loss: 3.83 (3.57)  Time: 0.124s, 1033.57/s  (0.127s, 1006.85/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 133.613s\n",
      "Train: 8 [1100/10009 ( 11%)]  Loss: 3.73 (3.57)  Time: 0.126s, 1014.26/s  (0.127s, 1007.24/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 139.914s\n",
      "Train: 8 [1150/10009 ( 11%)]  Loss: 3.79 (3.57)  Time: 0.154s,  830.23/s  (0.127s, 1007.05/s)  LR: 2.239e-05  Data: 0.012 (0.007)Time: 146.296s\n",
      "Train: 8 [1200/10009 ( 12%)]  Loss: 3.66 (3.57)  Time: 0.124s, 1033.43/s  (0.127s, 1007.19/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 152.630s\n",
      "Train: 8 [1250/10009 ( 12%)]  Loss: 3.61 (3.57)  Time: 0.127s, 1009.10/s  (0.127s, 1007.30/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 158.967s\n",
      "Train: 8 [1300/10009 ( 13%)]  Loss: 3.71 (3.57)  Time: 0.124s, 1033.22/s  (0.127s, 1006.77/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 165.408s\n",
      "Train: 8 [1350/10009 ( 13%)]  Loss: 3.56 (3.57)  Time: 0.124s, 1034.48/s  (0.127s, 1006.94/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 171.735s\n",
      "Train: 8 [1400/10009 ( 14%)]  Loss: 3.69 (3.57)  Time: 0.124s, 1034.57/s  (0.127s, 1006.90/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 178.099s\n",
      "Train: 8 [1450/10009 ( 14%)]  Loss: 3.61 (3.57)  Time: 0.125s, 1025.61/s  (0.127s, 1007.16/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 184.406s\n",
      "Train: 8 [1500/10009 ( 15%)]  Loss: 3.19 (3.57)  Time: 0.124s, 1030.63/s  (0.127s, 1007.31/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 190.734s\n",
      "Train: 8 [1550/10009 ( 15%)]  Loss: 3.24 (3.57)  Time: 0.125s, 1024.49/s  (0.127s, 1007.52/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 197.046s\n",
      "Train: 8 [1600/10009 ( 16%)]  Loss: 3.65 (3.57)  Time: 0.123s, 1041.24/s  (0.127s, 1007.52/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 203.397s\n",
      "Train: 8 [1650/10009 ( 16%)]  Loss: 3.64 (3.57)  Time: 0.132s,  972.27/s  (0.127s, 1007.27/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 209.803s\n",
      "Train: 8 [1700/10009 ( 17%)]  Loss: 3.67 (3.57)  Time: 0.125s, 1024.19/s  (0.127s, 1007.07/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 216.200s\n",
      "Train: 8 [1750/10009 ( 17%)]  Loss: 3.47 (3.57)  Time: 0.125s, 1028.10/s  (0.127s, 1007.23/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 222.518s\n",
      "Train: 8 [1800/10009 ( 18%)]  Loss: 3.49 (3.57)  Time: 0.125s, 1023.44/s  (0.127s, 1006.99/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 228.927s\n",
      "Train: 8 [1850/10009 ( 18%)]  Loss: 3.82 (3.57)  Time: 0.128s,  996.40/s  (0.127s, 1007.18/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 235.239s\n",
      "Train: 8 [1900/10009 ( 19%)]  Loss: 3.69 (3.57)  Time: 0.124s, 1036.32/s  (0.127s, 1007.14/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 241.602s\n",
      "Train: 8 [1950/10009 ( 19%)]  Loss: 3.39 (3.57)  Time: 0.125s, 1020.03/s  (0.127s, 1007.56/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 247.855s\n",
      "Train: 8 [2000/10009 ( 20%)]  Loss: 3.51 (3.57)  Time: 0.123s, 1039.90/s  (0.127s, 1007.87/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 254.127s\n",
      "Train: 8 [2050/10009 ( 20%)]  Loss: 3.70 (3.57)  Time: 0.123s, 1038.05/s  (0.127s, 1008.06/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 260.428s\n",
      "Train: 8 [2100/10009 ( 21%)]  Loss: 3.68 (3.57)  Time: 0.134s,  958.52/s  (0.127s, 1008.26/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 266.724s\n",
      "Train: 8 [2150/10009 ( 21%)]  Loss: 3.48 (3.57)  Time: 0.124s, 1031.89/s  (0.127s, 1008.42/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 273.028s\n",
      "Train: 8 [2200/10009 ( 22%)]  Loss: 3.58 (3.57)  Time: 0.124s, 1031.72/s  (0.127s, 1008.59/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 279.328s\n",
      "Train: 8 [2250/10009 ( 22%)]  Loss: 3.44 (3.57)  Time: 0.124s, 1036.28/s  (0.127s, 1008.76/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 285.625s\n",
      "Train: 8 [2300/10009 ( 23%)]  Loss: 3.47 (3.57)  Time: 0.126s, 1019.48/s  (0.127s, 1008.98/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 291.907s\n",
      "Train: 8 [2350/10009 ( 23%)]  Loss: 4.01 (3.57)  Time: 0.124s, 1032.62/s  (0.127s, 1009.33/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 298.146s\n",
      "Train: 8 [2400/10009 ( 24%)]  Loss: 3.75 (3.57)  Time: 0.125s, 1022.93/s  (0.127s, 1009.11/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 304.552s\n",
      "Train: 8 [2450/10009 ( 24%)]  Loss: 3.47 (3.57)  Time: 0.126s, 1019.80/s  (0.127s, 1009.07/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 310.907s\n",
      "Train: 8 [2500/10009 ( 25%)]  Loss: 3.43 (3.57)  Time: 0.126s, 1017.44/s  (0.127s, 1009.23/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 317.200s\n",
      "Train: 8 [2550/10009 ( 25%)]  Loss: 3.73 (3.57)  Time: 0.125s, 1023.91/s  (0.127s, 1009.16/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 323.562s\n",
      "Train: 8 [2600/10009 ( 26%)]  Loss: 3.55 (3.57)  Time: 0.125s, 1022.62/s  (0.127s, 1009.33/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 329.850s\n",
      "Train: 8 [2650/10009 ( 26%)]  Loss: 3.60 (3.57)  Time: 0.125s, 1021.43/s  (0.127s, 1009.20/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 336.235s\n",
      "Train: 8 [2700/10009 ( 27%)]  Loss: 3.54 (3.57)  Time: 0.125s, 1025.50/s  (0.127s, 1009.42/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 342.501s\n",
      "Train: 8 [2750/10009 ( 27%)]  Loss: 3.50 (3.57)  Time: 0.122s, 1045.22/s  (0.127s, 1009.20/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 348.919s\n",
      "Train: 8 [2800/10009 ( 28%)]  Loss: 3.58 (3.57)  Time: 0.125s, 1024.72/s  (0.127s, 1009.31/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 355.219s\n",
      "Train: 8 [2850/10009 ( 28%)]  Loss: 3.51 (3.57)  Time: 0.123s, 1043.77/s  (0.127s, 1009.35/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 361.548s\n",
      "Train: 8 [2900/10009 ( 29%)]  Loss: 3.43 (3.57)  Time: 0.124s, 1029.78/s  (0.127s, 1009.28/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 367.912s\n",
      "Train: 8 [2950/10009 ( 29%)]  Loss: 3.51 (3.57)  Time: 0.140s,  915.80/s  (0.127s, 1009.43/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 374.198s\n",
      "Train: 8 [3000/10009 ( 30%)]  Loss: 3.34 (3.57)  Time: 0.124s, 1032.82/s  (0.127s, 1009.45/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 380.530s\n",
      "Train: 8 [3050/10009 ( 30%)]  Loss: 3.17 (3.57)  Time: 0.145s,  883.52/s  (0.127s, 1009.46/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 386.869s\n",
      "Train: 8 [3100/10009 ( 31%)]  Loss: 3.45 (3.57)  Time: 0.125s, 1026.71/s  (0.127s, 1009.63/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 393.141s\n",
      "Train: 8 [3150/10009 ( 31%)]  Loss: 3.62 (3.56)  Time: 0.126s, 1018.99/s  (0.127s, 1009.53/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 399.518s\n",
      "Train: 8 [3200/10009 ( 32%)]  Loss: 3.38 (3.56)  Time: 0.125s, 1022.07/s  (0.127s, 1009.63/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 405.819s\n",
      "Train: 8 [3250/10009 ( 32%)]  Loss: 3.30 (3.56)  Time: 0.127s, 1008.26/s  (0.127s, 1009.77/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 412.100s\n",
      "Train: 8 [3300/10009 ( 33%)]  Loss: 3.45 (3.56)  Time: 0.125s, 1024.04/s  (0.127s, 1009.83/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 418.414s\n",
      "Train: 8 [3350/10009 ( 33%)]  Loss: 3.48 (3.56)  Time: 0.124s, 1029.29/s  (0.127s, 1010.06/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 424.653s\n",
      "Train: 8 [3400/10009 ( 34%)]  Loss: 3.67 (3.56)  Time: 0.125s, 1022.92/s  (0.127s, 1010.16/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 430.947s\n",
      "Train: 8 [3450/10009 ( 34%)]  Loss: 3.60 (3.56)  Time: 0.124s, 1030.74/s  (0.127s, 1010.30/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 437.225s\n",
      "Train: 8 [3500/10009 ( 35%)]  Loss: 3.62 (3.56)  Time: 0.123s, 1041.39/s  (0.127s, 1010.33/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 443.544s\n",
      "Train: 8 [3550/10009 ( 35%)]  Loss: 3.51 (3.56)  Time: 0.126s, 1013.21/s  (0.127s, 1010.37/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 449.863s\n",
      "Train: 8 [3600/10009 ( 36%)]  Loss: 3.54 (3.56)  Time: 0.124s, 1035.84/s  (0.127s, 1010.54/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 456.121s\n",
      "Train: 8 [3650/10009 ( 36%)]  Loss: 3.63 (3.56)  Time: 0.124s, 1035.72/s  (0.127s, 1010.60/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 462.427s\n",
      "Train: 8 [3700/10009 ( 37%)]  Loss: 3.47 (3.56)  Time: 0.124s, 1028.20/s  (0.127s, 1010.76/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 468.685s\n",
      "Train: 8 [3750/10009 ( 37%)]  Loss: 3.57 (3.56)  Time: 0.125s, 1025.08/s  (0.127s, 1010.95/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 474.927s\n",
      "Train: 8 [3800/10009 ( 38%)]  Loss: 3.58 (3.56)  Time: 0.124s, 1029.61/s  (0.127s, 1011.00/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 481.233s\n",
      "Train: 8 [3850/10009 ( 38%)]  Loss: 3.60 (3.56)  Time: 0.124s, 1028.40/s  (0.127s, 1011.15/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 487.493s\n",
      "Train: 8 [3900/10009 ( 39%)]  Loss: 3.50 (3.56)  Time: 0.124s, 1030.37/s  (0.127s, 1011.22/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 493.785s\n",
      "Train: 8 [3950/10009 ( 39%)]  Loss: 3.56 (3.56)  Time: 0.125s, 1022.97/s  (0.127s, 1011.33/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 500.062s\n",
      "Train: 8 [4000/10009 ( 40%)]  Loss: 3.21 (3.56)  Time: 0.125s, 1023.86/s  (0.127s, 1011.34/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 506.384s\n",
      "Train: 8 [4050/10009 ( 40%)]  Loss: 3.86 (3.56)  Time: 0.123s, 1042.82/s  (0.127s, 1011.43/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 512.667s\n",
      "Train: 8 [4100/10009 ( 41%)]  Loss: 3.63 (3.56)  Time: 0.123s, 1037.29/s  (0.127s, 1011.46/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 518.981s\n",
      "Train: 8 [4150/10009 ( 41%)]  Loss: 3.54 (3.56)  Time: 0.125s, 1020.33/s  (0.127s, 1011.40/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 525.338s\n",
      "Train: 8 [4200/10009 ( 42%)]  Loss: 3.85 (3.56)  Time: 0.125s, 1022.98/s  (0.127s, 1011.44/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 531.643s\n",
      "Train: 8 [4250/10009 ( 42%)]  Loss: 3.73 (3.56)  Time: 0.125s, 1020.70/s  (0.127s, 1011.46/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 537.960s\n",
      "Train: 8 [4300/10009 ( 43%)]  Loss: 3.40 (3.56)  Time: 0.159s,  805.19/s  (0.127s, 1011.40/s)  LR: 2.239e-05  Data: 0.008 (0.006)Time: 544.323s\n",
      "Train: 8 [4350/10009 ( 43%)]  Loss: 3.37 (3.56)  Time: 0.124s, 1028.33/s  (0.127s, 1011.48/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 550.603s\n",
      "Train: 8 [4400/10009 ( 44%)]  Loss: 3.53 (3.56)  Time: 0.125s, 1026.97/s  (0.127s, 1011.47/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 556.939s\n",
      "Train: 8 [4450/10009 ( 44%)]  Loss: 3.54 (3.56)  Time: 0.124s, 1034.42/s  (0.127s, 1011.51/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 563.243s\n",
      "Train: 8 [4500/10009 ( 45%)]  Loss: 3.51 (3.56)  Time: 0.125s, 1027.18/s  (0.127s, 1011.53/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 569.561s\n",
      "Train: 8 [4550/10009 ( 45%)]  Loss: 3.67 (3.56)  Time: 0.125s, 1020.08/s  (0.127s, 1011.62/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 575.832s\n",
      "Train: 8 [4600/10009 ( 46%)]  Loss: 3.74 (3.56)  Time: 0.125s, 1025.44/s  (0.127s, 1011.56/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 582.195s\n",
      "Train: 8 [4650/10009 ( 46%)]  Loss: 3.72 (3.56)  Time: 0.125s, 1020.78/s  (0.127s, 1011.69/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 588.447s\n",
      "Train: 8 [4700/10009 ( 47%)]  Loss: 3.37 (3.56)  Time: 0.125s, 1023.53/s  (0.127s, 1011.51/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 594.878s\n",
      "Train: 8 [4750/10009 ( 47%)]  Loss: 3.76 (3.56)  Time: 0.125s, 1027.50/s  (0.127s, 1011.61/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 601.149s\n",
      "Train: 8 [4800/10009 ( 48%)]  Loss: 3.62 (3.56)  Time: 0.144s,  891.71/s  (0.127s, 1011.39/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 607.603s\n",
      "Train: 8 [4850/10009 ( 48%)]  Loss: 3.64 (3.56)  Time: 0.126s, 1012.89/s  (0.127s, 1011.28/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 614.003s\n",
      "Train: 8 [4900/10009 ( 49%)]  Loss: 3.52 (3.56)  Time: 0.124s, 1030.84/s  (0.127s, 1011.35/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 620.287s\n",
      "Train: 8 [4950/10009 ( 49%)]  Loss: 3.80 (3.56)  Time: 0.125s, 1025.84/s  (0.127s, 1011.43/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 626.567s\n",
      "Train: 8 [5000/10009 ( 50%)]  Loss: 3.38 (3.56)  Time: 0.128s,  998.18/s  (0.127s, 1011.48/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 632.859s\n",
      "Train: 8 [5050/10009 ( 50%)]  Loss: 3.51 (3.56)  Time: 0.125s, 1021.25/s  (0.127s, 1011.48/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 639.186s\n",
      "Train: 8 [5100/10009 ( 51%)]  Loss: 3.43 (3.56)  Time: 0.123s, 1038.98/s  (0.127s, 1011.53/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 645.485s\n",
      "Train: 8 [5150/10009 ( 51%)]  Loss: 3.56 (3.56)  Time: 0.125s, 1023.68/s  (0.127s, 1011.64/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 651.739s\n",
      "Train: 8 [5200/10009 ( 52%)]  Loss: 3.51 (3.56)  Time: 0.125s, 1021.48/s  (0.127s, 1011.72/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 658.012s\n",
      "Train: 8 [5250/10009 ( 52%)]  Loss: 3.42 (3.56)  Time: 0.130s,  981.03/s  (0.127s, 1011.79/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 664.291s\n",
      "Train: 8 [5300/10009 ( 53%)]  Loss: 3.46 (3.56)  Time: 0.125s, 1020.89/s  (0.127s, 1011.70/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 670.681s\n",
      "Train: 8 [5350/10009 ( 53%)]  Loss: 3.38 (3.56)  Time: 0.153s,  839.34/s  (0.127s, 1011.54/s)  LR: 2.239e-05  Data: 0.008 (0.006)Time: 677.109s\n",
      "Train: 8 [5400/10009 ( 54%)]  Loss: 3.49 (3.56)  Time: 0.126s, 1019.47/s  (0.127s, 1011.58/s)  LR: 2.239e-05  Data: 0.008 (0.006)Time: 683.409s\n",
      "Train: 8 [5450/10009 ( 54%)]  Loss: 3.58 (3.56)  Time: 0.125s, 1021.50/s  (0.127s, 1011.64/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 689.695s\n",
      "Train: 8 [5500/10009 ( 55%)]  Loss: 3.48 (3.56)  Time: 0.124s, 1028.64/s  (0.127s, 1011.66/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 696.008s\n",
      "Train: 8 [5550/10009 ( 55%)]  Loss: 3.67 (3.56)  Time: 0.141s,  904.91/s  (0.127s, 1011.65/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 702.344s\n",
      "Train: 8 [5600/10009 ( 56%)]  Loss: 3.73 (3.56)  Time: 0.125s, 1025.59/s  (0.127s, 1011.70/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 708.632s\n",
      "Train: 8 [5650/10009 ( 56%)]  Loss: 3.39 (3.56)  Time: 0.125s, 1023.90/s  (0.127s, 1011.69/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 714.967s\n",
      "Train: 8 [5700/10009 ( 57%)]  Loss: 3.43 (3.56)  Time: 0.126s, 1019.41/s  (0.127s, 1011.77/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 721.236s\n",
      "Train: 8 [5750/10009 ( 57%)]  Loss: 3.83 (3.56)  Time: 0.123s, 1042.35/s  (0.127s, 1011.69/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 727.622s\n",
      "Train: 8 [5800/10009 ( 58%)]  Loss: 3.61 (3.56)  Time: 0.124s, 1030.03/s  (0.127s, 1011.71/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 733.933s\n",
      "Train: 8 [5850/10009 ( 58%)]  Loss: 3.58 (3.56)  Time: 0.124s, 1031.74/s  (0.127s, 1011.73/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 740.240s\n",
      "Train: 8 [5900/10009 ( 59%)]  Loss: 3.63 (3.56)  Time: 0.125s, 1026.46/s  (0.127s, 1011.75/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 746.556s\n",
      "Train: 8 [5950/10009 ( 59%)]  Loss: 3.66 (3.56)  Time: 0.124s, 1032.31/s  (0.127s, 1011.71/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 752.907s\n",
      "Train: 8 [6000/10009 ( 60%)]  Loss: 3.72 (3.56)  Time: 0.124s, 1033.37/s  (0.127s, 1011.74/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 759.209s\n",
      "Train: 8 [6050/10009 ( 60%)]  Loss: 3.74 (3.56)  Time: 0.129s,  991.74/s  (0.127s, 1011.68/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 765.580s\n",
      "Train: 8 [6100/10009 ( 61%)]  Loss: 3.42 (3.56)  Time: 0.134s,  956.75/s  (0.127s, 1011.65/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 771.933s\n",
      "Train: 8 [6150/10009 ( 61%)]  Loss: 3.56 (3.56)  Time: 0.125s, 1023.00/s  (0.127s, 1011.63/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 778.270s\n",
      "Train: 8 [6200/10009 ( 62%)]  Loss: 3.74 (3.56)  Time: 0.125s, 1026.54/s  (0.127s, 1011.67/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 784.569s\n",
      "Train: 8 [6250/10009 ( 62%)]  Loss: 3.66 (3.56)  Time: 0.132s,  967.10/s  (0.127s, 1011.75/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 790.832s\n",
      "Train: 8 [6300/10009 ( 63%)]  Loss: 3.39 (3.56)  Time: 0.123s, 1037.05/s  (0.127s, 1011.76/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 797.152s\n",
      "Train: 8 [6350/10009 ( 63%)]  Loss: 3.62 (3.56)  Time: 0.125s, 1024.78/s  (0.127s, 1011.59/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 803.613s\n",
      "Train: 8 [6400/10009 ( 64%)]  Loss: 3.56 (3.56)  Time: 0.125s, 1020.18/s  (0.127s, 1011.53/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 809.983s\n",
      "Train: 8 [6450/10009 ( 64%)]  Loss: 3.77 (3.56)  Time: 0.129s,  993.56/s  (0.127s, 1011.54/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 816.307s\n",
      "Train: 8 [6500/10009 ( 65%)]  Loss: 3.57 (3.56)  Time: 0.126s, 1012.62/s  (0.127s, 1011.55/s)  LR: 2.239e-05  Data: 0.008 (0.006)Time: 822.627s\n",
      "Train: 8 [6550/10009 ( 65%)]  Loss: 3.39 (3.56)  Time: 0.126s, 1017.02/s  (0.127s, 1011.56/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 828.944s\n",
      "Train: 8 [6600/10009 ( 66%)]  Loss: 3.35 (3.56)  Time: 0.124s, 1028.47/s  (0.127s, 1011.63/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 835.209s\n",
      "Train: 8 [6650/10009 ( 66%)]  Loss: 3.56 (3.56)  Time: 0.127s, 1004.59/s  (0.127s, 1011.58/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 841.578s\n",
      "Train: 8 [6700/10009 ( 67%)]  Loss: 3.43 (3.56)  Time: 0.124s, 1028.81/s  (0.127s, 1011.64/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 847.859s\n",
      "Train: 8 [6750/10009 ( 67%)]  Loss: 3.42 (3.56)  Time: 0.123s, 1036.56/s  (0.127s, 1011.74/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 854.096s\n",
      "Train: 8 [6800/10009 ( 68%)]  Loss: 3.58 (3.56)  Time: 0.123s, 1039.89/s  (0.127s, 1011.75/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 860.419s\n",
      "Train: 8 [6850/10009 ( 68%)]  Loss: 3.36 (3.56)  Time: 0.123s, 1038.31/s  (0.127s, 1011.71/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 866.772s\n",
      "Train: 8 [6900/10009 ( 69%)]  Loss: 3.58 (3.56)  Time: 0.125s, 1021.03/s  (0.127s, 1011.75/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 873.066s\n",
      "Train: 8 [6950/10009 ( 69%)]  Loss: 3.42 (3.56)  Time: 0.123s, 1036.57/s  (0.127s, 1011.67/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 879.459s\n",
      "Train: 8 [7000/10009 ( 70%)]  Loss: 3.56 (3.56)  Time: 0.125s, 1021.16/s  (0.127s, 1011.60/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 885.845s\n",
      "Train: 8 [7050/10009 ( 70%)]  Loss: 3.76 (3.56)  Time: 0.124s, 1034.67/s  (0.127s, 1011.68/s)  LR: 2.239e-05  Data: 0.005 (0.006)Time: 892.108s\n",
      "Train: 8 [7100/10009 ( 71%)]  Loss: 3.11 (3.56)  Time: 0.126s, 1019.59/s  (0.127s, 1011.59/s)  LR: 2.239e-05  Data: 0.007 (0.006)Time: 898.509s\n",
      "Train: 8 [7150/10009 ( 71%)]  Loss: 3.44 (3.56)  Time: 0.142s,  903.67/s  (0.127s, 1011.45/s)  LR: 2.239e-05  Data: 0.009 (0.006)Time: 904.966s\n",
      "Train: 8 [7200/10009 ( 72%)]  Loss: 3.87 (3.56)  Time: 0.124s, 1031.16/s  (0.127s, 1011.48/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 911.260s\n",
      "Train: 8 [7250/10009 ( 72%)]  Loss: 3.73 (3.56)  Time: 0.123s, 1039.19/s  (0.127s, 1011.40/s)  LR: 2.239e-05  Data: 0.006 (0.006)Time: 917.665s\n",
      "Train: 8 [7300/10009 ( 73%)]  Loss: 3.49 (3.56)  Time: 0.120s, 1063.74/s  (0.127s, 1009.91/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 925.356s\n",
      "Train: 8 [7350/10009 ( 73%)]  Loss: 3.44 (3.56)  Time: 0.123s, 1039.16/s  (0.127s, 1006.01/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 935.303s\n",
      "Train: 8 [7400/10009 ( 74%)]  Loss: 3.38 (3.56)  Time: 0.124s, 1028.38/s  (0.127s, 1005.56/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 942.088s\n",
      "Train: 8 [7450/10009 ( 74%)]  Loss: 3.55 (3.56)  Time: 0.124s, 1035.98/s  (0.127s, 1005.62/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 948.398s\n",
      "Train: 8 [7500/10009 ( 75%)]  Loss: 3.27 (3.56)  Time: 0.125s, 1027.17/s  (0.127s, 1005.70/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 954.685s\n",
      "Train: 8 [7550/10009 ( 75%)]  Loss: 3.52 (3.56)  Time: 0.136s,  942.13/s  (0.127s, 1005.78/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 960.968s\n",
      "Train: 8 [7600/10009 ( 76%)]  Loss: 3.55 (3.56)  Time: 0.124s, 1032.15/s  (0.127s, 1005.84/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 967.277s\n",
      "Train: 8 [7650/10009 ( 76%)]  Loss: 3.82 (3.56)  Time: 0.124s, 1031.73/s  (0.127s, 1005.83/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 973.649s\n",
      "Train: 8 [7700/10009 ( 77%)]  Loss: 3.51 (3.56)  Time: 0.124s, 1036.06/s  (0.127s, 1005.93/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 979.917s\n",
      "Train: 8 [7750/10009 ( 77%)]  Loss: 3.72 (3.56)  Time: 0.124s, 1029.13/s  (0.127s, 1006.01/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 986.200s\n",
      "Train: 8 [7800/10009 ( 78%)]  Loss: 3.59 (3.56)  Time: 0.124s, 1029.95/s  (0.127s, 1006.09/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 992.483s\n",
      "Train: 8 [7850/10009 ( 78%)]  Loss: 3.54 (3.56)  Time: 0.125s, 1024.94/s  (0.127s, 1006.13/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 998.802s\n",
      "Train: 8 [7900/10009 ( 79%)]  Loss: 3.61 (3.56)  Time: 0.126s, 1016.17/s  (0.127s, 1006.20/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1005.094s\n",
      "Train: 8 [7950/10009 ( 79%)]  Loss: 3.61 (3.56)  Time: 0.124s, 1031.73/s  (0.127s, 1006.25/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1011.406s\n",
      "Train: 8 [8000/10009 ( 80%)]  Loss: 3.53 (3.56)  Time: 0.139s,  923.20/s  (0.127s, 1006.25/s)  LR: 2.239e-05  Data: 0.010 (0.007)Time: 1017.763s\n",
      "Train: 8 [8050/10009 ( 80%)]  Loss: 3.36 (3.56)  Time: 0.124s, 1028.72/s  (0.127s, 1006.34/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1024.034s\n",
      "Train: 8 [8100/10009 ( 81%)]  Loss: 3.40 (3.56)  Time: 0.124s, 1035.26/s  (0.127s, 1006.30/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1030.428s\n",
      "Train: 8 [8150/10009 ( 81%)]  Loss: 3.49 (3.56)  Time: 0.122s, 1045.53/s  (0.127s, 1006.36/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 1036.728s\n",
      "Train: 8 [8200/10009 ( 82%)]  Loss: 3.43 (3.56)  Time: 0.124s, 1028.48/s  (0.127s, 1006.32/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 1043.128s\n",
      "Train: 8 [8250/10009 ( 82%)]  Loss: 3.55 (3.56)  Time: 0.123s, 1041.71/s  (0.127s, 1006.29/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1049.524s\n",
      "Train: 8 [8300/10009 ( 83%)]  Loss: 3.59 (3.56)  Time: 0.124s, 1032.43/s  (0.127s, 1006.34/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 1055.827s\n",
      "Train: 8 [8350/10009 ( 83%)]  Loss: 3.50 (3.56)  Time: 0.124s, 1034.59/s  (0.127s, 1006.31/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1062.226s\n",
      "Train: 8 [8400/10009 ( 84%)]  Loss: 3.43 (3.56)  Time: 0.124s, 1034.90/s  (0.127s, 1006.36/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1068.531s\n",
      "Train: 8 [8450/10009 ( 84%)]  Loss: 3.65 (3.56)  Time: 0.130s,  985.62/s  (0.127s, 1006.43/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1074.812s\n",
      "Train: 8 [8500/10009 ( 85%)]  Loss: 3.71 (3.56)  Time: 0.124s, 1028.96/s  (0.127s, 1006.42/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1081.183s\n",
      "Train: 8 [8550/10009 ( 85%)]  Loss: 3.18 (3.56)  Time: 0.126s, 1018.97/s  (0.127s, 1006.38/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1087.586s\n",
      "Train: 8 [8600/10009 ( 86%)]  Loss: 3.65 (3.56)  Time: 0.127s, 1004.73/s  (0.127s, 1006.39/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1093.933s\n",
      "Train: 8 [8650/10009 ( 86%)]  Loss: 3.42 (3.56)  Time: 0.126s, 1019.33/s  (0.127s, 1006.42/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1100.265s\n",
      "Train: 8 [8700/10009 ( 87%)]  Loss: 3.97 (3.56)  Time: 0.127s, 1008.47/s  (0.127s, 1006.46/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 1106.578s\n",
      "Train: 8 [8750/10009 ( 87%)]  Loss: 3.47 (3.56)  Time: 0.134s,  957.71/s  (0.127s, 1006.48/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 1112.907s\n",
      "Train: 8 [8800/10009 ( 88%)]  Loss: 3.50 (3.56)  Time: 0.141s,  910.32/s  (0.127s, 1006.47/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1119.278s\n",
      "Train: 8 [8850/10009 ( 88%)]  Loss: 3.28 (3.56)  Time: 0.124s, 1034.46/s  (0.127s, 1006.57/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1125.527s\n",
      "Train: 8 [8900/10009 ( 89%)]  Loss: 3.41 (3.56)  Time: 0.125s, 1027.74/s  (0.127s, 1006.60/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1131.850s\n",
      "Train: 8 [8950/10009 ( 89%)]  Loss: 3.50 (3.56)  Time: 0.125s, 1024.02/s  (0.127s, 1006.68/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1138.120s\n",
      "Train: 8 [9000/10009 ( 90%)]  Loss: 3.57 (3.56)  Time: 0.125s, 1023.06/s  (0.127s, 1006.73/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1144.417s\n",
      "Train: 8 [9050/10009 ( 90%)]  Loss: 3.67 (3.56)  Time: 0.128s, 1003.85/s  (0.127s, 1006.70/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1150.817s\n",
      "Train: 8 [9100/10009 ( 91%)]  Loss: 3.47 (3.56)  Time: 0.129s,  988.56/s  (0.127s, 1006.68/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1157.195s\n",
      "Train: 8 [9150/10009 ( 91%)]  Loss: 3.87 (3.56)  Time: 0.124s, 1030.44/s  (0.127s, 1006.73/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1163.497s\n",
      "Train: 8 [9200/10009 ( 92%)]  Loss: 3.61 (3.56)  Time: 0.127s, 1008.73/s  (0.127s, 1006.73/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 1169.845s\n",
      "Train: 8 [9250/10009 ( 92%)]  Loss: 3.27 (3.56)  Time: 0.124s, 1031.05/s  (0.127s, 1006.71/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1176.227s\n",
      "Train: 8 [9300/10009 ( 93%)]  Loss: 3.59 (3.56)  Time: 0.125s, 1023.75/s  (0.127s, 1006.77/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 1182.516s\n",
      "Train: 8 [9350/10009 ( 93%)]  Loss: 3.41 (3.56)  Time: 0.128s, 1002.78/s  (0.127s, 1006.85/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1188.775s\n",
      "Train: 8 [9400/10009 ( 94%)]  Loss: 3.46 (3.56)  Time: 0.127s, 1010.97/s  (0.127s, 1006.90/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1195.081s\n",
      "Train: 8 [9450/10009 ( 94%)]  Loss: 3.67 (3.56)  Time: 0.124s, 1032.24/s  (0.127s, 1006.88/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1201.459s\n",
      "Train: 8 [9500/10009 ( 95%)]  Loss: 3.73 (3.56)  Time: 0.125s, 1022.31/s  (0.127s, 1006.83/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1207.872s\n",
      "Train: 8 [9550/10009 ( 95%)]  Loss: 3.83 (3.56)  Time: 0.124s, 1029.13/s  (0.127s, 1006.89/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1214.162s\n",
      "Train: 8 [9600/10009 ( 96%)]  Loss: 3.55 (3.56)  Time: 0.123s, 1036.65/s  (0.127s, 1006.83/s)  LR: 2.239e-05  Data: 0.005 (0.007)Time: 1220.593s\n",
      "Train: 8 [9650/10009 ( 96%)]  Loss: 3.73 (3.56)  Time: 0.134s,  956.97/s  (0.127s, 1006.88/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1226.888s\n",
      "Train: 8 [9700/10009 ( 97%)]  Loss: 3.40 (3.56)  Time: 0.125s, 1020.29/s  (0.127s, 1006.85/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1233.275s\n",
      "Train: 8 [9750/10009 ( 97%)]  Loss: 3.34 (3.56)  Time: 0.126s, 1016.81/s  (0.127s, 1006.95/s)  LR: 2.239e-05  Data: 0.008 (0.007)Time: 1239.512s\n",
      "Train: 8 [9800/10009 ( 98%)]  Loss: 3.70 (3.56)  Time: 0.126s, 1012.39/s  (0.127s, 1006.97/s)  LR: 2.239e-05  Data: 0.007 (0.007)Time: 1245.834s\n",
      "Train: 8 [9850/10009 ( 98%)]  Loss: 3.60 (3.56)  Time: 0.123s, 1038.45/s  (0.127s, 1007.00/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1252.164s\n",
      "Train: 8 [9900/10009 ( 99%)]  Loss: 3.66 (3.56)  Time: 0.124s, 1034.07/s  (0.127s, 1007.04/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1258.466s\n",
      "Train: 8 [9950/10009 ( 99%)]  Loss: 3.48 (3.56)  Time: 0.123s, 1040.98/s  (0.127s, 1007.12/s)  LR: 2.239e-05  Data: 0.006 (0.007)Time: 1264.719s\n",
      "Train: 8 [10000/10009 (100%)]  Loss: 3.95 (3.56)  Time: 0.168s,  760.33/s  (0.127s, 1007.19/s)  LR: 2.239e-05  Data: 0.052 (0.007)Time: 1270.983s\n",
      "Test: [   0/390]  Time: 0.692 (0.692)  Loss:   1.523 ( 1.523)  Acc@1:  72.656 ( 72.656)  Acc@5:  84.375 ( 84.375)\n",
      "Test: [  50/390]  Time: 0.038 (0.139)  Loss:   1.638 ( 2.334)  Acc@1:  60.156 ( 51.363)  Acc@5:  82.812 ( 73.575)\n",
      "Test: [ 100/390]  Time: 0.075 (0.132)  Loss:   2.483 ( 2.385)  Acc@1:  42.188 ( 47.865)  Acc@5:  75.000 ( 73.554)\n",
      "Test: [ 150/390]  Time: 0.038 (0.137)  Loss:   2.644 ( 2.348)  Acc@1:  40.625 ( 48.453)  Acc@5:  68.750 ( 74.219)\n",
      "Test: [ 200/390]  Time: 0.038 (0.133)  Loss:   3.585 ( 2.544)  Acc@1:  21.875 ( 45.192)  Acc@5:  51.562 ( 70.810)\n",
      "Test: [ 250/390]  Time: 0.039 (0.132)  Loss:   2.502 ( 2.656)  Acc@1:  49.219 ( 43.576)  Acc@5:  67.969 ( 68.840)\n",
      "Test: [ 300/390]  Time: 0.454 (0.132)  Loss:   2.787 ( 2.748)  Acc@1:  45.312 ( 42.042)  Acc@5:  66.406 ( 67.071)\n",
      "Test: [ 350/390]  Time: 0.070 (0.130)  Loss:   3.097 ( 2.821)  Acc@1:  30.469 ( 40.781)  Acc@5:  63.281 ( 65.834)\n",
      "Test: [ 390/390]  Time: 0.025 (0.130)  Loss:   4.233 ( 2.789)  Acc@1:   7.500 ( 41.378)  Acc@5:  41.250 ( 66.388)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-8.pth.tar', 41.378)\n",
      " ('./output/budgeted/checkpoint-7.pth.tar', 39.734)\n",
      " ('./output/budgeted/checkpoint-6.pth.tar', 36.718)\n",
      " ('./output/budgeted/checkpoint-5.pth.tar', 34.288)\n",
      "\n",
      "Train: 9 [   0/10009 (  0%)]  Loss: 3.57 (3.57)  Time: 0.737s,  173.71/s  (0.737s,  173.71/s)  LR: 1.727e-05  Data: 0.621 (0.621)Time: 0.737s\n",
      "Train: 9 [  50/10009 (  0%)]  Loss: 3.36 (3.47)  Time: 0.126s, 1015.99/s  (0.140s,  911.57/s)  LR: 1.727e-05  Data: 0.007 (0.022)Time: 7.162s\n",
      "Train: 9 [ 100/10009 (  1%)]  Loss: 3.57 (3.48)  Time: 0.125s, 1026.88/s  (0.134s,  957.59/s)  LR: 1.727e-05  Data: 0.005 (0.014)Time: 13.501s\n",
      "Train: 9 [ 150/10009 (  1%)]  Loss: 3.27 (3.47)  Time: 0.124s, 1036.27/s  (0.131s,  976.38/s)  LR: 1.727e-05  Data: 0.006 (0.012)Time: 19.796s\n",
      "Train: 9 [ 200/10009 (  2%)]  Loss: 3.62 (3.47)  Time: 0.124s, 1031.66/s  (0.130s,  987.35/s)  LR: 1.727e-05  Data: 0.006 (0.010)Time: 26.058s\n",
      "Train: 9 [ 250/10009 (  2%)]  Loss: 3.52 (3.47)  Time: 0.123s, 1037.10/s  (0.129s,  994.66/s)  LR: 1.727e-05  Data: 0.007 (0.009)Time: 32.301s\n",
      "Train: 9 [ 300/10009 (  3%)]  Loss: 3.49 (3.46)  Time: 0.123s, 1044.45/s  (0.128s,  998.10/s)  LR: 1.727e-05  Data: 0.005 (0.009)Time: 38.602s\n",
      "Train: 9 [ 350/10009 (  3%)]  Loss: 3.49 (3.47)  Time: 0.129s,  991.67/s  (0.128s, 1000.02/s)  LR: 1.727e-05  Data: 0.006 (0.008)Time: 44.927s\n",
      "Train: 9 [ 400/10009 (  4%)]  Loss: 3.32 (3.47)  Time: 0.127s, 1010.60/s  (0.128s, 1001.91/s)  LR: 1.727e-05  Data: 0.007 (0.008)Time: 51.231s\n",
      "Train: 9 [ 450/10009 (  4%)]  Loss: 3.55 (3.47)  Time: 0.127s, 1011.78/s  (0.128s, 1003.51/s)  LR: 1.727e-05  Data: 0.007 (0.008)Time: 57.526s\n",
      "Train: 9 [ 500/10009 (  5%)]  Loss: 3.43 (3.47)  Time: 0.136s,  938.16/s  (0.127s, 1004.06/s)  LR: 1.727e-05  Data: 0.008 (0.008)Time: 63.869s\n",
      "Train: 9 [ 550/10009 (  5%)]  Loss: 3.74 (3.47)  Time: 0.124s, 1036.25/s  (0.127s, 1005.77/s)  LR: 1.727e-05  Data: 0.006 (0.008)Time: 70.123s\n",
      "Train: 9 [ 600/10009 (  6%)]  Loss: 3.72 (3.47)  Time: 0.124s, 1028.72/s  (0.127s, 1006.90/s)  LR: 1.727e-05  Data: 0.006 (0.008)Time: 76.401s\n",
      "Train: 9 [ 650/10009 (  6%)]  Loss: 3.25 (3.46)  Time: 0.124s, 1032.66/s  (0.127s, 1006.61/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 82.781s\n",
      "Train: 9 [ 700/10009 (  7%)]  Loss: 3.40 (3.47)  Time: 0.131s,  978.94/s  (0.127s, 1006.68/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 89.132s\n",
      "Train: 9 [ 750/10009 (  7%)]  Loss: 3.53 (3.46)  Time: 0.124s, 1028.72/s  (0.127s, 1006.66/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 95.492s\n",
      "Train: 9 [ 800/10009 (  8%)]  Loss: 3.32 (3.46)  Time: 0.124s, 1032.49/s  (0.127s, 1007.08/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 101.807s\n",
      "Train: 9 [ 850/10009 (  8%)]  Loss: 3.45 (3.46)  Time: 0.125s, 1020.08/s  (0.127s, 1007.33/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 108.135s\n",
      "Train: 9 [ 900/10009 (  9%)]  Loss: 3.13 (3.46)  Time: 0.124s, 1034.62/s  (0.127s, 1007.15/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 114.510s\n",
      "Train: 9 [ 950/10009 (  9%)]  Loss: 3.51 (3.46)  Time: 0.124s, 1031.95/s  (0.127s, 1007.30/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 120.846s\n",
      "Train: 9 [1000/10009 ( 10%)]  Loss: 3.61 (3.46)  Time: 0.124s, 1029.14/s  (0.127s, 1007.84/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 127.131s\n",
      "Train: 9 [1050/10009 ( 10%)]  Loss: 3.41 (3.46)  Time: 0.124s, 1029.15/s  (0.127s, 1008.56/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 133.386s\n",
      "Train: 9 [1100/10009 ( 11%)]  Loss: 3.42 (3.46)  Time: 0.144s,  886.85/s  (0.127s, 1008.79/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 139.700s\n",
      "Train: 9 [1150/10009 ( 11%)]  Loss: 3.40 (3.46)  Time: 0.131s,  978.29/s  (0.127s, 1008.74/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 146.052s\n",
      "Train: 9 [1200/10009 ( 12%)]  Loss: 3.41 (3.46)  Time: 0.124s, 1031.68/s  (0.127s, 1008.76/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 152.393s\n",
      "Train: 9 [1250/10009 ( 12%)]  Loss: 3.60 (3.46)  Time: 0.125s, 1025.57/s  (0.127s, 1009.18/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 158.671s\n",
      "Train: 9 [1300/10009 ( 13%)]  Loss: 3.74 (3.46)  Time: 0.125s, 1027.62/s  (0.127s, 1009.18/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 165.012s\n",
      "Train: 9 [1350/10009 ( 13%)]  Loss: 3.15 (3.46)  Time: 0.124s, 1031.27/s  (0.127s, 1009.05/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 171.377s\n",
      "Train: 9 [1400/10009 ( 14%)]  Loss: 3.29 (3.46)  Time: 0.125s, 1026.57/s  (0.127s, 1009.50/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 177.640s\n",
      "Train: 9 [1450/10009 ( 14%)]  Loss: 3.61 (3.46)  Time: 0.124s, 1029.20/s  (0.127s, 1009.89/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 183.908s\n",
      "Train: 9 [1500/10009 ( 15%)]  Loss: 3.61 (3.46)  Time: 0.125s, 1025.50/s  (0.127s, 1010.08/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 190.211s\n",
      "Train: 9 [1550/10009 ( 15%)]  Loss: 3.26 (3.46)  Time: 0.124s, 1031.84/s  (0.127s, 1009.66/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 196.629s\n",
      "Train: 9 [1600/10009 ( 16%)]  Loss: 3.38 (3.46)  Time: 0.126s, 1017.17/s  (0.127s, 1009.50/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 202.999s\n",
      "Train: 9 [1650/10009 ( 16%)]  Loss: 3.58 (3.46)  Time: 0.134s,  955.53/s  (0.127s, 1009.74/s)  LR: 1.727e-05  Data: 0.009 (0.007)Time: 209.289s\n",
      "Train: 9 [1700/10009 ( 17%)]  Loss: 3.62 (3.46)  Time: 0.124s, 1036.33/s  (0.127s, 1009.63/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 215.651s\n",
      "Train: 9 [1750/10009 ( 17%)]  Loss: 3.27 (3.46)  Time: 0.124s, 1030.82/s  (0.127s, 1009.86/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 221.938s\n",
      "Train: 9 [1800/10009 ( 18%)]  Loss: 3.50 (3.46)  Time: 0.125s, 1025.70/s  (0.127s, 1010.04/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 228.235s\n",
      "Train: 9 [1850/10009 ( 18%)]  Loss: 3.66 (3.46)  Time: 0.124s, 1032.32/s  (0.127s, 1010.15/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 234.547s\n",
      "Train: 9 [1900/10009 ( 19%)]  Loss: 3.46 (3.46)  Time: 0.123s, 1039.62/s  (0.127s, 1010.32/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 240.841s\n",
      "Train: 9 [1950/10009 ( 19%)]  Loss: 3.45 (3.46)  Time: 0.124s, 1032.20/s  (0.127s, 1010.35/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 247.169s\n",
      "Train: 9 [2000/10009 ( 20%)]  Loss: 3.23 (3.46)  Time: 0.125s, 1021.21/s  (0.127s, 1010.60/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 253.440s\n",
      "Train: 9 [2050/10009 ( 20%)]  Loss: 2.96 (3.46)  Time: 0.125s, 1024.14/s  (0.127s, 1010.46/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 259.809s\n",
      "Train: 9 [2100/10009 ( 21%)]  Loss: 3.23 (3.46)  Time: 0.124s, 1032.60/s  (0.127s, 1010.01/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 266.261s\n",
      "Train: 9 [2150/10009 ( 21%)]  Loss: 3.25 (3.46)  Time: 0.124s, 1028.25/s  (0.127s, 1009.96/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 272.613s\n",
      "Train: 9 [2200/10009 ( 22%)]  Loss: 3.53 (3.46)  Time: 0.125s, 1020.78/s  (0.127s, 1009.99/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 278.942s\n",
      "Train: 9 [2250/10009 ( 22%)]  Loss: 3.18 (3.46)  Time: 0.125s, 1024.30/s  (0.127s, 1009.69/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 285.363s\n",
      "Train: 9 [2300/10009 ( 23%)]  Loss: 3.59 (3.46)  Time: 0.125s, 1027.85/s  (0.127s, 1009.79/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 291.672s\n",
      "Train: 9 [2350/10009 ( 23%)]  Loss: 3.18 (3.46)  Time: 0.126s, 1014.58/s  (0.127s, 1009.92/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 297.970s\n",
      "Train: 9 [2400/10009 ( 24%)]  Loss: 3.26 (3.46)  Time: 0.125s, 1019.93/s  (0.127s, 1009.93/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 304.305s\n",
      "Train: 9 [2450/10009 ( 24%)]  Loss: 3.80 (3.46)  Time: 0.125s, 1020.36/s  (0.127s, 1009.76/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 310.694s\n",
      "Train: 9 [2500/10009 ( 25%)]  Loss: 3.46 (3.46)  Time: 0.125s, 1024.80/s  (0.127s, 1009.98/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 316.963s\n",
      "Train: 9 [2550/10009 ( 25%)]  Loss: 3.57 (3.46)  Time: 0.125s, 1024.23/s  (0.127s, 1009.94/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 323.315s\n",
      "Train: 9 [2600/10009 ( 26%)]  Loss: 3.59 (3.46)  Time: 0.123s, 1037.05/s  (0.127s, 1009.76/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 329.710s\n",
      "Train: 9 [2650/10009 ( 26%)]  Loss: 3.68 (3.46)  Time: 0.124s, 1031.71/s  (0.127s, 1009.71/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 336.064s\n",
      "Train: 9 [2700/10009 ( 27%)]  Loss: 3.65 (3.46)  Time: 0.125s, 1023.50/s  (0.127s, 1009.49/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 342.478s\n",
      "Train: 9 [2750/10009 ( 27%)]  Loss: 3.70 (3.46)  Time: 0.122s, 1048.95/s  (0.127s, 1009.61/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 348.777s\n",
      "Train: 9 [2800/10009 ( 28%)]  Loss: 3.67 (3.46)  Time: 0.125s, 1027.58/s  (0.127s, 1009.74/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 355.067s\n",
      "Train: 9 [2850/10009 ( 28%)]  Loss: 3.49 (3.46)  Time: 0.134s,  955.61/s  (0.127s, 1009.67/s)  LR: 1.727e-05  Data: 0.009 (0.007)Time: 361.431s\n",
      "Train: 9 [2900/10009 ( 29%)]  Loss: 3.48 (3.46)  Time: 0.125s, 1025.63/s  (0.127s, 1009.84/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 367.707s\n",
      "Train: 9 [2950/10009 ( 29%)]  Loss: 3.30 (3.46)  Time: 0.125s, 1020.11/s  (0.127s, 1009.47/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 374.184s\n",
      "Train: 9 [3000/10009 ( 30%)]  Loss: 3.68 (3.46)  Time: 0.124s, 1032.90/s  (0.127s, 1009.56/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 380.491s\n",
      "Train: 9 [3050/10009 ( 30%)]  Loss: 3.15 (3.46)  Time: 0.128s,  996.59/s  (0.127s, 1009.51/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 386.848s\n",
      "Train: 9 [3100/10009 ( 31%)]  Loss: 3.34 (3.46)  Time: 0.139s,  917.83/s  (0.127s, 1009.59/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 393.157s\n",
      "Train: 9 [3150/10009 ( 31%)]  Loss: 3.05 (3.46)  Time: 0.124s, 1028.97/s  (0.127s, 1009.53/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 399.520s\n",
      "Train: 9 [3200/10009 ( 32%)]  Loss: 3.46 (3.46)  Time: 0.123s, 1042.01/s  (0.127s, 1009.42/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 405.902s\n",
      "Train: 9 [3250/10009 ( 32%)]  Loss: 3.64 (3.46)  Time: 0.125s, 1026.25/s  (0.127s, 1009.54/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 412.193s\n",
      "Train: 9 [3300/10009 ( 33%)]  Loss: 3.08 (3.46)  Time: 0.138s,  924.68/s  (0.127s, 1009.63/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 418.498s\n",
      "Train: 9 [3350/10009 ( 33%)]  Loss: 3.17 (3.46)  Time: 0.126s, 1012.52/s  (0.127s, 1009.69/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 424.812s\n",
      "Train: 9 [3400/10009 ( 34%)]  Loss: 3.82 (3.46)  Time: 0.131s,  976.49/s  (0.127s, 1009.82/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 431.091s\n",
      "Train: 9 [3450/10009 ( 34%)]  Loss: 3.35 (3.46)  Time: 0.125s, 1024.66/s  (0.127s, 1009.92/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 437.390s\n",
      "Train: 9 [3500/10009 ( 35%)]  Loss: 3.65 (3.46)  Time: 0.127s, 1008.86/s  (0.127s, 1009.93/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 443.721s\n",
      "Train: 9 [3550/10009 ( 35%)]  Loss: 3.51 (3.46)  Time: 0.127s, 1004.06/s  (0.127s, 1009.98/s)  LR: 1.727e-05  Data: 0.009 (0.007)Time: 450.036s\n",
      "Train: 9 [3600/10009 ( 36%)]  Loss: 3.33 (3.46)  Time: 0.125s, 1024.89/s  (0.127s, 1010.00/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 456.361s\n",
      "Train: 9 [3650/10009 ( 36%)]  Loss: 3.36 (3.46)  Time: 0.127s, 1009.43/s  (0.127s, 1009.84/s)  LR: 1.727e-05  Data: 0.009 (0.007)Time: 462.774s\n",
      "Train: 9 [3700/10009 ( 37%)]  Loss: 3.51 (3.46)  Time: 0.124s, 1029.04/s  (0.127s, 1009.73/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 469.160s\n",
      "Train: 9 [3750/10009 ( 37%)]  Loss: 3.56 (3.46)  Time: 0.123s, 1040.77/s  (0.127s, 1009.72/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 475.506s\n",
      "Train: 9 [3800/10009 ( 38%)]  Loss: 3.07 (3.46)  Time: 0.125s, 1025.36/s  (0.127s, 1009.56/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 481.919s\n",
      "Train: 9 [3850/10009 ( 38%)]  Loss: 3.49 (3.46)  Time: 0.124s, 1033.32/s  (0.127s, 1009.67/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 488.206s\n",
      "Train: 9 [3900/10009 ( 39%)]  Loss: 3.60 (3.46)  Time: 0.125s, 1027.80/s  (0.127s, 1009.69/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 494.534s\n",
      "Train: 9 [3950/10009 ( 39%)]  Loss: 3.43 (3.46)  Time: 0.124s, 1035.20/s  (0.127s, 1009.80/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 500.817s\n",
      "Train: 9 [4000/10009 ( 40%)]  Loss: 3.92 (3.46)  Time: 0.124s, 1035.84/s  (0.127s, 1009.81/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 507.150s\n",
      "Train: 9 [4050/10009 ( 40%)]  Loss: 3.22 (3.46)  Time: 0.128s,  996.92/s  (0.127s, 1009.58/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 513.608s\n",
      "Train: 9 [4100/10009 ( 41%)]  Loss: 3.22 (3.46)  Time: 0.127s, 1009.30/s  (0.127s, 1009.31/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 520.086s\n",
      "Train: 9 [4150/10009 ( 41%)]  Loss: 3.50 (3.46)  Time: 0.131s,  979.56/s  (0.127s, 1009.24/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 526.461s\n",
      "Train: 9 [4200/10009 ( 42%)]  Loss: 3.47 (3.46)  Time: 0.123s, 1037.16/s  (0.127s, 1009.22/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 532.816s\n",
      "Train: 9 [4250/10009 ( 42%)]  Loss: 3.56 (3.46)  Time: 0.125s, 1022.66/s  (0.127s, 1009.29/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 539.116s\n",
      "Train: 9 [4300/10009 ( 43%)]  Loss: 3.29 (3.46)  Time: 0.125s, 1023.13/s  (0.127s, 1009.38/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 545.411s\n",
      "Train: 9 [4350/10009 ( 43%)]  Loss: 3.45 (3.46)  Time: 0.146s,  874.56/s  (0.127s, 1009.38/s)  LR: 1.727e-05  Data: 0.009 (0.007)Time: 551.749s\n",
      "Train: 9 [4400/10009 ( 44%)]  Loss: 3.62 (3.46)  Time: 0.123s, 1040.21/s  (0.127s, 1009.34/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 558.112s\n",
      "Train: 9 [4450/10009 ( 44%)]  Loss: 3.57 (3.46)  Time: 0.124s, 1030.06/s  (0.127s, 1009.46/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 564.387s\n",
      "Train: 9 [4500/10009 ( 45%)]  Loss: 3.58 (3.46)  Time: 0.124s, 1035.56/s  (0.127s, 1009.44/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 570.736s\n",
      "Train: 9 [4550/10009 ( 45%)]  Loss: 3.65 (3.46)  Time: 0.127s, 1008.35/s  (0.127s, 1009.48/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 577.056s\n",
      "Train: 9 [4600/10009 ( 46%)]  Loss: 3.32 (3.46)  Time: 0.140s,  914.57/s  (0.127s, 1009.48/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 583.398s\n",
      "Train: 9 [4650/10009 ( 46%)]  Loss: 3.66 (3.46)  Time: 0.125s, 1020.58/s  (0.127s, 1009.40/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 589.783s\n",
      "Train: 9 [4700/10009 ( 47%)]  Loss: 3.30 (3.46)  Time: 0.124s, 1032.94/s  (0.127s, 1009.35/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 596.155s\n",
      "Train: 9 [4750/10009 ( 47%)]  Loss: 3.51 (3.46)  Time: 0.125s, 1025.53/s  (0.127s, 1009.31/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 602.520s\n",
      "Train: 9 [4800/10009 ( 48%)]  Loss: 3.53 (3.46)  Time: 0.124s, 1031.27/s  (0.127s, 1009.29/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 608.870s\n",
      "Train: 9 [4850/10009 ( 48%)]  Loss: 3.75 (3.46)  Time: 0.123s, 1039.82/s  (0.127s, 1009.19/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 615.269s\n",
      "Train: 9 [4900/10009 ( 49%)]  Loss: 3.25 (3.46)  Time: 0.123s, 1038.20/s  (0.127s, 1009.15/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 621.639s\n",
      "Train: 9 [4950/10009 ( 49%)]  Loss: 3.29 (3.46)  Time: 0.124s, 1034.40/s  (0.127s, 1009.19/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 627.953s\n",
      "Train: 9 [5000/10009 ( 50%)]  Loss: 3.17 (3.46)  Time: 0.126s, 1015.04/s  (0.127s, 1009.33/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 634.208s\n",
      "Train: 9 [5050/10009 ( 50%)]  Loss: 3.70 (3.46)  Time: 0.128s, 1002.18/s  (0.127s, 1009.41/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 640.497s\n",
      "Train: 9 [5100/10009 ( 51%)]  Loss: 3.34 (3.46)  Time: 0.133s,  963.42/s  (0.127s, 1008.88/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 647.178s\n",
      "Train: 9 [5150/10009 ( 51%)]  Loss: 3.21 (3.46)  Time: 0.137s,  931.39/s  (0.127s, 1007.59/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 654.358s\n",
      "Train: 9 [5200/10009 ( 52%)]  Loss: 3.69 (3.46)  Time: 0.161s,  795.19/s  (0.127s, 1006.90/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 661.162s\n",
      "Train: 9 [5250/10009 ( 52%)]  Loss: 3.78 (3.46)  Time: 0.136s,  939.10/s  (0.127s, 1006.32/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 667.903s\n",
      "Train: 9 [5300/10009 ( 53%)]  Loss: 3.44 (3.46)  Time: 0.134s,  957.93/s  (0.127s, 1005.43/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 674.860s\n",
      "Train: 9 [5350/10009 ( 53%)]  Loss: 3.40 (3.46)  Time: 0.134s,  956.84/s  (0.127s, 1004.74/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 681.694s\n",
      "Train: 9 [5400/10009 ( 54%)]  Loss: 3.46 (3.46)  Time: 0.163s,  783.41/s  (0.128s, 1003.67/s)  LR: 1.727e-05  Data: 0.010 (0.007)Time: 688.796s\n",
      "Train: 9 [5450/10009 ( 54%)]  Loss: 3.28 (3.46)  Time: 0.133s,  961.20/s  (0.128s, 1003.10/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 695.572s\n",
      "Train: 9 [5500/10009 ( 55%)]  Loss: 3.48 (3.46)  Time: 0.138s,  928.62/s  (0.128s, 1002.64/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 702.269s\n",
      "Train: 9 [5550/10009 ( 55%)]  Loss: 3.02 (3.46)  Time: 0.146s,  878.93/s  (0.128s, 1002.03/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 709.084s\n",
      "Train: 9 [5600/10009 ( 56%)]  Loss: 3.22 (3.46)  Time: 0.134s,  957.21/s  (0.128s, 1001.43/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 715.905s\n",
      "Train: 9 [5650/10009 ( 56%)]  Loss: 3.54 (3.46)  Time: 0.139s,  917.58/s  (0.128s, 1000.98/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 722.618s\n",
      "Train: 9 [5700/10009 ( 57%)]  Loss: 3.12 (3.46)  Time: 0.133s,  962.59/s  (0.128s, 1000.45/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 729.399s\n",
      "Train: 9 [5750/10009 ( 57%)]  Loss: 3.43 (3.46)  Time: 0.133s,  964.99/s  (0.128s,  999.94/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 736.172s\n",
      "Train: 9 [5800/10009 ( 58%)]  Loss: 3.56 (3.46)  Time: 0.131s,  974.69/s  (0.128s,  999.30/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 743.044s\n",
      "Train: 9 [5850/10009 ( 58%)]  Loss: 3.31 (3.46)  Time: 0.132s,  972.27/s  (0.128s,  998.80/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 749.822s\n",
      "Train: 9 [5900/10009 ( 59%)]  Loss: 3.58 (3.46)  Time: 0.133s,  961.58/s  (0.128s,  998.21/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 756.678s\n",
      "Train: 9 [5950/10009 ( 59%)]  Loss: 3.43 (3.46)  Time: 0.133s,  962.82/s  (0.128s,  997.75/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 763.446s\n",
      "Train: 9 [6000/10009 ( 60%)]  Loss: 3.64 (3.46)  Time: 0.135s,  946.76/s  (0.128s,  997.28/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 770.221s\n",
      "Train: 9 [6050/10009 ( 60%)]  Loss: 3.34 (3.46)  Time: 0.163s,  786.94/s  (0.128s,  996.81/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 777.005s\n",
      "Train: 9 [6100/10009 ( 61%)]  Loss: 3.55 (3.46)  Time: 0.133s,  961.82/s  (0.128s,  996.36/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 783.781s\n",
      "Train: 9 [6150/10009 ( 61%)]  Loss: 3.54 (3.46)  Time: 0.135s,  948.36/s  (0.129s,  995.74/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 790.693s\n",
      "Train: 9 [6200/10009 ( 62%)]  Loss: 3.60 (3.46)  Time: 0.134s,  955.81/s  (0.129s,  995.04/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 797.681s\n",
      "Train: 9 [6250/10009 ( 62%)]  Loss: 3.19 (3.46)  Time: 0.142s,  899.33/s  (0.129s,  994.51/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 804.542s\n",
      "Train: 9 [6300/10009 ( 63%)]  Loss: 3.26 (3.46)  Time: 0.137s,  937.43/s  (0.129s,  993.74/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 811.606s\n",
      "Train: 9 [6350/10009 ( 63%)]  Loss: 3.67 (3.46)  Time: 0.136s,  942.88/s  (0.129s,  993.22/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 818.475s\n",
      "Train: 9 [6400/10009 ( 64%)]  Loss: 3.43 (3.46)  Time: 0.150s,  850.79/s  (0.129s,  992.47/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 825.542s\n",
      "Train: 9 [6450/10009 ( 64%)]  Loss: 3.39 (3.46)  Time: 0.149s,  858.92/s  (0.129s,  991.71/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 832.624s\n",
      "Train: 9 [6500/10009 ( 65%)]  Loss: 3.04 (3.46)  Time: 0.135s,  945.84/s  (0.129s,  990.85/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 839.811s\n",
      "Train: 9 [6550/10009 ( 65%)]  Loss: 3.52 (3.46)  Time: 0.134s,  954.34/s  (0.129s,  990.21/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 846.814s\n",
      "Train: 9 [6600/10009 ( 66%)]  Loss: 3.36 (3.46)  Time: 0.142s,  899.17/s  (0.129s,  989.81/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 853.621s\n",
      "Train: 9 [6650/10009 ( 66%)]  Loss: 3.50 (3.46)  Time: 0.134s,  958.55/s  (0.129s,  989.23/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 860.592s\n",
      "Train: 9 [6700/10009 ( 67%)]  Loss: 3.39 (3.46)  Time: 0.137s,  935.04/s  (0.129s,  988.84/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 867.409s\n",
      "Train: 9 [6750/10009 ( 67%)]  Loss: 3.75 (3.46)  Time: 0.132s,  970.25/s  (0.129s,  988.49/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 874.185s\n",
      "Train: 9 [6800/10009 ( 68%)]  Loss: 3.59 (3.46)  Time: 0.133s,  961.40/s  (0.130s,  988.02/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 881.078s\n",
      "Train: 9 [6850/10009 ( 68%)]  Loss: 3.27 (3.46)  Time: 0.153s,  838.00/s  (0.130s,  987.10/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 888.385s\n",
      "Train: 9 [6900/10009 ( 69%)]  Loss: 3.61 (3.46)  Time: 0.161s,  792.77/s  (0.130s,  986.32/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 895.572s\n",
      "Train: 9 [6950/10009 ( 69%)]  Loss: 3.88 (3.46)  Time: 0.135s,  950.99/s  (0.130s,  985.97/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 902.383s\n",
      "Train: 9 [7000/10009 ( 70%)]  Loss: 3.35 (3.46)  Time: 0.172s,  745.63/s  (0.130s,  985.65/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 909.169s\n",
      "Train: 9 [7050/10009 ( 70%)]  Loss: 3.49 (3.46)  Time: 0.134s,  952.99/s  (0.130s,  984.90/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 916.362s\n",
      "Train: 9 [7100/10009 ( 71%)]  Loss: 3.82 (3.46)  Time: 0.134s,  952.59/s  (0.130s,  984.39/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 923.342s\n",
      "Train: 9 [7150/10009 ( 71%)]  Loss: 3.29 (3.46)  Time: 0.133s,  964.23/s  (0.130s,  983.98/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 930.229s\n",
      "Train: 9 [7200/10009 ( 72%)]  Loss: 3.33 (3.46)  Time: 0.131s,  975.72/s  (0.130s,  983.83/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 936.877s\n",
      "Train: 9 [7250/10009 ( 72%)]  Loss: 3.30 (3.46)  Time: 0.128s, 1002.77/s  (0.130s,  983.70/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 943.507s\n",
      "Train: 9 [7300/10009 ( 73%)]  Loss: 3.40 (3.46)  Time: 0.133s,  964.22/s  (0.130s,  983.48/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 950.221s\n",
      "Train: 9 [7350/10009 ( 73%)]  Loss: 3.43 (3.46)  Time: 0.129s,  994.91/s  (0.130s,  983.37/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 956.834s\n",
      "Train: 9 [7400/10009 ( 74%)]  Loss: 3.30 (3.46)  Time: 0.129s,  992.57/s  (0.130s,  983.25/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 963.460s\n",
      "Train: 9 [7450/10009 ( 74%)]  Loss: 3.22 (3.46)  Time: 0.131s,  977.84/s  (0.130s,  983.20/s)  LR: 1.727e-05  Data: 0.009 (0.007)Time: 970.022s\n",
      "Train: 9 [7500/10009 ( 75%)]  Loss: 3.27 (3.46)  Time: 0.137s,  934.81/s  (0.130s,  982.99/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 976.736s\n",
      "Train: 9 [7550/10009 ( 75%)]  Loss: 3.38 (3.46)  Time: 0.130s,  984.47/s  (0.130s,  982.90/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 983.335s\n",
      "Train: 9 [7600/10009 ( 76%)]  Loss: 3.66 (3.46)  Time: 0.127s, 1004.82/s  (0.130s,  982.84/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 989.912s\n",
      "Train: 9 [7650/10009 ( 76%)]  Loss: 3.50 (3.46)  Time: 0.129s,  994.18/s  (0.130s,  982.72/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 996.540s\n",
      "Train: 9 [7700/10009 ( 77%)]  Loss: 3.81 (3.46)  Time: 0.138s,  928.53/s  (0.130s,  982.68/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1003.097s\n",
      "Train: 9 [7750/10009 ( 77%)]  Loss: 3.71 (3.46)  Time: 0.136s,  941.15/s  (0.130s,  982.59/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1009.708s\n",
      "Train: 9 [7800/10009 ( 78%)]  Loss: 3.45 (3.46)  Time: 0.130s,  987.54/s  (0.130s,  982.39/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1016.419s\n",
      "Train: 9 [7850/10009 ( 78%)]  Loss: 3.95 (3.46)  Time: 0.154s,  831.10/s  (0.130s,  982.25/s)  LR: 1.727e-05  Data: 0.010 (0.007)Time: 1023.083s\n",
      "Train: 9 [7900/10009 ( 79%)]  Loss: 3.15 (3.46)  Time: 0.129s,  989.75/s  (0.130s,  982.26/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1029.594s\n",
      "Train: 9 [7950/10009 ( 79%)]  Loss: 3.58 (3.46)  Time: 0.128s,  998.54/s  (0.130s,  982.16/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1036.206s\n",
      "Train: 9 [8000/10009 ( 80%)]  Loss: 3.25 (3.46)  Time: 0.130s,  987.61/s  (0.130s,  982.10/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1042.785s\n",
      "Train: 9 [8050/10009 ( 80%)]  Loss: 3.37 (3.46)  Time: 0.139s,  919.73/s  (0.130s,  981.94/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1049.476s\n",
      "Train: 9 [8100/10009 ( 81%)]  Loss: 3.42 (3.46)  Time: 0.129s,  994.09/s  (0.130s,  981.88/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1056.064s\n",
      "Train: 9 [8150/10009 ( 81%)]  Loss: 3.38 (3.46)  Time: 0.129s,  988.55/s  (0.130s,  981.83/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1062.634s\n",
      "Train: 9 [8200/10009 ( 82%)]  Loss: 3.29 (3.46)  Time: 0.128s, 1003.16/s  (0.130s,  981.81/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1069.168s\n",
      "Train: 9 [8250/10009 ( 82%)]  Loss: 3.30 (3.46)  Time: 0.128s,  996.16/s  (0.130s,  981.69/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1075.827s\n",
      "Train: 9 [8300/10009 ( 83%)]  Loss: 3.54 (3.46)  Time: 0.137s,  934.96/s  (0.130s,  981.48/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1082.575s\n",
      "Train: 9 [8350/10009 ( 83%)]  Loss: 3.71 (3.46)  Time: 0.130s,  983.18/s  (0.130s,  981.44/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1089.140s\n",
      "Train: 9 [8400/10009 ( 84%)]  Loss: 3.75 (3.46)  Time: 0.129s,  988.88/s  (0.130s,  981.33/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1095.778s\n",
      "Train: 9 [8450/10009 ( 84%)]  Loss: 3.46 (3.46)  Time: 0.163s,  783.81/s  (0.130s,  981.16/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1102.493s\n",
      "Train: 9 [8500/10009 ( 85%)]  Loss: 3.38 (3.46)  Time: 0.131s,  974.61/s  (0.130s,  981.02/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1109.181s\n",
      "Train: 9 [8550/10009 ( 85%)]  Loss: 3.59 (3.46)  Time: 0.128s,  996.39/s  (0.130s,  980.89/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1115.846s\n",
      "Train: 9 [8600/10009 ( 86%)]  Loss: 3.50 (3.46)  Time: 0.133s,  964.06/s  (0.131s,  980.71/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1122.579s\n",
      "Train: 9 [8650/10009 ( 86%)]  Loss: 3.39 (3.46)  Time: 0.131s,  975.82/s  (0.131s,  980.71/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1129.110s\n",
      "Train: 9 [8700/10009 ( 87%)]  Loss: 3.51 (3.46)  Time: 0.128s,  999.74/s  (0.131s,  980.69/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1135.658s\n",
      "Train: 9 [8750/10009 ( 87%)]  Loss: 3.36 (3.46)  Time: 0.146s,  876.03/s  (0.131s,  980.64/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1142.237s\n",
      "Train: 9 [8800/10009 ( 88%)]  Loss: 3.46 (3.46)  Time: 0.132s,  971.56/s  (0.131s,  980.54/s)  LR: 1.727e-05  Data: 0.008 (0.007)Time: 1148.876s\n",
      "Train: 9 [8850/10009 ( 88%)]  Loss: 3.48 (3.46)  Time: 0.131s,  980.70/s  (0.131s,  980.50/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1155.450s\n",
      "Train: 9 [8900/10009 ( 89%)]  Loss: 3.59 (3.46)  Time: 0.133s,  964.41/s  (0.131s,  980.46/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1162.025s\n",
      "Train: 9 [8950/10009 ( 89%)]  Loss: 3.51 (3.46)  Time: 0.131s,  979.59/s  (0.131s,  980.40/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1168.632s\n",
      "Train: 9 [9000/10009 ( 90%)]  Loss: 3.35 (3.46)  Time: 0.129s,  993.43/s  (0.131s,  980.34/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1175.223s\n",
      "Train: 9 [9050/10009 ( 90%)]  Loss: 3.68 (3.46)  Time: 0.132s,  966.30/s  (0.131s,  980.29/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1181.816s\n",
      "Train: 9 [9100/10009 ( 91%)]  Loss: 3.48 (3.46)  Time: 0.129s,  992.79/s  (0.131s,  980.24/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1188.407s\n",
      "Train: 9 [9150/10009 ( 91%)]  Loss: 3.69 (3.46)  Time: 0.132s,  970.23/s  (0.131s,  980.17/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1195.016s\n",
      "Train: 9 [9200/10009 ( 92%)]  Loss: 3.50 (3.46)  Time: 0.130s,  985.42/s  (0.131s,  980.05/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1201.700s\n",
      "Train: 9 [9250/10009 ( 92%)]  Loss: 3.49 (3.46)  Time: 0.130s,  987.43/s  (0.131s,  979.99/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1208.296s\n",
      "Train: 9 [9300/10009 ( 93%)]  Loss: 3.34 (3.46)  Time: 0.129s,  993.88/s  (0.131s,  979.94/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1214.900s\n",
      "Train: 9 [9350/10009 ( 93%)]  Loss: 3.68 (3.46)  Time: 0.128s, 1000.85/s  (0.131s,  979.91/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 1221.461s\n",
      "Train: 9 [9400/10009 ( 94%)]  Loss: 3.59 (3.46)  Time: 0.140s,  912.56/s  (0.131s,  979.89/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1228.018s\n",
      "Train: 9 [9450/10009 ( 94%)]  Loss: 3.53 (3.46)  Time: 0.130s,  986.03/s  (0.131s,  979.82/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1234.633s\n",
      "Train: 9 [9500/10009 ( 95%)]  Loss: 3.45 (3.46)  Time: 0.144s,  889.69/s  (0.131s,  979.69/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 1241.340s\n",
      "Train: 9 [9550/10009 ( 95%)]  Loss: 3.50 (3.46)  Time: 0.147s,  872.15/s  (0.131s,  979.63/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1247.943s\n",
      "Train: 9 [9600/10009 ( 96%)]  Loss: 3.54 (3.46)  Time: 0.131s,  975.83/s  (0.131s,  979.55/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1254.580s\n",
      "Train: 9 [9650/10009 ( 96%)]  Loss: 3.89 (3.46)  Time: 0.129s,  993.86/s  (0.131s,  979.46/s)  LR: 1.727e-05  Data: 0.005 (0.007)Time: 1261.225s\n",
      "Train: 9 [9700/10009 ( 97%)]  Loss: 3.58 (3.46)  Time: 0.128s,  997.67/s  (0.131s,  979.25/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1268.041s\n",
      "Train: 9 [9750/10009 ( 97%)]  Loss: 3.77 (3.46)  Time: 0.129s,  989.78/s  (0.131s,  979.11/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1274.759s\n",
      "Train: 9 [9800/10009 ( 98%)]  Loss: 3.26 (3.46)  Time: 0.157s,  815.88/s  (0.131s,  978.95/s)  LR: 1.727e-05  Data: 0.007 (0.007)Time: 1281.498s\n",
      "Train: 9 [9850/10009 ( 98%)]  Loss: 3.51 (3.46)  Time: 0.128s, 1003.19/s  (0.131s,  978.83/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1288.192s\n",
      "Train: 9 [9900/10009 ( 99%)]  Loss: 3.34 (3.46)  Time: 0.130s,  983.77/s  (0.131s,  978.66/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1294.952s\n",
      "Train: 9 [9950/10009 ( 99%)]  Loss: 3.62 (3.46)  Time: 0.128s,  997.46/s  (0.131s,  978.58/s)  LR: 1.727e-05  Data: 0.006 (0.007)Time: 1301.608s\n",
      "Train: 9 [10000/10009 (100%)]  Loss: 3.29 (3.46)  Time: 0.177s,  724.23/s  (0.131s,  978.53/s)  LR: 1.727e-05  Data: 0.054 (0.007)Time: 1308.208s\n",
      "Test: [   0/390]  Time: 0.667 (0.667)  Loss:   1.613 ( 1.613)  Acc@1:  65.625 ( 65.625)  Acc@5:  85.156 ( 85.156)\n",
      "Test: [  50/390]  Time: 0.039 (0.142)  Loss:   1.421 ( 2.237)  Acc@1:  67.188 ( 52.956)  Acc@5:  85.938 ( 75.000)\n",
      "Test: [ 100/390]  Time: 0.171 (0.134)  Loss:   2.276 ( 2.263)  Acc@1:  47.656 ( 49.776)  Acc@5:  78.125 ( 75.565)\n",
      "Test: [ 150/390]  Time: 0.039 (0.139)  Loss:   2.003 ( 2.222)  Acc@1:  50.000 ( 50.611)  Acc@5:  83.594 ( 76.293)\n",
      "Test: [ 200/390]  Time: 0.039 (0.135)  Loss:   3.483 ( 2.420)  Acc@1:  21.094 ( 47.458)  Acc@5:  55.469 ( 72.796)\n",
      "Test: [ 250/390]  Time: 0.040 (0.134)  Loss:   2.318 ( 2.538)  Acc@1:  55.469 ( 45.770)  Acc@5:  72.656 ( 70.835)\n",
      "Test: [ 300/390]  Time: 0.443 (0.134)  Loss:   2.822 ( 2.639)  Acc@1:  43.750 ( 44.129)  Acc@5:  66.406 ( 68.999)\n",
      "Test: [ 350/390]  Time: 0.040 (0.133)  Loss:   2.899 ( 2.715)  Acc@1:  38.281 ( 42.895)  Acc@5:  69.531 ( 67.677)\n",
      "Test: [ 390/390]  Time: 0.026 (0.132)  Loss:   3.884 ( 2.680)  Acc@1:  18.750 ( 43.538)  Acc@5:  50.000 ( 68.332)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-9.pth.tar', 43.538)\n",
      " ('./output/budgeted/checkpoint-8.pth.tar', 41.378)\n",
      " ('./output/budgeted/checkpoint-7.pth.tar', 39.734)\n",
      " ('./output/budgeted/checkpoint-6.pth.tar', 36.718)\n",
      " ('./output/budgeted/checkpoint-5.pth.tar', 34.288)\n",
      "\n",
      "*** Best metric: 43.538 (epoch 9)\n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        for epoch in range(start_epoch, 10):\n",
    "            if hasattr(dataset_train, 'set_epoch'):\n",
    "                dataset_train.set_epoch(epoch)\n",
    "            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n",
    "                loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                train_loss_fn,\n",
    "                args,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                saver=saver,\n",
    "                output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast,\n",
    "                loss_scaler=loss_scaler,\n",
    "                model_ema=model_ema,\n",
    "                mixup_fn=mixup_fn,\n",
    "                # fish: add preconditioner\n",
    "                preconditioner=preconditioner,\n",
    "            )\n",
    "\n",
    "            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                if utils.is_primary(args):\n",
    "                    _logger.info(\"Distributing BatchNorm running means and vars\")\n",
    "                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "            eval_metrics = validate(\n",
    "                model,\n",
    "                loader_eval,\n",
    "                validate_loss_fn,\n",
    "                args,\n",
    "                amp_autocast=amp_autocast,\n",
    "            )\n",
    "\n",
    "            if model_ema is not None and not args.model_ema_force_cpu:\n",
    "                if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                    utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "                ema_eval_metrics = validate(\n",
    "                    model_ema.module,\n",
    "                    loader_eval,\n",
    "                    validate_loss_fn,\n",
    "                    args,\n",
    "                    amp_autocast=amp_autocast,\n",
    "                    log_suffix=' (EMA)',\n",
    "                )\n",
    "                eval_metrics = ema_eval_metrics\n",
    "\n",
    "            if output_dir is not None:\n",
    "                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "                utils.update_summary(\n",
    "                    epoch,\n",
    "                    train_metrics,\n",
    "                    eval_metrics,\n",
    "                    filename=os.path.join(output_dir, 'summary.csv'),\n",
    "                    lr=sum(lrs) / len(lrs),\n",
    "                    write_header=best_metric is None,\n",
    "                    log_wandb=args.log_wandb and has_wandb,\n",
    "                )\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model bud_small_patch16_224_man created, param count:22050664\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n",
      "Using native Torch AMP. Training in mixed precision.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:euj4dvhf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c227e0ed613046389e925ffef65f85a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>eval_loss</td><td>█▆▄▃▁</td></tr><tr><td>eval_top1</td><td>▁▃▅▆█</td></tr><tr><td>eval_top5</td><td>▁▃▅▇█</td></tr><tr><td>lr</td><td>█▆▅▃▁</td></tr><tr><td>train_loss</td><td>█▅▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>eval_loss</td><td>2.67956</td></tr><tr><td>eval_top1</td><td>43.538</td></tr><tr><td>eval_top5</td><td>68.332</td></tr><tr><td>lr</td><td>2e-05</td></tr><tr><td>train_loss</td><td>3.45738</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">budgeted</strong> at: <a href='https://wandb.ai/fabfish/timm/runs/euj4dvhf' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/euj4dvhf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230827_175115-euj4dvhf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:euj4dvhf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d1b0a2c9154f8e953286f06c73f372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668962250696494, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fish/Documents/GitHub/pytorch-image-models/wandb/run-20230827_194901-pl7sw7b1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabfish/timm/runs/pl7sw7b1' target=\"_blank\">budgeted</a></strong> to <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">https://wandb.ai/fabfish/timm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabfish/timm/runs/pl7sw7b1' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/pl7sw7b1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scheduled epochs: 15. LR stepped per epoch.\n"
     ]
    }
   ],
   "source": [
    "    old_model = model\n",
    "    old_optimizer = optimizer\n",
    "    old_loss_scaler = loss_scaler\n",
    "    old_lr_scheduler = lr_scheduler\n",
    "    if args.use_eva:\n",
    "        old_preconditioner = preconditioner\n",
    "\n",
    "    # args.model = args.model.replace('baby', 'child')\n",
    "    args.model = args.model.replace('child', 'man')\n",
    "    # args.model = args.model.replace('man', 'baby')\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm',name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 10\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0047,  0.0291,  0.0084,  ...,  0.0061,  0.0207,  0.0245],\n",
      "        [-0.0064, -0.0107, -0.0039,  ..., -0.0381, -0.0248, -0.0018],\n",
      "        [ 0.0030,  0.0182, -0.0012,  ...,  0.0175, -0.0210, -0.0017],\n",
      "        ...,\n",
      "        [-0.0133,  0.0130,  0.0257,  ..., -0.0031,  0.0145,  0.0067],\n",
      "        [ 0.0034,  0.0407,  0.0003,  ..., -0.0187,  0.0027,  0.0183],\n",
      "        [ 0.0138, -0.0381, -0.0353,  ...,  0.0069, -0.0030,  0.0216]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0384,  0.0526,  0.0231,  ...,  0.0080, -0.0127, -0.0318],\n",
      "        [ 0.0123,  0.0325, -0.0157,  ..., -0.0021, -0.1333, -0.1377],\n",
      "        [-0.0235, -0.0248, -0.1257,  ...,  0.0762, -0.0377, -0.0869],\n",
      "        ...,\n",
      "        [-0.0528,  0.0271,  0.0363,  ...,  0.0818,  0.0323, -0.0372],\n",
      "        [ 0.0458,  0.0301, -0.0118,  ...,  0.0388, -0.0197,  0.0305],\n",
      "        [ 0.1512,  0.0830, -0.0156,  ...,  0.0436,  0.0078,  0.0451]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0384,  0.0526,  0.0231,  ...,  0.0080, -0.0127, -0.0318],\n",
      "        [ 0.0123,  0.0325, -0.0157,  ..., -0.0021, -0.1333, -0.1377],\n",
      "        [-0.0235, -0.0248, -0.1257,  ...,  0.0762, -0.0377, -0.0869],\n",
      "        ...,\n",
      "        [-0.0528,  0.0271,  0.0363,  ...,  0.0818,  0.0323, -0.0372],\n",
      "        [ 0.0458,  0.0301, -0.0118,  ...,  0.0388, -0.0197,  0.0305],\n",
      "        [ 0.1512,  0.0830, -0.0156,  ...,  0.0436,  0.0078,  0.0451]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "print(old_model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    tmp = old_model.state_dict()\n",
    "    for param_tensor in old_model.state_dict(): # 字典的遍历默认是遍历 key，所以param_tensor实际上是键值\n",
    "        # print(param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "        # 如果不是 attn 或者 mlp 层\n",
    "        if 'attn' not in param_tensor and 'mlp' not in param_tensor:\n",
    "            continue\n",
    "        old_size = old_model.state_dict()[param_tensor].size()\n",
    "        new_size = model.state_dict()[param_tensor].size()\n",
    "        flag = False\n",
    "        if len(old_size) != len(new_size):\n",
    "            continue\n",
    "        elif len(old_size) == 1:\n",
    "            # 如果整除\n",
    "            if new_size[0] % old_size[0] == 0:\n",
    "                times = new_size[0] // old_size[0]\n",
    "                tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "            # if old_size[0]*2 == new_size[0]:\n",
    "            #     tmp[param_tensor] = torch.cat((tmp[param_tensor], tmp[param_tensor]), 0)\n",
    "                flag = True\n",
    "            # 如果是 2:3\n",
    "            elif old_size[0]*3 == new_size[0]*2:\n",
    "                # 拼接tensor和它自己的一半\n",
    "                if old_size[0] % 2 == 0:\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                    # tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][]), 0)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    # 未实现\n",
    "                    print('error')\n",
    "        elif len(old_size) == 2:\n",
    "            if old_size[0] == new_size[0]:\n",
    "                # if old_size[1]*2 == new_size[1]:\n",
    "                if new_size[1] % old_size[1] == 0:\n",
    "                    times = new_size[1] // old_size[1]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 1)\n",
    "                    flag = True\n",
    "                elif old_size[1]*3 == new_size[1]*2:\n",
    "                    if old_size[1] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:,old_size[1]//2]), 1)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "            elif old_size[1] == new_size[1]:\n",
    "                # if old_size[0]*2 == new_size[0]:\n",
    "                if new_size[0] % old_size[0] == 0:\n",
    "                    times = new_size[0] // old_size[0]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "                    flag = True\n",
    "                elif old_size[0]*3 == new_size[0]*2:    \n",
    "                    if old_size[0] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "        if not flag:\n",
    "            if param_tensor in model.state_dict():\n",
    "                tmp[param_tensor] = model.state_dict()[param_tensor]\n",
    "            else:\n",
    "                continue\n",
    "    model.load_state_dict(tmp, strict=False)\n",
    "    print(model.state_dict()['blocks.0.attn.qkv.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 384]) torch.Size([1, 1, 384])\n",
      "torch.Size([1, 197, 384]) torch.Size([1, 197, 384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([1000]) torch.Size([1000])\n",
      "torch.Size([384, 3, 16, 16]) torch.Size([384, 3, 16, 16])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([1000, 384]) torch.Size([1000, 384])\n"
     ]
    }
   ],
   "source": [
    "len(old_optimizer.param_groups[0]['params'])\n",
    "# 接下来，遍历原始模型的参数，把他们加入新的优化器中\n",
    "for param_group in range(len(old_optimizer.param_groups)):\n",
    "    for i, (old_p, new_p) in enumerate(zip(old_optimizer.param_groups[param_group]['params'], optimizer.param_groups[param_group]['params'])):\n",
    "        # 查看 old_p 和 new_p 的 shape\n",
    "        print(old_p.shape, new_p.shape)\n",
    "        with torch.no_grad():\n",
    "            if old_p.shape == new_p.shape:\n",
    "                new_p = old_p\n",
    "            else:\n",
    "                # 否则，用 0 填补 new_p 的后半部分\n",
    "                # 如果 new_p 是 1 维\n",
    "                if len(new_p.shape) == 1:\n",
    "                    new_p[:old_p.shape[0]] = old_p\n",
    "                # 如果 new_p 是 2 维\n",
    "                elif len(new_p.shape) == 2:\n",
    "                    new_p[:old_p.shape[0], :old_p.shape[1]] = old_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 10 [   0/10009 (  0%)]  Loss: 7.12 (7.12)  Time: 1.011s,  126.66/s  (1.011s,  126.66/s)  LR: 1.250e-05  Data: 0.641 (0.641)Time: 1.011s\n",
      "Train: 10 [  50/10009 (  0%)]  Loss: 5.17 (5.95)  Time: 0.166s,  773.36/s  (0.183s,  699.49/s)  LR: 1.250e-05  Data: 0.006 (0.019)Time: 9.333s\n",
      "Train: 10 [ 100/10009 (  1%)]  Loss: 4.69 (5.42)  Time: 0.172s,  743.37/s  (0.176s,  727.37/s)  LR: 1.250e-05  Data: 0.008 (0.013)Time: 17.774s\n",
      "Train: 10 [ 150/10009 (  1%)]  Loss: 4.39 (5.10)  Time: 0.172s,  742.70/s  (0.174s,  734.21/s)  LR: 1.250e-05  Data: 0.008 (0.011)Time: 26.325s\n",
      "Train: 10 [ 200/10009 (  2%)]  Loss: 4.23 (4.89)  Time: 0.170s,  750.99/s  (0.174s,  737.72/s)  LR: 1.250e-05  Data: 0.005 (0.009)Time: 34.876s\n",
      "Train: 10 [ 250/10009 (  2%)]  Loss: 3.89 (4.73)  Time: 0.170s,  753.43/s  (0.173s,  740.10/s)  LR: 1.250e-05  Data: 0.006 (0.009)Time: 43.411s\n",
      "Train: 10 [ 300/10009 (  3%)]  Loss: 4.14 (4.61)  Time: 0.173s,  741.73/s  (0.173s,  741.90/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 51.932s\n",
      "Train: 10 [ 350/10009 (  3%)]  Loss: 3.91 (4.50)  Time: 0.171s,  749.14/s  (0.172s,  743.37/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 60.439s\n",
      "Train: 10 [ 400/10009 (  4%)]  Loss: 3.85 (4.42)  Time: 0.171s,  750.21/s  (0.172s,  744.42/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 68.950s\n",
      "Train: 10 [ 450/10009 (  4%)]  Loss: 3.73 (4.35)  Time: 0.170s,  754.08/s  (0.172s,  744.97/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 77.491s\n",
      "Train: 10 [ 500/10009 (  5%)]  Loss: 3.91 (4.29)  Time: 0.171s,  750.30/s  (0.172s,  745.69/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 85.999s\n",
      "Train: 10 [ 550/10009 (  5%)]  Loss: 3.31 (4.24)  Time: 0.170s,  752.69/s  (0.172s,  745.02/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 94.666s\n",
      "Train: 10 [ 600/10009 (  6%)]  Loss: 3.57 (4.20)  Time: 0.175s,  730.97/s  (0.172s,  744.80/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 103.288s\n",
      "Train: 10 [ 650/10009 (  6%)]  Loss: 3.83 (4.16)  Time: 0.175s,  730.44/s  (0.172s,  744.25/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 111.962s\n",
      "Train: 10 [ 700/10009 (  7%)]  Loss: 3.63 (4.12)  Time: 0.172s,  743.10/s  (0.172s,  743.65/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 120.659s\n",
      "Train: 10 [ 750/10009 (  7%)]  Loss: 3.59 (4.09)  Time: 0.175s,  732.46/s  (0.172s,  743.22/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 129.340s\n",
      "Train: 10 [ 800/10009 (  8%)]  Loss: 3.45 (4.07)  Time: 0.173s,  741.97/s  (0.172s,  742.77/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 138.034s\n",
      "Train: 10 [ 850/10009 (  8%)]  Loss: 3.47 (4.04)  Time: 0.174s,  737.28/s  (0.172s,  742.38/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 146.728s\n",
      "Train: 10 [ 900/10009 (  9%)]  Loss: 3.64 (4.02)  Time: 0.173s,  741.65/s  (0.172s,  742.03/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 155.422s\n",
      "Train: 10 [ 950/10009 (  9%)]  Loss: 3.70 (4.00)  Time: 0.174s,  735.57/s  (0.173s,  741.70/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 164.121s\n",
      "Train: 10 [1000/10009 ( 10%)]  Loss: 3.51 (3.98)  Time: 0.173s,  741.95/s  (0.173s,  741.38/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 172.824s\n",
      "Train: 10 [1050/10009 ( 10%)]  Loss: 3.61 (3.96)  Time: 0.171s,  749.46/s  (0.173s,  741.52/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 181.422s\n",
      "Train: 10 [1100/10009 ( 11%)]  Loss: 3.63 (3.94)  Time: 0.174s,  736.81/s  (0.173s,  741.69/s)  LR: 1.250e-05  Data: 0.010 (0.007)Time: 190.009s\n",
      "Train: 10 [1150/10009 ( 11%)]  Loss: 3.36 (3.93)  Time: 0.171s,  748.02/s  (0.173s,  741.84/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 198.597s\n",
      "Train: 10 [1200/10009 ( 12%)]  Loss: 3.68 (3.91)  Time: 0.175s,  732.87/s  (0.173s,  741.86/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 207.221s\n",
      "Train: 10 [1250/10009 ( 12%)]  Loss: 3.75 (3.90)  Time: 0.173s,  740.02/s  (0.173s,  741.75/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 215.879s\n",
      "Train: 10 [1300/10009 ( 13%)]  Loss: 3.33 (3.89)  Time: 0.173s,  739.48/s  (0.173s,  741.68/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 224.527s\n",
      "Train: 10 [1350/10009 ( 13%)]  Loss: 3.53 (3.88)  Time: 0.173s,  738.37/s  (0.173s,  741.65/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 233.166s\n",
      "Train: 10 [1400/10009 ( 14%)]  Loss: 3.53 (3.87)  Time: 0.174s,  734.61/s  (0.173s,  741.65/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 241.797s\n",
      "Train: 10 [1450/10009 ( 14%)]  Loss: 3.82 (3.86)  Time: 0.172s,  742.37/s  (0.173s,  741.67/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 250.418s\n",
      "Train: 10 [1500/10009 ( 15%)]  Loss: 3.72 (3.85)  Time: 0.171s,  746.94/s  (0.173s,  741.74/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 259.022s\n",
      "Train: 10 [1550/10009 ( 15%)]  Loss: 3.47 (3.84)  Time: 0.171s,  747.26/s  (0.173s,  741.83/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 267.620s\n",
      "Train: 10 [1600/10009 ( 16%)]  Loss: 3.57 (3.83)  Time: 0.175s,  730.36/s  (0.173s,  741.53/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 276.357s\n",
      "Train: 10 [1650/10009 ( 16%)]  Loss: 3.67 (3.82)  Time: 0.176s,  726.16/s  (0.173s,  741.24/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 285.102s\n",
      "Train: 10 [1700/10009 ( 17%)]  Loss: 3.82 (3.81)  Time: 0.175s,  733.46/s  (0.173s,  740.94/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 293.855s\n",
      "Train: 10 [1750/10009 ( 17%)]  Loss: 3.72 (3.81)  Time: 0.176s,  727.48/s  (0.173s,  740.66/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 302.604s\n",
      "Train: 10 [1800/10009 ( 18%)]  Loss: 3.70 (3.80)  Time: 0.174s,  734.05/s  (0.173s,  740.34/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 311.380s\n",
      "Train: 10 [1850/10009 ( 18%)]  Loss: 3.73 (3.79)  Time: 0.172s,  742.75/s  (0.173s,  740.15/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 320.110s\n",
      "Train: 10 [1900/10009 ( 19%)]  Loss: 3.14 (3.79)  Time: 0.172s,  744.00/s  (0.173s,  740.05/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 328.800s\n",
      "Train: 10 [1950/10009 ( 19%)]  Loss: 3.66 (3.78)  Time: 0.174s,  735.90/s  (0.173s,  739.89/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 337.518s\n",
      "Train: 10 [2000/10009 ( 20%)]  Loss: 3.36 (3.77)  Time: 0.174s,  734.58/s  (0.173s,  739.72/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 346.250s\n",
      "Train: 10 [2050/10009 ( 20%)]  Loss: 3.54 (3.77)  Time: 0.175s,  732.21/s  (0.173s,  739.52/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 354.999s\n",
      "Train: 10 [2100/10009 ( 21%)]  Loss: 3.68 (3.76)  Time: 0.175s,  730.02/s  (0.173s,  739.32/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 363.751s\n",
      "Train: 10 [2150/10009 ( 21%)]  Loss: 3.67 (3.76)  Time: 0.175s,  730.48/s  (0.173s,  739.13/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 372.503s\n",
      "Train: 10 [2200/10009 ( 22%)]  Loss: 3.38 (3.75)  Time: 0.172s,  742.36/s  (0.173s,  739.01/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 381.221s\n",
      "Train: 10 [2250/10009 ( 22%)]  Loss: 3.54 (3.75)  Time: 0.174s,  736.85/s  (0.173s,  738.96/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 389.909s\n",
      "Train: 10 [2300/10009 ( 23%)]  Loss: 3.93 (3.74)  Time: 0.173s,  741.40/s  (0.173s,  738.96/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 398.569s\n",
      "Train: 10 [2350/10009 ( 23%)]  Loss: 3.68 (3.74)  Time: 0.173s,  741.09/s  (0.173s,  739.08/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 407.167s\n",
      "Train: 10 [2400/10009 ( 24%)]  Loss: 3.37 (3.73)  Time: 0.172s,  745.30/s  (0.173s,  739.18/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 415.771s\n",
      "Train: 10 [2450/10009 ( 24%)]  Loss: 3.29 (3.73)  Time: 0.171s,  746.90/s  (0.173s,  739.27/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 424.377s\n",
      "Train: 10 [2500/10009 ( 25%)]  Loss: 3.64 (3.73)  Time: 0.172s,  744.51/s  (0.173s,  739.33/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 432.997s\n",
      "Train: 10 [2550/10009 ( 25%)]  Loss: 3.36 (3.72)  Time: 0.177s,  724.04/s  (0.173s,  739.24/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 441.706s\n",
      "Train: 10 [2600/10009 ( 26%)]  Loss: 3.48 (3.72)  Time: 0.172s,  742.92/s  (0.173s,  739.10/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 450.448s\n",
      "Train: 10 [2650/10009 ( 26%)]  Loss: 3.78 (3.71)  Time: 0.172s,  744.94/s  (0.173s,  739.12/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 459.095s\n",
      "Train: 10 [2700/10009 ( 27%)]  Loss: 3.83 (3.71)  Time: 0.175s,  731.28/s  (0.173s,  739.06/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 467.791s\n",
      "Train: 10 [2750/10009 ( 27%)]  Loss: 3.21 (3.71)  Time: 0.174s,  735.24/s  (0.173s,  738.91/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 476.547s\n",
      "Train: 10 [2800/10009 ( 28%)]  Loss: 3.62 (3.70)  Time: 0.175s,  730.88/s  (0.173s,  738.79/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 485.288s\n",
      "Train: 10 [2850/10009 ( 28%)]  Loss: 3.80 (3.70)  Time: 0.175s,  733.26/s  (0.173s,  738.66/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 494.037s\n",
      "Train: 10 [2900/10009 ( 29%)]  Loss: 3.44 (3.70)  Time: 0.175s,  732.72/s  (0.173s,  738.53/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 502.793s\n",
      "Train: 10 [2950/10009 ( 29%)]  Loss: 3.49 (3.69)  Time: 0.175s,  731.99/s  (0.173s,  738.40/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 511.551s\n",
      "Train: 10 [3000/10009 ( 30%)]  Loss: 3.38 (3.69)  Time: 0.176s,  728.79/s  (0.173s,  738.27/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 520.310s\n",
      "Train: 10 [3050/10009 ( 30%)]  Loss: 3.42 (3.69)  Time: 0.175s,  731.78/s  (0.173s,  738.14/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 529.068s\n",
      "Train: 10 [3100/10009 ( 31%)]  Loss: 3.02 (3.68)  Time: 0.175s,  730.07/s  (0.173s,  738.01/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 537.831s\n",
      "Train: 10 [3150/10009 ( 31%)]  Loss: 3.66 (3.68)  Time: 0.175s,  732.35/s  (0.173s,  737.90/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 546.587s\n",
      "Train: 10 [3200/10009 ( 32%)]  Loss: 3.60 (3.68)  Time: 0.175s,  730.89/s  (0.173s,  737.79/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 555.346s\n",
      "Train: 10 [3250/10009 ( 32%)]  Loss: 3.45 (3.68)  Time: 0.172s,  742.66/s  (0.174s,  737.68/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 564.106s\n",
      "Train: 10 [3300/10009 ( 33%)]  Loss: 3.57 (3.67)  Time: 0.176s,  728.33/s  (0.174s,  737.60/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 572.844s\n",
      "Train: 10 [3350/10009 ( 33%)]  Loss: 3.43 (3.67)  Time: 0.175s,  731.34/s  (0.174s,  737.51/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 581.585s\n",
      "Train: 10 [3400/10009 ( 34%)]  Loss: 3.60 (3.67)  Time: 0.173s,  738.30/s  (0.174s,  737.45/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 590.316s\n",
      "Train: 10 [3450/10009 ( 34%)]  Loss: 3.66 (3.67)  Time: 0.174s,  735.44/s  (0.174s,  737.39/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 599.046s\n",
      "Train: 10 [3500/10009 ( 35%)]  Loss: 3.51 (3.66)  Time: 0.173s,  740.14/s  (0.174s,  737.34/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 607.760s\n",
      "Train: 10 [3550/10009 ( 35%)]  Loss: 3.73 (3.66)  Time: 0.173s,  741.51/s  (0.174s,  737.31/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 616.469s\n",
      "Train: 10 [3600/10009 ( 36%)]  Loss: 3.20 (3.66)  Time: 0.173s,  737.95/s  (0.174s,  737.31/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 625.149s\n",
      "Train: 10 [3650/10009 ( 36%)]  Loss: 3.44 (3.66)  Time: 0.173s,  738.09/s  (0.174s,  737.27/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 633.863s\n",
      "Train: 10 [3700/10009 ( 37%)]  Loss: 3.42 (3.65)  Time: 0.175s,  729.97/s  (0.174s,  737.19/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 642.611s\n",
      "Train: 10 [3750/10009 ( 37%)]  Loss: 3.26 (3.65)  Time: 0.176s,  728.25/s  (0.174s,  737.11/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 651.365s\n",
      "Train: 10 [3800/10009 ( 38%)]  Loss: 3.42 (3.65)  Time: 0.175s,  731.05/s  (0.174s,  737.03/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 660.122s\n",
      "Train: 10 [3850/10009 ( 38%)]  Loss: 3.46 (3.65)  Time: 0.174s,  736.81/s  (0.174s,  736.97/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 668.858s\n",
      "Train: 10 [3900/10009 ( 39%)]  Loss: 3.28 (3.65)  Time: 0.172s,  742.73/s  (0.174s,  736.92/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 677.583s\n",
      "Train: 10 [3950/10009 ( 39%)]  Loss: 3.66 (3.64)  Time: 0.173s,  741.44/s  (0.174s,  736.96/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 686.234s\n",
      "Train: 10 [4000/10009 ( 40%)]  Loss: 3.64 (3.64)  Time: 0.174s,  735.93/s  (0.174s,  736.93/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 694.944s\n",
      "Train: 10 [4050/10009 ( 40%)]  Loss: 3.32 (3.64)  Time: 0.173s,  741.82/s  (0.174s,  736.98/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 703.586s\n",
      "Train: 10 [4100/10009 ( 41%)]  Loss: 3.63 (3.64)  Time: 0.173s,  739.03/s  (0.174s,  737.01/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 712.237s\n",
      "Train: 10 [4150/10009 ( 41%)]  Loss: 3.36 (3.64)  Time: 0.172s,  742.17/s  (0.174s,  737.07/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 720.865s\n",
      "Train: 10 [4200/10009 ( 42%)]  Loss: 3.35 (3.63)  Time: 0.172s,  744.86/s  (0.174s,  737.13/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 729.489s\n",
      "Train: 10 [4250/10009 ( 42%)]  Loss: 3.59 (3.63)  Time: 0.172s,  744.56/s  (0.174s,  737.20/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 738.096s\n",
      "Train: 10 [4300/10009 ( 43%)]  Loss: 3.57 (3.63)  Time: 0.172s,  745.49/s  (0.174s,  737.28/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 746.704s\n",
      "Train: 10 [4350/10009 ( 43%)]  Loss: 3.37 (3.63)  Time: 0.175s,  730.19/s  (0.174s,  737.25/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 755.410s\n",
      "Train: 10 [4400/10009 ( 44%)]  Loss: 3.64 (3.63)  Time: 0.174s,  733.89/s  (0.174s,  737.17/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 764.176s\n",
      "Train: 10 [4450/10009 ( 44%)]  Loss: 3.62 (3.63)  Time: 0.176s,  729.25/s  (0.174s,  737.11/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 772.923s\n",
      "Train: 10 [4500/10009 ( 45%)]  Loss: 3.81 (3.62)  Time: 0.175s,  729.67/s  (0.174s,  737.06/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 781.651s\n",
      "Train: 10 [4550/10009 ( 45%)]  Loss: 3.36 (3.62)  Time: 0.173s,  739.39/s  (0.174s,  737.02/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 790.387s\n",
      "Train: 10 [4600/10009 ( 46%)]  Loss: 3.51 (3.62)  Time: 0.174s,  733.56/s  (0.174s,  736.95/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 799.139s\n",
      "Train: 10 [4650/10009 ( 46%)]  Loss: 3.55 (3.62)  Time: 0.173s,  739.13/s  (0.174s,  736.99/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 807.778s\n",
      "Train: 10 [4700/10009 ( 47%)]  Loss: 3.50 (3.62)  Time: 0.173s,  739.20/s  (0.174s,  736.95/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 816.509s\n",
      "Train: 10 [4750/10009 ( 47%)]  Loss: 3.51 (3.62)  Time: 0.177s,  724.45/s  (0.174s,  736.90/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 825.252s\n",
      "Train: 10 [4800/10009 ( 48%)]  Loss: 3.53 (3.61)  Time: 0.175s,  731.70/s  (0.174s,  736.84/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 834.000s\n",
      "Train: 10 [4850/10009 ( 48%)]  Loss: 3.66 (3.61)  Time: 0.174s,  734.70/s  (0.174s,  736.42/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 843.165s\n",
      "Train: 10 [4900/10009 ( 49%)]  Loss: 3.26 (3.61)  Time: 0.175s,  732.97/s  (0.174s,  735.21/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 853.264s\n",
      "Train: 10 [4950/10009 ( 49%)]  Loss: 3.56 (3.61)  Time: 0.173s,  738.14/s  (0.174s,  735.18/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 862.004s\n",
      "Train: 10 [5000/10009 ( 50%)]  Loss: 3.87 (3.61)  Time: 0.174s,  737.27/s  (0.174s,  735.15/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 870.741s\n",
      "Train: 10 [5050/10009 ( 50%)]  Loss: 3.65 (3.61)  Time: 0.175s,  732.20/s  (0.174s,  735.13/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 879.475s\n",
      "Train: 10 [5100/10009 ( 51%)]  Loss: 3.16 (3.61)  Time: 0.174s,  734.01/s  (0.174s,  735.12/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 888.192s\n",
      "Train: 10 [5150/10009 ( 51%)]  Loss: 3.63 (3.61)  Time: 0.175s,  731.94/s  (0.174s,  735.09/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 896.931s\n",
      "Train: 10 [5200/10009 ( 52%)]  Loss: 3.23 (3.61)  Time: 0.174s,  735.26/s  (0.174s,  735.08/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 905.658s\n",
      "Train: 10 [5250/10009 ( 52%)]  Loss: 3.34 (3.60)  Time: 0.174s,  735.26/s  (0.174s,  735.06/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 914.382s\n",
      "Train: 10 [5300/10009 ( 53%)]  Loss: 3.63 (3.60)  Time: 0.171s,  750.14/s  (0.174s,  735.08/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 923.062s\n",
      "Train: 10 [5350/10009 ( 53%)]  Loss: 3.38 (3.60)  Time: 0.174s,  735.78/s  (0.174s,  735.08/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 931.776s\n",
      "Train: 10 [5400/10009 ( 54%)]  Loss: 3.36 (3.60)  Time: 0.176s,  728.60/s  (0.174s,  735.11/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 940.443s\n",
      "Train: 10 [5450/10009 ( 54%)]  Loss: 3.25 (3.60)  Time: 0.173s,  738.95/s  (0.174s,  735.07/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 949.191s\n",
      "Train: 10 [5500/10009 ( 55%)]  Loss: 3.35 (3.60)  Time: 0.175s,  732.35/s  (0.174s,  735.03/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 957.953s\n",
      "Train: 10 [5550/10009 ( 55%)]  Loss: 3.14 (3.60)  Time: 0.176s,  727.90/s  (0.174s,  735.00/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 966.709s\n",
      "Train: 10 [5600/10009 ( 56%)]  Loss: 3.75 (3.60)  Time: 0.176s,  728.07/s  (0.174s,  734.95/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 975.483s\n",
      "Train: 10 [5650/10009 ( 56%)]  Loss: 3.38 (3.59)  Time: 0.176s,  728.71/s  (0.174s,  734.95/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 984.190s\n",
      "Train: 10 [5700/10009 ( 57%)]  Loss: 3.45 (3.59)  Time: 0.177s,  724.01/s  (0.174s,  734.91/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 992.947s\n",
      "Train: 10 [5750/10009 ( 57%)]  Loss: 3.36 (3.59)  Time: 0.171s,  746.53/s  (0.174s,  734.97/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1001.579s\n",
      "Train: 10 [5800/10009 ( 58%)]  Loss: 3.62 (3.59)  Time: 0.171s,  748.91/s  (0.174s,  735.04/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 1010.190s\n",
      "Train: 10 [5850/10009 ( 58%)]  Loss: 3.15 (3.59)  Time: 0.173s,  738.70/s  (0.174s,  735.11/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 1018.798s\n",
      "Train: 10 [5900/10009 ( 59%)]  Loss: 3.38 (3.59)  Time: 0.172s,  745.90/s  (0.174s,  735.17/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1027.417s\n",
      "Train: 10 [5950/10009 ( 59%)]  Loss: 3.41 (3.59)  Time: 0.174s,  733.62/s  (0.174s,  735.22/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1036.051s\n",
      "Train: 10 [6000/10009 ( 60%)]  Loss: 3.34 (3.59)  Time: 0.174s,  737.02/s  (0.174s,  735.20/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1044.792s\n",
      "Train: 10 [6050/10009 ( 60%)]  Loss: 3.66 (3.59)  Time: 0.174s,  737.39/s  (0.174s,  735.18/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 1053.512s\n",
      "Train: 10 [6100/10009 ( 61%)]  Loss: 3.46 (3.58)  Time: 0.177s,  724.69/s  (0.174s,  735.16/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 1062.253s\n",
      "Train: 10 [6150/10009 ( 61%)]  Loss: 3.30 (3.58)  Time: 0.161s,  794.80/s  (0.174s,  735.39/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1070.626s\n",
      "Train: 10 [6200/10009 ( 62%)]  Loss: 3.45 (3.58)  Time: 0.160s,  799.58/s  (0.174s,  735.82/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1078.693s\n",
      "Train: 10 [6250/10009 ( 62%)]  Loss: 3.60 (3.58)  Time: 0.160s,  798.96/s  (0.174s,  736.26/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 1086.739s\n",
      "Train: 10 [6300/10009 ( 63%)]  Loss: 3.40 (3.58)  Time: 0.161s,  795.17/s  (0.174s,  736.70/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1094.788s\n",
      "Train: 10 [6350/10009 ( 63%)]  Loss: 3.78 (3.58)  Time: 0.161s,  796.64/s  (0.174s,  737.11/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1102.849s\n",
      "Train: 10 [6400/10009 ( 64%)]  Loss: 3.77 (3.58)  Time: 0.161s,  794.91/s  (0.174s,  737.54/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 1110.898s\n",
      "Train: 10 [6450/10009 ( 64%)]  Loss: 3.72 (3.58)  Time: 0.160s,  797.79/s  (0.173s,  737.96/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1118.932s\n",
      "Train: 10 [6500/10009 ( 65%)]  Loss: 3.65 (3.58)  Time: 0.160s,  799.41/s  (0.173s,  738.37/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1126.979s\n",
      "Train: 10 [6550/10009 ( 65%)]  Loss: 3.61 (3.58)  Time: 0.161s,  797.04/s  (0.173s,  738.76/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1135.040s\n",
      "Train: 10 [6600/10009 ( 66%)]  Loss: 3.47 (3.58)  Time: 0.161s,  795.63/s  (0.173s,  739.16/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1143.084s\n",
      "Train: 10 [6650/10009 ( 66%)]  Loss: 3.33 (3.57)  Time: 0.161s,  793.49/s  (0.173s,  739.55/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1151.140s\n",
      "Train: 10 [6700/10009 ( 67%)]  Loss: 3.48 (3.57)  Time: 0.162s,  787.95/s  (0.173s,  739.94/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1159.187s\n",
      "Train: 10 [6750/10009 ( 67%)]  Loss: 3.37 (3.57)  Time: 0.162s,  788.79/s  (0.173s,  740.31/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1167.242s\n",
      "Train: 10 [6800/10009 ( 68%)]  Loss: 3.37 (3.57)  Time: 0.161s,  796.58/s  (0.173s,  740.67/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1175.318s\n",
      "Train: 10 [6850/10009 ( 68%)]  Loss: 3.38 (3.57)  Time: 0.161s,  796.40/s  (0.173s,  741.04/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1183.365s\n",
      "Train: 10 [6900/10009 ( 69%)]  Loss: 3.48 (3.57)  Time: 0.162s,  791.96/s  (0.173s,  741.41/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1191.414s\n",
      "Train: 10 [6950/10009 ( 69%)]  Loss: 3.47 (3.57)  Time: 0.160s,  798.67/s  (0.173s,  741.77/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1199.456s\n",
      "Train: 10 [7000/10009 ( 70%)]  Loss: 3.37 (3.57)  Time: 0.161s,  794.40/s  (0.172s,  742.13/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1207.506s\n",
      "Train: 10 [7050/10009 ( 70%)]  Loss: 3.24 (3.57)  Time: 0.160s,  798.84/s  (0.172s,  742.48/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1215.558s\n",
      "Train: 10 [7100/10009 ( 71%)]  Loss: 3.22 (3.57)  Time: 0.162s,  790.59/s  (0.172s,  742.82/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1223.611s\n",
      "Train: 10 [7150/10009 ( 71%)]  Loss: 3.51 (3.57)  Time: 0.162s,  790.07/s  (0.172s,  743.16/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1231.668s\n",
      "Train: 10 [7200/10009 ( 72%)]  Loss: 3.44 (3.57)  Time: 0.161s,  795.34/s  (0.172s,  743.50/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1239.713s\n",
      "Train: 10 [7250/10009 ( 72%)]  Loss: 3.32 (3.57)  Time: 0.160s,  799.00/s  (0.172s,  743.83/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1247.758s\n",
      "Train: 10 [7300/10009 ( 73%)]  Loss: 3.37 (3.56)  Time: 0.160s,  798.05/s  (0.172s,  744.16/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1255.814s\n",
      "Train: 10 [7350/10009 ( 73%)]  Loss: 3.27 (3.56)  Time: 0.160s,  798.50/s  (0.172s,  744.47/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1263.880s\n",
      "Train: 10 [7400/10009 ( 74%)]  Loss: 3.78 (3.56)  Time: 0.161s,  796.97/s  (0.172s,  744.79/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1271.930s\n",
      "Train: 10 [7450/10009 ( 74%)]  Loss: 3.56 (3.56)  Time: 0.161s,  796.36/s  (0.172s,  745.11/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1279.987s\n",
      "Train: 10 [7500/10009 ( 75%)]  Loss: 3.34 (3.56)  Time: 0.161s,  793.52/s  (0.172s,  745.42/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1288.036s\n",
      "Train: 10 [7550/10009 ( 75%)]  Loss: 3.19 (3.56)  Time: 0.161s,  794.56/s  (0.172s,  745.73/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1296.083s\n",
      "Train: 10 [7600/10009 ( 76%)]  Loss: 3.25 (3.56)  Time: 0.163s,  785.94/s  (0.172s,  746.02/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1304.149s\n",
      "Train: 10 [7650/10009 ( 76%)]  Loss: 3.46 (3.56)  Time: 0.163s,  786.86/s  (0.172s,  746.33/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1312.186s\n",
      "Train: 10 [7700/10009 ( 77%)]  Loss: 3.61 (3.56)  Time: 0.162s,  787.98/s  (0.171s,  746.63/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1320.234s\n",
      "Train: 10 [7750/10009 ( 77%)]  Loss: 3.51 (3.56)  Time: 0.160s,  797.78/s  (0.171s,  746.92/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1328.295s\n",
      "Train: 10 [7800/10009 ( 78%)]  Loss: 3.48 (3.56)  Time: 0.161s,  796.39/s  (0.171s,  747.20/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1336.353s\n",
      "Train: 10 [7850/10009 ( 78%)]  Loss: 3.45 (3.56)  Time: 0.160s,  800.17/s  (0.171s,  747.49/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1344.406s\n",
      "Train: 10 [7900/10009 ( 79%)]  Loss: 3.37 (3.56)  Time: 0.161s,  796.41/s  (0.171s,  747.77/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1352.448s\n",
      "Train: 10 [7950/10009 ( 79%)]  Loss: 3.26 (3.55)  Time: 0.161s,  797.14/s  (0.171s,  748.06/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1360.495s\n",
      "Train: 10 [8000/10009 ( 80%)]  Loss: 3.22 (3.55)  Time: 0.160s,  797.60/s  (0.171s,  748.33/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1368.548s\n",
      "Train: 10 [8050/10009 ( 80%)]  Loss: 3.55 (3.55)  Time: 0.161s,  793.35/s  (0.171s,  748.60/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1376.600s\n",
      "Train: 10 [8100/10009 ( 81%)]  Loss: 3.27 (3.55)  Time: 0.161s,  796.73/s  (0.171s,  748.82/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1384.744s\n",
      "Train: 10 [8150/10009 ( 81%)]  Loss: 3.30 (3.55)  Time: 0.161s,  796.09/s  (0.171s,  749.08/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1392.806s\n",
      "Train: 10 [8200/10009 ( 82%)]  Loss: 3.69 (3.55)  Time: 0.160s,  799.47/s  (0.171s,  749.35/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1400.851s\n",
      "Train: 10 [8250/10009 ( 82%)]  Loss: 3.67 (3.55)  Time: 0.162s,  788.25/s  (0.171s,  749.61/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 1408.908s\n",
      "Train: 10 [8300/10009 ( 83%)]  Loss: 3.43 (3.55)  Time: 0.161s,  796.01/s  (0.171s,  749.86/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1416.958s\n",
      "Train: 10 [8350/10009 ( 83%)]  Loss: 3.32 (3.55)  Time: 0.160s,  798.46/s  (0.171s,  750.13/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1424.994s\n",
      "Train: 10 [8400/10009 ( 84%)]  Loss: 3.62 (3.55)  Time: 0.162s,  792.40/s  (0.171s,  750.38/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1433.044s\n",
      "Train: 10 [8450/10009 ( 84%)]  Loss: 3.35 (3.55)  Time: 0.160s,  798.09/s  (0.171s,  750.63/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1441.089s\n",
      "Train: 10 [8500/10009 ( 85%)]  Loss: 3.27 (3.55)  Time: 0.161s,  795.45/s  (0.170s,  750.88/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1449.133s\n",
      "Train: 10 [8550/10009 ( 85%)]  Loss: 3.34 (3.55)  Time: 0.162s,  791.34/s  (0.170s,  751.12/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1457.197s\n",
      "Train: 10 [8600/10009 ( 86%)]  Loss: 3.37 (3.55)  Time: 0.161s,  793.54/s  (0.170s,  751.35/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1465.266s\n",
      "Train: 10 [8650/10009 ( 86%)]  Loss: 3.34 (3.54)  Time: 0.162s,  791.39/s  (0.170s,  751.58/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1473.337s\n",
      "Train: 10 [8700/10009 ( 87%)]  Loss: 3.41 (3.54)  Time: 0.160s,  800.19/s  (0.170s,  751.81/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1481.393s\n",
      "Train: 10 [8750/10009 ( 87%)]  Loss: 3.44 (3.54)  Time: 0.161s,  793.41/s  (0.170s,  752.05/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1489.436s\n",
      "Train: 10 [8800/10009 ( 88%)]  Loss: 3.45 (3.54)  Time: 0.160s,  801.12/s  (0.170s,  752.27/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1497.497s\n",
      "Train: 10 [8850/10009 ( 88%)]  Loss: 3.21 (3.54)  Time: 0.160s,  799.01/s  (0.170s,  752.50/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1505.546s\n",
      "Train: 10 [8900/10009 ( 89%)]  Loss: 3.59 (3.54)  Time: 0.161s,  796.29/s  (0.170s,  752.73/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1513.595s\n",
      "Train: 10 [8950/10009 ( 89%)]  Loss: 3.55 (3.54)  Time: 0.161s,  795.53/s  (0.170s,  752.96/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1521.637s\n",
      "Train: 10 [9000/10009 ( 90%)]  Loss: 3.61 (3.54)  Time: 0.162s,  790.66/s  (0.170s,  753.17/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1529.693s\n",
      "Train: 10 [9050/10009 ( 90%)]  Loss: 3.35 (3.54)  Time: 0.162s,  792.02/s  (0.170s,  753.39/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 1537.744s\n",
      "Train: 10 [9100/10009 ( 91%)]  Loss: 3.44 (3.54)  Time: 0.161s,  794.94/s  (0.170s,  753.61/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1545.788s\n",
      "Train: 10 [9150/10009 ( 91%)]  Loss: 3.56 (3.54)  Time: 0.160s,  800.87/s  (0.170s,  753.83/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1553.838s\n",
      "Train: 10 [9200/10009 ( 92%)]  Loss: 3.17 (3.54)  Time: 0.161s,  796.69/s  (0.170s,  754.05/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1561.871s\n",
      "Train: 10 [9250/10009 ( 92%)]  Loss: 3.46 (3.54)  Time: 0.161s,  797.30/s  (0.170s,  754.25/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1569.934s\n",
      "Train: 10 [9300/10009 ( 93%)]  Loss: 3.18 (3.54)  Time: 0.161s,  794.66/s  (0.170s,  754.46/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1577.989s\n",
      "Train: 10 [9350/10009 ( 93%)]  Loss: 3.35 (3.54)  Time: 0.160s,  800.02/s  (0.170s,  754.67/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1586.028s\n",
      "Train: 10 [9400/10009 ( 94%)]  Loss: 2.96 (3.54)  Time: 0.161s,  793.60/s  (0.170s,  754.87/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1594.089s\n",
      "Train: 10 [9450/10009 ( 94%)]  Loss: 3.07 (3.53)  Time: 0.162s,  788.89/s  (0.170s,  755.06/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1602.157s\n",
      "Train: 10 [9500/10009 ( 95%)]  Loss: 3.27 (3.53)  Time: 0.162s,  792.10/s  (0.169s,  755.26/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1610.206s\n",
      "Train: 10 [9550/10009 ( 95%)]  Loss: 3.49 (3.53)  Time: 0.163s,  784.67/s  (0.169s,  755.46/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 1618.255s\n",
      "Train: 10 [9600/10009 ( 96%)]  Loss: 3.38 (3.53)  Time: 0.160s,  798.41/s  (0.169s,  755.65/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1626.310s\n",
      "Train: 10 [9650/10009 ( 96%)]  Loss: 3.11 (3.53)  Time: 0.160s,  797.82/s  (0.169s,  755.84/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1634.366s\n",
      "Train: 10 [9700/10009 ( 97%)]  Loss: 3.30 (3.53)  Time: 0.161s,  796.62/s  (0.169s,  756.03/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1642.434s\n",
      "Train: 10 [9750/10009 ( 97%)]  Loss: 3.08 (3.53)  Time: 0.160s,  798.35/s  (0.169s,  756.22/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1650.475s\n",
      "Train: 10 [9800/10009 ( 98%)]  Loss: 3.45 (3.53)  Time: 0.161s,  795.52/s  (0.169s,  756.41/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1658.518s\n",
      "Train: 10 [9850/10009 ( 98%)]  Loss: 3.49 (3.53)  Time: 0.161s,  793.36/s  (0.169s,  756.60/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1666.563s\n",
      "Train: 10 [9900/10009 ( 99%)]  Loss: 3.25 (3.53)  Time: 0.160s,  798.70/s  (0.169s,  756.79/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1674.600s\n",
      "Train: 10 [9950/10009 ( 99%)]  Loss: 3.15 (3.53)  Time: 0.161s,  797.19/s  (0.169s,  756.97/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1682.659s\n",
      "Train: 10 [10000/10009 (100%)]  Loss: 3.34 (3.53)  Time: 0.203s,  631.67/s  (0.169s,  757.13/s)  LR: 1.250e-05  Data: 0.049 (0.006)Time: 1690.765s\n",
      "Test: [   0/390]  Time: 0.674 (0.674)  Loss:   1.293 ( 1.293)  Acc@1:  77.344 ( 77.344)  Acc@5:  89.844 ( 89.844)\n",
      "Test: [  50/390]  Time: 0.053 (0.140)  Loss:   1.117 ( 2.156)  Acc@1:  72.656 ( 54.810)  Acc@5:  89.844 ( 76.210)\n",
      "Test: [ 100/390]  Time: 0.120 (0.135)  Loss:   2.358 ( 2.177)  Acc@1:  43.750 ( 51.323)  Acc@5:  77.344 ( 76.709)\n",
      "Test: [ 150/390]  Time: 0.051 (0.136)  Loss:   1.972 ( 2.167)  Acc@1:  48.438 ( 51.630)  Acc@5:  82.031 ( 76.857)\n",
      "Test: [ 200/390]  Time: 0.051 (0.134)  Loss:   3.377 ( 2.353)  Acc@1:  23.438 ( 48.453)  Acc@5:  57.031 ( 73.791)\n",
      "Test: [ 250/390]  Time: 0.051 (0.134)  Loss:   2.423 ( 2.468)  Acc@1:  55.469 ( 46.791)  Acc@5:  69.531 ( 71.704)\n",
      "Test: [ 300/390]  Time: 0.391 (0.134)  Loss:   2.856 ( 2.561)  Acc@1:  47.656 ( 45.315)  Acc@5:  63.281 ( 69.944)\n",
      "Test: [ 350/390]  Time: 0.051 (0.133)  Loss:   2.860 ( 2.633)  Acc@1:  42.188 ( 44.017)  Acc@5:  64.844 ( 68.739)\n",
      "Test: [ 390/390]  Time: 0.034 (0.134)  Loss:   3.787 ( 2.593)  Acc@1:  22.500 ( 44.822)  Acc@5:  55.000 ( 69.428)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 11 [   0/10009 (  0%)]  Loss: 3.15 (3.15)  Time: 0.790s,  162.02/s  (0.790s,  162.02/s)  LR: 8.272e-06  Data: 0.636 (0.636)Time: 0.790s\n",
      "Train: 11 [  50/10009 (  0%)]  Loss: 3.28 (3.38)  Time: 0.160s,  802.06/s  (0.172s,  745.25/s)  LR: 8.272e-06  Data: 0.006 (0.019)Time: 8.760s\n",
      "Train: 11 [ 100/10009 (  1%)]  Loss: 2.92 (3.36)  Time: 0.159s,  804.34/s  (0.166s,  772.79/s)  LR: 8.272e-06  Data: 0.006 (0.013)Time: 16.729s\n",
      "Train: 11 [ 150/10009 (  1%)]  Loss: 3.10 (3.35)  Time: 0.159s,  805.66/s  (0.164s,  781.88/s)  LR: 8.272e-06  Data: 0.006 (0.011)Time: 24.720s\n",
      "Train: 11 [ 200/10009 (  2%)]  Loss: 3.59 (3.37)  Time: 0.161s,  794.80/s  (0.163s,  785.96/s)  LR: 8.272e-06  Data: 0.006 (0.010)Time: 32.735s\n",
      "Train: 11 [ 250/10009 (  2%)]  Loss: 3.22 (3.36)  Time: 0.161s,  794.85/s  (0.162s,  788.32/s)  LR: 8.272e-06  Data: 0.006 (0.009)Time: 40.755s\n",
      "Train: 11 [ 300/10009 (  3%)]  Loss: 3.56 (3.37)  Time: 0.163s,  786.91/s  (0.162s,  789.43/s)  LR: 8.272e-06  Data: 0.007 (0.008)Time: 48.805s\n",
      "Train: 11 [ 350/10009 (  3%)]  Loss: 3.10 (3.36)  Time: 0.161s,  795.72/s  (0.162s,  790.56/s)  LR: 8.272e-06  Data: 0.005 (0.008)Time: 56.831s\n",
      "Train: 11 [ 400/10009 (  4%)]  Loss: 3.55 (3.37)  Time: 0.161s,  793.15/s  (0.162s,  791.46/s)  LR: 8.272e-06  Data: 0.007 (0.008)Time: 64.852s\n",
      "Train: 11 [ 450/10009 (  4%)]  Loss: 3.06 (3.37)  Time: 0.161s,  794.32/s  (0.162s,  791.87/s)  LR: 8.272e-06  Data: 0.007 (0.008)Time: 72.901s\n",
      "Train: 11 [ 500/10009 (  5%)]  Loss: 3.48 (3.37)  Time: 0.160s,  800.03/s  (0.162s,  792.21/s)  LR: 8.272e-06  Data: 0.005 (0.008)Time: 80.949s\n",
      "Train: 11 [ 550/10009 (  5%)]  Loss: 3.26 (3.37)  Time: 0.160s,  797.53/s  (0.162s,  792.39/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 89.007s\n",
      "Train: 11 [ 600/10009 (  6%)]  Loss: 3.51 (3.36)  Time: 0.160s,  798.86/s  (0.161s,  792.71/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 97.044s\n",
      "Train: 11 [ 650/10009 (  6%)]  Loss: 3.20 (3.36)  Time: 0.160s,  798.47/s  (0.161s,  792.98/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 105.082s\n",
      "Train: 11 [ 700/10009 (  7%)]  Loss: 3.09 (3.36)  Time: 0.161s,  793.18/s  (0.161s,  793.17/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 113.127s\n",
      "Train: 11 [ 750/10009 (  7%)]  Loss: 3.65 (3.37)  Time: 0.160s,  801.44/s  (0.161s,  793.38/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 121.162s\n",
      "Train: 11 [ 800/10009 (  8%)]  Loss: 3.63 (3.36)  Time: 0.160s,  799.70/s  (0.161s,  793.60/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 129.193s\n",
      "Train: 11 [ 850/10009 (  8%)]  Loss: 3.64 (3.36)  Time: 0.160s,  799.41/s  (0.161s,  793.73/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 137.235s\n",
      "Train: 11 [ 900/10009 (  9%)]  Loss: 3.44 (3.36)  Time: 0.162s,  788.51/s  (0.161s,  793.85/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 145.277s\n",
      "Train: 11 [ 950/10009 (  9%)]  Loss: 3.33 (3.37)  Time: 0.160s,  799.73/s  (0.161s,  793.88/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 153.332s\n",
      "Train: 11 [1000/10009 ( 10%)]  Loss: 3.40 (3.37)  Time: 0.162s,  792.32/s  (0.161s,  793.97/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 161.376s\n",
      "Train: 11 [1050/10009 ( 10%)]  Loss: 3.40 (3.37)  Time: 0.160s,  799.72/s  (0.161s,  794.07/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 169.417s\n",
      "Train: 11 [1100/10009 ( 11%)]  Loss: 3.60 (3.37)  Time: 0.161s,  794.75/s  (0.161s,  794.16/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 177.454s\n",
      "Train: 11 [1150/10009 ( 11%)]  Loss: 3.40 (3.37)  Time: 0.162s,  788.66/s  (0.161s,  794.00/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 185.553s\n",
      "Train: 11 [1200/10009 ( 12%)]  Loss: 3.32 (3.37)  Time: 0.162s,  788.72/s  (0.161s,  794.08/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 193.593s\n",
      "Train: 11 [1250/10009 ( 12%)]  Loss: 3.21 (3.37)  Time: 0.160s,  798.95/s  (0.161s,  794.18/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 201.628s\n",
      "Train: 11 [1300/10009 ( 13%)]  Loss: 3.26 (3.37)  Time: 0.160s,  800.72/s  (0.161s,  794.23/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 209.672s\n",
      "Train: 11 [1350/10009 ( 13%)]  Loss: 3.46 (3.36)  Time: 0.161s,  795.95/s  (0.161s,  794.22/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 217.733s\n",
      "Train: 11 [1400/10009 ( 14%)]  Loss: 3.68 (3.37)  Time: 0.161s,  793.61/s  (0.161s,  794.28/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 225.775s\n",
      "Train: 11 [1450/10009 ( 14%)]  Loss: 3.43 (3.36)  Time: 0.162s,  791.44/s  (0.161s,  794.36/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 233.809s\n",
      "Train: 11 [1500/10009 ( 15%)]  Loss: 3.54 (3.36)  Time: 0.161s,  793.72/s  (0.161s,  794.36/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 241.866s\n",
      "Train: 11 [1550/10009 ( 15%)]  Loss: 3.53 (3.36)  Time: 0.161s,  796.78/s  (0.161s,  794.41/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 249.906s\n",
      "Train: 11 [1600/10009 ( 16%)]  Loss: 3.41 (3.36)  Time: 0.160s,  798.25/s  (0.161s,  794.44/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 257.951s\n",
      "Train: 11 [1650/10009 ( 16%)]  Loss: 3.39 (3.36)  Time: 0.163s,  786.74/s  (0.161s,  794.47/s)  LR: 8.272e-06  Data: 0.009 (0.007)Time: 265.998s\n",
      "Train: 11 [1700/10009 ( 17%)]  Loss: 3.42 (3.36)  Time: 0.160s,  799.31/s  (0.161s,  794.50/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 274.043s\n",
      "Train: 11 [1750/10009 ( 17%)]  Loss: 3.53 (3.36)  Time: 0.160s,  800.23/s  (0.161s,  794.53/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 282.087s\n",
      "Train: 11 [1800/10009 ( 18%)]  Loss: 3.27 (3.36)  Time: 0.160s,  801.39/s  (0.161s,  794.60/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 290.120s\n",
      "Train: 11 [1850/10009 ( 18%)]  Loss: 3.20 (3.36)  Time: 0.161s,  794.32/s  (0.161s,  794.66/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 298.151s\n",
      "Train: 11 [1900/10009 ( 19%)]  Loss: 2.89 (3.36)  Time: 0.161s,  797.17/s  (0.161s,  794.67/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 306.198s\n",
      "Train: 11 [1950/10009 ( 19%)]  Loss: 3.14 (3.36)  Time: 0.162s,  788.23/s  (0.161s,  794.70/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 314.241s\n",
      "Train: 11 [2000/10009 ( 20%)]  Loss: 3.25 (3.36)  Time: 0.161s,  793.54/s  (0.161s,  794.69/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 322.300s\n",
      "Train: 11 [2050/10009 ( 20%)]  Loss: 3.42 (3.36)  Time: 0.161s,  795.88/s  (0.161s,  794.67/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 330.359s\n",
      "Train: 11 [2100/10009 ( 21%)]  Loss: 3.31 (3.36)  Time: 0.160s,  798.30/s  (0.161s,  794.70/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 338.401s\n",
      "Train: 11 [2150/10009 ( 21%)]  Loss: 3.29 (3.36)  Time: 0.161s,  795.51/s  (0.161s,  794.71/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 346.451s\n",
      "Train: 11 [2200/10009 ( 22%)]  Loss: 3.43 (3.36)  Time: 0.160s,  798.43/s  (0.161s,  794.74/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 354.490s\n",
      "Train: 11 [2250/10009 ( 22%)]  Loss: 3.40 (3.36)  Time: 0.160s,  800.26/s  (0.161s,  794.75/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 362.539s\n",
      "Train: 11 [2300/10009 ( 23%)]  Loss: 3.15 (3.36)  Time: 0.162s,  787.88/s  (0.161s,  794.77/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 370.583s\n",
      "Train: 11 [2350/10009 ( 23%)]  Loss: 3.64 (3.35)  Time: 0.160s,  798.46/s  (0.161s,  794.79/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 378.626s\n",
      "Train: 11 [2400/10009 ( 24%)]  Loss: 3.19 (3.35)  Time: 0.160s,  798.30/s  (0.161s,  794.78/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 386.682s\n",
      "Train: 11 [2450/10009 ( 24%)]  Loss: 3.41 (3.35)  Time: 0.162s,  788.95/s  (0.161s,  794.78/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 394.737s\n",
      "Train: 11 [2500/10009 ( 25%)]  Loss: 3.44 (3.35)  Time: 0.161s,  796.12/s  (0.161s,  794.78/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 402.786s\n",
      "Train: 11 [2550/10009 ( 25%)]  Loss: 3.38 (3.35)  Time: 0.162s,  788.75/s  (0.161s,  794.80/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 410.832s\n",
      "Train: 11 [2600/10009 ( 26%)]  Loss: 3.50 (3.35)  Time: 0.161s,  794.58/s  (0.161s,  794.82/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 418.869s\n",
      "Train: 11 [2650/10009 ( 26%)]  Loss: 3.33 (3.35)  Time: 0.160s,  798.41/s  (0.161s,  794.84/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 426.910s\n",
      "Train: 11 [2700/10009 ( 27%)]  Loss: 3.31 (3.35)  Time: 0.161s,  794.68/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 434.954s\n",
      "Train: 11 [2750/10009 ( 27%)]  Loss: 3.48 (3.35)  Time: 0.160s,  798.41/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 442.992s\n",
      "Train: 11 [2800/10009 ( 28%)]  Loss: 3.48 (3.35)  Time: 0.161s,  796.85/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 451.053s\n",
      "Train: 11 [2850/10009 ( 28%)]  Loss: 3.20 (3.35)  Time: 0.161s,  793.65/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 459.106s\n",
      "Train: 11 [2900/10009 ( 29%)]  Loss: 3.55 (3.35)  Time: 0.160s,  800.57/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 467.157s\n",
      "Train: 11 [2950/10009 ( 29%)]  Loss: 3.24 (3.35)  Time: 0.160s,  798.82/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 475.203s\n",
      "Train: 11 [3000/10009 ( 30%)]  Loss: 3.42 (3.35)  Time: 0.162s,  791.13/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 483.248s\n",
      "Train: 11 [3050/10009 ( 30%)]  Loss: 3.23 (3.35)  Time: 0.160s,  800.20/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 491.291s\n",
      "Train: 11 [3100/10009 ( 31%)]  Loss: 2.98 (3.35)  Time: 0.161s,  796.64/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 499.347s\n",
      "Train: 11 [3150/10009 ( 31%)]  Loss: 3.33 (3.35)  Time: 0.163s,  783.53/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 507.394s\n",
      "Train: 11 [3200/10009 ( 32%)]  Loss: 3.32 (3.35)  Time: 0.162s,  787.94/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 515.452s\n",
      "Train: 11 [3250/10009 ( 32%)]  Loss: 3.68 (3.35)  Time: 0.166s,  770.23/s  (0.161s,  794.85/s)  LR: 8.272e-06  Data: 0.011 (0.006)Time: 523.532s\n",
      "Train: 11 [3300/10009 ( 33%)]  Loss: 3.16 (3.35)  Time: 0.160s,  798.65/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 531.574s\n",
      "Train: 11 [3350/10009 ( 33%)]  Loss: 3.67 (3.35)  Time: 0.161s,  796.91/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 539.616s\n",
      "Train: 11 [3400/10009 ( 34%)]  Loss: 3.69 (3.35)  Time: 0.162s,  791.48/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 547.659s\n",
      "Train: 11 [3450/10009 ( 34%)]  Loss: 3.36 (3.35)  Time: 0.161s,  792.77/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 555.712s\n",
      "Train: 11 [3500/10009 ( 35%)]  Loss: 3.14 (3.35)  Time: 0.162s,  791.97/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 563.771s\n",
      "Train: 11 [3550/10009 ( 35%)]  Loss: 3.17 (3.35)  Time: 0.163s,  787.67/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 571.825s\n",
      "Train: 11 [3600/10009 ( 36%)]  Loss: 3.45 (3.35)  Time: 0.161s,  794.56/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 579.872s\n",
      "Train: 11 [3650/10009 ( 36%)]  Loss: 3.26 (3.35)  Time: 0.161s,  796.40/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 587.919s\n",
      "Train: 11 [3700/10009 ( 37%)]  Loss: 3.36 (3.35)  Time: 0.161s,  794.40/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 595.965s\n",
      "Train: 11 [3750/10009 ( 37%)]  Loss: 3.21 (3.35)  Time: 0.161s,  795.46/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 604.008s\n",
      "Train: 11 [3800/10009 ( 38%)]  Loss: 3.26 (3.35)  Time: 0.162s,  788.00/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 612.060s\n",
      "Train: 11 [3850/10009 ( 38%)]  Loss: 3.43 (3.35)  Time: 0.161s,  795.58/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 620.108s\n",
      "Train: 11 [3900/10009 ( 39%)]  Loss: 3.17 (3.35)  Time: 0.161s,  792.76/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 628.160s\n",
      "Train: 11 [3950/10009 ( 39%)]  Loss: 3.51 (3.35)  Time: 0.160s,  798.30/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 636.211s\n",
      "Train: 11 [4000/10009 ( 40%)]  Loss: 3.26 (3.35)  Time: 0.162s,  792.48/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 644.285s\n",
      "Train: 11 [4050/10009 ( 40%)]  Loss: 3.25 (3.35)  Time: 0.161s,  795.05/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 652.327s\n",
      "Train: 11 [4100/10009 ( 41%)]  Loss: 3.23 (3.35)  Time: 0.161s,  795.55/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 660.375s\n",
      "Train: 11 [4150/10009 ( 41%)]  Loss: 3.16 (3.35)  Time: 0.160s,  798.75/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 668.414s\n",
      "Train: 11 [4200/10009 ( 42%)]  Loss: 3.10 (3.35)  Time: 0.161s,  797.49/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 676.470s\n",
      "Train: 11 [4250/10009 ( 42%)]  Loss: 3.25 (3.35)  Time: 0.160s,  798.75/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 684.525s\n",
      "Train: 11 [4300/10009 ( 43%)]  Loss: 3.58 (3.35)  Time: 0.160s,  798.95/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 692.575s\n",
      "Train: 11 [4350/10009 ( 43%)]  Loss: 3.34 (3.35)  Time: 0.161s,  795.36/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 700.643s\n",
      "Train: 11 [4400/10009 ( 44%)]  Loss: 3.35 (3.35)  Time: 0.161s,  797.10/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 708.696s\n",
      "Train: 11 [4450/10009 ( 44%)]  Loss: 3.36 (3.35)  Time: 0.161s,  793.09/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 716.744s\n",
      "Train: 11 [4500/10009 ( 45%)]  Loss: 3.29 (3.35)  Time: 0.161s,  796.72/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 724.783s\n",
      "Train: 11 [4550/10009 ( 45%)]  Loss: 3.42 (3.34)  Time: 0.164s,  780.91/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 732.841s\n",
      "Train: 11 [4600/10009 ( 46%)]  Loss: 3.56 (3.34)  Time: 0.162s,  787.74/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 740.901s\n",
      "Train: 11 [4650/10009 ( 46%)]  Loss: 2.90 (3.34)  Time: 0.160s,  797.61/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 748.961s\n",
      "Train: 11 [4700/10009 ( 47%)]  Loss: 3.45 (3.34)  Time: 0.161s,  797.37/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 757.024s\n",
      "Train: 11 [4750/10009 ( 47%)]  Loss: 3.47 (3.34)  Time: 0.162s,  792.06/s  (0.161s,  794.85/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 765.083s\n",
      "Train: 11 [4800/10009 ( 48%)]  Loss: 3.41 (3.34)  Time: 0.161s,  796.13/s  (0.161s,  794.85/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 773.134s\n",
      "Train: 11 [4850/10009 ( 48%)]  Loss: 3.54 (3.35)  Time: 0.161s,  794.12/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 781.163s\n",
      "Train: 11 [4900/10009 ( 49%)]  Loss: 3.10 (3.35)  Time: 0.161s,  797.44/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 789.205s\n",
      "Train: 11 [4950/10009 ( 49%)]  Loss: 3.40 (3.35)  Time: 0.161s,  795.07/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 797.251s\n",
      "Train: 11 [5000/10009 ( 50%)]  Loss: 3.11 (3.35)  Time: 0.160s,  799.83/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 805.292s\n",
      "Train: 11 [5050/10009 ( 50%)]  Loss: 3.28 (3.35)  Time: 0.160s,  798.89/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 813.324s\n",
      "Train: 11 [5100/10009 ( 51%)]  Loss: 3.00 (3.34)  Time: 0.161s,  797.33/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 821.370s\n",
      "Train: 11 [5150/10009 ( 51%)]  Loss: 3.50 (3.34)  Time: 0.161s,  795.38/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 829.429s\n",
      "Train: 11 [5200/10009 ( 52%)]  Loss: 3.34 (3.34)  Time: 0.163s,  783.74/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 837.482s\n",
      "Train: 11 [5250/10009 ( 52%)]  Loss: 3.54 (3.34)  Time: 0.161s,  793.78/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 845.537s\n",
      "Train: 11 [5300/10009 ( 53%)]  Loss: 3.20 (3.34)  Time: 0.160s,  797.70/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 853.589s\n",
      "Train: 11 [5350/10009 ( 53%)]  Loss: 3.38 (3.34)  Time: 0.160s,  797.65/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 861.630s\n",
      "Train: 11 [5400/10009 ( 54%)]  Loss: 3.60 (3.34)  Time: 0.161s,  795.93/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 869.679s\n",
      "Train: 11 [5450/10009 ( 54%)]  Loss: 3.24 (3.34)  Time: 0.161s,  796.76/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 877.728s\n",
      "Train: 11 [5500/10009 ( 55%)]  Loss: 3.49 (3.34)  Time: 0.161s,  797.15/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 885.784s\n",
      "Train: 11 [5550/10009 ( 55%)]  Loss: 3.44 (3.34)  Time: 0.161s,  794.01/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 893.831s\n",
      "Train: 11 [5600/10009 ( 56%)]  Loss: 3.27 (3.34)  Time: 0.161s,  794.38/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 901.882s\n",
      "Train: 11 [5650/10009 ( 56%)]  Loss: 3.62 (3.34)  Time: 0.162s,  791.15/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 909.935s\n",
      "Train: 11 [5700/10009 ( 57%)]  Loss: 3.24 (3.34)  Time: 0.161s,  795.47/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 917.974s\n",
      "Train: 11 [5750/10009 ( 57%)]  Loss: 3.58 (3.34)  Time: 0.160s,  799.39/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 926.028s\n",
      "Train: 11 [5800/10009 ( 58%)]  Loss: 3.48 (3.34)  Time: 0.161s,  794.23/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 934.080s\n",
      "Train: 11 [5850/10009 ( 58%)]  Loss: 3.35 (3.34)  Time: 0.161s,  794.32/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 942.131s\n",
      "Train: 11 [5900/10009 ( 59%)]  Loss: 3.83 (3.34)  Time: 0.162s,  792.26/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 950.182s\n",
      "Train: 11 [5950/10009 ( 59%)]  Loss: 3.39 (3.34)  Time: 0.160s,  799.83/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 958.220s\n",
      "Train: 11 [6000/10009 ( 60%)]  Loss: 3.16 (3.34)  Time: 0.160s,  799.68/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 966.261s\n",
      "Train: 11 [6050/10009 ( 60%)]  Loss: 3.21 (3.34)  Time: 0.161s,  794.10/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 974.307s\n",
      "Train: 11 [6100/10009 ( 61%)]  Loss: 3.43 (3.34)  Time: 0.161s,  794.65/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 982.371s\n",
      "Train: 11 [6150/10009 ( 61%)]  Loss: 3.25 (3.34)  Time: 0.160s,  798.57/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 990.420s\n",
      "Train: 11 [6200/10009 ( 62%)]  Loss: 3.26 (3.34)  Time: 0.160s,  798.42/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 998.459s\n",
      "Train: 11 [6250/10009 ( 62%)]  Loss: 3.47 (3.34)  Time: 0.162s,  790.13/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1006.503s\n",
      "Train: 11 [6300/10009 ( 63%)]  Loss: 3.33 (3.34)  Time: 0.161s,  793.57/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1014.549s\n",
      "Train: 11 [6350/10009 ( 63%)]  Loss: 3.53 (3.34)  Time: 0.161s,  796.55/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1022.615s\n",
      "Train: 11 [6400/10009 ( 64%)]  Loss: 3.45 (3.34)  Time: 0.162s,  791.36/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 1030.663s\n",
      "Train: 11 [6450/10009 ( 64%)]  Loss: 3.23 (3.34)  Time: 0.162s,  789.45/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1038.729s\n",
      "Train: 11 [6500/10009 ( 65%)]  Loss: 3.53 (3.34)  Time: 0.161s,  795.70/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1046.776s\n",
      "Train: 11 [6550/10009 ( 65%)]  Loss: 3.39 (3.34)  Time: 0.161s,  795.69/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1054.816s\n",
      "Train: 11 [6600/10009 ( 66%)]  Loss: 3.30 (3.34)  Time: 0.161s,  792.85/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1062.877s\n",
      "Train: 11 [6650/10009 ( 66%)]  Loss: 3.38 (3.34)  Time: 0.161s,  793.09/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1070.923s\n",
      "Train: 11 [6700/10009 ( 67%)]  Loss: 3.61 (3.34)  Time: 0.160s,  797.77/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1078.963s\n",
      "Train: 11 [6750/10009 ( 67%)]  Loss: 3.45 (3.34)  Time: 0.163s,  785.59/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 1087.013s\n",
      "Train: 11 [6800/10009 ( 68%)]  Loss: 2.95 (3.34)  Time: 0.161s,  795.43/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1095.093s\n",
      "Train: 11 [6850/10009 ( 68%)]  Loss: 3.31 (3.34)  Time: 0.161s,  794.40/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1103.149s\n",
      "Train: 11 [6900/10009 ( 69%)]  Loss: 3.52 (3.34)  Time: 0.161s,  795.87/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1111.203s\n",
      "Train: 11 [6950/10009 ( 69%)]  Loss: 3.29 (3.34)  Time: 0.161s,  796.07/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1119.255s\n",
      "Train: 11 [7000/10009 ( 70%)]  Loss: 3.41 (3.34)  Time: 0.161s,  794.70/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1127.308s\n",
      "Train: 11 [7050/10009 ( 70%)]  Loss: 3.48 (3.34)  Time: 0.160s,  797.79/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1135.387s\n",
      "Train: 11 [7100/10009 ( 71%)]  Loss: 3.18 (3.34)  Time: 0.161s,  794.92/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1143.447s\n",
      "Train: 11 [7150/10009 ( 71%)]  Loss: 3.41 (3.34)  Time: 0.161s,  794.07/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1151.491s\n",
      "Train: 11 [7200/10009 ( 72%)]  Loss: 3.11 (3.34)  Time: 0.161s,  795.09/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1159.548s\n",
      "Train: 11 [7250/10009 ( 72%)]  Loss: 3.19 (3.34)  Time: 0.160s,  799.53/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1167.596s\n",
      "Train: 11 [7300/10009 ( 73%)]  Loss: 3.05 (3.34)  Time: 0.160s,  800.82/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1175.641s\n",
      "Train: 11 [7350/10009 ( 73%)]  Loss: 3.36 (3.34)  Time: 0.161s,  796.96/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1183.685s\n",
      "Train: 11 [7400/10009 ( 74%)]  Loss: 3.15 (3.34)  Time: 0.161s,  797.46/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1191.734s\n",
      "Train: 11 [7450/10009 ( 74%)]  Loss: 3.46 (3.34)  Time: 0.161s,  794.11/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1199.810s\n",
      "Train: 11 [7500/10009 ( 75%)]  Loss: 3.49 (3.34)  Time: 0.161s,  794.89/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1207.860s\n",
      "Train: 11 [7550/10009 ( 75%)]  Loss: 2.95 (3.34)  Time: 0.160s,  798.34/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1215.904s\n",
      "Train: 11 [7600/10009 ( 76%)]  Loss: 3.36 (3.34)  Time: 0.162s,  791.61/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1223.943s\n",
      "Train: 11 [7650/10009 ( 76%)]  Loss: 3.58 (3.34)  Time: 0.160s,  798.89/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1231.997s\n",
      "Train: 11 [7700/10009 ( 77%)]  Loss: 3.30 (3.34)  Time: 0.161s,  794.69/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1240.045s\n",
      "Train: 11 [7750/10009 ( 77%)]  Loss: 3.40 (3.34)  Time: 0.160s,  800.62/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1248.103s\n",
      "Train: 11 [7800/10009 ( 78%)]  Loss: 3.23 (3.34)  Time: 0.161s,  797.47/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1256.150s\n",
      "Train: 11 [7850/10009 ( 78%)]  Loss: 3.66 (3.34)  Time: 0.160s,  798.12/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1264.198s\n",
      "Train: 11 [7900/10009 ( 79%)]  Loss: 3.23 (3.34)  Time: 0.160s,  798.61/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1272.246s\n",
      "Train: 11 [7950/10009 ( 79%)]  Loss: 3.53 (3.34)  Time: 0.161s,  795.46/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1280.290s\n",
      "Train: 11 [8000/10009 ( 80%)]  Loss: 3.22 (3.34)  Time: 0.160s,  798.88/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1288.341s\n",
      "Train: 11 [8050/10009 ( 80%)]  Loss: 3.44 (3.34)  Time: 0.160s,  798.63/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1296.398s\n",
      "Train: 11 [8100/10009 ( 81%)]  Loss: 3.36 (3.34)  Time: 0.160s,  799.95/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1304.441s\n",
      "Train: 11 [8150/10009 ( 81%)]  Loss: 3.19 (3.34)  Time: 0.162s,  790.89/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1312.489s\n",
      "Train: 11 [8200/10009 ( 82%)]  Loss: 3.69 (3.34)  Time: 0.161s,  797.22/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1320.532s\n",
      "Train: 11 [8250/10009 ( 82%)]  Loss: 3.24 (3.34)  Time: 0.161s,  794.67/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1328.587s\n",
      "Train: 11 [8300/10009 ( 83%)]  Loss: 3.50 (3.34)  Time: 0.161s,  796.34/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1336.642s\n",
      "Train: 11 [8350/10009 ( 83%)]  Loss: 3.45 (3.34)  Time: 0.160s,  798.42/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1344.681s\n",
      "Train: 11 [8400/10009 ( 84%)]  Loss: 3.22 (3.34)  Time: 0.161s,  793.02/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1352.735s\n",
      "Train: 11 [8450/10009 ( 84%)]  Loss: 3.23 (3.34)  Time: 0.161s,  793.93/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1360.771s\n",
      "Train: 11 [8500/10009 ( 85%)]  Loss: 3.48 (3.34)  Time: 0.161s,  796.86/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1368.821s\n",
      "Train: 11 [8550/10009 ( 85%)]  Loss: 3.17 (3.34)  Time: 0.161s,  794.27/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1376.855s\n",
      "Train: 11 [8600/10009 ( 86%)]  Loss: 3.21 (3.34)  Time: 0.161s,  793.92/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1384.907s\n",
      "Train: 11 [8650/10009 ( 86%)]  Loss: 3.23 (3.34)  Time: 0.161s,  794.42/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1392.962s\n",
      "Train: 11 [8700/10009 ( 87%)]  Loss: 3.41 (3.34)  Time: 0.161s,  796.86/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1401.012s\n",
      "Train: 11 [8750/10009 ( 87%)]  Loss: 3.46 (3.34)  Time: 0.161s,  796.60/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1409.063s\n",
      "Train: 11 [8800/10009 ( 88%)]  Loss: 3.49 (3.34)  Time: 0.160s,  799.48/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1417.112s\n",
      "Train: 11 [8850/10009 ( 88%)]  Loss: 3.64 (3.34)  Time: 0.161s,  793.46/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1425.163s\n",
      "Train: 11 [8900/10009 ( 89%)]  Loss: 3.38 (3.34)  Time: 0.161s,  796.46/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1433.216s\n",
      "Train: 11 [8950/10009 ( 89%)]  Loss: 3.07 (3.34)  Time: 0.160s,  799.74/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1441.270s\n",
      "Train: 11 [9000/10009 ( 90%)]  Loss: 3.22 (3.34)  Time: 0.162s,  792.29/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1449.310s\n",
      "Train: 11 [9050/10009 ( 90%)]  Loss: 3.32 (3.34)  Time: 0.162s,  788.07/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1457.360s\n",
      "Train: 11 [9100/10009 ( 91%)]  Loss: 3.37 (3.34)  Time: 0.161s,  793.76/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1465.421s\n",
      "Train: 11 [9150/10009 ( 91%)]  Loss: 3.52 (3.34)  Time: 0.161s,  794.30/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1473.464s\n",
      "Train: 11 [9200/10009 ( 92%)]  Loss: 3.29 (3.34)  Time: 0.161s,  793.24/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1481.500s\n",
      "Train: 11 [9250/10009 ( 92%)]  Loss: 3.60 (3.34)  Time: 0.161s,  795.59/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1489.538s\n",
      "Train: 11 [9300/10009 ( 93%)]  Loss: 3.30 (3.34)  Time: 0.161s,  796.13/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1497.583s\n",
      "Train: 11 [9350/10009 ( 93%)]  Loss: 3.61 (3.34)  Time: 0.161s,  797.26/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1505.632s\n",
      "Train: 11 [9400/10009 ( 94%)]  Loss: 3.39 (3.34)  Time: 0.161s,  796.90/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1513.676s\n",
      "Train: 11 [9450/10009 ( 94%)]  Loss: 3.40 (3.34)  Time: 0.160s,  798.57/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1521.733s\n",
      "Train: 11 [9500/10009 ( 95%)]  Loss: 3.36 (3.34)  Time: 0.162s,  792.53/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1529.788s\n",
      "Train: 11 [9550/10009 ( 95%)]  Loss: 3.23 (3.34)  Time: 0.161s,  793.50/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1537.844s\n",
      "Train: 11 [9600/10009 ( 96%)]  Loss: 3.19 (3.34)  Time: 0.161s,  795.64/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1545.887s\n",
      "Train: 11 [9650/10009 ( 96%)]  Loss: 3.33 (3.34)  Time: 0.161s,  795.05/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1553.932s\n",
      "Train: 11 [9700/10009 ( 97%)]  Loss: 3.48 (3.34)  Time: 0.162s,  790.11/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 1561.983s\n",
      "Train: 11 [9750/10009 ( 97%)]  Loss: 3.04 (3.34)  Time: 0.161s,  792.58/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1570.034s\n",
      "Train: 11 [9800/10009 ( 98%)]  Loss: 3.34 (3.34)  Time: 0.161s,  794.65/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1578.092s\n",
      "Train: 11 [9850/10009 ( 98%)]  Loss: 3.15 (3.34)  Time: 0.161s,  796.87/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1586.148s\n",
      "Train: 11 [9900/10009 ( 99%)]  Loss: 3.43 (3.34)  Time: 0.161s,  797.40/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1594.207s\n",
      "Train: 11 [9950/10009 ( 99%)]  Loss: 3.59 (3.34)  Time: 0.161s,  795.32/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1602.261s\n",
      "Train: 11 [10000/10009 (100%)]  Loss: 3.42 (3.34)  Time: 0.202s,  632.24/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.048 (0.006)Time: 1610.344s\n",
      "Test: [   0/390]  Time: 0.678 (0.678)  Loss:   1.237 ( 1.237)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.145)  Loss:   1.066 ( 2.035)  Acc@1:  76.562 ( 56.878)  Acc@5:  90.625 ( 78.064)\n",
      "Test: [ 100/390]  Time: 0.052 (0.133)  Loss:   2.191 ( 2.074)  Acc@1:  50.781 ( 53.187)  Acc@5:  80.469 ( 78.434)\n",
      "Test: [ 150/390]  Time: 0.052 (0.138)  Loss:   1.975 ( 2.047)  Acc@1:  46.875 ( 53.839)  Acc@5:  79.688 ( 78.844)\n",
      "Test: [ 200/390]  Time: 0.052 (0.135)  Loss:   2.979 ( 2.231)  Acc@1:  32.031 ( 50.723)  Acc@5:  65.625 ( 75.836)\n",
      "Test: [ 250/390]  Time: 0.052 (0.134)  Loss:   2.435 ( 2.351)  Acc@1:  57.812 ( 49.044)  Acc@5:  72.656 ( 73.795)\n",
      "Test: [ 300/390]  Time: 0.262 (0.132)  Loss:   2.650 ( 2.449)  Acc@1:  50.000 ( 47.376)  Acc@5:  66.406 ( 72.000)\n",
      "Test: [ 350/390]  Time: 0.190 (0.131)  Loss:   2.732 ( 2.524)  Acc@1:  46.094 ( 46.049)  Acc@5:  70.312 ( 70.791)\n",
      "Test: [ 390/390]  Time: 0.035 (0.131)  Loss:   3.945 ( 2.490)  Acc@1:  16.250 ( 46.698)  Acc@5:  55.000 ( 71.312)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 12 [   0/10009 (  0%)]  Loss: 3.23 (3.23)  Time: 0.666s,  192.14/s  (0.666s,  192.14/s)  LR: 4.775e-06  Data: 0.515 (0.515)Time: 0.667s\n",
      "Train: 12 [  50/10009 (  0%)]  Loss: 3.32 (3.29)  Time: 0.159s,  804.93/s  (0.170s,  754.56/s)  LR: 4.775e-06  Data: 0.006 (0.017)Time: 8.652s\n",
      "Train: 12 [ 100/10009 (  1%)]  Loss: 3.13 (3.29)  Time: 0.159s,  803.15/s  (0.165s,  777.40/s)  LR: 4.775e-06  Data: 0.006 (0.012)Time: 16.630s\n",
      "Train: 12 [ 150/10009 (  1%)]  Loss: 3.17 (3.28)  Time: 0.160s,  801.51/s  (0.163s,  785.04/s)  LR: 4.775e-06  Data: 0.006 (0.010)Time: 24.621s\n",
      "Train: 12 [ 200/10009 (  2%)]  Loss: 3.11 (3.28)  Time: 0.160s,  799.54/s  (0.162s,  788.32/s)  LR: 4.775e-06  Data: 0.006 (0.009)Time: 32.637s\n",
      "Train: 12 [ 250/10009 (  2%)]  Loss: 3.22 (3.28)  Time: 0.160s,  801.55/s  (0.162s,  790.27/s)  LR: 4.775e-06  Data: 0.006 (0.008)Time: 40.655s\n",
      "Train: 12 [ 300/10009 (  3%)]  Loss: 3.25 (3.27)  Time: 0.160s,  800.98/s  (0.162s,  791.57/s)  LR: 4.775e-06  Data: 0.006 (0.008)Time: 48.673s\n",
      "Train: 12 [ 350/10009 (  3%)]  Loss: 3.16 (3.27)  Time: 0.160s,  800.27/s  (0.162s,  792.53/s)  LR: 4.775e-06  Data: 0.006 (0.008)Time: 56.689s\n",
      "Train: 12 [ 400/10009 (  4%)]  Loss: 3.29 (3.26)  Time: 0.161s,  792.75/s  (0.161s,  793.13/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 64.716s\n",
      "Train: 12 [ 450/10009 (  4%)]  Loss: 3.25 (3.26)  Time: 0.160s,  800.63/s  (0.161s,  793.57/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 72.745s\n",
      "Train: 12 [ 500/10009 (  5%)]  Loss: 3.24 (3.26)  Time: 0.160s,  798.50/s  (0.161s,  793.72/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 80.794s\n",
      "Train: 12 [ 550/10009 (  5%)]  Loss: 3.35 (3.26)  Time: 0.160s,  798.19/s  (0.161s,  794.03/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 88.823s\n",
      "Train: 12 [ 600/10009 (  6%)]  Loss: 3.19 (3.26)  Time: 0.160s,  800.70/s  (0.161s,  794.25/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 96.856s\n",
      "Train: 12 [ 650/10009 (  6%)]  Loss: 3.25 (3.26)  Time: 0.160s,  802.19/s  (0.161s,  794.45/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 104.888s\n",
      "Train: 12 [ 700/10009 (  7%)]  Loss: 3.37 (3.26)  Time: 0.162s,  790.25/s  (0.161s,  794.62/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 112.920s\n",
      "Train: 12 [ 750/10009 (  7%)]  Loss: 3.25 (3.25)  Time: 0.161s,  796.19/s  (0.161s,  794.75/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 120.953s\n",
      "Train: 12 [ 800/10009 (  8%)]  Loss: 3.06 (3.25)  Time: 0.161s,  796.00/s  (0.161s,  794.89/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 128.984s\n",
      "Train: 12 [ 850/10009 (  8%)]  Loss: 2.90 (3.25)  Time: 0.161s,  793.99/s  (0.161s,  794.98/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 137.020s\n",
      "Train: 12 [ 900/10009 (  9%)]  Loss: 3.40 (3.26)  Time: 0.161s,  797.30/s  (0.161s,  795.03/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 145.061s\n",
      "Train: 12 [ 950/10009 (  9%)]  Loss: 3.25 (3.26)  Time: 0.161s,  793.95/s  (0.161s,  795.05/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 153.107s\n",
      "Train: 12 [1000/10009 ( 10%)]  Loss: 3.23 (3.26)  Time: 0.160s,  797.69/s  (0.161s,  795.10/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 161.147s\n",
      "Train: 12 [1050/10009 ( 10%)]  Loss: 3.39 (3.26)  Time: 0.161s,  797.31/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 169.179s\n",
      "Train: 12 [1100/10009 ( 11%)]  Loss: 3.24 (3.26)  Time: 0.160s,  798.94/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 177.212s\n",
      "Train: 12 [1150/10009 ( 11%)]  Loss: 3.18 (3.26)  Time: 0.160s,  798.40/s  (0.161s,  795.28/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 185.254s\n",
      "Train: 12 [1200/10009 ( 12%)]  Loss: 3.02 (3.26)  Time: 0.160s,  798.59/s  (0.161s,  795.32/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 193.290s\n",
      "Train: 12 [1250/10009 ( 12%)]  Loss: 3.18 (3.26)  Time: 0.161s,  794.92/s  (0.161s,  795.35/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 201.330s\n",
      "Train: 12 [1300/10009 ( 13%)]  Loss: 3.29 (3.26)  Time: 0.161s,  793.58/s  (0.161s,  795.37/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 209.371s\n",
      "Train: 12 [1350/10009 ( 13%)]  Loss: 3.52 (3.26)  Time: 0.161s,  794.31/s  (0.161s,  795.38/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 217.415s\n",
      "Train: 12 [1400/10009 ( 14%)]  Loss: 3.23 (3.26)  Time: 0.160s,  798.90/s  (0.161s,  795.40/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 225.457s\n",
      "Train: 12 [1450/10009 ( 14%)]  Loss: 3.47 (3.26)  Time: 0.162s,  789.49/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 233.504s\n",
      "Train: 12 [1500/10009 ( 15%)]  Loss: 3.49 (3.26)  Time: 0.160s,  799.45/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 241.551s\n",
      "Train: 12 [1550/10009 ( 15%)]  Loss: 3.32 (3.26)  Time: 0.161s,  794.15/s  (0.161s,  795.40/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 249.593s\n",
      "Train: 12 [1600/10009 ( 16%)]  Loss: 3.34 (3.26)  Time: 0.160s,  800.96/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 257.624s\n",
      "Train: 12 [1650/10009 ( 16%)]  Loss: 3.19 (3.26)  Time: 0.161s,  796.17/s  (0.161s,  795.49/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 265.659s\n",
      "Train: 12 [1700/10009 ( 17%)]  Loss: 3.04 (3.26)  Time: 0.160s,  798.28/s  (0.161s,  795.49/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 273.704s\n",
      "Train: 12 [1750/10009 ( 17%)]  Loss: 3.33 (3.26)  Time: 0.161s,  796.91/s  (0.161s,  795.52/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 281.737s\n",
      "Train: 12 [1800/10009 ( 18%)]  Loss: 3.27 (3.26)  Time: 0.160s,  798.16/s  (0.161s,  795.50/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 289.788s\n",
      "Train: 12 [1850/10009 ( 18%)]  Loss: 3.01 (3.26)  Time: 0.160s,  799.02/s  (0.161s,  795.51/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 297.830s\n",
      "Train: 12 [1900/10009 ( 19%)]  Loss: 3.31 (3.26)  Time: 0.161s,  793.08/s  (0.161s,  795.53/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 305.869s\n",
      "Train: 12 [1950/10009 ( 19%)]  Loss: 3.55 (3.26)  Time: 0.161s,  793.14/s  (0.161s,  795.52/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 313.917s\n",
      "Train: 12 [2000/10009 ( 20%)]  Loss: 3.23 (3.26)  Time: 0.161s,  792.83/s  (0.161s,  795.53/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 321.957s\n",
      "Train: 12 [2050/10009 ( 20%)]  Loss: 2.87 (3.26)  Time: 0.162s,  789.77/s  (0.161s,  795.54/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 329.997s\n",
      "Train: 12 [2100/10009 ( 21%)]  Loss: 3.20 (3.26)  Time: 0.162s,  792.27/s  (0.161s,  795.54/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 338.044s\n",
      "Train: 12 [2150/10009 ( 21%)]  Loss: 3.24 (3.26)  Time: 0.162s,  790.08/s  (0.161s,  795.55/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 346.085s\n",
      "Train: 12 [2200/10009 ( 22%)]  Loss: 3.41 (3.26)  Time: 0.160s,  798.61/s  (0.161s,  795.55/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 354.130s\n",
      "Train: 12 [2250/10009 ( 22%)]  Loss: 3.00 (3.26)  Time: 0.160s,  797.90/s  (0.161s,  795.55/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 362.176s\n",
      "Train: 12 [2300/10009 ( 23%)]  Loss: 3.50 (3.26)  Time: 0.160s,  797.97/s  (0.161s,  795.50/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 370.244s\n",
      "Train: 12 [2350/10009 ( 23%)]  Loss: 3.32 (3.26)  Time: 0.161s,  793.84/s  (0.161s,  795.49/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 378.294s\n",
      "Train: 12 [2400/10009 ( 24%)]  Loss: 3.27 (3.26)  Time: 0.162s,  792.21/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 386.357s\n",
      "Train: 12 [2450/10009 ( 24%)]  Loss: 3.45 (3.26)  Time: 0.160s,  798.58/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 394.402s\n",
      "Train: 12 [2500/10009 ( 25%)]  Loss: 3.31 (3.26)  Time: 0.161s,  796.01/s  (0.161s,  795.46/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 402.445s\n",
      "Train: 12 [2550/10009 ( 25%)]  Loss: 3.29 (3.26)  Time: 0.161s,  793.37/s  (0.161s,  795.46/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 410.491s\n",
      "Train: 12 [2600/10009 ( 26%)]  Loss: 3.14 (3.26)  Time: 0.161s,  794.63/s  (0.161s,  795.46/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 418.537s\n",
      "Train: 12 [2650/10009 ( 26%)]  Loss: 3.09 (3.26)  Time: 0.161s,  797.01/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 426.584s\n",
      "Train: 12 [2700/10009 ( 27%)]  Loss: 3.25 (3.26)  Time: 0.163s,  787.32/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 434.631s\n",
      "Train: 12 [2750/10009 ( 27%)]  Loss: 3.24 (3.26)  Time: 0.161s,  797.23/s  (0.161s,  795.43/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 442.688s\n",
      "Train: 12 [2800/10009 ( 28%)]  Loss: 3.29 (3.26)  Time: 0.160s,  798.42/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 450.722s\n",
      "Train: 12 [2850/10009 ( 28%)]  Loss: 3.13 (3.26)  Time: 0.161s,  796.29/s  (0.161s,  795.43/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 458.777s\n",
      "Train: 12 [2900/10009 ( 29%)]  Loss: 3.06 (3.26)  Time: 0.160s,  797.64/s  (0.161s,  795.44/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 466.817s\n",
      "Train: 12 [2950/10009 ( 29%)]  Loss: 3.52 (3.26)  Time: 0.160s,  799.54/s  (0.161s,  795.43/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 474.871s\n",
      "Train: 12 [3000/10009 ( 30%)]  Loss: 3.26 (3.26)  Time: 0.161s,  792.70/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 482.942s\n",
      "Train: 12 [3050/10009 ( 30%)]  Loss: 3.24 (3.26)  Time: 0.162s,  790.21/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 490.991s\n",
      "Train: 12 [3100/10009 ( 31%)]  Loss: 3.15 (3.26)  Time: 0.161s,  796.59/s  (0.161s,  795.38/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 499.042s\n",
      "Train: 12 [3150/10009 ( 31%)]  Loss: 3.18 (3.26)  Time: 0.161s,  793.57/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 507.083s\n",
      "Train: 12 [3200/10009 ( 32%)]  Loss: 3.39 (3.26)  Time: 0.161s,  794.36/s  (0.161s,  795.38/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 515.135s\n",
      "Train: 12 [3250/10009 ( 32%)]  Loss: 3.30 (3.26)  Time: 0.160s,  797.71/s  (0.161s,  795.36/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 523.191s\n",
      "Train: 12 [3300/10009 ( 33%)]  Loss: 3.40 (3.26)  Time: 0.161s,  794.88/s  (0.161s,  795.35/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 531.249s\n",
      "Train: 12 [3350/10009 ( 33%)]  Loss: 3.03 (3.26)  Time: 0.161s,  797.15/s  (0.161s,  795.35/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 539.295s\n",
      "Train: 12 [3400/10009 ( 34%)]  Loss: 3.31 (3.26)  Time: 0.161s,  796.64/s  (0.161s,  795.34/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 547.345s\n",
      "Train: 12 [3450/10009 ( 34%)]  Loss: 3.42 (3.26)  Time: 0.161s,  797.19/s  (0.161s,  795.33/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 555.401s\n",
      "Train: 12 [3500/10009 ( 35%)]  Loss: 3.02 (3.26)  Time: 0.163s,  786.44/s  (0.161s,  795.31/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 563.466s\n",
      "Train: 12 [3550/10009 ( 35%)]  Loss: 3.38 (3.26)  Time: 0.160s,  799.87/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 571.537s\n",
      "Train: 12 [3600/10009 ( 36%)]  Loss: 3.24 (3.26)  Time: 0.160s,  797.84/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 579.588s\n",
      "Train: 12 [3650/10009 ( 36%)]  Loss: 3.05 (3.26)  Time: 0.162s,  789.32/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 587.645s\n",
      "Train: 12 [3700/10009 ( 37%)]  Loss: 3.28 (3.26)  Time: 0.161s,  796.49/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 595.695s\n",
      "Train: 12 [3750/10009 ( 37%)]  Loss: 3.25 (3.26)  Time: 0.161s,  793.03/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 603.736s\n",
      "Train: 12 [3800/10009 ( 38%)]  Loss: 3.33 (3.26)  Time: 0.161s,  797.29/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 611.776s\n",
      "Train: 12 [3850/10009 ( 38%)]  Loss: 2.93 (3.26)  Time: 0.161s,  796.47/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 619.826s\n",
      "Train: 12 [3900/10009 ( 39%)]  Loss: 3.35 (3.26)  Time: 0.160s,  799.54/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 627.879s\n",
      "Train: 12 [3950/10009 ( 39%)]  Loss: 3.54 (3.26)  Time: 0.161s,  793.16/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 635.930s\n",
      "Train: 12 [4000/10009 ( 40%)]  Loss: 3.28 (3.26)  Time: 0.161s,  793.01/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 643.975s\n",
      "Train: 12 [4050/10009 ( 40%)]  Loss: 2.96 (3.26)  Time: 0.161s,  794.81/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 652.024s\n",
      "Train: 12 [4100/10009 ( 41%)]  Loss: 3.29 (3.26)  Time: 0.161s,  793.39/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 660.068s\n",
      "Train: 12 [4150/10009 ( 41%)]  Loss: 3.27 (3.25)  Time: 0.162s,  789.70/s  (0.161s,  795.24/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 668.131s\n",
      "Train: 12 [4200/10009 ( 42%)]  Loss: 3.26 (3.25)  Time: 0.160s,  798.84/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 676.172s\n",
      "Train: 12 [4250/10009 ( 42%)]  Loss: 3.15 (3.26)  Time: 0.161s,  797.30/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 684.220s\n",
      "Train: 12 [4300/10009 ( 43%)]  Loss: 3.49 (3.25)  Time: 0.161s,  793.58/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 692.264s\n",
      "Train: 12 [4350/10009 ( 43%)]  Loss: 3.34 (3.26)  Time: 0.163s,  784.89/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 700.314s\n",
      "Train: 12 [4400/10009 ( 44%)]  Loss: 3.36 (3.26)  Time: 0.161s,  794.83/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 708.358s\n",
      "Train: 12 [4450/10009 ( 44%)]  Loss: 3.33 (3.25)  Time: 0.162s,  792.29/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 716.429s\n",
      "Train: 12 [4500/10009 ( 45%)]  Loss: 3.20 (3.25)  Time: 0.161s,  796.21/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 724.478s\n",
      "Train: 12 [4550/10009 ( 45%)]  Loss: 2.97 (3.26)  Time: 0.161s,  794.83/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 732.531s\n",
      "Train: 12 [4600/10009 ( 46%)]  Loss: 3.19 (3.25)  Time: 0.156s,  820.35/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 740.588s\n",
      "Train: 12 [4650/10009 ( 46%)]  Loss: 3.14 (3.26)  Time: 0.161s,  795.84/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 748.637s\n",
      "Train: 12 [4700/10009 ( 47%)]  Loss: 3.66 (3.26)  Time: 0.160s,  799.52/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 756.680s\n",
      "Train: 12 [4750/10009 ( 47%)]  Loss: 3.30 (3.25)  Time: 0.160s,  798.33/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 764.725s\n",
      "Train: 12 [4800/10009 ( 48%)]  Loss: 3.20 (3.26)  Time: 0.161s,  793.53/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 772.775s\n",
      "Train: 12 [4850/10009 ( 48%)]  Loss: 3.26 (3.25)  Time: 0.160s,  797.53/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 780.824s\n",
      "Train: 12 [4900/10009 ( 49%)]  Loss: 3.19 (3.26)  Time: 0.160s,  799.34/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 788.870s\n",
      "Train: 12 [4950/10009 ( 49%)]  Loss: 3.38 (3.26)  Time: 0.160s,  798.82/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 796.912s\n",
      "Train: 12 [5000/10009 ( 50%)]  Loss: 3.34 (3.25)  Time: 0.161s,  793.75/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 804.956s\n",
      "Train: 12 [5050/10009 ( 50%)]  Loss: 3.07 (3.25)  Time: 0.162s,  789.39/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 813.001s\n",
      "Train: 12 [5100/10009 ( 51%)]  Loss: 3.15 (3.25)  Time: 0.163s,  786.21/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 821.055s\n",
      "Train: 12 [5150/10009 ( 51%)]  Loss: 3.59 (3.25)  Time: 0.160s,  799.80/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 829.101s\n",
      "Train: 12 [5200/10009 ( 52%)]  Loss: 3.41 (3.25)  Time: 0.160s,  800.30/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 837.152s\n",
      "Train: 12 [5250/10009 ( 52%)]  Loss: 3.31 (3.25)  Time: 0.161s,  795.03/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 845.206s\n",
      "Train: 12 [5300/10009 ( 53%)]  Loss: 3.48 (3.25)  Time: 0.160s,  797.78/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 853.249s\n",
      "Train: 12 [5350/10009 ( 53%)]  Loss: 3.14 (3.25)  Time: 0.161s,  796.37/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 861.291s\n",
      "Train: 12 [5400/10009 ( 54%)]  Loss: 3.46 (3.25)  Time: 0.160s,  798.22/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 869.340s\n",
      "Train: 12 [5450/10009 ( 54%)]  Loss: 3.46 (3.26)  Time: 0.161s,  796.72/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 877.398s\n",
      "Train: 12 [5500/10009 ( 55%)]  Loss: 3.20 (3.26)  Time: 0.161s,  795.82/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 885.462s\n",
      "Train: 12 [5550/10009 ( 55%)]  Loss: 3.13 (3.26)  Time: 0.161s,  793.71/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 893.514s\n",
      "Train: 12 [5600/10009 ( 56%)]  Loss: 3.10 (3.26)  Time: 0.161s,  795.43/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 901.560s\n",
      "Train: 12 [5650/10009 ( 56%)]  Loss: 3.13 (3.26)  Time: 0.161s,  795.41/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 909.606s\n",
      "Train: 12 [5700/10009 ( 57%)]  Loss: 3.25 (3.26)  Time: 0.161s,  794.54/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 917.663s\n",
      "Train: 12 [5750/10009 ( 57%)]  Loss: 3.56 (3.26)  Time: 0.161s,  797.13/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 925.718s\n",
      "Train: 12 [5800/10009 ( 58%)]  Loss: 3.56 (3.26)  Time: 0.162s,  792.34/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 933.767s\n",
      "Train: 12 [5850/10009 ( 58%)]  Loss: 3.06 (3.26)  Time: 0.160s,  798.90/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 941.821s\n",
      "Train: 12 [5900/10009 ( 59%)]  Loss: 3.54 (3.26)  Time: 0.160s,  798.43/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 949.866s\n",
      "Train: 12 [5950/10009 ( 59%)]  Loss: 3.21 (3.26)  Time: 0.162s,  790.16/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 957.919s\n",
      "Train: 12 [6000/10009 ( 60%)]  Loss: 3.15 (3.26)  Time: 0.161s,  793.95/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 965.956s\n",
      "Train: 12 [6050/10009 ( 60%)]  Loss: 3.03 (3.26)  Time: 0.161s,  793.52/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 974.000s\n",
      "Train: 12 [6100/10009 ( 61%)]  Loss: 3.12 (3.26)  Time: 0.162s,  792.11/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 982.050s\n",
      "Train: 12 [6150/10009 ( 61%)]  Loss: 3.12 (3.26)  Time: 0.160s,  797.57/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 990.102s\n",
      "Train: 12 [6200/10009 ( 62%)]  Loss: 3.54 (3.26)  Time: 0.162s,  788.51/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 998.151s\n",
      "Train: 12 [6250/10009 ( 62%)]  Loss: 3.12 (3.26)  Time: 0.162s,  791.92/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1006.203s\n",
      "Train: 12 [6300/10009 ( 63%)]  Loss: 2.97 (3.26)  Time: 0.160s,  799.74/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1014.246s\n",
      "Train: 12 [6350/10009 ( 63%)]  Loss: 3.26 (3.26)  Time: 0.161s,  795.97/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1022.301s\n",
      "Train: 12 [6400/10009 ( 64%)]  Loss: 3.17 (3.26)  Time: 0.161s,  793.37/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1030.351s\n",
      "Train: 12 [6450/10009 ( 64%)]  Loss: 3.31 (3.26)  Time: 0.161s,  794.74/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1038.416s\n",
      "Train: 12 [6500/10009 ( 65%)]  Loss: 3.08 (3.26)  Time: 0.160s,  799.42/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1046.466s\n",
      "Train: 12 [6550/10009 ( 65%)]  Loss: 3.44 (3.26)  Time: 0.162s,  790.45/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1054.509s\n",
      "Train: 12 [6600/10009 ( 66%)]  Loss: 3.65 (3.26)  Time: 0.161s,  797.34/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1062.553s\n",
      "Train: 12 [6650/10009 ( 66%)]  Loss: 3.32 (3.26)  Time: 0.162s,  790.76/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1070.587s\n",
      "Train: 12 [6700/10009 ( 67%)]  Loss: 2.90 (3.26)  Time: 0.161s,  797.21/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1078.634s\n",
      "Train: 12 [6750/10009 ( 67%)]  Loss: 3.38 (3.26)  Time: 0.160s,  800.19/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1086.680s\n",
      "Train: 12 [6800/10009 ( 68%)]  Loss: 3.22 (3.26)  Time: 0.162s,  790.10/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1094.725s\n",
      "Train: 12 [6850/10009 ( 68%)]  Loss: 3.47 (3.26)  Time: 0.161s,  797.15/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1102.781s\n",
      "Train: 12 [6900/10009 ( 69%)]  Loss: 3.29 (3.26)  Time: 0.160s,  797.75/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1110.841s\n",
      "Train: 12 [6950/10009 ( 69%)]  Loss: 3.25 (3.26)  Time: 0.160s,  799.00/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1118.900s\n",
      "Train: 12 [7000/10009 ( 70%)]  Loss: 3.46 (3.26)  Time: 0.164s,  779.96/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1126.966s\n",
      "Train: 12 [7050/10009 ( 70%)]  Loss: 3.37 (3.26)  Time: 0.161s,  793.35/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1135.029s\n",
      "Train: 12 [7100/10009 ( 71%)]  Loss: 3.17 (3.26)  Time: 0.160s,  798.83/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1143.080s\n",
      "Train: 12 [7150/10009 ( 71%)]  Loss: 3.19 (3.26)  Time: 0.160s,  799.74/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1151.127s\n",
      "Train: 12 [7200/10009 ( 72%)]  Loss: 2.89 (3.26)  Time: 0.160s,  799.81/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1159.182s\n",
      "Train: 12 [7250/10009 ( 72%)]  Loss: 3.01 (3.26)  Time: 0.161s,  795.06/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1167.227s\n",
      "Train: 12 [7300/10009 ( 73%)]  Loss: 3.07 (3.26)  Time: 0.160s,  798.99/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1175.288s\n",
      "Train: 12 [7350/10009 ( 73%)]  Loss: 3.29 (3.26)  Time: 0.162s,  791.70/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1183.331s\n",
      "Train: 12 [7400/10009 ( 74%)]  Loss: 3.17 (3.26)  Time: 0.161s,  793.97/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1191.382s\n",
      "Train: 12 [7450/10009 ( 74%)]  Loss: 3.30 (3.26)  Time: 0.161s,  794.74/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1199.436s\n",
      "Train: 12 [7500/10009 ( 75%)]  Loss: 3.14 (3.26)  Time: 0.161s,  797.51/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1207.486s\n",
      "Train: 12 [7550/10009 ( 75%)]  Loss: 3.19 (3.26)  Time: 0.161s,  794.69/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1215.534s\n",
      "Train: 12 [7600/10009 ( 76%)]  Loss: 3.35 (3.26)  Time: 0.160s,  799.93/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1223.582s\n",
      "Train: 12 [7650/10009 ( 76%)]  Loss: 3.47 (3.26)  Time: 0.161s,  797.24/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1231.622s\n",
      "Train: 12 [7700/10009 ( 77%)]  Loss: 3.22 (3.26)  Time: 0.161s,  797.31/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1239.667s\n",
      "Train: 12 [7750/10009 ( 77%)]  Loss: 3.26 (3.26)  Time: 0.160s,  797.86/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1247.707s\n",
      "Train: 12 [7800/10009 ( 78%)]  Loss: 3.43 (3.26)  Time: 0.161s,  793.66/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1255.752s\n",
      "Train: 12 [7850/10009 ( 78%)]  Loss: 3.20 (3.26)  Time: 0.161s,  797.17/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1263.805s\n",
      "Train: 12 [7900/10009 ( 79%)]  Loss: 3.43 (3.26)  Time: 0.160s,  797.89/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1271.847s\n",
      "Train: 12 [7950/10009 ( 79%)]  Loss: 3.19 (3.26)  Time: 0.161s,  794.03/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1279.885s\n",
      "Train: 12 [8000/10009 ( 80%)]  Loss: 3.06 (3.26)  Time: 0.162s,  792.22/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1287.939s\n",
      "Train: 12 [8050/10009 ( 80%)]  Loss: 2.90 (3.26)  Time: 0.161s,  793.64/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1295.990s\n",
      "Train: 12 [8100/10009 ( 81%)]  Loss: 3.20 (3.26)  Time: 0.162s,  791.34/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1304.035s\n",
      "Train: 12 [8150/10009 ( 81%)]  Loss: 3.65 (3.26)  Time: 0.160s,  797.68/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1312.080s\n",
      "Train: 12 [8200/10009 ( 82%)]  Loss: 3.28 (3.26)  Time: 0.162s,  789.69/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 1320.134s\n",
      "Train: 12 [8250/10009 ( 82%)]  Loss: 2.92 (3.26)  Time: 0.161s,  797.39/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1328.188s\n",
      "Train: 12 [8300/10009 ( 83%)]  Loss: 3.12 (3.26)  Time: 0.161s,  795.92/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1336.259s\n",
      "Train: 12 [8350/10009 ( 83%)]  Loss: 3.19 (3.26)  Time: 0.161s,  794.43/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1344.345s\n",
      "Train: 12 [8400/10009 ( 84%)]  Loss: 3.26 (3.26)  Time: 0.160s,  797.68/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1352.392s\n",
      "Train: 12 [8450/10009 ( 84%)]  Loss: 3.24 (3.26)  Time: 0.162s,  788.38/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1360.439s\n",
      "Train: 12 [8500/10009 ( 85%)]  Loss: 2.97 (3.26)  Time: 0.163s,  784.06/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1368.486s\n",
      "Train: 12 [8550/10009 ( 85%)]  Loss: 3.27 (3.26)  Time: 0.160s,  800.71/s  (0.161s,  795.12/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1376.550s\n",
      "Train: 12 [8600/10009 ( 86%)]  Loss: 3.32 (3.26)  Time: 0.160s,  798.61/s  (0.161s,  795.12/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1384.594s\n",
      "Train: 12 [8650/10009 ( 86%)]  Loss: 3.04 (3.26)  Time: 0.160s,  798.13/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1392.632s\n",
      "Train: 12 [8700/10009 ( 87%)]  Loss: 3.11 (3.26)  Time: 0.161s,  794.38/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1400.681s\n",
      "Train: 12 [8750/10009 ( 87%)]  Loss: 3.29 (3.26)  Time: 0.160s,  797.61/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1408.726s\n",
      "Train: 12 [8800/10009 ( 88%)]  Loss: 3.18 (3.26)  Time: 0.161s,  797.44/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1416.763s\n",
      "Train: 12 [8850/10009 ( 88%)]  Loss: 3.36 (3.26)  Time: 0.161s,  796.35/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1424.827s\n",
      "Train: 12 [8900/10009 ( 89%)]  Loss: 3.21 (3.26)  Time: 0.161s,  794.25/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1432.882s\n",
      "Train: 12 [8950/10009 ( 89%)]  Loss: 3.38 (3.26)  Time: 0.161s,  796.64/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1440.929s\n",
      "Train: 12 [9000/10009 ( 90%)]  Loss: 3.08 (3.26)  Time: 0.162s,  788.86/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1448.969s\n",
      "Train: 12 [9050/10009 ( 90%)]  Loss: 3.09 (3.26)  Time: 0.161s,  795.60/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1457.019s\n",
      "Train: 12 [9100/10009 ( 91%)]  Loss: 3.39 (3.26)  Time: 0.162s,  788.10/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 1465.066s\n",
      "Train: 12 [9150/10009 ( 91%)]  Loss: 3.19 (3.26)  Time: 0.160s,  799.87/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1473.113s\n",
      "Train: 12 [9200/10009 ( 92%)]  Loss: 3.59 (3.26)  Time: 0.160s,  797.70/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1481.158s\n",
      "Train: 12 [9250/10009 ( 92%)]  Loss: 3.33 (3.26)  Time: 0.160s,  798.76/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1489.204s\n",
      "Train: 12 [9300/10009 ( 93%)]  Loss: 3.14 (3.26)  Time: 0.161s,  795.91/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1497.245s\n",
      "Train: 12 [9350/10009 ( 93%)]  Loss: 3.06 (3.26)  Time: 0.160s,  799.11/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1505.293s\n",
      "Train: 12 [9400/10009 ( 94%)]  Loss: 3.28 (3.26)  Time: 0.160s,  798.17/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1513.341s\n",
      "Train: 12 [9450/10009 ( 94%)]  Loss: 3.39 (3.26)  Time: 0.160s,  799.71/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1521.390s\n",
      "Train: 12 [9500/10009 ( 95%)]  Loss: 3.30 (3.26)  Time: 0.161s,  793.31/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1529.434s\n",
      "Train: 12 [9550/10009 ( 95%)]  Loss: 3.37 (3.26)  Time: 0.161s,  796.07/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1537.486s\n",
      "Train: 12 [9600/10009 ( 96%)]  Loss: 3.08 (3.26)  Time: 0.161s,  792.99/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1545.528s\n",
      "Train: 12 [9650/10009 ( 96%)]  Loss: 3.48 (3.26)  Time: 0.160s,  800.13/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1553.563s\n",
      "Train: 12 [9700/10009 ( 97%)]  Loss: 3.30 (3.26)  Time: 0.160s,  801.20/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1561.605s\n",
      "Train: 12 [9750/10009 ( 97%)]  Loss: 3.43 (3.26)  Time: 0.160s,  800.41/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1569.642s\n",
      "Train: 12 [9800/10009 ( 98%)]  Loss: 3.50 (3.26)  Time: 0.161s,  794.29/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1577.690s\n",
      "Train: 12 [9850/10009 ( 98%)]  Loss: 3.22 (3.26)  Time: 0.160s,  800.02/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1585.743s\n",
      "Train: 12 [9900/10009 ( 99%)]  Loss: 3.31 (3.26)  Time: 0.160s,  797.89/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1593.785s\n",
      "Train: 12 [9950/10009 ( 99%)]  Loss: 3.52 (3.26)  Time: 0.160s,  802.18/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1601.830s\n",
      "Train: 12 [10000/10009 (100%)]  Loss: 3.07 (3.26)  Time: 0.204s,  626.37/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.050 (0.006)Time: 1609.913s\n",
      "Test: [   0/390]  Time: 0.679 (0.679)  Loss:   1.249 ( 1.249)  Acc@1:  79.688 ( 79.688)  Acc@5:  91.406 ( 91.406)\n",
      "Test: [  50/390]  Time: 0.052 (0.138)  Loss:   1.147 ( 2.019)  Acc@1:  72.656 ( 57.430)  Acc@5:  92.188 ( 78.202)\n",
      "Test: [ 100/390]  Time: 0.263 (0.132)  Loss:   1.933 ( 2.045)  Acc@1:  56.250 ( 54.022)  Acc@5:  82.031 ( 78.837)\n",
      "Test: [ 150/390]  Time: 0.052 (0.135)  Loss:   1.790 ( 2.008)  Acc@1:  57.031 ( 54.838)  Acc@5:  83.594 ( 79.367)\n",
      "Test: [ 200/390]  Time: 0.051 (0.132)  Loss:   3.167 ( 2.190)  Acc@1:  29.688 ( 51.710)  Acc@5:  60.938 ( 76.294)\n",
      "Test: [ 250/390]  Time: 0.052 (0.131)  Loss:   2.287 ( 2.303)  Acc@1:  56.250 ( 50.093)  Acc@5:  71.094 ( 74.406)\n",
      "Test: [ 300/390]  Time: 0.363 (0.131)  Loss:   2.656 ( 2.401)  Acc@1:  51.562 ( 48.438)  Acc@5:  67.188 ( 72.654)\n",
      "Test: [ 350/390]  Time: 0.052 (0.129)  Loss:   2.729 ( 2.473)  Acc@1:  42.969 ( 47.086)  Acc@5:  67.188 ( 71.443)\n",
      "Test: [ 390/390]  Time: 0.034 (0.130)  Loss:   3.650 ( 2.440)  Acc@1:  23.750 ( 47.692)  Acc@5:  57.500 ( 72.006)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 13 [   0/10009 (  0%)]  Loss: 3.24 (3.24)  Time: 0.666s,  192.14/s  (0.666s,  192.14/s)  LR: 2.161e-06  Data: 0.515 (0.515)Time: 0.667s\n",
      "Train: 13 [  50/10009 (  0%)]  Loss: 3.30 (3.22)  Time: 0.159s,  803.23/s  (0.169s,  758.09/s)  LR: 2.161e-06  Data: 0.006 (0.016)Time: 8.612s\n",
      "Train: 13 [ 100/10009 (  1%)]  Loss: 2.74 (3.20)  Time: 0.161s,  795.50/s  (0.164s,  779.95/s)  LR: 2.161e-06  Data: 0.008 (0.011)Time: 16.576s\n",
      "Train: 13 [ 150/10009 (  1%)]  Loss: 3.17 (3.20)  Time: 0.159s,  803.50/s  (0.163s,  786.67/s)  LR: 2.161e-06  Data: 0.006 (0.010)Time: 24.570s\n",
      "Train: 13 [ 200/10009 (  2%)]  Loss: 3.21 (3.20)  Time: 0.161s,  796.79/s  (0.162s,  789.59/s)  LR: 2.161e-06  Data: 0.007 (0.009)Time: 32.585s\n",
      "Train: 13 [ 250/10009 (  2%)]  Loss: 3.08 (3.21)  Time: 0.161s,  793.79/s  (0.162s,  791.22/s)  LR: 2.161e-06  Data: 0.007 (0.008)Time: 40.606s\n",
      "Train: 13 [ 300/10009 (  3%)]  Loss: 3.10 (3.21)  Time: 0.161s,  795.77/s  (0.162s,  792.31/s)  LR: 2.161e-06  Data: 0.006 (0.008)Time: 48.628s\n",
      "Train: 13 [ 350/10009 (  3%)]  Loss: 3.12 (3.20)  Time: 0.160s,  798.53/s  (0.161s,  792.96/s)  LR: 2.161e-06  Data: 0.006 (0.008)Time: 56.659s\n",
      "Train: 13 [ 400/10009 (  4%)]  Loss: 3.09 (3.20)  Time: 0.160s,  798.48/s  (0.161s,  793.35/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 64.698s\n",
      "Train: 13 [ 450/10009 (  4%)]  Loss: 3.60 (3.20)  Time: 0.160s,  801.08/s  (0.161s,  793.65/s)  LR: 2.161e-06  Data: 0.005 (0.007)Time: 72.738s\n",
      "Train: 13 [ 500/10009 (  5%)]  Loss: 3.30 (3.20)  Time: 0.160s,  799.46/s  (0.161s,  793.96/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 80.770s\n",
      "Train: 13 [ 550/10009 (  5%)]  Loss: 3.42 (3.20)  Time: 0.161s,  794.48/s  (0.161s,  794.23/s)  LR: 2.161e-06  Data: 0.007 (0.007)Time: 88.801s\n",
      "Train: 13 [ 600/10009 (  6%)]  Loss: 3.18 (3.20)  Time: 0.161s,  794.51/s  (0.161s,  794.46/s)  LR: 2.161e-06  Data: 0.007 (0.007)Time: 96.831s\n",
      "Train: 13 [ 650/10009 (  6%)]  Loss: 3.04 (3.20)  Time: 0.160s,  797.92/s  (0.161s,  794.67/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 104.859s\n",
      "Train: 13 [ 700/10009 (  7%)]  Loss: 3.73 (3.20)  Time: 0.161s,  796.99/s  (0.161s,  794.83/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 112.890s\n",
      "Train: 13 [ 750/10009 (  7%)]  Loss: 3.37 (3.20)  Time: 0.162s,  792.12/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 120.925s\n",
      "Train: 13 [ 800/10009 (  8%)]  Loss: 3.12 (3.21)  Time: 0.161s,  795.29/s  (0.161s,  794.97/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 128.972s\n",
      "Train: 13 [ 850/10009 (  8%)]  Loss: 3.04 (3.21)  Time: 0.161s,  794.25/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.005 (0.007)Time: 137.012s\n",
      "Train: 13 [ 900/10009 (  9%)]  Loss: 2.95 (3.21)  Time: 0.161s,  793.42/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 145.061s\n",
      "Train: 13 [ 950/10009 (  9%)]  Loss: 3.26 (3.21)  Time: 0.160s,  798.38/s  (0.161s,  795.10/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 153.099s\n",
      "Train: 13 [1000/10009 ( 10%)]  Loss: 3.14 (3.21)  Time: 0.160s,  798.19/s  (0.161s,  795.18/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 161.132s\n",
      "Train: 13 [1050/10009 ( 10%)]  Loss: 3.23 (3.21)  Time: 0.160s,  797.98/s  (0.161s,  795.27/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 169.161s\n",
      "Train: 13 [1100/10009 ( 11%)]  Loss: 3.35 (3.21)  Time: 0.160s,  798.17/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 177.193s\n",
      "Train: 13 [1150/10009 ( 11%)]  Loss: 2.98 (3.21)  Time: 0.162s,  789.10/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.007 (0.007)Time: 185.238s\n",
      "Train: 13 [1200/10009 ( 12%)]  Loss: 3.32 (3.21)  Time: 0.160s,  798.87/s  (0.161s,  795.36/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 193.282s\n",
      "Train: 13 [1250/10009 ( 12%)]  Loss: 3.40 (3.21)  Time: 0.160s,  798.40/s  (0.161s,  795.39/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 201.321s\n",
      "Train: 13 [1300/10009 ( 13%)]  Loss: 3.10 (3.21)  Time: 0.160s,  798.23/s  (0.161s,  795.46/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 209.349s\n",
      "Train: 13 [1350/10009 ( 13%)]  Loss: 3.15 (3.21)  Time: 0.160s,  798.97/s  (0.161s,  795.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 217.392s\n",
      "Train: 13 [1400/10009 ( 14%)]  Loss: 3.32 (3.21)  Time: 0.162s,  788.17/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.009 (0.006)Time: 225.434s\n",
      "Train: 13 [1450/10009 ( 14%)]  Loss: 3.33 (3.21)  Time: 0.161s,  794.28/s  (0.161s,  795.46/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 233.484s\n",
      "Train: 13 [1500/10009 ( 15%)]  Loss: 3.11 (3.21)  Time: 0.161s,  795.46/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 241.526s\n",
      "Train: 13 [1550/10009 ( 15%)]  Loss: 3.12 (3.21)  Time: 0.160s,  799.16/s  (0.161s,  795.51/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 249.560s\n",
      "Train: 13 [1600/10009 ( 16%)]  Loss: 3.23 (3.21)  Time: 0.161s,  792.91/s  (0.161s,  795.52/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 257.603s\n",
      "Train: 13 [1650/10009 ( 16%)]  Loss: 3.26 (3.21)  Time: 0.160s,  800.01/s  (0.161s,  795.45/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 265.671s\n",
      "Train: 13 [1700/10009 ( 17%)]  Loss: 3.15 (3.21)  Time: 0.161s,  792.95/s  (0.161s,  795.41/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 273.730s\n",
      "Train: 13 [1750/10009 ( 17%)]  Loss: 3.45 (3.20)  Time: 0.160s,  800.82/s  (0.161s,  795.43/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 281.769s\n",
      "Train: 13 [1800/10009 ( 18%)]  Loss: 3.27 (3.20)  Time: 0.160s,  797.98/s  (0.161s,  795.45/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 289.807s\n",
      "Train: 13 [1850/10009 ( 18%)]  Loss: 3.06 (3.21)  Time: 0.160s,  797.52/s  (0.161s,  795.46/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 297.851s\n",
      "Train: 13 [1900/10009 ( 19%)]  Loss: 3.34 (3.21)  Time: 0.162s,  789.37/s  (0.161s,  795.44/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 305.905s\n",
      "Train: 13 [1950/10009 ( 19%)]  Loss: 3.42 (3.20)  Time: 0.162s,  792.25/s  (0.161s,  795.45/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 313.946s\n",
      "Train: 13 [2000/10009 ( 20%)]  Loss: 3.06 (3.20)  Time: 0.162s,  792.00/s  (0.161s,  795.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 321.982s\n",
      "Train: 13 [2050/10009 ( 20%)]  Loss: 3.13 (3.20)  Time: 0.161s,  794.87/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 330.025s\n",
      "Train: 13 [2100/10009 ( 21%)]  Loss: 3.60 (3.20)  Time: 0.161s,  794.56/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 338.070s\n",
      "Train: 13 [2150/10009 ( 21%)]  Loss: 3.21 (3.20)  Time: 0.160s,  802.11/s  (0.161s,  795.50/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 346.108s\n",
      "Train: 13 [2200/10009 ( 22%)]  Loss: 3.47 (3.21)  Time: 0.160s,  797.85/s  (0.161s,  795.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 354.167s\n",
      "Train: 13 [2250/10009 ( 22%)]  Loss: 2.99 (3.20)  Time: 0.161s,  795.57/s  (0.161s,  795.44/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 362.224s\n",
      "Train: 13 [2300/10009 ( 23%)]  Loss: 2.93 (3.20)  Time: 0.161s,  797.38/s  (0.161s,  795.44/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 370.272s\n",
      "Train: 13 [2350/10009 ( 23%)]  Loss: 3.30 (3.20)  Time: 0.160s,  798.18/s  (0.161s,  795.43/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 378.321s\n",
      "Train: 13 [2400/10009 ( 24%)]  Loss: 3.26 (3.20)  Time: 0.160s,  800.15/s  (0.161s,  795.40/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 386.379s\n",
      "Train: 13 [2450/10009 ( 24%)]  Loss: 3.41 (3.20)  Time: 0.161s,  794.81/s  (0.161s,  795.41/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 394.423s\n",
      "Train: 13 [2500/10009 ( 25%)]  Loss: 3.15 (3.20)  Time: 0.161s,  794.81/s  (0.161s,  795.41/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 402.470s\n",
      "Train: 13 [2550/10009 ( 25%)]  Loss: 3.04 (3.20)  Time: 0.161s,  797.41/s  (0.161s,  795.40/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 410.522s\n",
      "Train: 13 [2600/10009 ( 26%)]  Loss: 3.12 (3.20)  Time: 0.162s,  792.57/s  (0.161s,  795.39/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 418.571s\n",
      "Train: 13 [2650/10009 ( 26%)]  Loss: 3.21 (3.20)  Time: 0.161s,  797.05/s  (0.161s,  795.39/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 426.619s\n",
      "Train: 13 [2700/10009 ( 27%)]  Loss: 3.06 (3.20)  Time: 0.160s,  798.02/s  (0.161s,  795.35/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 434.686s\n",
      "Train: 13 [2750/10009 ( 27%)]  Loss: 3.18 (3.20)  Time: 0.161s,  794.76/s  (0.161s,  795.35/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 442.733s\n",
      "Train: 13 [2800/10009 ( 28%)]  Loss: 3.32 (3.20)  Time: 0.161s,  795.72/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 450.790s\n",
      "Train: 13 [2850/10009 ( 28%)]  Loss: 2.98 (3.20)  Time: 0.160s,  798.56/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 458.841s\n",
      "Train: 13 [2900/10009 ( 29%)]  Loss: 3.20 (3.20)  Time: 0.160s,  799.89/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 466.905s\n",
      "Train: 13 [2950/10009 ( 29%)]  Loss: 3.29 (3.20)  Time: 0.161s,  793.91/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 474.945s\n",
      "Train: 13 [3000/10009 ( 30%)]  Loss: 3.12 (3.20)  Time: 0.161s,  793.49/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 482.998s\n",
      "Train: 13 [3050/10009 ( 30%)]  Loss: 2.94 (3.20)  Time: 0.162s,  791.35/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 491.044s\n",
      "Train: 13 [3100/10009 ( 31%)]  Loss: 3.10 (3.20)  Time: 0.161s,  794.57/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 499.095s\n",
      "Train: 13 [3150/10009 ( 31%)]  Loss: 3.29 (3.20)  Time: 0.160s,  797.96/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 507.143s\n",
      "Train: 13 [3200/10009 ( 32%)]  Loss: 3.57 (3.20)  Time: 0.161s,  796.18/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 515.188s\n",
      "Train: 13 [3250/10009 ( 32%)]  Loss: 3.03 (3.20)  Time: 0.160s,  799.73/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 523.218s\n",
      "Train: 13 [3300/10009 ( 33%)]  Loss: 3.43 (3.20)  Time: 0.160s,  798.91/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 531.266s\n",
      "Train: 13 [3350/10009 ( 33%)]  Loss: 3.14 (3.20)  Time: 0.160s,  798.69/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 539.314s\n",
      "Train: 13 [3400/10009 ( 34%)]  Loss: 3.39 (3.20)  Time: 0.162s,  789.18/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 547.358s\n",
      "Train: 13 [3450/10009 ( 34%)]  Loss: 3.40 (3.20)  Time: 0.161s,  795.81/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 555.403s\n",
      "Train: 13 [3500/10009 ( 35%)]  Loss: 3.09 (3.20)  Time: 0.162s,  792.04/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 563.448s\n",
      "Train: 13 [3550/10009 ( 35%)]  Loss: 3.19 (3.20)  Time: 0.161s,  793.01/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 571.490s\n",
      "Train: 13 [3600/10009 ( 36%)]  Loss: 3.28 (3.20)  Time: 0.161s,  795.29/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 579.534s\n",
      "Train: 13 [3650/10009 ( 36%)]  Loss: 3.37 (3.20)  Time: 0.161s,  797.24/s  (0.161s,  795.36/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 587.568s\n",
      "Train: 13 [3700/10009 ( 37%)]  Loss: 3.29 (3.20)  Time: 0.160s,  798.70/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 595.603s\n",
      "Train: 13 [3750/10009 ( 37%)]  Loss: 3.16 (3.20)  Time: 0.161s,  796.91/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 603.650s\n",
      "Train: 13 [3800/10009 ( 38%)]  Loss: 3.01 (3.20)  Time: 0.160s,  800.06/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 611.703s\n",
      "Train: 13 [3850/10009 ( 38%)]  Loss: 3.20 (3.20)  Time: 0.160s,  798.47/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 619.747s\n",
      "Train: 13 [3900/10009 ( 39%)]  Loss: 3.11 (3.20)  Time: 0.160s,  798.87/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 627.812s\n",
      "Train: 13 [3950/10009 ( 39%)]  Loss: 3.38 (3.20)  Time: 0.160s,  797.94/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 635.869s\n",
      "Train: 13 [4000/10009 ( 40%)]  Loss: 2.83 (3.20)  Time: 0.161s,  795.08/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 643.921s\n",
      "Train: 13 [4050/10009 ( 40%)]  Loss: 3.24 (3.20)  Time: 0.162s,  791.25/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 651.982s\n",
      "Train: 13 [4100/10009 ( 41%)]  Loss: 2.80 (3.20)  Time: 0.163s,  787.69/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 660.029s\n",
      "Train: 13 [4150/10009 ( 41%)]  Loss: 3.21 (3.20)  Time: 0.161s,  793.57/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 668.080s\n",
      "Train: 13 [4200/10009 ( 42%)]  Loss: 3.28 (3.20)  Time: 0.161s,  795.53/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 676.132s\n",
      "Train: 13 [4250/10009 ( 42%)]  Loss: 3.20 (3.20)  Time: 0.161s,  796.02/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 684.175s\n",
      "Train: 13 [4300/10009 ( 43%)]  Loss: 3.19 (3.20)  Time: 0.161s,  796.47/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 692.219s\n",
      "Train: 13 [4350/10009 ( 43%)]  Loss: 3.31 (3.20)  Time: 0.160s,  798.88/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 700.261s\n",
      "Train: 13 [4400/10009 ( 44%)]  Loss: 3.36 (3.20)  Time: 0.160s,  798.66/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 708.313s\n",
      "Train: 13 [4450/10009 ( 44%)]  Loss: 3.16 (3.20)  Time: 0.160s,  799.22/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 716.357s\n",
      "Train: 13 [4500/10009 ( 45%)]  Loss: 3.17 (3.20)  Time: 0.160s,  799.13/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 724.397s\n",
      "Train: 13 [4550/10009 ( 45%)]  Loss: 3.04 (3.20)  Time: 0.160s,  799.63/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 732.455s\n",
      "Train: 13 [4600/10009 ( 46%)]  Loss: 2.85 (3.20)  Time: 0.160s,  798.22/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 740.507s\n",
      "Train: 13 [4650/10009 ( 46%)]  Loss: 3.13 (3.20)  Time: 0.161s,  793.94/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 748.568s\n",
      "Train: 13 [4700/10009 ( 47%)]  Loss: 2.87 (3.20)  Time: 0.161s,  796.08/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 756.616s\n",
      "Train: 13 [4750/10009 ( 47%)]  Loss: 3.25 (3.20)  Time: 0.161s,  796.91/s  (0.161s,  795.28/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 764.666s\n",
      "Train: 13 [4800/10009 ( 48%)]  Loss: 3.08 (3.20)  Time: 0.160s,  798.14/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 772.694s\n",
      "Train: 13 [4850/10009 ( 48%)]  Loss: 2.97 (3.20)  Time: 0.161s,  796.84/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 780.741s\n",
      "Train: 13 [4900/10009 ( 49%)]  Loss: 2.98 (3.20)  Time: 0.160s,  797.84/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 788.790s\n",
      "Train: 13 [4950/10009 ( 49%)]  Loss: 3.37 (3.20)  Time: 0.161s,  797.20/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 796.833s\n",
      "Train: 13 [5000/10009 ( 50%)]  Loss: 3.12 (3.20)  Time: 0.161s,  796.45/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 804.878s\n",
      "Train: 13 [5050/10009 ( 50%)]  Loss: 3.31 (3.20)  Time: 0.161s,  795.50/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 812.936s\n",
      "Train: 13 [5100/10009 ( 51%)]  Loss: 3.24 (3.20)  Time: 0.162s,  790.48/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 820.981s\n",
      "Train: 13 [5150/10009 ( 51%)]  Loss: 3.36 (3.20)  Time: 0.161s,  795.55/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 829.033s\n",
      "Train: 13 [5200/10009 ( 52%)]  Loss: 3.25 (3.20)  Time: 0.161s,  796.95/s  (0.161s,  795.24/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 837.136s\n",
      "Train: 13 [5250/10009 ( 52%)]  Loss: 3.73 (3.20)  Time: 0.160s,  798.93/s  (0.161s,  795.24/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 845.184s\n",
      "Train: 13 [5300/10009 ( 53%)]  Loss: 3.02 (3.20)  Time: 0.161s,  796.78/s  (0.161s,  795.25/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 853.229s\n",
      "Train: 13 [5350/10009 ( 53%)]  Loss: 2.93 (3.20)  Time: 0.160s,  799.59/s  (0.161s,  795.25/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 861.274s\n",
      "Train: 13 [5400/10009 ( 54%)]  Loss: 3.15 (3.20)  Time: 0.160s,  798.90/s  (0.161s,  795.25/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 869.315s\n",
      "Train: 13 [5450/10009 ( 54%)]  Loss: 3.26 (3.20)  Time: 0.163s,  787.55/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 877.385s\n",
      "Train: 13 [5500/10009 ( 55%)]  Loss: 3.19 (3.20)  Time: 0.160s,  800.23/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 885.432s\n",
      "Train: 13 [5550/10009 ( 55%)]  Loss: 3.41 (3.20)  Time: 0.161s,  793.70/s  (0.161s,  795.24/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 893.478s\n",
      "Train: 13 [5600/10009 ( 56%)]  Loss: 3.42 (3.20)  Time: 0.161s,  794.29/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 901.533s\n",
      "Train: 13 [5650/10009 ( 56%)]  Loss: 2.74 (3.20)  Time: 0.160s,  801.16/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 909.580s\n",
      "Train: 13 [5700/10009 ( 57%)]  Loss: 3.21 (3.20)  Time: 0.161s,  796.34/s  (0.161s,  795.22/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 917.640s\n",
      "Train: 13 [5750/10009 ( 57%)]  Loss: 3.05 (3.20)  Time: 0.161s,  793.79/s  (0.161s,  795.18/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 925.738s\n",
      "Train: 13 [5800/10009 ( 58%)]  Loss: 3.05 (3.20)  Time: 0.161s,  796.42/s  (0.161s,  795.17/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 933.796s\n",
      "Train: 13 [5850/10009 ( 58%)]  Loss: 3.11 (3.20)  Time: 0.160s,  800.12/s  (0.161s,  795.17/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 941.841s\n",
      "Train: 13 [5900/10009 ( 59%)]  Loss: 3.19 (3.20)  Time: 0.160s,  798.77/s  (0.161s,  795.17/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 949.893s\n",
      "Train: 13 [5950/10009 ( 59%)]  Loss: 3.20 (3.20)  Time: 0.162s,  791.78/s  (0.161s,  795.15/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 957.968s\n",
      "Train: 13 [6000/10009 ( 60%)]  Loss: 3.05 (3.20)  Time: 0.161s,  795.99/s  (0.161s,  795.10/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 966.079s\n",
      "Train: 13 [6050/10009 ( 60%)]  Loss: 3.59 (3.20)  Time: 0.161s,  796.37/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 974.136s\n",
      "Train: 13 [6100/10009 ( 61%)]  Loss: 3.33 (3.20)  Time: 0.161s,  796.39/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 982.184s\n",
      "Train: 13 [6150/10009 ( 61%)]  Loss: 3.31 (3.20)  Time: 0.160s,  799.08/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 990.231s\n",
      "Train: 13 [6200/10009 ( 62%)]  Loss: 3.21 (3.20)  Time: 0.160s,  798.09/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 998.288s\n",
      "Train: 13 [6250/10009 ( 62%)]  Loss: 3.09 (3.20)  Time: 0.161s,  795.69/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1006.335s\n",
      "Train: 13 [6300/10009 ( 63%)]  Loss: 3.10 (3.20)  Time: 0.161s,  792.83/s  (0.161s,  795.08/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1014.392s\n",
      "Train: 13 [6350/10009 ( 63%)]  Loss: 3.02 (3.20)  Time: 0.161s,  796.87/s  (0.161s,  795.08/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1022.449s\n",
      "Train: 13 [6400/10009 ( 64%)]  Loss: 3.31 (3.20)  Time: 0.161s,  794.01/s  (0.161s,  795.07/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1030.512s\n",
      "Train: 13 [6450/10009 ( 64%)]  Loss: 3.12 (3.20)  Time: 0.161s,  796.62/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1038.605s\n",
      "Train: 13 [6500/10009 ( 65%)]  Loss: 2.99 (3.20)  Time: 0.161s,  796.54/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1046.655s\n",
      "Train: 13 [6550/10009 ( 65%)]  Loss: 3.45 (3.20)  Time: 0.162s,  790.81/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1054.707s\n",
      "Train: 13 [6600/10009 ( 66%)]  Loss: 2.87 (3.20)  Time: 0.162s,  789.40/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1062.763s\n",
      "Train: 13 [6650/10009 ( 66%)]  Loss: 3.35 (3.20)  Time: 0.160s,  798.04/s  (0.161s,  795.02/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1070.819s\n",
      "Train: 13 [6700/10009 ( 67%)]  Loss: 3.23 (3.20)  Time: 0.161s,  796.27/s  (0.161s,  795.01/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1078.887s\n",
      "Train: 13 [6750/10009 ( 67%)]  Loss: 3.06 (3.20)  Time: 0.162s,  788.46/s  (0.161s,  794.99/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1086.964s\n",
      "Train: 13 [6800/10009 ( 68%)]  Loss: 3.21 (3.20)  Time: 0.161s,  795.67/s  (0.161s,  794.97/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1095.037s\n",
      "Train: 13 [6850/10009 ( 68%)]  Loss: 3.05 (3.20)  Time: 0.161s,  797.40/s  (0.161s,  794.96/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1103.101s\n",
      "Train: 13 [6900/10009 ( 69%)]  Loss: 3.22 (3.20)  Time: 0.160s,  797.79/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1111.186s\n",
      "Train: 13 [6950/10009 ( 69%)]  Loss: 3.04 (3.20)  Time: 0.161s,  797.36/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1119.235s\n",
      "Train: 13 [7000/10009 ( 70%)]  Loss: 3.19 (3.20)  Time: 0.163s,  782.90/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.009 (0.006)Time: 1127.289s\n",
      "Train: 13 [7050/10009 ( 70%)]  Loss: 3.33 (3.20)  Time: 0.162s,  792.43/s  (0.161s,  794.91/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1135.387s\n",
      "Train: 13 [7100/10009 ( 71%)]  Loss: 3.10 (3.20)  Time: 0.160s,  798.51/s  (0.161s,  794.90/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1143.447s\n",
      "Train: 13 [7150/10009 ( 71%)]  Loss: 3.27 (3.20)  Time: 0.161s,  794.75/s  (0.161s,  794.89/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1151.509s\n",
      "Train: 13 [7200/10009 ( 72%)]  Loss: 3.48 (3.20)  Time: 0.160s,  798.78/s  (0.161s,  794.88/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1159.576s\n",
      "Train: 13 [7250/10009 ( 72%)]  Loss: 3.34 (3.20)  Time: 0.160s,  797.95/s  (0.161s,  794.88/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1167.625s\n",
      "Train: 13 [7300/10009 ( 73%)]  Loss: 2.98 (3.20)  Time: 0.162s,  789.76/s  (0.161s,  794.87/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1175.703s\n",
      "Train: 13 [7350/10009 ( 73%)]  Loss: 3.18 (3.20)  Time: 0.160s,  798.92/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1183.757s\n",
      "Train: 13 [7400/10009 ( 74%)]  Loss: 3.30 (3.20)  Time: 0.161s,  794.69/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1191.813s\n",
      "Train: 13 [7450/10009 ( 74%)]  Loss: 3.01 (3.20)  Time: 0.163s,  784.85/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 1199.868s\n",
      "Train: 13 [7500/10009 ( 75%)]  Loss: 2.87 (3.20)  Time: 0.162s,  791.44/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1207.925s\n",
      "Train: 13 [7550/10009 ( 75%)]  Loss: 3.15 (3.20)  Time: 0.162s,  788.39/s  (0.161s,  794.85/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1215.981s\n",
      "Train: 13 [7600/10009 ( 76%)]  Loss: 3.31 (3.20)  Time: 0.161s,  794.60/s  (0.161s,  794.85/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1224.040s\n",
      "Train: 13 [7650/10009 ( 76%)]  Loss: 3.15 (3.20)  Time: 0.161s,  794.60/s  (0.161s,  794.83/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1232.121s\n",
      "Train: 13 [7700/10009 ( 77%)]  Loss: 3.06 (3.20)  Time: 0.162s,  790.73/s  (0.161s,  794.80/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1240.215s\n",
      "Train: 13 [7750/10009 ( 77%)]  Loss: 2.99 (3.20)  Time: 0.160s,  799.08/s  (0.161s,  794.77/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1248.315s\n",
      "Train: 13 [7800/10009 ( 78%)]  Loss: 3.37 (3.20)  Time: 0.162s,  788.51/s  (0.161s,  794.75/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 1256.400s\n",
      "Train: 13 [7850/10009 ( 78%)]  Loss: 3.45 (3.20)  Time: 0.161s,  796.41/s  (0.161s,  794.74/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1264.467s\n",
      "Train: 13 [7900/10009 ( 79%)]  Loss: 2.94 (3.20)  Time: 0.160s,  797.65/s  (0.161s,  794.74/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1272.525s\n",
      "Train: 13 [7950/10009 ( 79%)]  Loss: 3.43 (3.20)  Time: 0.161s,  797.43/s  (0.161s,  794.73/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1280.594s\n",
      "Train: 13 [8000/10009 ( 80%)]  Loss: 3.19 (3.20)  Time: 0.161s,  793.89/s  (0.161s,  794.72/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1288.661s\n",
      "Train: 13 [8050/10009 ( 80%)]  Loss: 3.31 (3.20)  Time: 0.164s,  781.88/s  (0.161s,  794.71/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1296.735s\n",
      "Train: 13 [8100/10009 ( 81%)]  Loss: 3.13 (3.20)  Time: 0.162s,  791.80/s  (0.161s,  794.69/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1304.811s\n",
      "Train: 13 [8150/10009 ( 81%)]  Loss: 2.98 (3.20)  Time: 0.161s,  795.79/s  (0.161s,  794.69/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1312.877s\n",
      "Train: 13 [8200/10009 ( 82%)]  Loss: 3.13 (3.20)  Time: 0.162s,  791.95/s  (0.161s,  794.68/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1320.945s\n",
      "Train: 13 [8250/10009 ( 82%)]  Loss: 3.20 (3.20)  Time: 0.161s,  795.56/s  (0.161s,  794.66/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1329.030s\n",
      "Train: 13 [8300/10009 ( 83%)]  Loss: 3.24 (3.20)  Time: 0.161s,  796.42/s  (0.161s,  794.66/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1337.083s\n",
      "Train: 13 [8350/10009 ( 83%)]  Loss: 3.33 (3.20)  Time: 0.161s,  796.22/s  (0.161s,  794.64/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1345.162s\n",
      "Train: 13 [8400/10009 ( 84%)]  Loss: 3.03 (3.20)  Time: 0.160s,  799.08/s  (0.161s,  794.65/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1353.213s\n",
      "Train: 13 [8450/10009 ( 84%)]  Loss: 3.19 (3.20)  Time: 0.161s,  795.67/s  (0.161s,  794.64/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1361.279s\n",
      "Train: 13 [8500/10009 ( 85%)]  Loss: 3.18 (3.20)  Time: 0.162s,  792.51/s  (0.161s,  794.61/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1369.387s\n",
      "Train: 13 [8550/10009 ( 85%)]  Loss: 3.60 (3.20)  Time: 0.160s,  798.70/s  (0.161s,  794.61/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1377.439s\n",
      "Train: 13 [8600/10009 ( 86%)]  Loss: 3.15 (3.20)  Time: 0.161s,  795.57/s  (0.161s,  794.61/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1385.493s\n",
      "Train: 13 [8650/10009 ( 86%)]  Loss: 3.05 (3.20)  Time: 0.161s,  793.65/s  (0.161s,  794.60/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1393.555s\n",
      "Train: 13 [8700/10009 ( 87%)]  Loss: 3.24 (3.20)  Time: 0.161s,  796.35/s  (0.161s,  794.60/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1401.625s\n",
      "Train: 13 [8750/10009 ( 87%)]  Loss: 3.09 (3.20)  Time: 0.161s,  797.06/s  (0.161s,  794.59/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1409.697s\n",
      "Train: 13 [8800/10009 ( 88%)]  Loss: 3.18 (3.20)  Time: 0.161s,  793.87/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1417.761s\n",
      "Train: 13 [8850/10009 ( 88%)]  Loss: 3.36 (3.20)  Time: 0.160s,  799.46/s  (0.161s,  794.59/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1425.806s\n",
      "Train: 13 [8900/10009 ( 89%)]  Loss: 3.09 (3.20)  Time: 0.161s,  793.62/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1433.873s\n",
      "Train: 13 [8950/10009 ( 89%)]  Loss: 3.47 (3.20)  Time: 0.162s,  787.93/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1441.927s\n",
      "Train: 13 [9000/10009 ( 90%)]  Loss: 3.42 (3.20)  Time: 0.162s,  789.61/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1449.980s\n",
      "Train: 13 [9050/10009 ( 90%)]  Loss: 3.28 (3.20)  Time: 0.161s,  793.96/s  (0.161s,  794.57/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1458.061s\n",
      "Train: 13 [9100/10009 ( 91%)]  Loss: 3.15 (3.20)  Time: 0.161s,  796.27/s  (0.161s,  794.56/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1466.117s\n",
      "Train: 13 [9150/10009 ( 91%)]  Loss: 3.05 (3.20)  Time: 0.160s,  797.68/s  (0.161s,  794.55/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1474.204s\n",
      "Train: 13 [9200/10009 ( 92%)]  Loss: 3.16 (3.20)  Time: 0.165s,  773.55/s  (0.161s,  794.54/s)  LR: 2.161e-06  Data: 0.010 (0.006)Time: 1482.270s\n",
      "Train: 13 [9250/10009 ( 92%)]  Loss: 3.24 (3.20)  Time: 0.162s,  789.73/s  (0.161s,  794.54/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1490.326s\n",
      "Train: 13 [9300/10009 ( 93%)]  Loss: 3.03 (3.20)  Time: 0.161s,  796.95/s  (0.161s,  794.54/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1498.392s\n",
      "Train: 13 [9350/10009 ( 93%)]  Loss: 3.28 (3.20)  Time: 0.160s,  799.80/s  (0.161s,  794.51/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1506.496s\n",
      "Train: 13 [9400/10009 ( 94%)]  Loss: 3.47 (3.20)  Time: 0.161s,  797.06/s  (0.161s,  794.50/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1514.571s\n",
      "Train: 13 [9450/10009 ( 94%)]  Loss: 3.02 (3.20)  Time: 0.161s,  794.26/s  (0.161s,  794.50/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1522.632s\n",
      "Train: 13 [9500/10009 ( 95%)]  Loss: 3.32 (3.20)  Time: 0.161s,  797.26/s  (0.161s,  794.50/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1530.685s\n",
      "Train: 13 [9550/10009 ( 95%)]  Loss: 3.47 (3.20)  Time: 0.160s,  801.37/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1538.747s\n",
      "Train: 13 [9600/10009 ( 96%)]  Loss: 3.13 (3.20)  Time: 0.162s,  792.34/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1546.806s\n",
      "Train: 13 [9650/10009 ( 96%)]  Loss: 3.19 (3.20)  Time: 0.162s,  791.85/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1554.865s\n",
      "Train: 13 [9700/10009 ( 97%)]  Loss: 3.25 (3.20)  Time: 0.161s,  796.32/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1562.926s\n",
      "Train: 13 [9750/10009 ( 97%)]  Loss: 3.25 (3.20)  Time: 0.163s,  786.38/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 1570.977s\n",
      "Train: 13 [9800/10009 ( 98%)]  Loss: 3.30 (3.20)  Time: 0.162s,  790.56/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1579.029s\n",
      "Train: 13 [9850/10009 ( 98%)]  Loss: 3.26 (3.20)  Time: 0.160s,  799.48/s  (0.161s,  794.47/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1587.126s\n",
      "Train: 13 [9900/10009 ( 99%)]  Loss: 3.57 (3.20)  Time: 0.161s,  794.83/s  (0.161s,  794.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1595.177s\n",
      "Train: 13 [9950/10009 ( 99%)]  Loss: 3.30 (3.20)  Time: 0.160s,  800.29/s  (0.161s,  794.47/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1603.237s\n",
      "Train: 13 [10000/10009 (100%)]  Loss: 3.12 (3.20)  Time: 0.204s,  627.09/s  (0.161s,  794.44/s)  LR: 2.161e-06  Data: 0.051 (0.006)Time: 1611.360s\n",
      "Test: [   0/390]  Time: 0.675 (0.675)  Loss:   1.272 ( 1.272)  Acc@1:  73.438 ( 73.438)  Acc@5:  89.844 ( 89.844)\n",
      "Test: [  50/390]  Time: 0.051 (0.149)  Loss:   1.183 ( 1.967)  Acc@1:  74.219 ( 58.195)  Acc@5:  92.969 ( 79.228)\n",
      "Test: [ 100/390]  Time: 0.145 (0.144)  Loss:   2.003 ( 1.993)  Acc@1:  53.906 ( 54.749)  Acc@5:  83.594 ( 79.858)\n",
      "Test: [ 150/390]  Time: 0.051 (0.144)  Loss:   1.810 ( 1.965)  Acc@1:  56.250 ( 55.552)  Acc@5:  83.594 ( 80.262)\n",
      "Test: [ 200/390]  Time: 0.052 (0.143)  Loss:   3.043 ( 2.146)  Acc@1:  29.688 ( 52.363)  Acc@5:  66.406 ( 77.235)\n",
      "Test: [ 250/390]  Time: 0.053 (0.145)  Loss:   2.273 ( 2.260)  Acc@1:  58.594 ( 50.710)  Acc@5:  71.875 ( 75.271)\n",
      "Test: [ 300/390]  Time: 0.054 (0.144)  Loss:   2.626 ( 2.355)  Acc@1:  49.219 ( 49.084)  Acc@5:  67.969 ( 73.572)\n",
      "Test: [ 350/390]  Time: 0.216 (0.143)  Loss:   2.775 ( 2.429)  Acc@1:  43.750 ( 47.723)  Acc@5:  67.188 ( 72.291)\n",
      "Test: [ 390/390]  Time: 0.034 (0.144)  Loss:   3.664 ( 2.395)  Acc@1:  21.250 ( 48.326)  Acc@5:  58.750 ( 72.896)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 14 [   0/10009 (  0%)]  Loss: 3.08 (3.08)  Time: 0.741s,  172.72/s  (0.741s,  172.72/s)  LR: 5.463e-07  Data: 0.589 (0.589)Time: 0.742s\n",
      "Train: 14 [  50/10009 (  0%)]  Loss: 3.04 (3.19)  Time: 0.159s,  803.56/s  (0.175s,  731.00/s)  LR: 5.463e-07  Data: 0.006 (0.022)Time: 8.931s\n",
      "Train: 14 [ 100/10009 (  1%)]  Loss: 3.13 (3.19)  Time: 0.162s,  791.61/s  (0.168s,  763.21/s)  LR: 5.463e-07  Data: 0.008 (0.015)Time: 16.939s\n",
      "Train: 14 [ 150/10009 (  1%)]  Loss: 3.10 (3.18)  Time: 0.160s,  801.75/s  (0.165s,  774.25/s)  LR: 5.463e-07  Data: 0.006 (0.012)Time: 24.964s\n",
      "Train: 14 [ 200/10009 (  2%)]  Loss: 2.94 (3.18)  Time: 0.161s,  793.76/s  (0.164s,  779.44/s)  LR: 5.463e-07  Data: 0.006 (0.011)Time: 33.009s\n",
      "Train: 14 [ 250/10009 (  2%)]  Loss: 2.94 (3.18)  Time: 0.161s,  795.01/s  (0.163s,  783.10/s)  LR: 5.463e-07  Data: 0.006 (0.010)Time: 41.027s\n",
      "Train: 14 [ 300/10009 (  3%)]  Loss: 3.19 (3.18)  Time: 0.160s,  802.08/s  (0.163s,  785.44/s)  LR: 5.463e-07  Data: 0.005 (0.009)Time: 49.053s\n",
      "Train: 14 [ 350/10009 (  3%)]  Loss: 3.18 (3.18)  Time: 0.160s,  802.46/s  (0.163s,  787.09/s)  LR: 5.463e-07  Data: 0.006 (0.009)Time: 57.081s\n",
      "Train: 14 [ 400/10009 (  4%)]  Loss: 3.01 (3.18)  Time: 0.162s,  789.69/s  (0.162s,  788.17/s)  LR: 5.463e-07  Data: 0.007 (0.008)Time: 65.123s\n",
      "Train: 14 [ 450/10009 (  4%)]  Loss: 3.41 (3.18)  Time: 0.160s,  798.07/s  (0.162s,  788.67/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 73.197s\n",
      "Train: 14 [ 500/10009 (  5%)]  Loss: 3.10 (3.18)  Time: 0.160s,  802.07/s  (0.162s,  789.52/s)  LR: 5.463e-07  Data: 0.005 (0.008)Time: 81.224s\n",
      "Train: 14 [ 550/10009 (  5%)]  Loss: 2.89 (3.18)  Time: 0.161s,  795.52/s  (0.162s,  790.12/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 89.262s\n",
      "Train: 14 [ 600/10009 (  6%)]  Loss: 3.19 (3.18)  Time: 0.161s,  795.90/s  (0.162s,  790.55/s)  LR: 5.463e-07  Data: 0.007 (0.008)Time: 97.310s\n",
      "Train: 14 [ 650/10009 (  6%)]  Loss: 3.37 (3.18)  Time: 0.160s,  797.53/s  (0.162s,  790.97/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 105.349s\n",
      "Train: 14 [ 700/10009 (  7%)]  Loss: 3.49 (3.18)  Time: 0.160s,  799.04/s  (0.162s,  791.28/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 113.396s\n",
      "Train: 14 [ 750/10009 (  7%)]  Loss: 3.30 (3.18)  Time: 0.160s,  799.21/s  (0.162s,  791.64/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 121.429s\n",
      "Train: 14 [ 800/10009 (  8%)]  Loss: 3.08 (3.18)  Time: 0.160s,  799.02/s  (0.162s,  791.83/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 129.482s\n",
      "Train: 14 [ 850/10009 (  8%)]  Loss: 2.94 (3.17)  Time: 0.161s,  797.19/s  (0.162s,  792.07/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 137.523s\n",
      "Train: 14 [ 900/10009 (  9%)]  Loss: 3.26 (3.17)  Time: 0.160s,  798.85/s  (0.162s,  792.16/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 145.587s\n",
      "Train: 14 [ 950/10009 (  9%)]  Loss: 3.23 (3.17)  Time: 0.161s,  793.25/s  (0.162s,  792.27/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 153.645s\n",
      "Train: 14 [1000/10009 ( 10%)]  Loss: 3.22 (3.17)  Time: 0.166s,  768.95/s  (0.162s,  791.76/s)  LR: 5.463e-07  Data: 0.012 (0.007)Time: 161.826s\n",
      "Train: 14 [1050/10009 ( 10%)]  Loss: 3.08 (3.17)  Time: 0.161s,  793.66/s  (0.162s,  791.76/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 169.910s\n",
      "Train: 14 [1100/10009 ( 11%)]  Loss: 3.15 (3.17)  Time: 0.162s,  788.26/s  (0.162s,  791.86/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 177.972s\n",
      "Train: 14 [1150/10009 ( 11%)]  Loss: 3.06 (3.17)  Time: 0.160s,  797.75/s  (0.162s,  791.90/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 186.045s\n",
      "Train: 14 [1200/10009 ( 12%)]  Loss: 2.91 (3.17)  Time: 0.161s,  797.46/s  (0.162s,  792.03/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 194.093s\n",
      "Train: 14 [1250/10009 ( 12%)]  Loss: 3.33 (3.17)  Time: 0.160s,  799.98/s  (0.162s,  791.75/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 202.245s\n",
      "Train: 14 [1300/10009 ( 13%)]  Loss: 3.27 (3.17)  Time: 0.161s,  795.00/s  (0.162s,  791.81/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 210.312s\n",
      "Train: 14 [1350/10009 ( 13%)]  Loss: 3.33 (3.17)  Time: 0.160s,  798.55/s  (0.162s,  791.66/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 218.437s\n",
      "Train: 14 [1400/10009 ( 14%)]  Loss: 3.29 (3.17)  Time: 0.160s,  799.45/s  (0.162s,  791.80/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 226.480s\n",
      "Train: 14 [1450/10009 ( 14%)]  Loss: 3.21 (3.17)  Time: 0.160s,  799.06/s  (0.162s,  791.92/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 234.529s\n",
      "Train: 14 [1500/10009 ( 15%)]  Loss: 3.06 (3.17)  Time: 0.161s,  793.18/s  (0.162s,  791.94/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 242.604s\n",
      "Train: 14 [1550/10009 ( 15%)]  Loss: 3.07 (3.17)  Time: 0.160s,  798.48/s  (0.162s,  791.99/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 250.669s\n",
      "Train: 14 [1600/10009 ( 16%)]  Loss: 3.16 (3.17)  Time: 0.160s,  797.69/s  (0.162s,  792.07/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 258.724s\n",
      "Train: 14 [1650/10009 ( 16%)]  Loss: 3.07 (3.17)  Time: 0.161s,  796.09/s  (0.162s,  792.16/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 266.774s\n",
      "Train: 14 [1700/10009 ( 17%)]  Loss: 3.11 (3.17)  Time: 0.161s,  794.83/s  (0.162s,  792.26/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 274.817s\n",
      "Train: 14 [1750/10009 ( 17%)]  Loss: 3.07 (3.17)  Time: 0.160s,  797.56/s  (0.162s,  792.21/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 282.915s\n",
      "Train: 14 [1800/10009 ( 18%)]  Loss: 3.36 (3.17)  Time: 0.161s,  794.30/s  (0.162s,  792.29/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 290.963s\n",
      "Train: 14 [1850/10009 ( 18%)]  Loss: 3.12 (3.17)  Time: 0.160s,  798.22/s  (0.162s,  792.20/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 299.077s\n",
      "Train: 14 [1900/10009 ( 19%)]  Loss: 3.01 (3.17)  Time: 0.161s,  793.58/s  (0.162s,  792.25/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 307.134s\n",
      "Train: 14 [1950/10009 ( 19%)]  Loss: 3.32 (3.17)  Time: 0.161s,  796.75/s  (0.162s,  792.18/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 315.242s\n",
      "Train: 14 [2000/10009 ( 20%)]  Loss: 3.26 (3.17)  Time: 0.162s,  788.13/s  (0.162s,  792.11/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 323.349s\n",
      "Train: 14 [2050/10009 ( 20%)]  Loss: 2.88 (3.17)  Time: 0.160s,  798.36/s  (0.162s,  792.08/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 331.440s\n",
      "Train: 14 [2100/10009 ( 21%)]  Loss: 3.15 (3.17)  Time: 0.160s,  800.35/s  (0.162s,  792.14/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 339.496s\n",
      "Train: 14 [2150/10009 ( 21%)]  Loss: 3.13 (3.17)  Time: 0.161s,  797.42/s  (0.162s,  792.20/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 347.549s\n",
      "Train: 14 [2200/10009 ( 22%)]  Loss: 3.48 (3.17)  Time: 0.160s,  798.52/s  (0.162s,  792.28/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 355.590s\n",
      "Train: 14 [2250/10009 ( 22%)]  Loss: 3.23 (3.17)  Time: 0.161s,  793.99/s  (0.162s,  792.36/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 363.632s\n",
      "Train: 14 [2300/10009 ( 23%)]  Loss: 2.88 (3.17)  Time: 0.163s,  783.65/s  (0.162s,  792.30/s)  LR: 5.463e-07  Data: 0.009 (0.007)Time: 371.736s\n",
      "Train: 14 [2350/10009 ( 23%)]  Loss: 2.99 (3.17)  Time: 0.161s,  793.50/s  (0.162s,  792.21/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 379.857s\n",
      "Train: 14 [2400/10009 ( 24%)]  Loss: 3.30 (3.17)  Time: 0.163s,  787.42/s  (0.162s,  792.25/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 387.919s\n",
      "Train: 14 [2450/10009 ( 24%)]  Loss: 3.49 (3.17)  Time: 0.161s,  792.64/s  (0.162s,  792.32/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 395.962s\n",
      "Train: 14 [2500/10009 ( 25%)]  Loss: 3.26 (3.17)  Time: 0.162s,  789.17/s  (0.162s,  792.37/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 404.013s\n",
      "Train: 14 [2550/10009 ( 25%)]  Loss: 3.07 (3.17)  Time: 0.164s,  781.92/s  (0.162s,  792.37/s)  LR: 5.463e-07  Data: 0.009 (0.007)Time: 412.090s\n",
      "Train: 14 [2600/10009 ( 26%)]  Loss: 3.12 (3.17)  Time: 0.161s,  796.27/s  (0.162s,  792.32/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 420.193s\n",
      "Train: 14 [2650/10009 ( 26%)]  Loss: 3.09 (3.17)  Time: 0.161s,  796.82/s  (0.162s,  792.33/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 428.264s\n",
      "Train: 14 [2700/10009 ( 27%)]  Loss: 3.15 (3.17)  Time: 0.161s,  797.16/s  (0.162s,  792.36/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 436.325s\n",
      "Train: 14 [2750/10009 ( 27%)]  Loss: 3.00 (3.17)  Time: 0.160s,  797.73/s  (0.162s,  792.39/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 444.385s\n",
      "Train: 14 [2800/10009 ( 28%)]  Loss: 3.31 (3.17)  Time: 0.163s,  783.24/s  (0.162s,  792.41/s)  LR: 5.463e-07  Data: 0.009 (0.007)Time: 452.451s\n",
      "Train: 14 [2850/10009 ( 28%)]  Loss: 2.91 (3.17)  Time: 0.162s,  788.35/s  (0.162s,  792.42/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 460.523s\n",
      "Train: 14 [2900/10009 ( 29%)]  Loss: 3.24 (3.17)  Time: 0.161s,  796.65/s  (0.162s,  792.45/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 468.582s\n",
      "Train: 14 [2950/10009 ( 29%)]  Loss: 2.86 (3.17)  Time: 0.161s,  793.93/s  (0.162s,  792.50/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 476.630s\n",
      "Train: 14 [3000/10009 ( 30%)]  Loss: 3.10 (3.17)  Time: 0.160s,  802.37/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 484.677s\n",
      "Train: 14 [3050/10009 ( 30%)]  Loss: 3.58 (3.17)  Time: 0.161s,  797.42/s  (0.162s,  792.57/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 492.738s\n",
      "Train: 14 [3100/10009 ( 31%)]  Loss: 3.19 (3.17)  Time: 0.161s,  795.69/s  (0.162s,  792.55/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 500.824s\n",
      "Train: 14 [3150/10009 ( 31%)]  Loss: 2.82 (3.17)  Time: 0.161s,  795.16/s  (0.161s,  792.59/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 508.875s\n",
      "Train: 14 [3200/10009 ( 32%)]  Loss: 3.21 (3.17)  Time: 0.164s,  779.83/s  (0.162s,  792.56/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 516.964s\n",
      "Train: 14 [3250/10009 ( 32%)]  Loss: 3.10 (3.17)  Time: 0.160s,  797.85/s  (0.161s,  792.60/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 525.017s\n",
      "Train: 14 [3300/10009 ( 33%)]  Loss: 3.13 (3.17)  Time: 0.161s,  796.09/s  (0.162s,  792.52/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 533.142s\n",
      "Train: 14 [3350/10009 ( 33%)]  Loss: 3.26 (3.17)  Time: 0.161s,  796.09/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 541.208s\n",
      "Train: 14 [3400/10009 ( 34%)]  Loss: 3.06 (3.17)  Time: 0.162s,  790.27/s  (0.162s,  792.50/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 549.311s\n",
      "Train: 14 [3450/10009 ( 34%)]  Loss: 2.94 (3.17)  Time: 0.160s,  798.63/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 557.360s\n",
      "Train: 14 [3500/10009 ( 35%)]  Loss: 3.20 (3.17)  Time: 0.162s,  787.85/s  (0.161s,  792.57/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 565.408s\n",
      "Train: 14 [3550/10009 ( 35%)]  Loss: 3.24 (3.17)  Time: 0.162s,  789.97/s  (0.162s,  792.51/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 573.527s\n",
      "Train: 14 [3600/10009 ( 36%)]  Loss: 2.96 (3.17)  Time: 0.160s,  798.77/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 581.586s\n",
      "Train: 14 [3650/10009 ( 36%)]  Loss: 3.27 (3.17)  Time: 0.160s,  800.41/s  (0.161s,  792.58/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 589.627s\n",
      "Train: 14 [3700/10009 ( 37%)]  Loss: 3.18 (3.17)  Time: 0.161s,  794.70/s  (0.161s,  792.59/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 597.695s\n",
      "Train: 14 [3750/10009 ( 37%)]  Loss: 2.98 (3.17)  Time: 0.161s,  795.10/s  (0.161s,  792.60/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 605.765s\n",
      "Train: 14 [3800/10009 ( 38%)]  Loss: 3.04 (3.17)  Time: 0.160s,  798.15/s  (0.161s,  792.62/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 613.820s\n",
      "Train: 14 [3850/10009 ( 38%)]  Loss: 3.12 (3.17)  Time: 0.160s,  798.20/s  (0.161s,  792.65/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 621.868s\n",
      "Train: 14 [3900/10009 ( 39%)]  Loss: 3.24 (3.17)  Time: 0.162s,  789.99/s  (0.161s,  792.66/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 629.937s\n",
      "Train: 14 [3950/10009 ( 39%)]  Loss: 2.96 (3.17)  Time: 0.162s,  789.22/s  (0.161s,  792.69/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 637.989s\n",
      "Train: 14 [4000/10009 ( 40%)]  Loss: 3.35 (3.17)  Time: 0.161s,  795.64/s  (0.161s,  792.73/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 646.031s\n",
      "Train: 14 [4050/10009 ( 40%)]  Loss: 3.07 (3.17)  Time: 0.161s,  793.43/s  (0.161s,  792.76/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 654.075s\n",
      "Train: 14 [4100/10009 ( 41%)]  Loss: 3.29 (3.17)  Time: 0.161s,  795.85/s  (0.161s,  792.80/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 662.120s\n",
      "Train: 14 [4150/10009 ( 41%)]  Loss: 3.06 (3.17)  Time: 0.161s,  797.42/s  (0.161s,  792.83/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 670.167s\n",
      "Train: 14 [4200/10009 ( 42%)]  Loss: 3.12 (3.17)  Time: 0.160s,  799.49/s  (0.161s,  792.84/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 678.227s\n",
      "Train: 14 [4250/10009 ( 42%)]  Loss: 2.95 (3.17)  Time: 0.161s,  792.96/s  (0.161s,  792.87/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 686.272s\n",
      "Train: 14 [4300/10009 ( 43%)]  Loss: 3.04 (3.17)  Time: 0.161s,  794.02/s  (0.161s,  792.90/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 694.319s\n",
      "Train: 14 [4350/10009 ( 43%)]  Loss: 3.24 (3.17)  Time: 0.160s,  798.65/s  (0.161s,  792.93/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 702.367s\n",
      "Train: 14 [4400/10009 ( 44%)]  Loss: 3.14 (3.17)  Time: 0.162s,  788.86/s  (0.161s,  792.92/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 710.442s\n",
      "Train: 14 [4450/10009 ( 44%)]  Loss: 3.40 (3.17)  Time: 0.162s,  787.83/s  (0.161s,  792.93/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 718.511s\n",
      "Train: 14 [4500/10009 ( 45%)]  Loss: 3.03 (3.17)  Time: 0.160s,  799.45/s  (0.161s,  792.94/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 726.571s\n",
      "Train: 14 [4550/10009 ( 45%)]  Loss: 3.51 (3.17)  Time: 0.161s,  796.82/s  (0.161s,  792.96/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 734.622s\n",
      "Train: 14 [4600/10009 ( 46%)]  Loss: 3.34 (3.17)  Time: 0.162s,  792.38/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 742.676s\n",
      "Train: 14 [4650/10009 ( 46%)]  Loss: 3.17 (3.17)  Time: 0.162s,  792.33/s  (0.161s,  792.94/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 750.785s\n",
      "Train: 14 [4700/10009 ( 47%)]  Loss: 2.92 (3.17)  Time: 0.161s,  796.06/s  (0.161s,  792.96/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 758.836s\n",
      "Train: 14 [4750/10009 ( 47%)]  Loss: 2.92 (3.17)  Time: 0.162s,  791.82/s  (0.161s,  792.99/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 766.882s\n",
      "Train: 14 [4800/10009 ( 48%)]  Loss: 3.00 (3.17)  Time: 0.162s,  791.55/s  (0.161s,  792.99/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 774.952s\n",
      "Train: 14 [4850/10009 ( 48%)]  Loss: 3.29 (3.17)  Time: 0.161s,  793.57/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 783.001s\n",
      "Train: 14 [4900/10009 ( 49%)]  Loss: 3.38 (3.17)  Time: 0.160s,  799.39/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 791.097s\n",
      "Train: 14 [4950/10009 ( 49%)]  Loss: 3.12 (3.17)  Time: 0.162s,  788.36/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 799.144s\n",
      "Train: 14 [5000/10009 ( 50%)]  Loss: 3.21 (3.17)  Time: 0.161s,  793.55/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 807.241s\n",
      "Train: 14 [5050/10009 ( 50%)]  Loss: 3.30 (3.17)  Time: 0.163s,  786.82/s  (0.161s,  792.95/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 815.339s\n",
      "Train: 14 [5100/10009 ( 51%)]  Loss: 3.16 (3.17)  Time: 0.162s,  791.24/s  (0.161s,  792.96/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 823.402s\n",
      "Train: 14 [5150/10009 ( 51%)]  Loss: 3.36 (3.17)  Time: 0.160s,  799.56/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 831.449s\n",
      "Train: 14 [5200/10009 ( 52%)]  Loss: 3.31 (3.17)  Time: 0.162s,  791.13/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 839.496s\n",
      "Train: 14 [5250/10009 ( 52%)]  Loss: 3.03 (3.17)  Time: 0.161s,  794.51/s  (0.161s,  793.02/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 847.556s\n",
      "Train: 14 [5300/10009 ( 53%)]  Loss: 3.17 (3.17)  Time: 0.163s,  787.21/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 855.635s\n",
      "Train: 14 [5350/10009 ( 53%)]  Loss: 3.43 (3.17)  Time: 0.162s,  790.92/s  (0.161s,  793.00/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 863.714s\n",
      "Train: 14 [5400/10009 ( 54%)]  Loss: 3.00 (3.17)  Time: 0.160s,  800.36/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 871.777s\n",
      "Train: 14 [5450/10009 ( 54%)]  Loss: 3.00 (3.17)  Time: 0.161s,  797.49/s  (0.161s,  793.03/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 879.824s\n",
      "Train: 14 [5500/10009 ( 55%)]  Loss: 3.21 (3.17)  Time: 0.164s,  781.56/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 887.916s\n",
      "Train: 14 [5550/10009 ( 55%)]  Loss: 3.25 (3.17)  Time: 0.161s,  794.39/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 895.990s\n",
      "Train: 14 [5600/10009 ( 56%)]  Loss: 3.17 (3.17)  Time: 0.161s,  796.30/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 904.054s\n",
      "Train: 14 [5650/10009 ( 56%)]  Loss: 3.23 (3.17)  Time: 0.161s,  796.22/s  (0.161s,  793.02/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 912.111s\n",
      "Train: 14 [5700/10009 ( 57%)]  Loss: 3.02 (3.17)  Time: 0.161s,  797.44/s  (0.161s,  793.03/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 920.170s\n",
      "Train: 14 [5750/10009 ( 57%)]  Loss: 2.98 (3.17)  Time: 0.161s,  795.44/s  (0.161s,  793.04/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 928.229s\n",
      "Train: 14 [5800/10009 ( 58%)]  Loss: 3.45 (3.17)  Time: 0.161s,  796.57/s  (0.161s,  793.05/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 936.286s\n",
      "Train: 14 [5850/10009 ( 58%)]  Loss: 3.33 (3.17)  Time: 0.161s,  794.03/s  (0.161s,  793.07/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 944.342s\n",
      "Train: 14 [5900/10009 ( 59%)]  Loss: 3.23 (3.17)  Time: 0.160s,  798.29/s  (0.161s,  793.06/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 952.421s\n",
      "Train: 14 [5950/10009 ( 59%)]  Loss: 3.19 (3.17)  Time: 0.162s,  792.09/s  (0.161s,  793.08/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 960.465s\n",
      "Train: 14 [6000/10009 ( 60%)]  Loss: 3.60 (3.17)  Time: 0.161s,  792.94/s  (0.161s,  793.06/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 968.561s\n",
      "Train: 14 [6050/10009 ( 60%)]  Loss: 3.21 (3.17)  Time: 0.162s,  788.72/s  (0.161s,  793.08/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 976.609s\n",
      "Train: 14 [6100/10009 ( 61%)]  Loss: 2.89 (3.17)  Time: 0.160s,  797.56/s  (0.161s,  793.10/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 984.651s\n",
      "Train: 14 [6150/10009 ( 61%)]  Loss: 3.40 (3.17)  Time: 0.162s,  789.79/s  (0.161s,  793.11/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 992.711s\n",
      "Train: 14 [6200/10009 ( 62%)]  Loss: 3.62 (3.17)  Time: 0.160s,  798.12/s  (0.161s,  793.13/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1000.751s\n",
      "Train: 14 [6250/10009 ( 62%)]  Loss: 3.30 (3.17)  Time: 0.161s,  796.20/s  (0.161s,  793.12/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1008.838s\n",
      "Train: 14 [6300/10009 ( 63%)]  Loss: 3.33 (3.17)  Time: 0.162s,  792.28/s  (0.161s,  793.13/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1016.890s\n",
      "Train: 14 [6350/10009 ( 63%)]  Loss: 3.16 (3.17)  Time: 0.161s,  794.86/s  (0.161s,  793.14/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1024.946s\n",
      "Train: 14 [6400/10009 ( 64%)]  Loss: 3.12 (3.17)  Time: 0.163s,  786.76/s  (0.161s,  793.15/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1033.006s\n",
      "Train: 14 [6450/10009 ( 64%)]  Loss: 3.13 (3.17)  Time: 0.161s,  792.80/s  (0.161s,  793.16/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1041.061s\n",
      "Train: 14 [6500/10009 ( 65%)]  Loss: 3.20 (3.17)  Time: 0.160s,  800.04/s  (0.161s,  793.17/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1049.108s\n",
      "Train: 14 [6550/10009 ( 65%)]  Loss: 3.15 (3.17)  Time: 0.160s,  799.46/s  (0.161s,  793.19/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1057.154s\n",
      "Train: 14 [6600/10009 ( 66%)]  Loss: 3.21 (3.17)  Time: 0.162s,  790.89/s  (0.161s,  793.21/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1065.194s\n",
      "Train: 14 [6650/10009 ( 66%)]  Loss: 3.24 (3.17)  Time: 0.160s,  799.98/s  (0.161s,  793.22/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1073.252s\n",
      "Train: 14 [6700/10009 ( 67%)]  Loss: 3.07 (3.17)  Time: 0.160s,  798.08/s  (0.161s,  793.23/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1081.301s\n",
      "Train: 14 [6750/10009 ( 67%)]  Loss: 2.81 (3.17)  Time: 0.160s,  797.66/s  (0.161s,  793.25/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1089.350s\n",
      "Train: 14 [6800/10009 ( 68%)]  Loss: 3.17 (3.17)  Time: 0.160s,  798.89/s  (0.161s,  793.26/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1097.401s\n",
      "Train: 14 [6850/10009 ( 68%)]  Loss: 2.96 (3.17)  Time: 0.161s,  797.44/s  (0.161s,  793.27/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1105.454s\n",
      "Train: 14 [6900/10009 ( 69%)]  Loss: 3.26 (3.17)  Time: 0.162s,  789.70/s  (0.161s,  793.25/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1113.552s\n",
      "Train: 14 [6950/10009 ( 69%)]  Loss: 3.41 (3.17)  Time: 0.162s,  791.42/s  (0.161s,  793.24/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1121.629s\n",
      "Train: 14 [7000/10009 ( 70%)]  Loss: 3.37 (3.17)  Time: 0.160s,  800.74/s  (0.161s,  793.26/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1129.674s\n",
      "Train: 14 [7050/10009 ( 70%)]  Loss: 3.29 (3.17)  Time: 0.160s,  800.65/s  (0.161s,  793.28/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1137.716s\n",
      "Train: 14 [7100/10009 ( 71%)]  Loss: 3.32 (3.17)  Time: 0.161s,  796.28/s  (0.161s,  793.29/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1145.766s\n",
      "Train: 14 [7150/10009 ( 71%)]  Loss: 3.10 (3.17)  Time: 0.160s,  798.67/s  (0.161s,  793.30/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1153.813s\n",
      "Train: 14 [7200/10009 ( 72%)]  Loss: 2.95 (3.17)  Time: 0.160s,  799.58/s  (0.161s,  793.32/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1161.860s\n",
      "Train: 14 [7250/10009 ( 72%)]  Loss: 3.21 (3.17)  Time: 0.161s,  793.49/s  (0.161s,  793.33/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1169.910s\n",
      "Train: 14 [7300/10009 ( 73%)]  Loss: 3.49 (3.17)  Time: 0.161s,  795.07/s  (0.161s,  793.34/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1177.966s\n",
      "Train: 14 [7350/10009 ( 73%)]  Loss: 3.32 (3.17)  Time: 0.162s,  789.27/s  (0.161s,  793.36/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1186.002s\n",
      "Train: 14 [7400/10009 ( 74%)]  Loss: 3.08 (3.17)  Time: 0.161s,  794.39/s  (0.161s,  793.36/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1194.070s\n",
      "Train: 14 [7450/10009 ( 74%)]  Loss: 3.13 (3.17)  Time: 0.161s,  795.83/s  (0.161s,  793.37/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1202.125s\n",
      "Train: 14 [7500/10009 ( 75%)]  Loss: 3.20 (3.17)  Time: 0.161s,  796.17/s  (0.161s,  793.37/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1210.190s\n",
      "Train: 14 [7550/10009 ( 75%)]  Loss: 3.16 (3.17)  Time: 0.162s,  792.45/s  (0.161s,  793.38/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1218.240s\n",
      "Train: 14 [7600/10009 ( 76%)]  Loss: 3.36 (3.17)  Time: 0.160s,  797.64/s  (0.161s,  793.38/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1226.302s\n",
      "Train: 14 [7650/10009 ( 76%)]  Loss: 3.23 (3.17)  Time: 0.160s,  797.55/s  (0.161s,  793.39/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1234.354s\n",
      "Train: 14 [7700/10009 ( 77%)]  Loss: 3.40 (3.17)  Time: 0.160s,  798.55/s  (0.161s,  793.40/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1242.408s\n",
      "Train: 14 [7750/10009 ( 77%)]  Loss: 3.35 (3.17)  Time: 0.160s,  799.62/s  (0.161s,  793.42/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1250.448s\n",
      "Train: 14 [7800/10009 ( 78%)]  Loss: 3.46 (3.17)  Time: 0.160s,  799.00/s  (0.161s,  793.43/s)  LR: 5.463e-07  Data: 0.005 (0.006)Time: 1258.494s\n",
      "Train: 14 [7850/10009 ( 78%)]  Loss: 3.19 (3.17)  Time: 0.161s,  794.72/s  (0.161s,  793.44/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1266.545s\n",
      "Train: 14 [7900/10009 ( 79%)]  Loss: 3.04 (3.17)  Time: 0.160s,  798.56/s  (0.161s,  793.45/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1274.589s\n",
      "Train: 14 [7950/10009 ( 79%)]  Loss: 3.36 (3.17)  Time: 0.161s,  795.75/s  (0.161s,  793.46/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1282.642s\n",
      "Train: 14 [8000/10009 ( 80%)]  Loss: 3.35 (3.17)  Time: 0.160s,  800.21/s  (0.161s,  793.47/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1290.690s\n",
      "Train: 14 [8050/10009 ( 80%)]  Loss: 3.09 (3.17)  Time: 0.160s,  799.62/s  (0.161s,  793.48/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1298.746s\n",
      "Train: 14 [8100/10009 ( 81%)]  Loss: 3.22 (3.17)  Time: 0.162s,  790.52/s  (0.161s,  793.49/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1306.791s\n",
      "Train: 14 [8150/10009 ( 81%)]  Loss: 3.03 (3.17)  Time: 0.160s,  797.81/s  (0.161s,  793.50/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1314.833s\n",
      "Train: 14 [8200/10009 ( 82%)]  Loss: 3.14 (3.17)  Time: 0.161s,  796.06/s  (0.161s,  793.52/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1322.876s\n",
      "Train: 14 [8250/10009 ( 82%)]  Loss: 3.32 (3.17)  Time: 0.162s,  791.95/s  (0.161s,  793.53/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1330.926s\n",
      "Train: 14 [8300/10009 ( 83%)]  Loss: 3.09 (3.17)  Time: 0.162s,  792.01/s  (0.161s,  793.54/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1338.974s\n",
      "Train: 14 [8350/10009 ( 83%)]  Loss: 2.79 (3.17)  Time: 0.162s,  790.51/s  (0.161s,  793.54/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1347.027s\n",
      "Train: 14 [8400/10009 ( 84%)]  Loss: 3.36 (3.17)  Time: 0.160s,  800.31/s  (0.161s,  793.55/s)  LR: 5.463e-07  Data: 0.005 (0.006)Time: 1355.080s\n",
      "Train: 14 [8450/10009 ( 84%)]  Loss: 2.94 (3.17)  Time: 0.161s,  794.84/s  (0.161s,  793.56/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1363.134s\n",
      "Train: 14 [8500/10009 ( 85%)]  Loss: 3.07 (3.17)  Time: 0.161s,  793.94/s  (0.161s,  793.57/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1371.178s\n",
      "Train: 14 [8550/10009 ( 85%)]  Loss: 3.43 (3.17)  Time: 0.161s,  796.56/s  (0.161s,  793.58/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1379.227s\n",
      "Train: 14 [8600/10009 ( 86%)]  Loss: 3.17 (3.17)  Time: 0.161s,  794.24/s  (0.161s,  793.59/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1387.270s\n",
      "Train: 14 [8650/10009 ( 86%)]  Loss: 3.31 (3.17)  Time: 0.161s,  793.35/s  (0.161s,  793.59/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1395.329s\n",
      "Train: 14 [8700/10009 ( 87%)]  Loss: 3.44 (3.17)  Time: 0.161s,  792.60/s  (0.161s,  793.60/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1403.386s\n",
      "Train: 14 [8750/10009 ( 87%)]  Loss: 3.35 (3.17)  Time: 0.161s,  792.95/s  (0.161s,  793.61/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1411.433s\n",
      "Train: 14 [8800/10009 ( 88%)]  Loss: 3.47 (3.17)  Time: 0.162s,  791.18/s  (0.161s,  793.62/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1419.480s\n",
      "Train: 14 [8850/10009 ( 88%)]  Loss: 3.15 (3.17)  Time: 0.160s,  798.38/s  (0.161s,  793.61/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1427.560s\n",
      "Train: 14 [8900/10009 ( 89%)]  Loss: 3.15 (3.17)  Time: 0.162s,  791.99/s  (0.161s,  793.62/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1435.612s\n",
      "Train: 14 [8950/10009 ( 89%)]  Loss: 3.27 (3.17)  Time: 0.161s,  796.14/s  (0.161s,  793.62/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1443.668s\n",
      "Train: 14 [9000/10009 ( 90%)]  Loss: 3.03 (3.17)  Time: 0.160s,  799.70/s  (0.161s,  793.63/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1451.720s\n",
      "Train: 14 [9050/10009 ( 90%)]  Loss: 3.15 (3.17)  Time: 0.161s,  796.17/s  (0.161s,  793.63/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1459.781s\n",
      "Train: 14 [9100/10009 ( 91%)]  Loss: 3.25 (3.17)  Time: 0.160s,  799.12/s  (0.161s,  793.64/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1467.821s\n",
      "Train: 14 [9150/10009 ( 91%)]  Loss: 3.02 (3.17)  Time: 0.162s,  790.73/s  (0.161s,  793.65/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1475.866s\n",
      "Train: 14 [9200/10009 ( 92%)]  Loss: 3.08 (3.17)  Time: 0.162s,  790.84/s  (0.161s,  793.66/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1483.910s\n",
      "Train: 14 [9250/10009 ( 92%)]  Loss: 2.98 (3.17)  Time: 0.162s,  791.74/s  (0.161s,  793.67/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1491.967s\n",
      "Train: 14 [9300/10009 ( 93%)]  Loss: 3.09 (3.17)  Time: 0.161s,  794.34/s  (0.161s,  793.66/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1500.052s\n",
      "Train: 14 [9350/10009 ( 93%)]  Loss: 3.13 (3.17)  Time: 0.160s,  797.59/s  (0.161s,  793.67/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1508.092s\n",
      "Train: 14 [9400/10009 ( 94%)]  Loss: 3.26 (3.17)  Time: 0.161s,  795.61/s  (0.161s,  793.68/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1516.141s\n",
      "Train: 14 [9450/10009 ( 94%)]  Loss: 3.32 (3.17)  Time: 0.161s,  796.95/s  (0.161s,  793.68/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1524.194s\n",
      "Train: 14 [9500/10009 ( 95%)]  Loss: 3.16 (3.17)  Time: 0.160s,  798.85/s  (0.161s,  793.69/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1532.242s\n",
      "Train: 14 [9550/10009 ( 95%)]  Loss: 3.31 (3.17)  Time: 0.161s,  795.32/s  (0.161s,  793.69/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1540.305s\n",
      "Train: 14 [9600/10009 ( 96%)]  Loss: 3.17 (3.17)  Time: 0.161s,  794.09/s  (0.161s,  793.70/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1548.352s\n",
      "Train: 14 [9650/10009 ( 96%)]  Loss: 3.21 (3.17)  Time: 0.160s,  801.51/s  (0.161s,  793.71/s)  LR: 5.463e-07  Data: 0.005 (0.006)Time: 1556.400s\n",
      "Train: 14 [9700/10009 ( 97%)]  Loss: 3.30 (3.17)  Time: 0.162s,  790.44/s  (0.161s,  793.72/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1564.446s\n",
      "Train: 14 [9750/10009 ( 97%)]  Loss: 3.09 (3.17)  Time: 0.161s,  797.03/s  (0.161s,  793.72/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1572.496s\n",
      "Train: 14 [9800/10009 ( 98%)]  Loss: 3.28 (3.17)  Time: 0.161s,  793.67/s  (0.161s,  793.73/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1580.543s\n",
      "Train: 14 [9850/10009 ( 98%)]  Loss: 3.37 (3.17)  Time: 0.161s,  794.81/s  (0.161s,  793.74/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1588.587s\n",
      "Train: 14 [9900/10009 ( 99%)]  Loss: 3.02 (3.17)  Time: 0.161s,  796.07/s  (0.161s,  793.75/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1596.634s\n",
      "Train: 14 [9950/10009 ( 99%)]  Loss: 3.30 (3.17)  Time: 0.161s,  797.13/s  (0.161s,  793.76/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1604.677s\n",
      "Train: 14 [10000/10009 (100%)]  Loss: 3.24 (3.17)  Time: 0.203s,  630.36/s  (0.161s,  793.74/s)  LR: 5.463e-07  Data: 0.050 (0.006)Time: 1612.767s\n",
      "Test: [   0/390]  Time: 0.669 (0.669)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.140)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.236 (0.133)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.052 (0.136)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.053 (0.133)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.052 (0.133)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.195 (0.131)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.272 (0.130)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.034 (0.130)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "*** Best metric: 48.84 (epoch 14)\n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        for epoch in range(start_epoch, 15):\n",
    "            if hasattr(dataset_train, 'set_epoch'):\n",
    "                dataset_train.set_epoch(epoch)\n",
    "            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n",
    "                loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                train_loss_fn,\n",
    "                args,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                saver=saver,\n",
    "                output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast,\n",
    "                loss_scaler=loss_scaler,\n",
    "                model_ema=model_ema,\n",
    "                mixup_fn=mixup_fn,\n",
    "                # fish: add preconditioner\n",
    "                preconditioner=preconditioner,\n",
    "            )\n",
    "\n",
    "            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                if utils.is_primary(args):\n",
    "                    _logger.info(\"Distributing BatchNorm running means and vars\")\n",
    "                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "            eval_metrics = validate(\n",
    "                model,\n",
    "                loader_eval,\n",
    "                validate_loss_fn,\n",
    "                args,\n",
    "                amp_autocast=amp_autocast,\n",
    "            )\n",
    "\n",
    "            if model_ema is not None and not args.model_ema_force_cpu:\n",
    "                if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                    utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "                ema_eval_metrics = validate(\n",
    "                    model_ema.module,\n",
    "                    loader_eval,\n",
    "                    validate_loss_fn,\n",
    "                    args,\n",
    "                    amp_autocast=amp_autocast,\n",
    "                    log_suffix=' (EMA)',\n",
    "                )\n",
    "                eval_metrics = ema_eval_metrics\n",
    "\n",
    "            if output_dir is not None:\n",
    "                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "                utils.update_summary(\n",
    "                    epoch,\n",
    "                    train_metrics,\n",
    "                    eval_metrics,\n",
    "                    filename=os.path.join(output_dir, 'summary.csv'),\n",
    "                    lr=sum(lrs) / len(lrs),\n",
    "                    write_header=best_metric is None,\n",
    "                    log_wandb=args.log_wandb and has_wandb,\n",
    "                )\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small   1310s\n",
    "\n",
    "mid     1310s\n",
    "\n",
    "large   1650s\n",
    "\n",
    "跑小模型时显卡未达到瓶颈，利用率不高，因此对于small和mid大小可以适当放大batchsize。按时间比较，22min:27.5min=1:1.25，因此可以再训练2个epoch。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 15 [   0/10009 (  0%)]  Loss: 3.18 (3.18)  Time: 0.931s,  137.45/s  (0.931s,  137.45/s)  LR: 0.000e+00  Data: 0.740 (0.740)Time: 0.932s\n",
      "Train: 15 [  50/10009 (  0%)]  Loss: 2.97 (3.14)  Time: 0.168s,  761.74/s  (0.182s,  702.40/s)  LR: 0.000e+00  Data: 0.006 (0.022)Time: 9.295s\n",
      "Train: 15 [ 100/10009 (  1%)]  Loss: 2.83 (3.15)  Time: 0.166s,  769.20/s  (0.174s,  734.08/s)  LR: 0.000e+00  Data: 0.007 (0.014)Time: 17.612s\n",
      "Train: 15 [ 150/10009 (  1%)]  Loss: 3.56 (3.17)  Time: 0.168s,  763.81/s  (0.172s,  742.46/s)  LR: 0.000e+00  Data: 0.006 (0.011)Time: 26.033s\n",
      "Train: 15 [ 200/10009 (  2%)]  Loss: 3.18 (3.16)  Time: 0.169s,  758.29/s  (0.171s,  746.94/s)  LR: 0.000e+00  Data: 0.007 (0.010)Time: 34.445s\n",
      "Train: 15 [ 250/10009 (  2%)]  Loss: 3.25 (3.16)  Time: 0.169s,  756.61/s  (0.171s,  748.17/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 42.943s\n",
      "Train: 15 [ 300/10009 (  3%)]  Loss: 3.22 (3.16)  Time: 0.170s,  752.03/s  (0.171s,  748.26/s)  LR: 0.000e+00  Data: 0.008 (0.009)Time: 51.491s\n",
      "Train: 15 [ 350/10009 (  3%)]  Loss: 3.27 (3.16)  Time: 0.173s,  741.88/s  (0.171s,  747.93/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 60.071s\n",
      "Train: 15 [ 400/10009 (  4%)]  Loss: 3.19 (3.16)  Time: 0.172s,  746.26/s  (0.171s,  747.32/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 68.684s\n",
      "Train: 15 [ 450/10009 (  4%)]  Loss: 3.04 (3.16)  Time: 0.173s,  741.43/s  (0.172s,  746.23/s)  LR: 0.000e+00  Data: 0.008 (0.008)Time: 77.361s\n",
      "Train: 15 [ 500/10009 (  5%)]  Loss: 3.31 (3.17)  Time: 0.171s,  746.93/s  (0.172s,  746.12/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 85.950s\n",
      "Train: 15 [ 550/10009 (  5%)]  Loss: 2.99 (3.17)  Time: 0.173s,  741.60/s  (0.172s,  745.99/s)  LR: 0.000e+00  Data: 0.008 (0.008)Time: 94.544s\n",
      "Train: 15 [ 600/10009 (  6%)]  Loss: 3.29 (3.17)  Time: 0.172s,  743.87/s  (0.172s,  745.99/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 103.123s\n",
      "Train: 15 [ 650/10009 (  6%)]  Loss: 3.07 (3.17)  Time: 0.170s,  753.83/s  (0.172s,  746.19/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 111.671s\n",
      "Train: 15 [ 700/10009 (  7%)]  Loss: 3.22 (3.17)  Time: 0.170s,  752.34/s  (0.171s,  746.50/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 120.199s\n",
      "Train: 15 [ 750/10009 (  7%)]  Loss: 3.26 (3.17)  Time: 0.173s,  740.27/s  (0.171s,  746.36/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 128.796s\n",
      "Train: 15 [ 800/10009 (  8%)]  Loss: 2.98 (3.17)  Time: 0.170s,  753.44/s  (0.172s,  746.20/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 137.400s\n",
      "Train: 15 [ 850/10009 (  8%)]  Loss: 3.07 (3.17)  Time: 0.173s,  739.32/s  (0.172s,  746.30/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 145.958s\n",
      "Train: 15 [ 900/10009 (  9%)]  Loss: 3.19 (3.16)  Time: 0.172s,  745.90/s  (0.172s,  745.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 154.623s\n",
      "Train: 15 [ 950/10009 (  9%)]  Loss: 2.71 (3.16)  Time: 0.172s,  745.31/s  (0.172s,  745.79/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 163.221s\n",
      "Train: 15 [1000/10009 ( 10%)]  Loss: 3.30 (3.16)  Time: 0.173s,  741.22/s  (0.172s,  745.85/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 171.788s\n",
      "Train: 15 [1050/10009 ( 10%)]  Loss: 3.49 (3.16)  Time: 0.169s,  758.38/s  (0.172s,  745.76/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 180.390s\n",
      "Train: 15 [1100/10009 ( 11%)]  Loss: 3.33 (3.16)  Time: 0.172s,  744.99/s  (0.172s,  745.71/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 188.984s\n",
      "Train: 15 [1150/10009 ( 11%)]  Loss: 3.14 (3.16)  Time: 0.171s,  747.57/s  (0.172s,  745.81/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 197.542s\n",
      "Train: 15 [1200/10009 ( 12%)]  Loss: 3.11 (3.16)  Time: 0.172s,  742.45/s  (0.172s,  745.55/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 206.193s\n",
      "Train: 15 [1250/10009 ( 12%)]  Loss: 3.17 (3.16)  Time: 0.174s,  736.71/s  (0.172s,  745.16/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 214.891s\n",
      "Train: 15 [1300/10009 ( 13%)]  Loss: 3.39 (3.16)  Time: 0.169s,  757.89/s  (0.172s,  745.26/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 223.451s\n",
      "Train: 15 [1350/10009 ( 13%)]  Loss: 3.31 (3.16)  Time: 0.172s,  743.96/s  (0.172s,  744.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 232.142s\n",
      "Train: 15 [1400/10009 ( 14%)]  Loss: 2.76 (3.16)  Time: 0.173s,  738.81/s  (0.172s,  744.60/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 240.837s\n",
      "Train: 15 [1450/10009 ( 14%)]  Loss: 3.16 (3.16)  Time: 0.171s,  750.04/s  (0.172s,  744.74/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 249.388s\n",
      "Train: 15 [1500/10009 ( 15%)]  Loss: 2.89 (3.16)  Time: 0.173s,  738.96/s  (0.172s,  744.84/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 257.947s\n",
      "Train: 15 [1550/10009 ( 15%)]  Loss: 2.98 (3.16)  Time: 0.170s,  754.59/s  (0.172s,  744.98/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 266.487s\n",
      "Train: 15 [1600/10009 ( 16%)]  Loss: 2.97 (3.16)  Time: 0.170s,  754.52/s  (0.172s,  745.08/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 275.043s\n",
      "Train: 15 [1650/10009 ( 16%)]  Loss: 3.13 (3.16)  Time: 0.169s,  756.25/s  (0.172s,  745.17/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 283.599s\n",
      "Train: 15 [1700/10009 ( 17%)]  Loss: 3.00 (3.16)  Time: 0.173s,  739.99/s  (0.172s,  745.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 292.235s\n",
      "Train: 15 [1750/10009 ( 17%)]  Loss: 3.14 (3.16)  Time: 0.173s,  738.30/s  (0.172s,  744.86/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 300.898s\n",
      "Train: 15 [1800/10009 ( 18%)]  Loss: 3.32 (3.16)  Time: 0.172s,  743.14/s  (0.172s,  744.58/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 309.610s\n",
      "Train: 15 [1850/10009 ( 18%)]  Loss: 3.12 (3.16)  Time: 0.173s,  741.73/s  (0.172s,  744.28/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 318.332s\n",
      "Train: 15 [1900/10009 ( 19%)]  Loss: 3.13 (3.16)  Time: 0.175s,  733.26/s  (0.172s,  744.03/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 327.041s\n",
      "Train: 15 [1950/10009 ( 19%)]  Loss: 2.91 (3.16)  Time: 0.175s,  731.15/s  (0.172s,  743.71/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 335.787s\n",
      "Train: 15 [2000/10009 ( 20%)]  Loss: 3.23 (3.16)  Time: 0.172s,  743.17/s  (0.172s,  743.54/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 344.472s\n",
      "Train: 15 [2050/10009 ( 20%)]  Loss: 2.91 (3.16)  Time: 0.170s,  751.69/s  (0.172s,  743.59/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 353.055s\n",
      "Train: 15 [2100/10009 ( 21%)]  Loss: 3.19 (3.16)  Time: 0.173s,  739.91/s  (0.172s,  743.45/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 361.732s\n",
      "Train: 15 [2150/10009 ( 21%)]  Loss: 3.15 (3.16)  Time: 0.171s,  750.09/s  (0.172s,  743.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 370.359s\n",
      "Train: 15 [2200/10009 ( 22%)]  Loss: 3.05 (3.16)  Time: 0.170s,  754.15/s  (0.172s,  743.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 378.911s\n",
      "Train: 15 [2250/10009 ( 22%)]  Loss: 3.04 (3.16)  Time: 0.174s,  736.10/s  (0.172s,  743.50/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 387.527s\n",
      "Train: 15 [2300/10009 ( 23%)]  Loss: 3.14 (3.16)  Time: 0.180s,  710.77/s  (0.172s,  743.29/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 396.249s\n",
      "Train: 15 [2350/10009 ( 23%)]  Loss: 3.01 (3.16)  Time: 0.174s,  734.40/s  (0.172s,  743.08/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 404.975s\n",
      "Train: 15 [2400/10009 ( 24%)]  Loss: 3.28 (3.16)  Time: 0.173s,  741.41/s  (0.172s,  743.06/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 413.599s\n",
      "Train: 15 [2450/10009 ( 24%)]  Loss: 3.53 (3.16)  Time: 0.171s,  747.17/s  (0.172s,  743.14/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 422.163s\n",
      "Train: 15 [2500/10009 ( 25%)]  Loss: 3.38 (3.16)  Time: 0.172s,  745.31/s  (0.172s,  743.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 430.777s\n",
      "Train: 15 [2550/10009 ( 25%)]  Loss: 3.07 (3.16)  Time: 0.170s,  752.62/s  (0.172s,  743.21/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 439.350s\n",
      "Train: 15 [2600/10009 ( 26%)]  Loss: 3.13 (3.16)  Time: 0.172s,  743.65/s  (0.172s,  743.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 447.899s\n",
      "Train: 15 [2650/10009 ( 26%)]  Loss: 3.27 (3.16)  Time: 0.170s,  754.08/s  (0.172s,  743.41/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 456.450s\n",
      "Train: 15 [2700/10009 ( 27%)]  Loss: 3.33 (3.16)  Time: 0.171s,  746.36/s  (0.172s,  743.45/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 465.030s\n",
      "Train: 15 [2750/10009 ( 27%)]  Loss: 3.20 (3.16)  Time: 0.172s,  745.75/s  (0.172s,  743.55/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 473.577s\n",
      "Train: 15 [2800/10009 ( 28%)]  Loss: 3.19 (3.16)  Time: 0.172s,  742.65/s  (0.172s,  743.43/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 482.264s\n",
      "Train: 15 [2850/10009 ( 28%)]  Loss: 3.16 (3.16)  Time: 0.172s,  742.13/s  (0.172s,  743.34/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 490.931s\n",
      "Train: 15 [2900/10009 ( 29%)]  Loss: 3.28 (3.16)  Time: 0.172s,  744.44/s  (0.172s,  743.26/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 499.592s\n",
      "Train: 15 [2950/10009 ( 29%)]  Loss: 3.05 (3.16)  Time: 0.173s,  738.22/s  (0.172s,  743.14/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 508.283s\n",
      "Train: 15 [3000/10009 ( 30%)]  Loss: 3.15 (3.16)  Time: 0.172s,  744.61/s  (0.172s,  742.97/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 517.014s\n",
      "Train: 15 [3050/10009 ( 30%)]  Loss: 3.39 (3.16)  Time: 0.173s,  738.92/s  (0.172s,  742.89/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 525.690s\n",
      "Train: 15 [3100/10009 ( 31%)]  Loss: 2.91 (3.16)  Time: 0.172s,  743.17/s  (0.172s,  742.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 534.355s\n",
      "Train: 15 [3150/10009 ( 31%)]  Loss: 3.30 (3.16)  Time: 0.174s,  735.71/s  (0.172s,  742.76/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 543.009s\n",
      "Train: 15 [3200/10009 ( 32%)]  Loss: 3.18 (3.16)  Time: 0.172s,  742.54/s  (0.172s,  742.73/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 551.652s\n",
      "Train: 15 [3250/10009 ( 32%)]  Loss: 3.22 (3.16)  Time: 0.172s,  742.15/s  (0.172s,  742.71/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 560.284s\n",
      "Train: 15 [3300/10009 ( 33%)]  Loss: 3.07 (3.16)  Time: 0.174s,  736.49/s  (0.172s,  742.70/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 568.908s\n",
      "Train: 15 [3350/10009 ( 33%)]  Loss: 3.01 (3.16)  Time: 0.173s,  741.89/s  (0.172s,  742.68/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 577.543s\n",
      "Train: 15 [3400/10009 ( 34%)]  Loss: 3.00 (3.16)  Time: 0.175s,  732.67/s  (0.172s,  742.56/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 586.256s\n",
      "Train: 15 [3450/10009 ( 34%)]  Loss: 3.11 (3.16)  Time: 0.171s,  746.66/s  (0.172s,  742.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 594.907s\n",
      "Train: 15 [3500/10009 ( 35%)]  Loss: 3.13 (3.16)  Time: 0.174s,  734.42/s  (0.172s,  742.40/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 603.623s\n",
      "Train: 15 [3550/10009 ( 35%)]  Loss: 2.96 (3.16)  Time: 0.172s,  742.92/s  (0.172s,  742.32/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 612.309s\n",
      "Train: 15 [3600/10009 ( 36%)]  Loss: 3.29 (3.16)  Time: 0.173s,  740.36/s  (0.172s,  742.28/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 620.962s\n",
      "Train: 15 [3650/10009 ( 36%)]  Loss: 3.27 (3.16)  Time: 0.174s,  736.26/s  (0.172s,  742.15/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 629.694s\n",
      "Train: 15 [3700/10009 ( 37%)]  Loss: 3.22 (3.16)  Time: 0.175s,  731.91/s  (0.173s,  742.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 638.433s\n",
      "Train: 15 [3750/10009 ( 37%)]  Loss: 3.50 (3.16)  Time: 0.171s,  747.08/s  (0.173s,  741.89/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 647.165s\n",
      "Train: 15 [3800/10009 ( 38%)]  Loss: 3.04 (3.16)  Time: 0.173s,  740.63/s  (0.173s,  741.86/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 655.825s\n",
      "Train: 15 [3850/10009 ( 38%)]  Loss: 2.99 (3.16)  Time: 0.173s,  740.08/s  (0.173s,  741.81/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 664.495s\n",
      "Train: 15 [3900/10009 ( 39%)]  Loss: 2.94 (3.16)  Time: 0.172s,  744.72/s  (0.173s,  741.79/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 673.135s\n",
      "Train: 15 [3950/10009 ( 39%)]  Loss: 3.17 (3.16)  Time: 0.173s,  741.38/s  (0.173s,  741.76/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 681.795s\n",
      "Train: 15 [4000/10009 ( 40%)]  Loss: 3.05 (3.16)  Time: 0.175s,  732.85/s  (0.173s,  741.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 690.428s\n",
      "Train: 15 [4050/10009 ( 40%)]  Loss: 2.97 (3.16)  Time: 0.172s,  742.60/s  (0.173s,  741.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 699.045s\n",
      "Train: 15 [4100/10009 ( 41%)]  Loss: 2.95 (3.16)  Time: 0.175s,  730.14/s  (0.173s,  741.76/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 707.673s\n",
      "Train: 15 [4150/10009 ( 41%)]  Loss: 3.23 (3.16)  Time: 0.174s,  736.26/s  (0.173s,  741.70/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 716.361s\n",
      "Train: 15 [4200/10009 ( 42%)]  Loss: 3.29 (3.16)  Time: 0.171s,  749.74/s  (0.173s,  741.62/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 725.071s\n",
      "Train: 15 [4250/10009 ( 42%)]  Loss: 3.38 (3.16)  Time: 0.171s,  749.79/s  (0.173s,  741.63/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 733.695s\n",
      "Train: 15 [4300/10009 ( 43%)]  Loss: 2.80 (3.16)  Time: 0.171s,  746.95/s  (0.173s,  741.62/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 742.328s\n",
      "Train: 15 [4350/10009 ( 43%)]  Loss: 3.09 (3.16)  Time: 0.172s,  742.38/s  (0.173s,  741.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 750.912s\n",
      "Train: 15 [4400/10009 ( 44%)]  Loss: 3.16 (3.16)  Time: 0.172s,  742.50/s  (0.173s,  741.66/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 759.548s\n",
      "Train: 15 [4450/10009 ( 44%)]  Loss: 3.35 (3.16)  Time: 0.172s,  744.93/s  (0.173s,  741.69/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 768.143s\n",
      "Train: 15 [4500/10009 ( 45%)]  Loss: 2.94 (3.16)  Time: 0.173s,  738.73/s  (0.173s,  741.71/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 776.752s\n",
      "Train: 15 [4550/10009 ( 45%)]  Loss: 3.35 (3.16)  Time: 0.173s,  739.37/s  (0.173s,  741.74/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 785.351s\n",
      "Train: 15 [4600/10009 ( 46%)]  Loss: 3.19 (3.16)  Time: 0.172s,  745.12/s  (0.173s,  741.78/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 793.941s\n",
      "Train: 15 [4650/10009 ( 46%)]  Loss: 3.29 (3.16)  Time: 0.171s,  746.79/s  (0.173s,  741.81/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 802.531s\n",
      "Train: 15 [4700/10009 ( 47%)]  Loss: 2.91 (3.16)  Time: 0.175s,  733.36/s  (0.173s,  741.73/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 811.244s\n",
      "Train: 15 [4750/10009 ( 47%)]  Loss: 2.81 (3.16)  Time: 0.175s,  731.92/s  (0.173s,  741.61/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 820.009s\n",
      "Train: 15 [4800/10009 ( 48%)]  Loss: 3.51 (3.16)  Time: 0.175s,  731.90/s  (0.173s,  741.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 828.753s\n",
      "Train: 15 [4850/10009 ( 48%)]  Loss: 3.23 (3.16)  Time: 0.174s,  735.67/s  (0.173s,  741.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 837.499s\n",
      "Train: 15 [4900/10009 ( 49%)]  Loss: 3.48 (3.16)  Time: 0.174s,  735.32/s  (0.173s,  741.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 846.239s\n",
      "Train: 15 [4950/10009 ( 49%)]  Loss: 3.58 (3.16)  Time: 0.175s,  732.48/s  (0.173s,  741.23/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 854.969s\n",
      "Train: 15 [5000/10009 ( 50%)]  Loss: 3.00 (3.16)  Time: 0.173s,  740.20/s  (0.173s,  741.16/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 863.685s\n",
      "Train: 15 [5050/10009 ( 50%)]  Loss: 3.17 (3.16)  Time: 0.176s,  728.84/s  (0.173s,  741.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 872.420s\n",
      "Train: 15 [5100/10009 ( 51%)]  Loss: 3.26 (3.16)  Time: 0.174s,  734.28/s  (0.173s,  741.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 881.120s\n",
      "Train: 15 [5150/10009 ( 51%)]  Loss: 3.01 (3.16)  Time: 0.174s,  735.71/s  (0.173s,  740.93/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 889.869s\n",
      "Train: 15 [5200/10009 ( 52%)]  Loss: 3.24 (3.16)  Time: 0.173s,  738.68/s  (0.173s,  740.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 898.537s\n",
      "Train: 15 [5250/10009 ( 52%)]  Loss: 3.12 (3.16)  Time: 0.174s,  737.65/s  (0.173s,  740.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 907.213s\n",
      "Train: 15 [5300/10009 ( 53%)]  Loss: 3.53 (3.16)  Time: 0.174s,  737.47/s  (0.173s,  740.83/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 915.896s\n",
      "Train: 15 [5350/10009 ( 53%)]  Loss: 3.11 (3.16)  Time: 0.173s,  738.18/s  (0.173s,  740.82/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 924.555s\n",
      "Train: 15 [5400/10009 ( 54%)]  Loss: 2.98 (3.16)  Time: 0.171s,  747.26/s  (0.173s,  740.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 933.192s\n",
      "Train: 15 [5450/10009 ( 54%)]  Loss: 3.17 (3.16)  Time: 0.173s,  738.02/s  (0.173s,  740.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 941.914s\n",
      "Train: 15 [5500/10009 ( 55%)]  Loss: 3.15 (3.16)  Time: 0.176s,  728.36/s  (0.173s,  740.66/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 950.671s\n",
      "Train: 15 [5550/10009 ( 55%)]  Loss: 3.01 (3.16)  Time: 0.160s,  801.53/s  (0.173s,  740.81/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 959.127s\n",
      "Train: 15 [5600/10009 ( 56%)]  Loss: 3.20 (3.16)  Time: 0.161s,  794.90/s  (0.173s,  741.26/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 967.167s\n",
      "Train: 15 [5650/10009 ( 56%)]  Loss: 2.89 (3.16)  Time: 0.160s,  799.23/s  (0.173s,  741.70/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 975.221s\n",
      "Train: 15 [5700/10009 ( 57%)]  Loss: 3.29 (3.16)  Time: 0.160s,  802.29/s  (0.172s,  742.15/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 983.265s\n",
      "Train: 15 [5750/10009 ( 57%)]  Loss: 3.16 (3.16)  Time: 0.160s,  799.57/s  (0.172s,  742.59/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 991.302s\n",
      "Train: 15 [5800/10009 ( 58%)]  Loss: 3.43 (3.16)  Time: 0.161s,  794.05/s  (0.172s,  743.02/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 999.342s\n",
      "Train: 15 [5850/10009 ( 58%)]  Loss: 2.90 (3.16)  Time: 0.161s,  796.76/s  (0.172s,  743.43/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1007.392s\n",
      "Train: 15 [5900/10009 ( 59%)]  Loss: 3.48 (3.16)  Time: 0.161s,  796.76/s  (0.172s,  743.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1015.431s\n",
      "Train: 15 [5950/10009 ( 59%)]  Loss: 3.18 (3.16)  Time: 0.162s,  790.85/s  (0.172s,  744.25/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1023.482s\n",
      "Train: 15 [6000/10009 ( 60%)]  Loss: 3.29 (3.16)  Time: 0.161s,  796.36/s  (0.172s,  744.65/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1031.527s\n",
      "Train: 15 [6050/10009 ( 60%)]  Loss: 2.85 (3.16)  Time: 0.161s,  793.73/s  (0.172s,  745.03/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1039.594s\n",
      "Train: 15 [6100/10009 ( 61%)]  Loss: 3.05 (3.16)  Time: 0.160s,  799.51/s  (0.172s,  745.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1047.633s\n",
      "Train: 15 [6150/10009 ( 61%)]  Loss: 2.79 (3.16)  Time: 0.162s,  790.21/s  (0.172s,  745.81/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1055.659s\n",
      "Train: 15 [6200/10009 ( 62%)]  Loss: 3.20 (3.16)  Time: 0.162s,  789.85/s  (0.172s,  746.18/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1063.715s\n",
      "Train: 15 [6250/10009 ( 62%)]  Loss: 3.22 (3.16)  Time: 0.161s,  796.85/s  (0.172s,  746.22/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1072.232s\n",
      "Train: 15 [6300/10009 ( 63%)]  Loss: 2.88 (3.16)  Time: 0.160s,  800.44/s  (0.171s,  746.57/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1080.304s\n",
      "Train: 15 [6350/10009 ( 63%)]  Loss: 3.37 (3.16)  Time: 0.166s,  768.84/s  (0.171s,  746.86/s)  LR: 0.000e+00  Data: 0.011 (0.007)Time: 1088.453s\n",
      "Train: 15 [6400/10009 ( 64%)]  Loss: 3.26 (3.16)  Time: 0.161s,  794.53/s  (0.171s,  747.23/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1096.488s\n",
      "Train: 15 [6450/10009 ( 64%)]  Loss: 3.33 (3.16)  Time: 0.162s,  791.36/s  (0.171s,  747.58/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1104.539s\n",
      "Train: 15 [6500/10009 ( 65%)]  Loss: 2.91 (3.16)  Time: 0.160s,  799.23/s  (0.171s,  747.93/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1112.577s\n",
      "Train: 15 [6550/10009 ( 65%)]  Loss: 2.78 (3.16)  Time: 0.161s,  797.31/s  (0.171s,  748.27/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1120.614s\n",
      "Train: 15 [6600/10009 ( 66%)]  Loss: 3.17 (3.16)  Time: 0.161s,  793.09/s  (0.171s,  748.61/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1128.658s\n",
      "Train: 15 [6650/10009 ( 66%)]  Loss: 3.26 (3.16)  Time: 0.162s,  792.02/s  (0.171s,  748.94/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1136.710s\n",
      "Train: 15 [6700/10009 ( 67%)]  Loss: 2.95 (3.16)  Time: 0.161s,  795.63/s  (0.171s,  749.26/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1144.762s\n",
      "Train: 15 [6750/10009 ( 67%)]  Loss: 3.14 (3.16)  Time: 0.163s,  786.56/s  (0.171s,  749.58/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1152.810s\n",
      "Train: 15 [6800/10009 ( 68%)]  Loss: 2.94 (3.16)  Time: 0.160s,  798.71/s  (0.171s,  749.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1160.850s\n",
      "Train: 15 [6850/10009 ( 68%)]  Loss: 3.30 (3.16)  Time: 0.160s,  798.70/s  (0.171s,  750.21/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1168.902s\n",
      "Train: 15 [6900/10009 ( 69%)]  Loss: 3.30 (3.16)  Time: 0.161s,  796.87/s  (0.171s,  750.53/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1176.943s\n",
      "Train: 15 [6950/10009 ( 69%)]  Loss: 3.29 (3.16)  Time: 0.160s,  799.33/s  (0.170s,  750.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1185.006s\n",
      "Train: 15 [7000/10009 ( 70%)]  Loss: 3.04 (3.16)  Time: 0.160s,  801.30/s  (0.170s,  751.13/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1193.039s\n",
      "Train: 15 [7050/10009 ( 70%)]  Loss: 3.31 (3.16)  Time: 0.160s,  800.36/s  (0.170s,  751.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1201.089s\n",
      "Train: 15 [7100/10009 ( 71%)]  Loss: 3.11 (3.16)  Time: 0.164s,  782.29/s  (0.170s,  751.71/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 1209.137s\n",
      "Train: 15 [7150/10009 ( 71%)]  Loss: 3.12 (3.16)  Time: 0.161s,  794.00/s  (0.170s,  752.00/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1217.188s\n",
      "Train: 15 [7200/10009 ( 72%)]  Loss: 3.48 (3.16)  Time: 0.161s,  795.85/s  (0.170s,  752.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1225.219s\n",
      "Train: 15 [7250/10009 ( 72%)]  Loss: 3.16 (3.16)  Time: 0.160s,  799.16/s  (0.170s,  752.58/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1233.253s\n",
      "Train: 15 [7300/10009 ( 73%)]  Loss: 3.29 (3.16)  Time: 0.160s,  798.58/s  (0.170s,  752.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1241.292s\n",
      "Train: 15 [7350/10009 ( 73%)]  Loss: 3.13 (3.16)  Time: 0.161s,  796.70/s  (0.170s,  753.15/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1249.327s\n",
      "Train: 15 [7400/10009 ( 74%)]  Loss: 2.98 (3.16)  Time: 0.166s,  772.68/s  (0.170s,  753.41/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1257.386s\n",
      "Train: 15 [7450/10009 ( 74%)]  Loss: 3.24 (3.16)  Time: 0.161s,  797.50/s  (0.170s,  753.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1265.436s\n",
      "Train: 15 [7500/10009 ( 75%)]  Loss: 2.86 (3.16)  Time: 0.160s,  800.20/s  (0.170s,  753.89/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1273.569s\n",
      "Train: 15 [7550/10009 ( 75%)]  Loss: 3.29 (3.16)  Time: 0.160s,  798.72/s  (0.170s,  754.16/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1281.599s\n",
      "Train: 15 [7600/10009 ( 76%)]  Loss: 3.18 (3.16)  Time: 0.160s,  799.63/s  (0.170s,  754.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1289.636s\n",
      "Train: 15 [7650/10009 ( 76%)]  Loss: 3.31 (3.16)  Time: 0.161s,  794.09/s  (0.170s,  754.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1297.694s\n",
      "Train: 15 [7700/10009 ( 77%)]  Loss: 3.07 (3.16)  Time: 0.162s,  792.42/s  (0.170s,  754.93/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1305.721s\n",
      "Train: 15 [7750/10009 ( 77%)]  Loss: 3.24 (3.16)  Time: 0.161s,  794.14/s  (0.169s,  755.18/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1313.762s\n",
      "Train: 15 [7800/10009 ( 78%)]  Loss: 3.10 (3.16)  Time: 0.161s,  794.65/s  (0.169s,  755.43/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1321.792s\n",
      "Train: 15 [7850/10009 ( 78%)]  Loss: 3.06 (3.16)  Time: 0.160s,  799.46/s  (0.169s,  755.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1329.853s\n",
      "Train: 15 [7900/10009 ( 79%)]  Loss: 3.13 (3.16)  Time: 0.161s,  796.60/s  (0.169s,  755.91/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1337.884s\n",
      "Train: 15 [7950/10009 ( 79%)]  Loss: 3.04 (3.16)  Time: 0.161s,  794.68/s  (0.169s,  756.16/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1345.914s\n",
      "Train: 15 [8000/10009 ( 80%)]  Loss: 3.02 (3.16)  Time: 0.161s,  792.86/s  (0.169s,  756.40/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1353.950s\n",
      "Train: 15 [8050/10009 ( 80%)]  Loss: 3.03 (3.16)  Time: 0.161s,  792.94/s  (0.169s,  756.63/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1361.998s\n",
      "Train: 15 [8100/10009 ( 81%)]  Loss: 3.01 (3.16)  Time: 0.160s,  797.93/s  (0.169s,  756.86/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1370.035s\n",
      "Train: 15 [8150/10009 ( 81%)]  Loss: 3.13 (3.16)  Time: 0.161s,  795.74/s  (0.169s,  757.08/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1378.086s\n",
      "Train: 15 [8200/10009 ( 82%)]  Loss: 3.25 (3.16)  Time: 0.160s,  800.66/s  (0.169s,  757.31/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1386.124s\n",
      "Train: 15 [8250/10009 ( 82%)]  Loss: 3.06 (3.16)  Time: 0.163s,  785.67/s  (0.169s,  757.53/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1394.162s\n",
      "Train: 15 [8300/10009 ( 83%)]  Loss: 3.08 (3.16)  Time: 0.162s,  789.82/s  (0.169s,  757.76/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1402.198s\n",
      "Train: 15 [8350/10009 ( 83%)]  Loss: 3.14 (3.16)  Time: 0.160s,  800.18/s  (0.169s,  757.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1410.264s\n",
      "Train: 15 [8400/10009 ( 84%)]  Loss: 3.27 (3.16)  Time: 0.160s,  800.32/s  (0.169s,  758.18/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1418.300s\n",
      "Train: 15 [8450/10009 ( 84%)]  Loss: 3.14 (3.16)  Time: 0.161s,  794.94/s  (0.169s,  758.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1426.346s\n",
      "Train: 15 [8500/10009 ( 85%)]  Loss: 3.30 (3.16)  Time: 0.160s,  801.22/s  (0.169s,  758.60/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1434.392s\n",
      "Train: 15 [8550/10009 ( 85%)]  Loss: 3.07 (3.16)  Time: 0.160s,  799.07/s  (0.169s,  758.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1442.435s\n",
      "Train: 15 [8600/10009 ( 86%)]  Loss: 3.28 (3.16)  Time: 0.160s,  800.53/s  (0.169s,  759.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1450.454s\n",
      "Train: 15 [8650/10009 ( 86%)]  Loss: 3.11 (3.16)  Time: 0.160s,  797.87/s  (0.169s,  759.23/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1458.491s\n",
      "Train: 15 [8700/10009 ( 87%)]  Loss: 3.14 (3.16)  Time: 0.160s,  800.44/s  (0.169s,  759.43/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1466.532s\n",
      "Train: 15 [8750/10009 ( 87%)]  Loss: 3.16 (3.16)  Time: 0.161s,  794.79/s  (0.169s,  759.63/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1474.568s\n",
      "Train: 15 [8800/10009 ( 88%)]  Loss: 3.10 (3.16)  Time: 0.160s,  799.32/s  (0.168s,  759.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1482.611s\n",
      "Train: 15 [8850/10009 ( 88%)]  Loss: 3.15 (3.16)  Time: 0.161s,  795.04/s  (0.168s,  760.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1490.659s\n",
      "Train: 15 [8900/10009 ( 89%)]  Loss: 3.09 (3.16)  Time: 0.164s,  781.46/s  (0.168s,  760.21/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1498.706s\n",
      "Train: 15 [8950/10009 ( 89%)]  Loss: 3.09 (3.16)  Time: 0.160s,  798.23/s  (0.168s,  760.39/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1506.754s\n",
      "Train: 15 [9000/10009 ( 90%)]  Loss: 3.11 (3.16)  Time: 0.160s,  797.77/s  (0.168s,  760.58/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1514.803s\n",
      "Train: 15 [9050/10009 ( 90%)]  Loss: 3.28 (3.16)  Time: 0.160s,  799.89/s  (0.168s,  760.76/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1522.854s\n",
      "Train: 15 [9100/10009 ( 91%)]  Loss: 3.36 (3.16)  Time: 0.160s,  799.24/s  (0.168s,  760.94/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1530.904s\n",
      "Train: 15 [9150/10009 ( 91%)]  Loss: 3.02 (3.16)  Time: 0.161s,  796.74/s  (0.168s,  761.12/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1538.944s\n",
      "Train: 15 [9200/10009 ( 92%)]  Loss: 3.24 (3.16)  Time: 0.162s,  789.38/s  (0.168s,  761.30/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1546.986s\n",
      "Train: 15 [9250/10009 ( 92%)]  Loss: 3.01 (3.16)  Time: 0.160s,  802.50/s  (0.168s,  761.48/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1555.038s\n",
      "Train: 15 [9300/10009 ( 93%)]  Loss: 3.09 (3.16)  Time: 0.162s,  790.55/s  (0.168s,  761.65/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1563.085s\n",
      "Train: 15 [9350/10009 ( 93%)]  Loss: 3.29 (3.16)  Time: 0.160s,  801.97/s  (0.168s,  761.82/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1571.133s\n",
      "Train: 15 [9400/10009 ( 94%)]  Loss: 3.06 (3.16)  Time: 0.161s,  794.68/s  (0.168s,  761.98/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1579.198s\n",
      "Train: 15 [9450/10009 ( 94%)]  Loss: 3.53 (3.16)  Time: 0.160s,  799.27/s  (0.168s,  762.16/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1587.240s\n",
      "Train: 15 [9500/10009 ( 95%)]  Loss: 3.46 (3.16)  Time: 0.162s,  792.45/s  (0.168s,  762.33/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1595.274s\n",
      "Train: 15 [9550/10009 ( 95%)]  Loss: 3.11 (3.16)  Time: 0.160s,  798.17/s  (0.168s,  762.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1603.318s\n",
      "Train: 15 [9600/10009 ( 96%)]  Loss: 2.87 (3.16)  Time: 0.161s,  796.77/s  (0.168s,  762.66/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1611.359s\n",
      "Train: 15 [9650/10009 ( 96%)]  Loss: 3.32 (3.16)  Time: 0.160s,  798.46/s  (0.168s,  762.83/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1619.393s\n",
      "Train: 15 [9700/10009 ( 97%)]  Loss: 2.82 (3.16)  Time: 0.160s,  799.05/s  (0.168s,  762.99/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1627.440s\n",
      "Train: 15 [9750/10009 ( 97%)]  Loss: 2.85 (3.16)  Time: 0.161s,  797.38/s  (0.168s,  763.15/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1635.484s\n",
      "Train: 15 [9800/10009 ( 98%)]  Loss: 3.21 (3.16)  Time: 0.160s,  797.92/s  (0.168s,  763.31/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1643.527s\n",
      "Train: 15 [9850/10009 ( 98%)]  Loss: 3.06 (3.16)  Time: 0.160s,  799.73/s  (0.168s,  763.47/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1651.577s\n",
      "Train: 15 [9900/10009 ( 99%)]  Loss: 3.03 (3.16)  Time: 0.166s,  772.24/s  (0.168s,  763.61/s)  LR: 0.000e+00  Data: 0.010 (0.006)Time: 1659.644s\n",
      "Train: 15 [9950/10009 ( 99%)]  Loss: 3.20 (3.16)  Time: 0.163s,  786.01/s  (0.168s,  763.76/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1667.711s\n",
      "Train: 15 [10000/10009 (100%)]  Loss: 3.00 (3.16)  Time: 0.216s,  591.80/s  (0.168s,  763.88/s)  LR: 0.000e+00  Data: 0.063 (0.006)Time: 1675.818s\n",
      "Test: [   0/390]  Time: 0.704 (0.704)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.050 (0.154)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.051 (0.141)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.052 (0.145)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.052 (0.142)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.052 (0.141)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.806 (0.147)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.053 (0.150)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.034 (0.150)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 16 [   0/10009 (  0%)]  Loss: 3.29 (3.29)  Time: 0.714s,  179.37/s  (0.714s,  179.37/s)  LR: 0.000e+00  Data: 0.564 (0.564)Time: 0.714s\n",
      "Train: 16 [  50/10009 (  0%)]  Loss: 3.25 (3.17)  Time: 0.160s,  801.05/s  (0.170s,  753.17/s)  LR: 0.000e+00  Data: 0.007 (0.018)Time: 8.668s\n",
      "Train: 16 [ 100/10009 (  1%)]  Loss: 2.99 (3.18)  Time: 0.161s,  796.84/s  (0.165s,  778.05/s)  LR: 0.000e+00  Data: 0.006 (0.012)Time: 16.617s\n",
      "Train: 16 [ 150/10009 (  1%)]  Loss: 3.17 (3.17)  Time: 0.161s,  795.69/s  (0.163s,  785.80/s)  LR: 0.000e+00  Data: 0.007 (0.010)Time: 24.597s\n",
      "Train: 16 [ 200/10009 (  2%)]  Loss: 3.37 (3.17)  Time: 0.160s,  800.35/s  (0.162s,  789.64/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 32.583s\n",
      "Train: 16 [ 250/10009 (  2%)]  Loss: 3.31 (3.16)  Time: 0.162s,  789.94/s  (0.162s,  791.99/s)  LR: 0.000e+00  Data: 0.007 (0.008)Time: 40.567s\n",
      "Train: 16 [ 300/10009 (  3%)]  Loss: 3.05 (3.17)  Time: 0.159s,  805.19/s  (0.161s,  793.58/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 48.550s\n",
      "Train: 16 [ 350/10009 (  3%)]  Loss: 3.32 (3.16)  Time: 0.160s,  798.94/s  (0.161s,  794.27/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 56.565s\n",
      "Train: 16 [ 400/10009 (  4%)]  Loss: 3.66 (3.16)  Time: 0.160s,  800.73/s  (0.161s,  794.90/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 64.573s\n",
      "Train: 16 [ 450/10009 (  4%)]  Loss: 3.00 (3.16)  Time: 0.160s,  797.80/s  (0.161s,  795.07/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 72.608s\n",
      "Train: 16 [ 500/10009 (  5%)]  Loss: 3.23 (3.16)  Time: 0.162s,  789.81/s  (0.161s,  795.08/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 80.657s\n",
      "Train: 16 [ 550/10009 (  5%)]  Loss: 3.03 (3.16)  Time: 0.161s,  794.73/s  (0.161s,  795.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 88.680s\n",
      "Train: 16 [ 600/10009 (  6%)]  Loss: 3.09 (3.16)  Time: 0.161s,  797.16/s  (0.161s,  795.46/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 96.709s\n",
      "Train: 16 [ 650/10009 (  6%)]  Loss: 3.03 (3.16)  Time: 0.161s,  797.26/s  (0.161s,  795.53/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 104.746s\n",
      "Train: 16 [ 700/10009 (  7%)]  Loss: 3.40 (3.16)  Time: 0.161s,  795.46/s  (0.161s,  795.60/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 112.780s\n",
      "Train: 16 [ 750/10009 (  7%)]  Loss: 3.13 (3.16)  Time: 0.162s,  790.68/s  (0.161s,  795.45/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 120.848s\n",
      "Train: 16 [ 800/10009 (  8%)]  Loss: 3.28 (3.16)  Time: 0.160s,  800.20/s  (0.161s,  795.54/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 128.879s\n",
      "Train: 16 [ 850/10009 (  8%)]  Loss: 3.10 (3.17)  Time: 0.160s,  800.10/s  (0.161s,  795.55/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 136.922s\n",
      "Train: 16 [ 900/10009 (  9%)]  Loss: 3.29 (3.17)  Time: 0.161s,  794.39/s  (0.161s,  795.50/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 144.976s\n",
      "Train: 16 [ 950/10009 (  9%)]  Loss: 3.10 (3.17)  Time: 0.160s,  801.71/s  (0.161s,  795.53/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 153.015s\n",
      "Train: 16 [1000/10009 ( 10%)]  Loss: 3.21 (3.16)  Time: 0.161s,  795.20/s  (0.161s,  795.47/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 161.072s\n",
      "Train: 16 [1050/10009 ( 10%)]  Loss: 3.11 (3.16)  Time: 0.159s,  802.53/s  (0.161s,  795.41/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 169.130s\n",
      "Train: 16 [1100/10009 ( 11%)]  Loss: 2.88 (3.16)  Time: 0.160s,  800.29/s  (0.161s,  795.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 177.178s\n",
      "Train: 16 [1150/10009 ( 11%)]  Loss: 3.23 (3.16)  Time: 0.160s,  798.49/s  (0.161s,  795.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 185.226s\n",
      "Train: 16 [1200/10009 ( 12%)]  Loss: 3.27 (3.16)  Time: 0.162s,  788.50/s  (0.161s,  795.27/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 193.304s\n",
      "Train: 16 [1250/10009 ( 12%)]  Loss: 3.28 (3.16)  Time: 0.161s,  794.04/s  (0.161s,  795.26/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 201.353s\n",
      "Train: 16 [1300/10009 ( 13%)]  Loss: 3.48 (3.17)  Time: 0.161s,  795.78/s  (0.161s,  795.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 209.388s\n",
      "Train: 16 [1350/10009 ( 13%)]  Loss: 3.28 (3.17)  Time: 0.160s,  798.92/s  (0.161s,  795.36/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 217.422s\n",
      "Train: 16 [1400/10009 ( 14%)]  Loss: 3.12 (3.16)  Time: 0.161s,  795.59/s  (0.161s,  795.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 225.459s\n",
      "Train: 16 [1450/10009 ( 14%)]  Loss: 3.18 (3.16)  Time: 0.161s,  795.42/s  (0.161s,  795.44/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 233.490s\n",
      "Train: 16 [1500/10009 ( 15%)]  Loss: 2.90 (3.16)  Time: 0.162s,  788.99/s  (0.161s,  795.46/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 241.529s\n",
      "Train: 16 [1550/10009 ( 15%)]  Loss: 3.18 (3.16)  Time: 0.161s,  794.59/s  (0.161s,  795.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 249.592s\n",
      "Train: 16 [1600/10009 ( 16%)]  Loss: 3.04 (3.16)  Time: 0.162s,  790.03/s  (0.161s,  795.43/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 257.631s\n",
      "Train: 16 [1650/10009 ( 16%)]  Loss: 3.24 (3.16)  Time: 0.160s,  798.38/s  (0.161s,  795.41/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 265.683s\n",
      "Train: 16 [1700/10009 ( 17%)]  Loss: 3.05 (3.17)  Time: 0.161s,  793.87/s  (0.161s,  795.40/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 273.734s\n",
      "Train: 16 [1750/10009 ( 17%)]  Loss: 3.01 (3.17)  Time: 0.160s,  798.88/s  (0.161s,  795.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 281.778s\n",
      "Train: 16 [1800/10009 ( 18%)]  Loss: 3.27 (3.17)  Time: 0.160s,  799.26/s  (0.161s,  795.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 289.860s\n",
      "Train: 16 [1850/10009 ( 18%)]  Loss: 3.07 (3.17)  Time: 0.162s,  792.36/s  (0.161s,  795.27/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 297.922s\n",
      "Train: 16 [1900/10009 ( 19%)]  Loss: 3.10 (3.16)  Time: 0.160s,  799.11/s  (0.161s,  795.27/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 305.968s\n",
      "Train: 16 [1950/10009 ( 19%)]  Loss: 3.34 (3.16)  Time: 0.160s,  802.15/s  (0.161s,  795.30/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 314.004s\n",
      "Train: 16 [2000/10009 ( 20%)]  Loss: 3.10 (3.16)  Time: 0.169s,  755.60/s  (0.161s,  795.13/s)  LR: 0.000e+00  Data: 0.013 (0.007)Time: 322.119s\n",
      "Train: 16 [2050/10009 ( 20%)]  Loss: 3.16 (3.16)  Time: 0.163s,  786.19/s  (0.161s,  794.88/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 330.273s\n",
      "Train: 16 [2100/10009 ( 21%)]  Loss: 3.16 (3.16)  Time: 0.160s,  798.32/s  (0.161s,  794.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 338.351s\n",
      "Train: 16 [2150/10009 ( 21%)]  Loss: 3.09 (3.16)  Time: 0.160s,  797.93/s  (0.161s,  794.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 346.401s\n",
      "Train: 16 [2200/10009 ( 22%)]  Loss: 2.89 (3.16)  Time: 0.162s,  790.13/s  (0.161s,  794.74/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 354.489s\n",
      "Train: 16 [2250/10009 ( 22%)]  Loss: 3.28 (3.16)  Time: 0.160s,  798.32/s  (0.161s,  794.73/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 362.549s\n",
      "Train: 16 [2300/10009 ( 23%)]  Loss: 3.04 (3.16)  Time: 0.163s,  783.29/s  (0.161s,  794.64/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 370.644s\n",
      "Train: 16 [2350/10009 ( 23%)]  Loss: 3.19 (3.16)  Time: 0.171s,  749.71/s  (0.161s,  794.53/s)  LR: 0.000e+00  Data: 0.016 (0.007)Time: 378.748s\n",
      "Train: 16 [2400/10009 ( 24%)]  Loss: 3.13 (3.16)  Time: 0.162s,  787.77/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 386.860s\n",
      "Train: 16 [2450/10009 ( 24%)]  Loss: 3.29 (3.16)  Time: 0.160s,  798.11/s  (0.161s,  794.33/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 394.958s\n",
      "Train: 16 [2500/10009 ( 25%)]  Loss: 3.17 (3.16)  Time: 0.161s,  797.34/s  (0.161s,  794.34/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 403.012s\n",
      "Train: 16 [2550/10009 ( 25%)]  Loss: 3.26 (3.16)  Time: 0.161s,  794.35/s  (0.161s,  794.26/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 411.108s\n",
      "Train: 16 [2600/10009 ( 26%)]  Loss: 3.00 (3.16)  Time: 0.166s,  773.31/s  (0.161s,  794.17/s)  LR: 0.000e+00  Data: 0.011 (0.007)Time: 419.212s\n",
      "Train: 16 [2650/10009 ( 26%)]  Loss: 3.40 (3.16)  Time: 0.161s,  794.15/s  (0.161s,  794.16/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 427.281s\n",
      "Train: 16 [2700/10009 ( 27%)]  Loss: 3.07 (3.16)  Time: 0.163s,  784.34/s  (0.161s,  794.12/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 435.361s\n",
      "Train: 16 [2750/10009 ( 27%)]  Loss: 3.49 (3.16)  Time: 0.160s,  798.83/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 443.448s\n",
      "Train: 16 [2800/10009 ( 28%)]  Loss: 2.98 (3.16)  Time: 0.161s,  795.42/s  (0.161s,  794.10/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 451.486s\n",
      "Train: 16 [2850/10009 ( 28%)]  Loss: 3.45 (3.16)  Time: 0.161s,  797.04/s  (0.161s,  794.15/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 459.518s\n",
      "Train: 16 [2900/10009 ( 29%)]  Loss: 3.05 (3.16)  Time: 0.160s,  800.46/s  (0.161s,  794.13/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 467.589s\n",
      "Train: 16 [2950/10009 ( 29%)]  Loss: 3.26 (3.16)  Time: 0.161s,  793.42/s  (0.161s,  794.16/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 475.629s\n",
      "Train: 16 [3000/10009 ( 30%)]  Loss: 3.26 (3.16)  Time: 0.161s,  797.20/s  (0.161s,  794.13/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 483.709s\n",
      "Train: 16 [3050/10009 ( 30%)]  Loss: 3.18 (3.16)  Time: 0.163s,  784.50/s  (0.161s,  794.09/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 491.792s\n",
      "Train: 16 [3100/10009 ( 31%)]  Loss: 2.88 (3.16)  Time: 0.162s,  791.56/s  (0.161s,  793.97/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 499.929s\n",
      "Train: 16 [3150/10009 ( 31%)]  Loss: 3.26 (3.16)  Time: 0.160s,  801.22/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 508.053s\n",
      "Train: 16 [3200/10009 ( 32%)]  Loss: 3.25 (3.16)  Time: 0.160s,  798.79/s  (0.161s,  793.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 516.179s\n",
      "Train: 16 [3250/10009 ( 32%)]  Loss: 2.86 (3.16)  Time: 0.161s,  796.49/s  (0.161s,  793.78/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 524.234s\n",
      "Train: 16 [3300/10009 ( 33%)]  Loss: 2.82 (3.16)  Time: 0.160s,  799.27/s  (0.161s,  793.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 532.272s\n",
      "Train: 16 [3350/10009 ( 33%)]  Loss: 3.48 (3.16)  Time: 0.161s,  796.96/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 540.315s\n",
      "Train: 16 [3400/10009 ( 34%)]  Loss: 3.05 (3.16)  Time: 0.160s,  800.74/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 548.360s\n",
      "Train: 16 [3450/10009 ( 34%)]  Loss: 3.32 (3.16)  Time: 0.160s,  799.29/s  (0.161s,  793.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 556.473s\n",
      "Train: 16 [3500/10009 ( 35%)]  Loss: 3.38 (3.16)  Time: 0.161s,  795.79/s  (0.161s,  793.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 564.557s\n",
      "Train: 16 [3550/10009 ( 35%)]  Loss: 3.35 (3.16)  Time: 0.162s,  790.51/s  (0.161s,  793.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 572.617s\n",
      "Train: 16 [3600/10009 ( 36%)]  Loss: 2.98 (3.16)  Time: 0.161s,  794.94/s  (0.161s,  793.79/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 580.663s\n",
      "Train: 16 [3650/10009 ( 36%)]  Loss: 3.27 (3.16)  Time: 0.160s,  799.67/s  (0.161s,  793.79/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 588.727s\n",
      "Train: 16 [3700/10009 ( 37%)]  Loss: 3.00 (3.16)  Time: 0.161s,  796.23/s  (0.161s,  793.78/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 596.799s\n",
      "Train: 16 [3750/10009 ( 37%)]  Loss: 3.13 (3.16)  Time: 0.160s,  798.88/s  (0.161s,  793.79/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 604.854s\n",
      "Train: 16 [3800/10009 ( 38%)]  Loss: 2.91 (3.16)  Time: 0.160s,  800.70/s  (0.161s,  793.76/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 612.937s\n",
      "Train: 16 [3850/10009 ( 38%)]  Loss: 2.86 (3.16)  Time: 0.160s,  801.10/s  (0.161s,  793.79/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 620.977s\n",
      "Train: 16 [3900/10009 ( 39%)]  Loss: 3.17 (3.16)  Time: 0.161s,  794.87/s  (0.161s,  793.82/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 629.020s\n",
      "Train: 16 [3950/10009 ( 39%)]  Loss: 3.44 (3.16)  Time: 0.161s,  794.15/s  (0.161s,  793.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 637.069s\n",
      "Train: 16 [4000/10009 ( 40%)]  Loss: 2.75 (3.16)  Time: 0.160s,  799.25/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 645.121s\n",
      "Train: 16 [4050/10009 ( 40%)]  Loss: 3.42 (3.16)  Time: 0.160s,  801.62/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 653.179s\n",
      "Train: 16 [4100/10009 ( 41%)]  Loss: 3.33 (3.16)  Time: 0.162s,  788.80/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 661.222s\n",
      "Train: 16 [4150/10009 ( 41%)]  Loss: 3.23 (3.16)  Time: 0.161s,  794.32/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 669.289s\n",
      "Train: 16 [4200/10009 ( 42%)]  Loss: 3.33 (3.16)  Time: 0.161s,  796.63/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 677.338s\n",
      "Train: 16 [4250/10009 ( 42%)]  Loss: 3.37 (3.16)  Time: 0.161s,  796.63/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 685.404s\n",
      "Train: 16 [4300/10009 ( 43%)]  Loss: 3.08 (3.16)  Time: 0.160s,  801.00/s  (0.161s,  793.89/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 693.453s\n",
      "Train: 16 [4350/10009 ( 43%)]  Loss: 3.28 (3.16)  Time: 0.161s,  793.95/s  (0.161s,  793.91/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 701.495s\n",
      "Train: 16 [4400/10009 ( 44%)]  Loss: 3.34 (3.16)  Time: 0.162s,  792.22/s  (0.161s,  793.91/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 709.564s\n",
      "Train: 16 [4450/10009 ( 44%)]  Loss: 3.02 (3.16)  Time: 0.161s,  793.41/s  (0.161s,  793.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 717.608s\n",
      "Train: 16 [4500/10009 ( 45%)]  Loss: 3.12 (3.16)  Time: 0.163s,  786.37/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 725.713s\n",
      "Train: 16 [4550/10009 ( 45%)]  Loss: 3.06 (3.16)  Time: 0.161s,  794.89/s  (0.161s,  793.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 733.758s\n",
      "Train: 16 [4600/10009 ( 46%)]  Loss: 3.28 (3.16)  Time: 0.161s,  797.06/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 741.853s\n",
      "Train: 16 [4650/10009 ( 46%)]  Loss: 3.57 (3.16)  Time: 0.162s,  789.09/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 749.900s\n",
      "Train: 16 [4700/10009 ( 47%)]  Loss: 3.37 (3.16)  Time: 0.161s,  796.25/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 757.973s\n",
      "Train: 16 [4750/10009 ( 47%)]  Loss: 3.21 (3.16)  Time: 0.163s,  782.96/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 766.039s\n",
      "Train: 16 [4800/10009 ( 48%)]  Loss: 3.41 (3.16)  Time: 0.161s,  795.02/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 774.085s\n",
      "Train: 16 [4850/10009 ( 48%)]  Loss: 2.93 (3.16)  Time: 0.161s,  795.22/s  (0.161s,  793.84/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 782.182s\n",
      "Train: 16 [4900/10009 ( 49%)]  Loss: 3.00 (3.16)  Time: 0.160s,  798.64/s  (0.161s,  793.83/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 790.250s\n",
      "Train: 16 [4950/10009 ( 49%)]  Loss: 3.16 (3.16)  Time: 0.161s,  795.18/s  (0.161s,  793.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 798.310s\n",
      "Train: 16 [5000/10009 ( 50%)]  Loss: 3.10 (3.16)  Time: 0.160s,  798.27/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 806.356s\n",
      "Train: 16 [5050/10009 ( 50%)]  Loss: 2.99 (3.16)  Time: 0.162s,  792.05/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 814.421s\n",
      "Train: 16 [5100/10009 ( 51%)]  Loss: 3.11 (3.16)  Time: 0.162s,  790.78/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 822.480s\n",
      "Train: 16 [5150/10009 ( 51%)]  Loss: 3.06 (3.16)  Time: 0.162s,  790.19/s  (0.161s,  793.83/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 830.564s\n",
      "Train: 16 [5200/10009 ( 52%)]  Loss: 2.91 (3.16)  Time: 0.165s,  777.02/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 838.608s\n",
      "Train: 16 [5250/10009 ( 52%)]  Loss: 3.60 (3.16)  Time: 0.161s,  793.09/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 846.652s\n",
      "Train: 16 [5300/10009 ( 53%)]  Loss: 3.14 (3.16)  Time: 0.162s,  788.82/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 854.706s\n",
      "Train: 16 [5350/10009 ( 53%)]  Loss: 3.28 (3.16)  Time: 0.161s,  794.10/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 862.755s\n",
      "Train: 16 [5400/10009 ( 54%)]  Loss: 3.11 (3.16)  Time: 0.162s,  789.30/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 870.823s\n",
      "Train: 16 [5450/10009 ( 54%)]  Loss: 3.21 (3.16)  Time: 0.160s,  800.23/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 878.908s\n",
      "Train: 16 [5500/10009 ( 55%)]  Loss: 3.36 (3.16)  Time: 0.161s,  795.58/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 886.955s\n",
      "Train: 16 [5550/10009 ( 55%)]  Loss: 3.33 (3.16)  Time: 0.163s,  784.29/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 895.022s\n",
      "Train: 16 [5600/10009 ( 56%)]  Loss: 3.10 (3.16)  Time: 0.161s,  794.11/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 903.082s\n",
      "Train: 16 [5650/10009 ( 56%)]  Loss: 3.37 (3.16)  Time: 0.161s,  795.92/s  (0.161s,  793.89/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 911.120s\n",
      "Train: 16 [5700/10009 ( 57%)]  Loss: 3.31 (3.16)  Time: 0.161s,  797.48/s  (0.161s,  793.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 919.162s\n",
      "Train: 16 [5750/10009 ( 57%)]  Loss: 3.53 (3.16)  Time: 0.160s,  800.38/s  (0.161s,  793.92/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 927.206s\n",
      "Train: 16 [5800/10009 ( 58%)]  Loss: 3.16 (3.16)  Time: 0.160s,  798.95/s  (0.161s,  793.94/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 935.246s\n",
      "Train: 16 [5850/10009 ( 58%)]  Loss: 3.16 (3.16)  Time: 0.161s,  795.76/s  (0.161s,  793.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 943.282s\n",
      "Train: 16 [5900/10009 ( 59%)]  Loss: 3.12 (3.16)  Time: 0.160s,  801.01/s  (0.161s,  793.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 951.343s\n",
      "Train: 16 [5950/10009 ( 59%)]  Loss: 3.02 (3.16)  Time: 0.160s,  797.92/s  (0.161s,  793.97/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 959.389s\n",
      "Train: 16 [6000/10009 ( 60%)]  Loss: 3.11 (3.16)  Time: 0.160s,  798.00/s  (0.161s,  793.98/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 967.433s\n",
      "Train: 16 [6050/10009 ( 60%)]  Loss: 2.99 (3.16)  Time: 0.160s,  797.92/s  (0.161s,  793.98/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 975.503s\n",
      "Train: 16 [6100/10009 ( 61%)]  Loss: 3.09 (3.16)  Time: 0.160s,  801.34/s  (0.161s,  794.00/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 983.539s\n",
      "Train: 16 [6150/10009 ( 61%)]  Loss: 3.09 (3.16)  Time: 0.160s,  799.00/s  (0.161s,  794.02/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 991.574s\n",
      "Train: 16 [6200/10009 ( 62%)]  Loss: 3.03 (3.16)  Time: 0.162s,  789.32/s  (0.161s,  794.04/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 999.607s\n",
      "Train: 16 [6250/10009 ( 62%)]  Loss: 2.89 (3.16)  Time: 0.162s,  791.34/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1007.653s\n",
      "Train: 16 [6300/10009 ( 63%)]  Loss: 3.26 (3.16)  Time: 0.161s,  794.79/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1015.709s\n",
      "Train: 16 [6350/10009 ( 63%)]  Loss: 3.17 (3.16)  Time: 0.161s,  796.09/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1023.750s\n",
      "Train: 16 [6400/10009 ( 64%)]  Loss: 3.38 (3.16)  Time: 0.161s,  794.06/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1031.795s\n",
      "Train: 16 [6450/10009 ( 64%)]  Loss: 3.17 (3.16)  Time: 0.161s,  795.04/s  (0.161s,  794.09/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1039.834s\n",
      "Train: 16 [6500/10009 ( 65%)]  Loss: 3.32 (3.16)  Time: 0.160s,  798.86/s  (0.161s,  794.10/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1047.880s\n",
      "Train: 16 [6550/10009 ( 65%)]  Loss: 3.11 (3.16)  Time: 0.160s,  798.63/s  (0.161s,  794.12/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1055.920s\n",
      "Train: 16 [6600/10009 ( 66%)]  Loss: 2.87 (3.16)  Time: 0.163s,  787.14/s  (0.161s,  794.11/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1063.991s\n",
      "Train: 16 [6650/10009 ( 66%)]  Loss: 3.06 (3.16)  Time: 0.161s,  797.44/s  (0.161s,  794.12/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1072.038s\n",
      "Train: 16 [6700/10009 ( 67%)]  Loss: 3.37 (3.16)  Time: 0.161s,  794.09/s  (0.161s,  794.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1080.067s\n",
      "Train: 16 [6750/10009 ( 67%)]  Loss: 3.10 (3.16)  Time: 0.162s,  789.48/s  (0.161s,  794.12/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1088.150s\n",
      "Train: 16 [6800/10009 ( 68%)]  Loss: 3.20 (3.16)  Time: 0.160s,  800.53/s  (0.161s,  794.14/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1096.190s\n",
      "Train: 16 [6850/10009 ( 68%)]  Loss: 3.26 (3.16)  Time: 0.161s,  796.14/s  (0.161s,  794.16/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1104.221s\n",
      "Train: 16 [6900/10009 ( 69%)]  Loss: 3.23 (3.16)  Time: 0.161s,  794.83/s  (0.161s,  794.17/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1112.262s\n",
      "Train: 16 [6950/10009 ( 69%)]  Loss: 3.06 (3.16)  Time: 0.162s,  789.17/s  (0.161s,  794.18/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1120.314s\n",
      "Train: 16 [7000/10009 ( 70%)]  Loss: 3.08 (3.16)  Time: 0.163s,  786.08/s  (0.161s,  794.18/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1128.364s\n",
      "Train: 16 [7050/10009 ( 70%)]  Loss: 3.34 (3.16)  Time: 0.164s,  782.62/s  (0.161s,  794.17/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1136.440s\n",
      "Train: 16 [7100/10009 ( 71%)]  Loss: 3.13 (3.16)  Time: 0.160s,  798.46/s  (0.161s,  794.17/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1144.498s\n",
      "Train: 16 [7150/10009 ( 71%)]  Loss: 3.44 (3.16)  Time: 0.160s,  798.48/s  (0.161s,  794.18/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1152.541s\n",
      "Train: 16 [7200/10009 ( 72%)]  Loss: 2.97 (3.16)  Time: 0.161s,  792.77/s  (0.161s,  794.19/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1160.579s\n",
      "Train: 16 [7250/10009 ( 72%)]  Loss: 3.33 (3.16)  Time: 0.161s,  793.70/s  (0.161s,  794.20/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1168.632s\n",
      "Train: 16 [7300/10009 ( 73%)]  Loss: 3.12 (3.16)  Time: 0.161s,  793.63/s  (0.161s,  794.20/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1176.686s\n",
      "Train: 16 [7350/10009 ( 73%)]  Loss: 3.01 (3.16)  Time: 0.160s,  799.45/s  (0.161s,  794.21/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1184.728s\n",
      "Train: 16 [7400/10009 ( 74%)]  Loss: 2.88 (3.16)  Time: 0.160s,  800.77/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1192.760s\n",
      "Train: 16 [7450/10009 ( 74%)]  Loss: 3.08 (3.16)  Time: 0.160s,  800.96/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1200.816s\n",
      "Train: 16 [7500/10009 ( 75%)]  Loss: 3.16 (3.16)  Time: 0.161s,  794.84/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1208.869s\n",
      "Train: 16 [7550/10009 ( 75%)]  Loss: 3.24 (3.16)  Time: 0.161s,  793.48/s  (0.161s,  794.21/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1216.964s\n",
      "Train: 16 [7600/10009 ( 76%)]  Loss: 3.41 (3.16)  Time: 0.161s,  797.31/s  (0.161s,  794.21/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1225.028s\n",
      "Train: 16 [7650/10009 ( 76%)]  Loss: 2.73 (3.16)  Time: 0.162s,  787.90/s  (0.161s,  794.22/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1233.070s\n",
      "Train: 16 [7700/10009 ( 77%)]  Loss: 3.26 (3.16)  Time: 0.163s,  783.12/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1241.116s\n",
      "Train: 16 [7750/10009 ( 77%)]  Loss: 3.53 (3.16)  Time: 0.161s,  796.56/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1249.166s\n",
      "Train: 16 [7800/10009 ( 78%)]  Loss: 3.07 (3.16)  Time: 0.162s,  789.38/s  (0.161s,  794.24/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1257.202s\n",
      "Train: 16 [7850/10009 ( 78%)]  Loss: 3.18 (3.16)  Time: 0.161s,  795.82/s  (0.161s,  794.26/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1265.241s\n",
      "Train: 16 [7900/10009 ( 79%)]  Loss: 3.19 (3.16)  Time: 0.161s,  797.08/s  (0.161s,  794.27/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1273.273s\n",
      "Train: 16 [7950/10009 ( 79%)]  Loss: 3.11 (3.16)  Time: 0.162s,  791.00/s  (0.161s,  794.28/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1281.312s\n",
      "Train: 16 [8000/10009 ( 80%)]  Loss: 3.07 (3.16)  Time: 0.160s,  797.55/s  (0.161s,  794.28/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1289.377s\n",
      "Train: 16 [8050/10009 ( 80%)]  Loss: 3.13 (3.16)  Time: 0.160s,  798.25/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1297.424s\n",
      "Train: 16 [8100/10009 ( 81%)]  Loss: 2.99 (3.16)  Time: 0.160s,  798.11/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1305.476s\n",
      "Train: 16 [8150/10009 ( 81%)]  Loss: 3.35 (3.16)  Time: 0.160s,  800.45/s  (0.161s,  794.30/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1313.523s\n",
      "Train: 16 [8200/10009 ( 82%)]  Loss: 3.19 (3.16)  Time: 0.161s,  795.25/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1321.582s\n",
      "Train: 16 [8250/10009 ( 82%)]  Loss: 3.03 (3.16)  Time: 0.161s,  793.58/s  (0.161s,  794.30/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1329.631s\n",
      "Train: 16 [8300/10009 ( 83%)]  Loss: 2.91 (3.16)  Time: 0.161s,  796.63/s  (0.161s,  794.30/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1337.683s\n",
      "Train: 16 [8350/10009 ( 83%)]  Loss: 2.96 (3.16)  Time: 0.161s,  794.00/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1345.725s\n",
      "Train: 16 [8400/10009 ( 84%)]  Loss: 3.34 (3.16)  Time: 0.160s,  800.49/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1353.784s\n",
      "Train: 16 [8450/10009 ( 84%)]  Loss: 3.25 (3.16)  Time: 0.160s,  797.67/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1361.836s\n",
      "Train: 16 [8500/10009 ( 85%)]  Loss: 2.85 (3.16)  Time: 0.160s,  797.90/s  (0.161s,  794.32/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1369.881s\n",
      "Train: 16 [8550/10009 ( 85%)]  Loss: 3.35 (3.16)  Time: 0.160s,  799.95/s  (0.161s,  794.33/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1377.931s\n",
      "Train: 16 [8600/10009 ( 86%)]  Loss: 3.12 (3.16)  Time: 0.160s,  797.79/s  (0.161s,  794.33/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1385.972s\n",
      "Train: 16 [8650/10009 ( 86%)]  Loss: 3.15 (3.16)  Time: 0.160s,  798.00/s  (0.161s,  794.34/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1394.011s\n",
      "Train: 16 [8700/10009 ( 87%)]  Loss: 3.19 (3.16)  Time: 0.163s,  786.34/s  (0.161s,  794.35/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1402.059s\n",
      "Train: 16 [8750/10009 ( 87%)]  Loss: 2.96 (3.16)  Time: 0.161s,  795.21/s  (0.161s,  794.36/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1410.101s\n",
      "Train: 16 [8800/10009 ( 88%)]  Loss: 3.31 (3.16)  Time: 0.161s,  797.49/s  (0.161s,  794.36/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1418.152s\n",
      "Train: 16 [8850/10009 ( 88%)]  Loss: 3.10 (3.16)  Time: 0.161s,  797.16/s  (0.161s,  794.36/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1426.206s\n",
      "Train: 16 [8900/10009 ( 89%)]  Loss: 3.29 (3.16)  Time: 0.161s,  796.58/s  (0.161s,  794.38/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1434.239s\n",
      "Train: 16 [8950/10009 ( 89%)]  Loss: 3.34 (3.16)  Time: 0.163s,  786.37/s  (0.161s,  794.38/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1442.288s\n",
      "Train: 16 [9000/10009 ( 90%)]  Loss: 3.20 (3.16)  Time: 0.161s,  795.29/s  (0.161s,  794.39/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1450.328s\n",
      "Train: 16 [9050/10009 ( 90%)]  Loss: 3.23 (3.16)  Time: 0.161s,  795.34/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1458.364s\n",
      "Train: 16 [9100/10009 ( 91%)]  Loss: 3.01 (3.16)  Time: 0.160s,  801.13/s  (0.161s,  794.41/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1466.401s\n",
      "Train: 16 [9150/10009 ( 91%)]  Loss: 3.22 (3.16)  Time: 0.160s,  801.63/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1474.444s\n",
      "Train: 16 [9200/10009 ( 92%)]  Loss: 3.14 (3.16)  Time: 0.161s,  797.34/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1482.487s\n",
      "Train: 16 [9250/10009 ( 92%)]  Loss: 3.33 (3.16)  Time: 0.161s,  796.57/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1490.527s\n",
      "Train: 16 [9300/10009 ( 93%)]  Loss: 3.17 (3.16)  Time: 0.161s,  793.18/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1498.564s\n",
      "Train: 16 [9350/10009 ( 93%)]  Loss: 3.03 (3.16)  Time: 0.161s,  795.70/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1506.610s\n",
      "Train: 16 [9400/10009 ( 94%)]  Loss: 3.16 (3.16)  Time: 0.161s,  795.49/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1514.664s\n",
      "Train: 16 [9450/10009 ( 94%)]  Loss: 3.06 (3.16)  Time: 0.162s,  792.06/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1522.706s\n",
      "Train: 16 [9500/10009 ( 95%)]  Loss: 3.33 (3.16)  Time: 0.160s,  798.93/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1530.748s\n",
      "Train: 16 [9550/10009 ( 95%)]  Loss: 3.15 (3.16)  Time: 0.161s,  796.33/s  (0.161s,  794.47/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1538.787s\n",
      "Train: 16 [9600/10009 ( 96%)]  Loss: 3.36 (3.16)  Time: 0.160s,  798.64/s  (0.161s,  794.48/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1546.827s\n",
      "Train: 16 [9650/10009 ( 96%)]  Loss: 3.29 (3.16)  Time: 0.160s,  797.92/s  (0.161s,  794.47/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1554.899s\n",
      "Train: 16 [9700/10009 ( 97%)]  Loss: 3.55 (3.16)  Time: 0.160s,  797.86/s  (0.161s,  794.47/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1562.965s\n",
      "Train: 16 [9750/10009 ( 97%)]  Loss: 3.32 (3.16)  Time: 0.161s,  797.30/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1571.034s\n",
      "Train: 16 [9800/10009 ( 98%)]  Loss: 3.23 (3.16)  Time: 0.160s,  798.57/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1579.107s\n",
      "Train: 16 [9850/10009 ( 98%)]  Loss: 3.07 (3.16)  Time: 0.160s,  800.09/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1587.173s\n",
      "Train: 16 [9900/10009 ( 99%)]  Loss: 3.35 (3.16)  Time: 0.161s,  794.11/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1595.237s\n",
      "Train: 16 [9950/10009 ( 99%)]  Loss: 3.10 (3.16)  Time: 0.162s,  792.37/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1603.283s\n",
      "Train: 16 [10000/10009 (100%)]  Loss: 3.12 (3.16)  Time: 0.203s,  631.94/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.049 (0.006)Time: 1611.373s\n",
      "Test: [   0/390]  Time: 0.671 (0.671)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.143)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.051 (0.134)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.051 (0.138)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.053 (0.135)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.051 (0.135)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.052 (0.134)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.051 (0.133)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.032 (0.133)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 17 [   0/10009 (  0%)]  Loss: 2.94 (2.94)  Time: 0.697s,  183.68/s  (0.697s,  183.68/s)  LR: 0.000e+00  Data: 0.547 (0.547)Time: 0.697s\n",
      "Train: 17 [  50/10009 (  0%)]  Loss: 3.20 (3.18)  Time: 0.159s,  806.68/s  (0.171s,  748.99/s)  LR: 0.000e+00  Data: 0.006 (0.018)Time: 8.716s\n",
      "Train: 17 [ 100/10009 (  1%)]  Loss: 3.16 (3.16)  Time: 0.159s,  804.05/s  (0.165s,  774.89/s)  LR: 0.000e+00  Data: 0.006 (0.012)Time: 16.684s\n",
      "Train: 17 [ 150/10009 (  1%)]  Loss: 3.21 (3.17)  Time: 0.159s,  805.19/s  (0.163s,  783.40/s)  LR: 0.000e+00  Data: 0.005 (0.010)Time: 24.672s\n",
      "Train: 17 [ 200/10009 (  2%)]  Loss: 3.12 (3.17)  Time: 0.160s,  801.20/s  (0.163s,  786.42/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 32.715s\n",
      "Train: 17 [ 250/10009 (  2%)]  Loss: 3.18 (3.17)  Time: 0.159s,  804.04/s  (0.162s,  789.07/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 40.716s\n",
      "Train: 17 [ 300/10009 (  3%)]  Loss: 3.08 (3.17)  Time: 0.161s,  796.37/s  (0.162s,  790.62/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 48.732s\n",
      "Train: 17 [ 350/10009 (  3%)]  Loss: 3.18 (3.17)  Time: 0.161s,  795.23/s  (0.162s,  791.69/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 56.750s\n",
      "Train: 17 [ 400/10009 (  4%)]  Loss: 3.47 (3.17)  Time: 0.162s,  788.06/s  (0.162s,  792.38/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 64.777s\n",
      "Train: 17 [ 450/10009 (  4%)]  Loss: 3.24 (3.17)  Time: 0.161s,  792.92/s  (0.162s,  792.49/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 72.844s\n",
      "Train: 17 [ 500/10009 (  5%)]  Loss: 3.22 (3.17)  Time: 0.161s,  795.52/s  (0.161s,  792.95/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 80.873s\n",
      "Train: 17 [ 550/10009 (  5%)]  Loss: 3.10 (3.17)  Time: 0.161s,  796.04/s  (0.161s,  793.22/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 88.914s\n",
      "Train: 17 [ 600/10009 (  6%)]  Loss: 2.91 (3.17)  Time: 0.160s,  798.64/s  (0.161s,  793.30/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 96.972s\n",
      "Train: 17 [ 650/10009 (  6%)]  Loss: 3.10 (3.17)  Time: 0.162s,  791.21/s  (0.161s,  793.43/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 105.022s\n",
      "Train: 17 [ 700/10009 (  7%)]  Loss: 2.83 (3.17)  Time: 0.160s,  798.21/s  (0.161s,  793.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 113.055s\n",
      "Train: 17 [ 750/10009 (  7%)]  Loss: 3.62 (3.17)  Time: 0.161s,  794.20/s  (0.161s,  793.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 121.093s\n",
      "Train: 17 [ 800/10009 (  8%)]  Loss: 3.34 (3.17)  Time: 0.160s,  798.62/s  (0.161s,  793.89/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 129.147s\n",
      "Train: 17 [ 850/10009 (  8%)]  Loss: 3.09 (3.17)  Time: 0.160s,  799.98/s  (0.161s,  794.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 137.179s\n",
      "Train: 17 [ 900/10009 (  9%)]  Loss: 3.29 (3.17)  Time: 0.160s,  797.76/s  (0.161s,  794.11/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 145.230s\n",
      "Train: 17 [ 950/10009 (  9%)]  Loss: 3.13 (3.17)  Time: 0.160s,  798.51/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 153.301s\n",
      "Train: 17 [1000/10009 ( 10%)]  Loss: 3.01 (3.17)  Time: 0.160s,  797.71/s  (0.161s,  794.15/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 161.340s\n",
      "Train: 17 [1050/10009 ( 10%)]  Loss: 3.13 (3.17)  Time: 0.160s,  798.91/s  (0.161s,  794.25/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 169.376s\n",
      "Train: 17 [1100/10009 ( 11%)]  Loss: 3.19 (3.17)  Time: 0.161s,  797.30/s  (0.161s,  794.35/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 177.412s\n",
      "Train: 17 [1150/10009 ( 11%)]  Loss: 3.23 (3.17)  Time: 0.160s,  799.42/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 185.453s\n",
      "Train: 17 [1200/10009 ( 12%)]  Loss: 3.26 (3.17)  Time: 0.160s,  799.67/s  (0.161s,  794.47/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 193.497s\n",
      "Train: 17 [1250/10009 ( 12%)]  Loss: 3.30 (3.17)  Time: 0.164s,  778.23/s  (0.161s,  794.27/s)  LR: 0.000e+00  Data: 0.010 (0.007)Time: 201.604s\n",
      "Train: 17 [1300/10009 ( 13%)]  Loss: 3.15 (3.17)  Time: 0.161s,  796.06/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 209.651s\n",
      "Train: 17 [1350/10009 ( 13%)]  Loss: 3.03 (3.17)  Time: 0.162s,  791.40/s  (0.161s,  794.35/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 217.696s\n",
      "Train: 17 [1400/10009 ( 14%)]  Loss: 3.23 (3.17)  Time: 0.161s,  793.34/s  (0.161s,  794.22/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 225.792s\n",
      "Train: 17 [1450/10009 ( 14%)]  Loss: 3.29 (3.17)  Time: 0.162s,  788.73/s  (0.161s,  794.30/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 233.826s\n",
      "Train: 17 [1500/10009 ( 15%)]  Loss: 2.95 (3.17)  Time: 0.161s,  793.52/s  (0.161s,  794.32/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 241.876s\n",
      "Train: 17 [1550/10009 ( 15%)]  Loss: 3.16 (3.17)  Time: 0.159s,  803.02/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 249.936s\n",
      "Train: 17 [1600/10009 ( 16%)]  Loss: 3.16 (3.17)  Time: 0.160s,  797.88/s  (0.161s,  794.37/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 257.974s\n",
      "Train: 17 [1650/10009 ( 16%)]  Loss: 3.07 (3.17)  Time: 0.161s,  797.04/s  (0.161s,  794.35/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 266.038s\n",
      "Train: 17 [1700/10009 ( 17%)]  Loss: 3.11 (3.17)  Time: 0.160s,  799.73/s  (0.161s,  794.37/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 274.088s\n",
      "Train: 17 [1750/10009 ( 17%)]  Loss: 3.18 (3.17)  Time: 0.161s,  796.82/s  (0.161s,  794.37/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 282.143s\n",
      "Train: 17 [1800/10009 ( 18%)]  Loss: 3.09 (3.17)  Time: 0.160s,  797.69/s  (0.161s,  794.36/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 290.204s\n",
      "Train: 17 [1850/10009 ( 18%)]  Loss: 3.45 (3.17)  Time: 0.160s,  799.36/s  (0.161s,  794.38/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 298.255s\n",
      "Train: 17 [1900/10009 ( 19%)]  Loss: 3.23 (3.17)  Time: 0.161s,  796.64/s  (0.161s,  794.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 306.301s\n",
      "Train: 17 [1950/10009 ( 19%)]  Loss: 3.33 (3.17)  Time: 0.161s,  794.85/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 314.353s\n",
      "Train: 17 [2000/10009 ( 20%)]  Loss: 3.09 (3.17)  Time: 0.162s,  791.93/s  (0.161s,  794.28/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 322.467s\n",
      "Train: 17 [2050/10009 ( 20%)]  Loss: 2.96 (3.17)  Time: 0.162s,  790.87/s  (0.161s,  794.32/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 330.506s\n",
      "Train: 17 [2100/10009 ( 21%)]  Loss: 2.98 (3.17)  Time: 0.162s,  788.22/s  (0.161s,  794.32/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 338.563s\n",
      "Train: 17 [2150/10009 ( 21%)]  Loss: 3.19 (3.17)  Time: 0.161s,  796.03/s  (0.161s,  794.36/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 346.601s\n",
      "Train: 17 [2200/10009 ( 22%)]  Loss: 3.02 (3.17)  Time: 0.161s,  793.34/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 354.644s\n",
      "Train: 17 [2250/10009 ( 22%)]  Loss: 3.18 (3.17)  Time: 0.160s,  799.28/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 362.686s\n",
      "Train: 17 [2300/10009 ( 23%)]  Loss: 3.08 (3.17)  Time: 0.162s,  791.56/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 370.740s\n",
      "Train: 17 [2350/10009 ( 23%)]  Loss: 3.20 (3.17)  Time: 0.161s,  792.69/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 378.809s\n",
      "Train: 17 [2400/10009 ( 24%)]  Loss: 3.19 (3.17)  Time: 0.162s,  790.59/s  (0.161s,  794.38/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 386.877s\n",
      "Train: 17 [2450/10009 ( 24%)]  Loss: 3.33 (3.17)  Time: 0.159s,  803.09/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 394.911s\n",
      "Train: 17 [2500/10009 ( 25%)]  Loss: 2.88 (3.17)  Time: 0.160s,  799.87/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 402.947s\n",
      "Train: 17 [2550/10009 ( 25%)]  Loss: 3.24 (3.17)  Time: 0.161s,  795.31/s  (0.161s,  794.48/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 410.993s\n",
      "Train: 17 [2600/10009 ( 26%)]  Loss: 3.13 (3.17)  Time: 0.160s,  801.83/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 419.027s\n",
      "Train: 17 [2650/10009 ( 26%)]  Loss: 3.01 (3.17)  Time: 0.161s,  795.14/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 427.095s\n",
      "Train: 17 [2700/10009 ( 27%)]  Loss: 3.34 (3.17)  Time: 0.161s,  794.41/s  (0.161s,  794.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 435.146s\n",
      "Train: 17 [2750/10009 ( 27%)]  Loss: 3.31 (3.17)  Time: 0.162s,  791.74/s  (0.161s,  794.53/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 443.191s\n",
      "Train: 17 [2800/10009 ( 28%)]  Loss: 3.16 (3.17)  Time: 0.161s,  793.63/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 451.252s\n",
      "Train: 17 [2850/10009 ( 28%)]  Loss: 3.32 (3.17)  Time: 0.161s,  795.23/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 459.353s\n",
      "Train: 17 [2900/10009 ( 29%)]  Loss: 3.34 (3.17)  Time: 0.160s,  799.94/s  (0.161s,  794.12/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 467.593s\n",
      "Train: 17 [2950/10009 ( 29%)]  Loss: 3.44 (3.17)  Time: 0.161s,  795.55/s  (0.161s,  794.14/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 475.642s\n",
      "Train: 17 [3000/10009 ( 30%)]  Loss: 3.07 (3.17)  Time: 0.161s,  794.00/s  (0.161s,  794.17/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 483.682s\n",
      "Train: 17 [3050/10009 ( 30%)]  Loss: 3.16 (3.17)  Time: 0.161s,  793.07/s  (0.161s,  794.16/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 491.746s\n",
      "Train: 17 [3100/10009 ( 31%)]  Loss: 2.96 (3.17)  Time: 0.161s,  794.71/s  (0.161s,  794.20/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 499.782s\n",
      "Train: 17 [3150/10009 ( 31%)]  Loss: 2.99 (3.17)  Time: 0.161s,  794.48/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 507.818s\n",
      "Train: 17 [3200/10009 ( 32%)]  Loss: 3.02 (3.17)  Time: 0.162s,  789.56/s  (0.161s,  794.27/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 515.853s\n",
      "Train: 17 [3250/10009 ( 32%)]  Loss: 3.13 (3.17)  Time: 0.160s,  799.28/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 523.897s\n",
      "Train: 17 [3300/10009 ( 33%)]  Loss: 2.92 (3.17)  Time: 0.160s,  798.87/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 531.939s\n",
      "Train: 17 [3350/10009 ( 33%)]  Loss: 3.17 (3.17)  Time: 0.164s,  778.43/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 540.010s\n",
      "Train: 17 [3400/10009 ( 34%)]  Loss: 3.23 (3.17)  Time: 0.161s,  794.42/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 548.069s\n",
      "Train: 17 [3450/10009 ( 34%)]  Loss: 3.34 (3.17)  Time: 0.161s,  794.48/s  (0.161s,  794.32/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 556.106s\n",
      "Train: 17 [3500/10009 ( 35%)]  Loss: 2.95 (3.17)  Time: 0.161s,  793.04/s  (0.161s,  794.24/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 564.218s\n",
      "Train: 17 [3550/10009 ( 35%)]  Loss: 3.45 (3.17)  Time: 0.160s,  797.82/s  (0.161s,  794.24/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 572.277s\n",
      "Train: 17 [3600/10009 ( 36%)]  Loss: 3.09 (3.17)  Time: 0.160s,  798.91/s  (0.161s,  794.26/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 580.325s\n",
      "Train: 17 [3650/10009 ( 36%)]  Loss: 3.01 (3.17)  Time: 0.160s,  797.69/s  (0.161s,  794.27/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 588.370s\n",
      "Train: 17 [3700/10009 ( 37%)]  Loss: 3.41 (3.17)  Time: 0.160s,  797.80/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 596.419s\n",
      "Train: 17 [3750/10009 ( 37%)]  Loss: 3.22 (3.17)  Time: 0.160s,  798.52/s  (0.161s,  794.30/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 604.467s\n",
      "Train: 17 [3800/10009 ( 38%)]  Loss: 3.51 (3.17)  Time: 0.162s,  791.03/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 612.517s\n",
      "Train: 17 [3850/10009 ( 38%)]  Loss: 3.19 (3.17)  Time: 0.163s,  787.43/s  (0.161s,  794.33/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 620.558s\n",
      "Train: 17 [3900/10009 ( 39%)]  Loss: 3.08 (3.17)  Time: 0.163s,  787.37/s  (0.161s,  794.35/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 628.601s\n",
      "Train: 17 [3950/10009 ( 39%)]  Loss: 3.37 (3.17)  Time: 0.161s,  795.10/s  (0.161s,  794.36/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 636.648s\n",
      "Train: 17 [4000/10009 ( 40%)]  Loss: 3.13 (3.17)  Time: 0.161s,  797.18/s  (0.161s,  794.38/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 644.690s\n",
      "Train: 17 [4050/10009 ( 40%)]  Loss: 3.12 (3.17)  Time: 0.162s,  791.83/s  (0.161s,  794.38/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 652.743s\n",
      "Train: 17 [4100/10009 ( 41%)]  Loss: 3.07 (3.17)  Time: 0.161s,  794.64/s  (0.161s,  794.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 660.792s\n",
      "Train: 17 [4150/10009 ( 41%)]  Loss: 2.99 (3.17)  Time: 0.162s,  790.05/s  (0.161s,  794.39/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 668.845s\n",
      "Train: 17 [4200/10009 ( 42%)]  Loss: 3.01 (3.17)  Time: 0.160s,  800.53/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 676.900s\n",
      "Train: 17 [4250/10009 ( 42%)]  Loss: 3.24 (3.17)  Time: 0.162s,  789.36/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 684.951s\n",
      "Train: 17 [4300/10009 ( 43%)]  Loss: 3.38 (3.17)  Time: 0.160s,  799.28/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 692.995s\n",
      "Train: 17 [4350/10009 ( 43%)]  Loss: 3.11 (3.17)  Time: 0.161s,  794.02/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 701.037s\n",
      "Train: 17 [4400/10009 ( 44%)]  Loss: 3.02 (3.17)  Time: 0.161s,  796.38/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 709.124s\n",
      "Train: 17 [4450/10009 ( 44%)]  Loss: 3.18 (3.17)  Time: 0.160s,  799.19/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 717.164s\n",
      "Train: 17 [4500/10009 ( 45%)]  Loss: 3.03 (3.17)  Time: 0.160s,  799.49/s  (0.161s,  794.41/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 725.226s\n",
      "Train: 17 [4550/10009 ( 45%)]  Loss: 3.26 (3.17)  Time: 0.161s,  796.35/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 733.272s\n",
      "Train: 17 [4600/10009 ( 46%)]  Loss: 3.21 (3.17)  Time: 0.160s,  799.20/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 741.313s\n",
      "Train: 17 [4650/10009 ( 46%)]  Loss: 3.54 (3.17)  Time: 0.160s,  798.73/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 749.379s\n",
      "Train: 17 [4700/10009 ( 47%)]  Loss: 3.17 (3.17)  Time: 0.161s,  797.49/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 757.429s\n",
      "Train: 17 [4750/10009 ( 47%)]  Loss: 3.18 (3.17)  Time: 0.161s,  795.34/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 765.477s\n",
      "Train: 17 [4800/10009 ( 48%)]  Loss: 3.07 (3.17)  Time: 0.163s,  784.20/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.009 (0.006)Time: 773.532s\n",
      "Train: 17 [4850/10009 ( 48%)]  Loss: 3.18 (3.17)  Time: 0.161s,  796.56/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 781.587s\n",
      "Train: 17 [4900/10009 ( 49%)]  Loss: 3.28 (3.17)  Time: 0.160s,  798.88/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 789.637s\n",
      "Train: 17 [4950/10009 ( 49%)]  Loss: 3.13 (3.17)  Time: 0.160s,  797.69/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 797.712s\n",
      "Train: 17 [5000/10009 ( 50%)]  Loss: 3.09 (3.17)  Time: 0.162s,  789.59/s  (0.161s,  794.41/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 805.789s\n",
      "Train: 17 [5050/10009 ( 50%)]  Loss: 3.42 (3.17)  Time: 0.162s,  791.66/s  (0.161s,  794.40/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 813.854s\n",
      "Train: 17 [5100/10009 ( 51%)]  Loss: 3.25 (3.17)  Time: 0.161s,  793.48/s  (0.161s,  794.41/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 821.898s\n",
      "Train: 17 [5150/10009 ( 51%)]  Loss: 3.15 (3.17)  Time: 0.161s,  792.78/s  (0.161s,  794.42/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 829.949s\n",
      "Train: 17 [5200/10009 ( 52%)]  Loss: 3.30 (3.17)  Time: 0.160s,  797.63/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 837.987s\n",
      "Train: 17 [5250/10009 ( 52%)]  Loss: 3.42 (3.17)  Time: 0.160s,  799.41/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 846.031s\n",
      "Train: 17 [5300/10009 ( 53%)]  Loss: 3.16 (3.17)  Time: 0.162s,  788.66/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 854.091s\n",
      "Train: 17 [5350/10009 ( 53%)]  Loss: 2.94 (3.17)  Time: 0.162s,  790.75/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 862.144s\n",
      "Train: 17 [5400/10009 ( 54%)]  Loss: 3.27 (3.17)  Time: 0.161s,  797.31/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 870.202s\n",
      "Train: 17 [5450/10009 ( 54%)]  Loss: 3.34 (3.17)  Time: 0.160s,  800.25/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 878.276s\n",
      "Train: 17 [5500/10009 ( 55%)]  Loss: 3.10 (3.17)  Time: 0.160s,  799.97/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 886.333s\n",
      "Train: 17 [5550/10009 ( 55%)]  Loss: 3.34 (3.17)  Time: 0.161s,  795.85/s  (0.161s,  794.43/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 894.385s\n",
      "Train: 17 [5600/10009 ( 56%)]  Loss: 3.21 (3.17)  Time: 0.160s,  798.00/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 902.431s\n",
      "Train: 17 [5650/10009 ( 56%)]  Loss: 2.97 (3.17)  Time: 0.161s,  797.44/s  (0.161s,  794.44/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 910.488s\n",
      "Train: 17 [5700/10009 ( 57%)]  Loss: 3.35 (3.17)  Time: 0.160s,  798.15/s  (0.161s,  794.45/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 918.529s\n",
      "Train: 17 [5750/10009 ( 57%)]  Loss: 3.28 (3.17)  Time: 0.160s,  800.78/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 926.571s\n",
      "Train: 17 [5800/10009 ( 58%)]  Loss: 3.12 (3.17)  Time: 0.160s,  798.51/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 934.632s\n",
      "Train: 17 [5850/10009 ( 58%)]  Loss: 3.23 (3.17)  Time: 0.160s,  798.16/s  (0.161s,  794.46/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 942.680s\n",
      "Train: 17 [5900/10009 ( 59%)]  Loss: 3.35 (3.17)  Time: 0.160s,  800.15/s  (0.161s,  794.48/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 950.721s\n",
      "Train: 17 [5950/10009 ( 59%)]  Loss: 3.19 (3.17)  Time: 0.164s,  781.66/s  (0.161s,  794.48/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 958.776s\n",
      "Train: 17 [6000/10009 ( 60%)]  Loss: 2.92 (3.17)  Time: 0.161s,  794.00/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 966.821s\n",
      "Train: 17 [6050/10009 ( 60%)]  Loss: 2.88 (3.17)  Time: 0.168s,  762.33/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 974.873s\n",
      "Train: 17 [6100/10009 ( 61%)]  Loss: 3.11 (3.17)  Time: 0.161s,  796.49/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 982.926s\n",
      "Train: 17 [6150/10009 ( 61%)]  Loss: 3.11 (3.17)  Time: 0.160s,  801.39/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 990.970s\n",
      "Train: 17 [6200/10009 ( 62%)]  Loss: 3.17 (3.17)  Time: 0.161s,  795.03/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 999.024s\n",
      "Train: 17 [6250/10009 ( 62%)]  Loss: 3.39 (3.17)  Time: 0.161s,  795.99/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1007.079s\n",
      "Train: 17 [6300/10009 ( 63%)]  Loss: 3.25 (3.17)  Time: 0.161s,  792.98/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1015.131s\n",
      "Train: 17 [6350/10009 ( 63%)]  Loss: 3.33 (3.17)  Time: 0.162s,  788.00/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1023.187s\n",
      "Train: 17 [6400/10009 ( 64%)]  Loss: 2.91 (3.17)  Time: 0.160s,  797.89/s  (0.161s,  794.51/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1031.236s\n",
      "Train: 17 [6450/10009 ( 64%)]  Loss: 3.21 (3.17)  Time: 0.160s,  797.83/s  (0.161s,  794.51/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1039.292s\n",
      "Train: 17 [6500/10009 ( 65%)]  Loss: 3.21 (3.17)  Time: 0.161s,  797.33/s  (0.161s,  794.51/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1047.338s\n",
      "Train: 17 [6550/10009 ( 65%)]  Loss: 3.01 (3.17)  Time: 0.160s,  797.92/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1055.392s\n",
      "Train: 17 [6600/10009 ( 66%)]  Loss: 3.18 (3.17)  Time: 0.160s,  801.01/s  (0.161s,  794.53/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1063.435s\n",
      "Train: 17 [6650/10009 ( 66%)]  Loss: 3.11 (3.17)  Time: 0.160s,  798.86/s  (0.161s,  794.53/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1071.487s\n",
      "Train: 17 [6700/10009 ( 67%)]  Loss: 3.28 (3.17)  Time: 0.160s,  798.71/s  (0.161s,  794.54/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1079.521s\n",
      "Train: 17 [6750/10009 ( 67%)]  Loss: 3.12 (3.17)  Time: 0.160s,  798.20/s  (0.161s,  794.54/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1087.578s\n",
      "Train: 17 [6800/10009 ( 68%)]  Loss: 3.36 (3.17)  Time: 0.161s,  794.84/s  (0.161s,  794.55/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1095.624s\n",
      "Train: 17 [6850/10009 ( 68%)]  Loss: 3.49 (3.17)  Time: 0.161s,  794.66/s  (0.161s,  794.55/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1103.671s\n",
      "Train: 17 [6900/10009 ( 69%)]  Loss: 3.23 (3.17)  Time: 0.160s,  799.28/s  (0.161s,  794.57/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1111.708s\n",
      "Train: 17 [6950/10009 ( 69%)]  Loss: 3.34 (3.17)  Time: 0.161s,  792.62/s  (0.161s,  794.56/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1119.765s\n",
      "Train: 17 [7000/10009 ( 70%)]  Loss: 3.15 (3.17)  Time: 0.161s,  796.67/s  (0.161s,  794.57/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1127.818s\n",
      "Train: 17 [7050/10009 ( 70%)]  Loss: 3.21 (3.17)  Time: 0.160s,  798.48/s  (0.161s,  794.54/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1135.913s\n",
      "Train: 17 [7100/10009 ( 71%)]  Loss: 3.16 (3.17)  Time: 0.160s,  797.62/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1143.995s\n",
      "Train: 17 [7150/10009 ( 71%)]  Loss: 3.23 (3.17)  Time: 0.162s,  791.64/s  (0.161s,  794.51/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1152.059s\n",
      "Train: 17 [7200/10009 ( 72%)]  Loss: 3.12 (3.17)  Time: 0.161s,  797.04/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1160.110s\n",
      "Train: 17 [7250/10009 ( 72%)]  Loss: 3.18 (3.17)  Time: 0.160s,  799.95/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1168.196s\n",
      "Train: 17 [7300/10009 ( 73%)]  Loss: 2.87 (3.17)  Time: 0.161s,  795.17/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1176.248s\n",
      "Train: 17 [7350/10009 ( 73%)]  Loss: 3.09 (3.17)  Time: 0.162s,  790.68/s  (0.161s,  794.48/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1184.327s\n",
      "Train: 17 [7400/10009 ( 74%)]  Loss: 3.06 (3.17)  Time: 0.160s,  799.56/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1192.370s\n",
      "Train: 17 [7450/10009 ( 74%)]  Loss: 3.14 (3.17)  Time: 0.161s,  795.06/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1200.419s\n",
      "Train: 17 [7500/10009 ( 75%)]  Loss: 3.28 (3.17)  Time: 0.160s,  799.66/s  (0.161s,  794.48/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1208.498s\n",
      "Train: 17 [7550/10009 ( 75%)]  Loss: 3.19 (3.17)  Time: 0.161s,  795.27/s  (0.161s,  794.47/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1216.559s\n",
      "Train: 17 [7600/10009 ( 76%)]  Loss: 2.93 (3.17)  Time: 0.160s,  798.30/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1224.595s\n",
      "Train: 17 [7650/10009 ( 76%)]  Loss: 2.88 (3.17)  Time: 0.160s,  798.47/s  (0.161s,  794.49/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1232.648s\n",
      "Train: 17 [7700/10009 ( 77%)]  Loss: 3.40 (3.17)  Time: 0.160s,  798.94/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1240.692s\n",
      "Train: 17 [7750/10009 ( 77%)]  Loss: 3.19 (3.17)  Time: 0.162s,  791.44/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1248.747s\n",
      "Train: 17 [7800/10009 ( 78%)]  Loss: 2.95 (3.17)  Time: 0.161s,  794.80/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1256.791s\n",
      "Train: 17 [7850/10009 ( 78%)]  Loss: 3.24 (3.17)  Time: 0.161s,  792.79/s  (0.161s,  794.50/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1264.850s\n",
      "Train: 17 [7900/10009 ( 79%)]  Loss: 2.97 (3.17)  Time: 0.162s,  791.65/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1272.882s\n",
      "Train: 17 [7950/10009 ( 79%)]  Loss: 3.20 (3.17)  Time: 0.160s,  800.21/s  (0.161s,  794.53/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1280.920s\n",
      "Train: 17 [8000/10009 ( 80%)]  Loss: 3.08 (3.17)  Time: 0.160s,  798.59/s  (0.161s,  794.54/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1288.958s\n",
      "Train: 17 [8050/10009 ( 80%)]  Loss: 3.21 (3.17)  Time: 0.161s,  794.74/s  (0.161s,  794.53/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1297.018s\n",
      "Train: 17 [8100/10009 ( 81%)]  Loss: 3.58 (3.17)  Time: 0.160s,  798.14/s  (0.161s,  794.52/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1305.093s\n",
      "Train: 17 [8150/10009 ( 81%)]  Loss: 3.21 (3.17)  Time: 0.315s,  406.73/s  (0.162s,  792.28/s)  LR: 0.000e+00  Data: 0.161 (0.007)Time: 1316.871s\n",
      "Train: 17 [8200/10009 ( 82%)]  Loss: 3.15 (3.17)  Time: 0.162s,  790.75/s  (0.162s,  792.25/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1324.996s\n",
      "Train: 17 [8250/10009 ( 82%)]  Loss: 2.84 (3.16)  Time: 0.162s,  790.64/s  (0.162s,  792.27/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1333.029s\n",
      "Train: 17 [8300/10009 ( 83%)]  Loss: 3.19 (3.16)  Time: 0.161s,  795.91/s  (0.162s,  792.30/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1341.067s\n",
      "Train: 17 [8350/10009 ( 83%)]  Loss: 3.34 (3.16)  Time: 0.161s,  794.33/s  (0.162s,  792.32/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1349.105s\n",
      "Train: 17 [8400/10009 ( 84%)]  Loss: 2.89 (3.16)  Time: 0.160s,  798.68/s  (0.162s,  792.34/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1357.144s\n",
      "Train: 17 [8450/10009 ( 84%)]  Loss: 3.12 (3.17)  Time: 0.281s,  455.26/s  (0.162s,  792.25/s)  LR: 0.000e+00  Data: 0.128 (0.007)Time: 1365.375s\n",
      "Train: 17 [8500/10009 ( 85%)]  Loss: 3.18 (3.17)  Time: 0.159s,  803.79/s  (0.162s,  790.83/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1375.932s\n",
      "Train: 17 [8550/10009 ( 85%)]  Loss: 3.28 (3.17)  Time: 0.161s,  795.93/s  (0.162s,  790.84/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1384.005s\n",
      "Train: 17 [8600/10009 ( 86%)]  Loss: 3.13 (3.16)  Time: 0.161s,  796.47/s  (0.162s,  790.87/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1392.048s\n",
      "Train: 17 [8650/10009 ( 86%)]  Loss: 3.11 (3.16)  Time: 0.160s,  800.41/s  (0.162s,  790.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1400.083s\n",
      "Train: 17 [8700/10009 ( 87%)]  Loss: 3.09 (3.16)  Time: 0.160s,  800.26/s  (0.162s,  790.92/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1408.132s\n",
      "Train: 17 [8750/10009 ( 87%)]  Loss: 2.91 (3.16)  Time: 0.162s,  790.12/s  (0.162s,  790.96/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1416.164s\n",
      "Train: 17 [8800/10009 ( 88%)]  Loss: 3.02 (3.16)  Time: 0.160s,  798.75/s  (0.162s,  790.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1424.205s\n",
      "Train: 17 [8850/10009 ( 88%)]  Loss: 3.14 (3.16)  Time: 0.160s,  801.43/s  (0.162s,  791.02/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1432.236s\n",
      "Train: 17 [8900/10009 ( 89%)]  Loss: 3.13 (3.16)  Time: 0.160s,  797.67/s  (0.162s,  791.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1440.295s\n",
      "Train: 17 [8950/10009 ( 89%)]  Loss: 3.10 (3.16)  Time: 0.161s,  797.10/s  (0.162s,  791.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1448.332s\n",
      "Train: 17 [9000/10009 ( 90%)]  Loss: 3.10 (3.16)  Time: 0.162s,  788.60/s  (0.162s,  791.09/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1456.369s\n",
      "Train: 17 [9050/10009 ( 90%)]  Loss: 3.22 (3.16)  Time: 0.160s,  799.20/s  (0.162s,  791.12/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1464.409s\n",
      "Train: 17 [9100/10009 ( 91%)]  Loss: 3.44 (3.16)  Time: 0.163s,  787.65/s  (0.162s,  791.13/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1472.477s\n",
      "Train: 17 [9150/10009 ( 91%)]  Loss: 2.96 (3.16)  Time: 0.162s,  791.52/s  (0.162s,  791.16/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1480.520s\n",
      "Train: 17 [9200/10009 ( 92%)]  Loss: 2.71 (3.16)  Time: 0.160s,  797.95/s  (0.162s,  791.18/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1488.574s\n",
      "Train: 17 [9250/10009 ( 92%)]  Loss: 3.47 (3.16)  Time: 0.161s,  794.94/s  (0.162s,  791.19/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1496.638s\n",
      "Train: 17 [9300/10009 ( 93%)]  Loss: 3.00 (3.16)  Time: 0.163s,  786.30/s  (0.162s,  791.21/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1504.696s\n",
      "Train: 17 [9350/10009 ( 93%)]  Loss: 3.19 (3.16)  Time: 0.161s,  793.81/s  (0.162s,  791.21/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1512.776s\n",
      "Train: 17 [9400/10009 ( 94%)]  Loss: 3.31 (3.16)  Time: 0.162s,  792.51/s  (0.162s,  791.24/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1520.818s\n",
      "Train: 17 [9450/10009 ( 94%)]  Loss: 3.33 (3.16)  Time: 0.161s,  792.68/s  (0.162s,  791.26/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1528.858s\n",
      "Train: 17 [9500/10009 ( 95%)]  Loss: 3.14 (3.16)  Time: 0.160s,  801.50/s  (0.162s,  791.28/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1536.903s\n",
      "Train: 17 [9550/10009 ( 95%)]  Loss: 3.35 (3.16)  Time: 0.161s,  794.59/s  (0.162s,  791.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1544.942s\n",
      "Train: 17 [9600/10009 ( 96%)]  Loss: 3.17 (3.16)  Time: 0.160s,  802.03/s  (0.162s,  791.33/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1552.982s\n",
      "Train: 17 [9650/10009 ( 96%)]  Loss: 3.05 (3.16)  Time: 0.160s,  801.62/s  (0.162s,  791.35/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1561.024s\n",
      "Train: 17 [9700/10009 ( 97%)]  Loss: 3.36 (3.16)  Time: 0.160s,  801.86/s  (0.162s,  791.38/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1569.070s\n",
      "Train: 17 [9750/10009 ( 97%)]  Loss: 2.96 (3.16)  Time: 0.160s,  798.50/s  (0.162s,  791.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1577.120s\n",
      "Train: 17 [9800/10009 ( 98%)]  Loss: 2.90 (3.16)  Time: 0.160s,  798.78/s  (0.162s,  791.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1585.162s\n",
      "Train: 17 [9850/10009 ( 98%)]  Loss: 3.34 (3.16)  Time: 0.160s,  801.48/s  (0.162s,  791.44/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1593.202s\n",
      "Train: 17 [9900/10009 ( 99%)]  Loss: 3.21 (3.16)  Time: 0.162s,  791.16/s  (0.162s,  791.46/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1601.245s\n",
      "Train: 17 [9950/10009 ( 99%)]  Loss: 3.05 (3.16)  Time: 0.162s,  792.27/s  (0.162s,  791.48/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1609.293s\n",
      "Train: 17 [10000/10009 (100%)]  Loss: 3.39 (3.16)  Time: 0.206s,  621.31/s  (0.162s,  791.47/s)  LR: 0.000e+00  Data: 0.053 (0.007)Time: 1617.400s\n",
      "Test: [   0/390]  Time: 0.672 (0.672)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.143)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.053 (0.137)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.052 (0.141)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.052 (0.140)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.051 (0.138)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.052 (0.136)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.052 (0.135)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.035 (0.135)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 18 [   0/10009 (  0%)]  Loss: 3.23 (3.23)  Time: 0.852s,  150.30/s  (0.852s,  150.30/s)  LR: 0.000e+00  Data: 0.701 (0.701)Time: 0.852s\n",
      "Train: 18 [  50/10009 (  0%)]  Loss: 3.06 (3.18)  Time: 0.160s,  800.64/s  (0.173s,  739.39/s)  LR: 0.000e+00  Data: 0.007 (0.020)Time: 8.829s\n",
      "Train: 18 [ 100/10009 (  1%)]  Loss: 3.25 (3.17)  Time: 0.159s,  803.67/s  (0.166s,  769.54/s)  LR: 0.000e+00  Data: 0.006 (0.013)Time: 16.800s\n",
      "Train: 18 [ 150/10009 (  1%)]  Loss: 2.96 (3.16)  Time: 0.159s,  805.66/s  (0.164s,  779.92/s)  LR: 0.000e+00  Data: 0.005 (0.011)Time: 24.783s\n",
      "Train: 18 [ 200/10009 (  2%)]  Loss: 3.28 (3.17)  Time: 0.160s,  800.60/s  (0.163s,  784.78/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 32.784s\n",
      "Train: 18 [ 250/10009 (  2%)]  Loss: 3.07 (3.17)  Time: 0.162s,  792.47/s  (0.163s,  787.60/s)  LR: 0.000e+00  Data: 0.008 (0.009)Time: 40.792s\n",
      "Train: 18 [ 300/10009 (  3%)]  Loss: 3.04 (3.17)  Time: 0.161s,  796.70/s  (0.162s,  789.61/s)  LR: 0.000e+00  Data: 0.007 (0.009)Time: 48.794s\n",
      "Train: 18 [ 350/10009 (  3%)]  Loss: 3.12 (3.17)  Time: 0.161s,  797.37/s  (0.162s,  790.80/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 56.813s\n",
      "Train: 18 [ 400/10009 (  4%)]  Loss: 3.06 (3.17)  Time: 0.161s,  796.95/s  (0.162s,  791.56/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 64.844s\n",
      "Train: 18 [ 450/10009 (  4%)]  Loss: 2.90 (3.17)  Time: 0.160s,  799.60/s  (0.162s,  791.97/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 72.892s\n",
      "Train: 18 [ 500/10009 (  5%)]  Loss: 2.80 (3.17)  Time: 0.160s,  799.29/s  (0.162s,  792.42/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 80.927s\n",
      "Train: 18 [ 550/10009 (  5%)]  Loss: 3.61 (3.17)  Time: 0.161s,  796.75/s  (0.161s,  792.74/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 88.967s\n",
      "Train: 18 [ 600/10009 (  6%)]  Loss: 3.30 (3.17)  Time: 0.163s,  787.14/s  (0.161s,  792.98/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 97.011s\n",
      "Train: 18 [ 650/10009 (  6%)]  Loss: 2.89 (3.17)  Time: 0.160s,  798.72/s  (0.161s,  793.26/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 105.046s\n",
      "Train: 18 [ 700/10009 (  7%)]  Loss: 3.20 (3.17)  Time: 0.160s,  801.11/s  (0.161s,  793.47/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 113.083s\n",
      "Train: 18 [ 750/10009 (  7%)]  Loss: 3.07 (3.17)  Time: 0.161s,  797.35/s  (0.161s,  793.58/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 121.133s\n",
      "Train: 18 [ 800/10009 (  8%)]  Loss: 3.13 (3.17)  Time: 0.160s,  800.18/s  (0.161s,  793.57/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 129.199s\n",
      "Train: 18 [ 850/10009 (  8%)]  Loss: 3.00 (3.16)  Time: 0.162s,  788.84/s  (0.161s,  793.70/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 137.241s\n",
      "Train: 18 [ 900/10009 (  9%)]  Loss: 3.15 (3.16)  Time: 0.162s,  791.00/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 145.278s\n",
      "Train: 18 [ 950/10009 (  9%)]  Loss: 3.07 (3.17)  Time: 0.161s,  793.55/s  (0.161s,  793.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 153.311s\n",
      "Train: 18 [1000/10009 ( 10%)]  Loss: 3.30 (3.16)  Time: 0.161s,  795.09/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 161.360s\n",
      "Train: 18 [1050/10009 ( 10%)]  Loss: 3.31 (3.16)  Time: 0.162s,  790.28/s  (0.161s,  794.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 169.402s\n",
      "Train: 18 [1100/10009 ( 11%)]  Loss: 3.04 (3.16)  Time: 0.162s,  788.90/s  (0.161s,  794.16/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 177.456s\n",
      "Train: 18 [1150/10009 ( 11%)]  Loss: 3.46 (3.16)  Time: 0.160s,  798.06/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 185.533s\n",
      "Train: 18 [1200/10009 ( 12%)]  Loss: 3.51 (3.16)  Time: 0.160s,  799.59/s  (0.161s,  794.11/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 193.586s\n",
      "Train: 18 [1250/10009 ( 12%)]  Loss: 3.17 (3.16)  Time: 0.161s,  795.15/s  (0.161s,  794.17/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 201.629s\n",
      "Train: 18 [1300/10009 ( 13%)]  Loss: 3.02 (3.16)  Time: 0.163s,  784.02/s  (0.161s,  794.18/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 209.685s\n",
      "Train: 18 [1350/10009 ( 13%)]  Loss: 3.14 (3.16)  Time: 0.162s,  791.57/s  (0.161s,  794.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 217.755s\n",
      "Train: 18 [1400/10009 ( 14%)]  Loss: 3.19 (3.16)  Time: 0.160s,  797.55/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 225.834s\n",
      "Train: 18 [1450/10009 ( 14%)]  Loss: 3.29 (3.16)  Time: 0.160s,  800.49/s  (0.161s,  794.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 233.897s\n",
      "Train: 18 [1500/10009 ( 15%)]  Loss: 3.06 (3.16)  Time: 0.161s,  796.53/s  (0.161s,  794.11/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 241.941s\n",
      "Train: 18 [1550/10009 ( 15%)]  Loss: 3.21 (3.16)  Time: 0.162s,  790.26/s  (0.161s,  794.18/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 249.979s\n",
      "Train: 18 [1600/10009 ( 16%)]  Loss: 3.24 (3.16)  Time: 0.160s,  799.34/s  (0.161s,  794.23/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 258.022s\n",
      "Train: 18 [1650/10009 ( 16%)]  Loss: 3.08 (3.16)  Time: 0.162s,  789.56/s  (0.161s,  794.26/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 266.068s\n",
      "Train: 18 [1700/10009 ( 17%)]  Loss: 3.26 (3.16)  Time: 0.161s,  796.26/s  (0.161s,  794.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 274.115s\n",
      "Train: 18 [1750/10009 ( 17%)]  Loss: 2.91 (3.16)  Time: 0.161s,  796.74/s  (0.161s,  794.19/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 282.211s\n",
      "Train: 18 [1800/10009 ( 18%)]  Loss: 3.23 (3.16)  Time: 0.163s,  784.69/s  (0.161s,  794.19/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 290.267s\n",
      "Train: 18 [1850/10009 ( 18%)]  Loss: 3.19 (3.16)  Time: 0.161s,  796.56/s  (0.161s,  794.25/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 298.305s\n",
      "Train: 18 [1900/10009 ( 19%)]  Loss: 3.63 (3.16)  Time: 0.161s,  794.56/s  (0.161s,  794.27/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 306.353s\n",
      "Train: 18 [1950/10009 ( 19%)]  Loss: 2.88 (3.16)  Time: 0.161s,  795.46/s  (0.161s,  794.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 314.397s\n",
      "Train: 18 [2000/10009 ( 20%)]  Loss: 3.25 (3.16)  Time: 0.161s,  793.58/s  (0.161s,  794.16/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 322.513s\n",
      "Train: 18 [2050/10009 ( 20%)]  Loss: 3.22 (3.16)  Time: 0.160s,  799.09/s  (0.161s,  793.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 330.643s\n",
      "Train: 18 [2100/10009 ( 21%)]  Loss: 3.19 (3.16)  Time: 0.159s,  803.24/s  (0.161s,  794.02/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 338.691s\n",
      "Train: 18 [2150/10009 ( 21%)]  Loss: 3.18 (3.16)  Time: 0.160s,  797.70/s  (0.161s,  794.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 346.732s\n",
      "Train: 18 [2200/10009 ( 22%)]  Loss: 3.29 (3.16)  Time: 0.161s,  797.17/s  (0.161s,  794.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 354.812s\n",
      "Train: 18 [2250/10009 ( 22%)]  Loss: 2.83 (3.16)  Time: 0.160s,  800.62/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 362.851s\n",
      "Train: 18 [2300/10009 ( 23%)]  Loss: 3.41 (3.16)  Time: 0.161s,  792.86/s  (0.161s,  794.09/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 370.898s\n",
      "Train: 18 [2350/10009 ( 23%)]  Loss: 3.28 (3.16)  Time: 0.163s,  785.58/s  (0.161s,  794.11/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 378.951s\n",
      "Train: 18 [2400/10009 ( 24%)]  Loss: 3.01 (3.17)  Time: 0.162s,  791.63/s  (0.161s,  794.10/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 387.016s\n",
      "Train: 18 [2450/10009 ( 24%)]  Loss: 3.28 (3.16)  Time: 0.161s,  796.88/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 395.088s\n",
      "Train: 18 [2500/10009 ( 25%)]  Loss: 3.29 (3.16)  Time: 0.161s,  794.89/s  (0.161s,  794.02/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 403.173s\n",
      "Train: 18 [2550/10009 ( 25%)]  Loss: 3.14 (3.16)  Time: 0.160s,  801.53/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 411.236s\n",
      "Train: 18 [2600/10009 ( 26%)]  Loss: 3.24 (3.16)  Time: 0.161s,  793.64/s  (0.161s,  794.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 419.292s\n",
      "Train: 18 [2650/10009 ( 26%)]  Loss: 3.15 (3.16)  Time: 0.161s,  794.04/s  (0.161s,  794.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 427.341s\n",
      "Train: 18 [2700/10009 ( 27%)]  Loss: 3.09 (3.16)  Time: 0.161s,  792.72/s  (0.161s,  793.95/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 435.452s\n",
      "Train: 18 [2750/10009 ( 27%)]  Loss: 3.15 (3.16)  Time: 0.160s,  797.76/s  (0.161s,  793.93/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 443.525s\n",
      "Train: 18 [2800/10009 ( 28%)]  Loss: 3.40 (3.16)  Time: 0.162s,  792.26/s  (0.161s,  793.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 451.590s\n",
      "Train: 18 [2850/10009 ( 28%)]  Loss: 3.15 (3.16)  Time: 0.162s,  790.23/s  (0.161s,  793.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 459.654s\n",
      "Train: 18 [2900/10009 ( 29%)]  Loss: 2.91 (3.16)  Time: 0.160s,  797.96/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 467.753s\n",
      "Train: 18 [2950/10009 ( 29%)]  Loss: 3.36 (3.16)  Time: 0.160s,  799.43/s  (0.161s,  793.88/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 475.798s\n",
      "Train: 18 [3000/10009 ( 30%)]  Loss: 3.10 (3.16)  Time: 0.160s,  798.73/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 483.869s\n",
      "Train: 18 [3050/10009 ( 30%)]  Loss: 3.05 (3.16)  Time: 0.161s,  793.82/s  (0.161s,  793.89/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 491.915s\n",
      "Train: 18 [3100/10009 ( 31%)]  Loss: 3.28 (3.16)  Time: 0.162s,  789.51/s  (0.161s,  793.91/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 499.968s\n",
      "Train: 18 [3150/10009 ( 31%)]  Loss: 3.24 (3.16)  Time: 0.162s,  788.99/s  (0.161s,  793.91/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 508.024s\n",
      "Train: 18 [3200/10009 ( 32%)]  Loss: 3.20 (3.16)  Time: 0.163s,  784.25/s  (0.161s,  793.93/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 516.074s\n",
      "Train: 18 [3250/10009 ( 32%)]  Loss: 2.60 (3.16)  Time: 0.162s,  790.98/s  (0.161s,  793.94/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 524.132s\n",
      "Train: 18 [3300/10009 ( 33%)]  Loss: 3.26 (3.16)  Time: 0.160s,  799.28/s  (0.161s,  793.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 532.176s\n",
      "Train: 18 [3350/10009 ( 33%)]  Loss: 2.78 (3.16)  Time: 0.161s,  795.92/s  (0.161s,  793.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 540.220s\n",
      "Train: 18 [3400/10009 ( 34%)]  Loss: 3.18 (3.17)  Time: 0.162s,  789.20/s  (0.161s,  794.03/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 548.252s\n",
      "Train: 18 [3450/10009 ( 34%)]  Loss: 3.28 (3.17)  Time: 0.161s,  795.97/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 556.294s\n",
      "Train: 18 [3500/10009 ( 35%)]  Loss: 3.22 (3.17)  Time: 0.161s,  794.30/s  (0.161s,  794.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 564.360s\n",
      "Train: 18 [3550/10009 ( 35%)]  Loss: 3.01 (3.17)  Time: 0.161s,  796.19/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 572.414s\n",
      "Train: 18 [3600/10009 ( 36%)]  Loss: 2.88 (3.17)  Time: 0.161s,  795.11/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 580.464s\n",
      "Train: 18 [3650/10009 ( 36%)]  Loss: 3.24 (3.17)  Time: 0.161s,  796.79/s  (0.161s,  794.09/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 588.507s\n",
      "Train: 18 [3700/10009 ( 37%)]  Loss: 3.08 (3.17)  Time: 0.162s,  792.09/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 596.624s\n",
      "Train: 18 [3750/10009 ( 37%)]  Loss: 3.10 (3.17)  Time: 0.161s,  793.79/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 604.792s\n",
      "Train: 18 [3800/10009 ( 38%)]  Loss: 3.20 (3.17)  Time: 0.161s,  796.95/s  (0.161s,  793.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 612.832s\n",
      "Train: 18 [3850/10009 ( 38%)]  Loss: 3.05 (3.17)  Time: 0.162s,  789.59/s  (0.161s,  793.92/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 620.876s\n",
      "Train: 18 [3900/10009 ( 39%)]  Loss: 2.96 (3.17)  Time: 0.161s,  795.48/s  (0.161s,  793.94/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 628.921s\n",
      "Train: 18 [3950/10009 ( 39%)]  Loss: 3.51 (3.17)  Time: 0.162s,  792.27/s  (0.161s,  793.95/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 636.972s\n",
      "Train: 18 [4000/10009 ( 40%)]  Loss: 3.05 (3.17)  Time: 0.163s,  787.58/s  (0.161s,  793.98/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 645.014s\n",
      "Train: 18 [4050/10009 ( 40%)]  Loss: 3.26 (3.17)  Time: 0.160s,  799.84/s  (0.161s,  794.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 653.055s\n",
      "Train: 18 [4100/10009 ( 41%)]  Loss: 2.89 (3.16)  Time: 0.162s,  788.55/s  (0.161s,  794.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 661.115s\n",
      "Train: 18 [4150/10009 ( 41%)]  Loss: 3.08 (3.16)  Time: 0.162s,  790.76/s  (0.161s,  793.99/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 669.189s\n",
      "Train: 18 [4200/10009 ( 42%)]  Loss: 3.33 (3.17)  Time: 0.161s,  795.23/s  (0.161s,  793.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 677.244s\n",
      "Train: 18 [4250/10009 ( 42%)]  Loss: 2.93 (3.17)  Time: 0.161s,  794.23/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 685.293s\n",
      "Train: 18 [4300/10009 ( 43%)]  Loss: 3.00 (3.16)  Time: 0.161s,  792.66/s  (0.161s,  794.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 693.355s\n",
      "Train: 18 [4350/10009 ( 43%)]  Loss: 3.42 (3.16)  Time: 0.161s,  796.72/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 701.406s\n",
      "Train: 18 [4400/10009 ( 44%)]  Loss: 3.11 (3.16)  Time: 0.160s,  797.81/s  (0.161s,  794.03/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 709.450s\n",
      "Train: 18 [4450/10009 ( 44%)]  Loss: 2.77 (3.16)  Time: 0.161s,  795.26/s  (0.161s,  794.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 717.502s\n",
      "Train: 18 [4500/10009 ( 45%)]  Loss: 3.23 (3.16)  Time: 0.161s,  796.56/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 725.552s\n",
      "Train: 18 [4550/10009 ( 45%)]  Loss: 3.44 (3.16)  Time: 0.162s,  792.55/s  (0.161s,  794.03/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 733.633s\n",
      "Train: 18 [4600/10009 ( 46%)]  Loss: 3.18 (3.16)  Time: 0.160s,  798.01/s  (0.161s,  794.03/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 741.689s\n",
      "Train: 18 [4650/10009 ( 46%)]  Loss: 3.00 (3.16)  Time: 0.161s,  796.70/s  (0.161s,  794.03/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 749.750s\n",
      "Train: 18 [4700/10009 ( 47%)]  Loss: 3.10 (3.16)  Time: 0.169s,  759.15/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.013 (0.007)Time: 757.829s\n",
      "Train: 18 [4750/10009 ( 47%)]  Loss: 3.15 (3.16)  Time: 0.161s,  796.01/s  (0.161s,  793.99/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 765.910s\n",
      "Train: 18 [4800/10009 ( 48%)]  Loss: 3.30 (3.16)  Time: 0.161s,  796.55/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 773.953s\n",
      "Train: 18 [4850/10009 ( 48%)]  Loss: 3.31 (3.16)  Time: 0.162s,  792.09/s  (0.161s,  794.00/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 782.026s\n",
      "Train: 18 [4900/10009 ( 49%)]  Loss: 3.36 (3.16)  Time: 0.160s,  798.00/s  (0.161s,  794.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 790.071s\n",
      "Train: 18 [4950/10009 ( 49%)]  Loss: 3.07 (3.16)  Time: 0.160s,  799.55/s  (0.161s,  794.04/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 798.106s\n",
      "Train: 18 [5000/10009 ( 50%)]  Loss: 3.45 (3.16)  Time: 0.162s,  790.57/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 806.151s\n",
      "Train: 18 [5050/10009 ( 50%)]  Loss: 3.03 (3.16)  Time: 0.161s,  795.82/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 814.193s\n",
      "Train: 18 [5100/10009 ( 51%)]  Loss: 3.14 (3.16)  Time: 0.166s,  773.36/s  (0.161s,  794.06/s)  LR: 0.000e+00  Data: 0.010 (0.007)Time: 822.258s\n",
      "Train: 18 [5150/10009 ( 51%)]  Loss: 3.14 (3.16)  Time: 0.161s,  793.69/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 830.305s\n",
      "Train: 18 [5200/10009 ( 52%)]  Loss: 2.86 (3.16)  Time: 0.163s,  785.59/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 838.365s\n",
      "Train: 18 [5250/10009 ( 52%)]  Loss: 3.37 (3.16)  Time: 0.160s,  798.01/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 846.456s\n",
      "Train: 18 [5300/10009 ( 53%)]  Loss: 3.21 (3.16)  Time: 0.160s,  797.76/s  (0.161s,  794.06/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 854.497s\n",
      "Train: 18 [5350/10009 ( 53%)]  Loss: 3.11 (3.16)  Time: 0.161s,  795.22/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 862.539s\n",
      "Train: 18 [5400/10009 ( 54%)]  Loss: 3.21 (3.16)  Time: 0.160s,  798.90/s  (0.161s,  794.06/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 870.624s\n",
      "Train: 18 [5450/10009 ( 54%)]  Loss: 3.23 (3.16)  Time: 0.160s,  799.34/s  (0.161s,  794.07/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 878.669s\n",
      "Train: 18 [5500/10009 ( 55%)]  Loss: 3.05 (3.16)  Time: 0.164s,  779.79/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 886.724s\n",
      "Train: 18 [5550/10009 ( 55%)]  Loss: 3.25 (3.16)  Time: 0.162s,  788.52/s  (0.161s,  792.67/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 896.369s\n",
      "Train: 18 [5600/10009 ( 56%)]  Loss: 3.34 (3.16)  Time: 0.161s,  796.71/s  (0.162s,  791.25/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 906.072s\n",
      "Train: 18 [5650/10009 ( 56%)]  Loss: 3.00 (3.16)  Time: 0.161s,  797.40/s  (0.162s,  791.28/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 914.116s\n",
      "Train: 18 [5700/10009 ( 57%)]  Loss: 3.35 (3.16)  Time: 0.161s,  793.77/s  (0.162s,  791.33/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 922.156s\n",
      "Train: 18 [5750/10009 ( 57%)]  Loss: 3.13 (3.16)  Time: 0.160s,  801.86/s  (0.162s,  791.36/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 930.203s\n",
      "Train: 18 [5800/10009 ( 58%)]  Loss: 3.24 (3.16)  Time: 0.162s,  791.52/s  (0.162s,  791.40/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 938.243s\n",
      "Train: 18 [5850/10009 ( 58%)]  Loss: 3.16 (3.16)  Time: 0.161s,  793.73/s  (0.162s,  791.44/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 946.284s\n",
      "Train: 18 [5900/10009 ( 59%)]  Loss: 3.11 (3.16)  Time: 0.163s,  785.30/s  (0.162s,  791.48/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 954.327s\n",
      "Train: 18 [5950/10009 ( 59%)]  Loss: 3.53 (3.16)  Time: 0.162s,  791.90/s  (0.162s,  791.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 962.367s\n",
      "Train: 18 [6000/10009 ( 60%)]  Loss: 3.14 (3.16)  Time: 0.161s,  796.82/s  (0.162s,  791.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 970.441s\n",
      "Train: 18 [6050/10009 ( 60%)]  Loss: 3.28 (3.16)  Time: 0.162s,  791.65/s  (0.162s,  791.54/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 978.508s\n",
      "Train: 18 [6100/10009 ( 61%)]  Loss: 3.13 (3.16)  Time: 0.160s,  800.70/s  (0.162s,  791.56/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 986.562s\n",
      "Train: 18 [6150/10009 ( 61%)]  Loss: 3.17 (3.16)  Time: 0.162s,  791.74/s  (0.162s,  791.59/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 994.610s\n",
      "Train: 18 [6200/10009 ( 62%)]  Loss: 2.99 (3.16)  Time: 0.166s,  771.22/s  (0.162s,  791.61/s)  LR: 0.000e+00  Data: 0.013 (0.007)Time: 1002.670s\n",
      "Train: 18 [6250/10009 ( 62%)]  Loss: 2.92 (3.16)  Time: 0.161s,  797.22/s  (0.162s,  791.65/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1010.711s\n",
      "Train: 18 [6300/10009 ( 63%)]  Loss: 3.02 (3.16)  Time: 0.161s,  796.32/s  (0.162s,  791.68/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1018.755s\n",
      "Train: 18 [6350/10009 ( 63%)]  Loss: 3.16 (3.16)  Time: 0.160s,  798.12/s  (0.162s,  791.66/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1026.859s\n",
      "Train: 18 [6400/10009 ( 64%)]  Loss: 3.21 (3.16)  Time: 0.160s,  800.41/s  (0.162s,  791.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1034.931s\n",
      "Train: 18 [6450/10009 ( 64%)]  Loss: 3.19 (3.16)  Time: 0.160s,  799.95/s  (0.162s,  791.71/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1042.970s\n",
      "Train: 18 [6500/10009 ( 65%)]  Loss: 3.16 (3.16)  Time: 0.161s,  794.98/s  (0.162s,  791.73/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1051.020s\n",
      "Train: 18 [6550/10009 ( 65%)]  Loss: 3.63 (3.16)  Time: 0.160s,  799.46/s  (0.162s,  791.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1059.081s\n",
      "Train: 18 [6600/10009 ( 66%)]  Loss: 3.15 (3.16)  Time: 0.161s,  795.40/s  (0.162s,  791.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1067.157s\n",
      "Train: 18 [6650/10009 ( 66%)]  Loss: 3.37 (3.16)  Time: 0.160s,  798.52/s  (0.162s,  791.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1075.214s\n",
      "Train: 18 [6700/10009 ( 67%)]  Loss: 3.24 (3.16)  Time: 0.161s,  793.13/s  (0.162s,  791.78/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1083.294s\n",
      "Train: 18 [6750/10009 ( 67%)]  Loss: 3.21 (3.16)  Time: 0.161s,  793.57/s  (0.162s,  791.79/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1091.355s\n",
      "Train: 18 [6800/10009 ( 68%)]  Loss: 3.18 (3.16)  Time: 0.160s,  799.16/s  (0.162s,  791.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1099.397s\n",
      "Train: 18 [6850/10009 ( 68%)]  Loss: 2.87 (3.16)  Time: 0.160s,  797.63/s  (0.162s,  791.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1107.469s\n",
      "Train: 18 [6900/10009 ( 69%)]  Loss: 3.30 (3.16)  Time: 0.162s,  787.70/s  (0.162s,  791.85/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1115.517s\n",
      "Train: 18 [6950/10009 ( 69%)]  Loss: 3.21 (3.16)  Time: 0.160s,  798.51/s  (0.162s,  791.88/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1123.568s\n",
      "Train: 18 [7000/10009 ( 70%)]  Loss: 2.99 (3.16)  Time: 0.160s,  797.71/s  (0.162s,  791.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1131.611s\n",
      "Train: 18 [7050/10009 ( 70%)]  Loss: 3.10 (3.16)  Time: 0.161s,  794.73/s  (0.162s,  791.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1139.669s\n",
      "Train: 18 [7100/10009 ( 71%)]  Loss: 2.93 (3.16)  Time: 0.160s,  800.42/s  (0.162s,  791.95/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1147.713s\n",
      "Train: 18 [7150/10009 ( 71%)]  Loss: 2.72 (3.16)  Time: 0.160s,  799.41/s  (0.162s,  791.97/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1155.754s\n",
      "Train: 18 [7200/10009 ( 72%)]  Loss: 3.24 (3.16)  Time: 0.162s,  791.67/s  (0.162s,  791.99/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1163.816s\n",
      "Train: 18 [7250/10009 ( 72%)]  Loss: 3.05 (3.16)  Time: 0.162s,  790.66/s  (0.162s,  792.01/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1171.864s\n",
      "Train: 18 [7300/10009 ( 73%)]  Loss: 3.12 (3.16)  Time: 0.160s,  798.70/s  (0.162s,  792.02/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1179.929s\n",
      "Train: 18 [7350/10009 ( 73%)]  Loss: 3.10 (3.16)  Time: 0.161s,  796.38/s  (0.162s,  792.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1187.972s\n",
      "Train: 18 [7400/10009 ( 74%)]  Loss: 3.29 (3.16)  Time: 0.160s,  799.35/s  (0.162s,  792.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1196.058s\n",
      "Train: 18 [7450/10009 ( 74%)]  Loss: 3.19 (3.16)  Time: 0.161s,  797.33/s  (0.162s,  792.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1204.100s\n",
      "Train: 18 [7500/10009 ( 75%)]  Loss: 3.49 (3.16)  Time: 0.160s,  798.54/s  (0.162s,  792.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1212.165s\n",
      "Train: 18 [7550/10009 ( 75%)]  Loss: 3.08 (3.16)  Time: 0.162s,  791.00/s  (0.162s,  792.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1220.298s\n",
      "Train: 18 [7600/10009 ( 76%)]  Loss: 3.32 (3.16)  Time: 0.161s,  793.37/s  (0.162s,  792.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1228.348s\n",
      "Train: 18 [7650/10009 ( 76%)]  Loss: 2.87 (3.16)  Time: 0.161s,  796.66/s  (0.162s,  792.09/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1236.388s\n",
      "Train: 18 [7700/10009 ( 77%)]  Loss: 2.92 (3.16)  Time: 0.160s,  798.63/s  (0.162s,  792.11/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1244.428s\n",
      "Train: 18 [7750/10009 ( 77%)]  Loss: 3.38 (3.16)  Time: 0.161s,  796.36/s  (0.162s,  792.13/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1252.476s\n",
      "Train: 18 [7800/10009 ( 78%)]  Loss: 3.17 (3.16)  Time: 0.432s,  296.05/s  (0.162s,  791.07/s)  LR: 0.000e+00  Data: 0.278 (0.007)Time: 1262.244s\n",
      "Train: 18 [7850/10009 ( 78%)]  Loss: 3.57 (3.16)  Time: 0.161s,  797.13/s  (0.162s,  790.63/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1271.038s\n",
      "Train: 18 [7900/10009 ( 79%)]  Loss: 3.41 (3.16)  Time: 0.161s,  795.16/s  (0.162s,  790.66/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1279.085s\n",
      "Train: 18 [7950/10009 ( 79%)]  Loss: 3.28 (3.16)  Time: 0.161s,  796.12/s  (0.162s,  790.70/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1287.124s\n",
      "Train: 18 [8000/10009 ( 80%)]  Loss: 3.37 (3.16)  Time: 0.161s,  792.59/s  (0.162s,  790.72/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1295.176s\n",
      "Train: 18 [8050/10009 ( 80%)]  Loss: 3.16 (3.16)  Time: 0.161s,  796.08/s  (0.162s,  790.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1303.222s\n",
      "Train: 18 [8100/10009 ( 81%)]  Loss: 3.47 (3.16)  Time: 0.161s,  796.03/s  (0.162s,  790.77/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1311.281s\n",
      "Train: 18 [8150/10009 ( 81%)]  Loss: 3.31 (3.16)  Time: 0.162s,  789.54/s  (0.162s,  790.75/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1319.412s\n",
      "Train: 18 [8200/10009 ( 82%)]  Loss: 3.12 (3.16)  Time: 0.160s,  797.74/s  (0.162s,  790.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1327.467s\n",
      "Train: 18 [8250/10009 ( 82%)]  Loss: 2.91 (3.16)  Time: 0.160s,  797.94/s  (0.162s,  790.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1335.511s\n",
      "Train: 18 [8300/10009 ( 83%)]  Loss: 3.16 (3.16)  Time: 0.162s,  788.10/s  (0.162s,  790.81/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1343.593s\n",
      "Train: 18 [8350/10009 ( 83%)]  Loss: 2.94 (3.16)  Time: 0.161s,  794.01/s  (0.162s,  790.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1351.648s\n",
      "Train: 18 [8400/10009 ( 84%)]  Loss: 3.17 (3.16)  Time: 0.161s,  793.98/s  (0.162s,  790.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1359.701s\n",
      "Train: 18 [8450/10009 ( 84%)]  Loss: 3.20 (3.16)  Time: 0.160s,  798.14/s  (0.162s,  790.88/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1367.750s\n",
      "Train: 18 [8500/10009 ( 85%)]  Loss: 3.35 (3.16)  Time: 0.161s,  793.98/s  (0.162s,  790.91/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1375.793s\n",
      "Train: 18 [8550/10009 ( 85%)]  Loss: 2.94 (3.16)  Time: 0.161s,  795.54/s  (0.162s,  790.93/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1383.847s\n",
      "Train: 18 [8600/10009 ( 86%)]  Loss: 3.40 (3.16)  Time: 0.162s,  790.24/s  (0.162s,  790.95/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1391.906s\n",
      "Train: 18 [8650/10009 ( 86%)]  Loss: 3.27 (3.16)  Time: 0.161s,  793.29/s  (0.162s,  790.97/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1399.959s\n",
      "Train: 18 [8700/10009 ( 87%)]  Loss: 3.26 (3.16)  Time: 0.163s,  787.38/s  (0.162s,  790.96/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1408.060s\n",
      "Train: 18 [8750/10009 ( 87%)]  Loss: 3.05 (3.16)  Time: 0.167s,  766.68/s  (0.162s,  790.98/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1416.131s\n",
      "Train: 18 [8800/10009 ( 88%)]  Loss: 3.13 (3.16)  Time: 0.160s,  797.65/s  (0.162s,  791.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1424.180s\n",
      "Train: 18 [8850/10009 ( 88%)]  Loss: 3.36 (3.16)  Time: 0.161s,  793.30/s  (0.162s,  790.98/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1432.296s\n",
      "Train: 18 [8900/10009 ( 89%)]  Loss: 3.08 (3.16)  Time: 0.161s,  795.50/s  (0.162s,  790.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1440.386s\n",
      "Train: 18 [8950/10009 ( 89%)]  Loss: 3.35 (3.16)  Time: 0.160s,  799.18/s  (0.162s,  790.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1448.472s\n",
      "Train: 18 [9000/10009 ( 90%)]  Loss: 2.88 (3.16)  Time: 0.161s,  794.57/s  (0.162s,  790.98/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1456.581s\n",
      "Train: 18 [9050/10009 ( 90%)]  Loss: 2.91 (3.16)  Time: 0.165s,  776.93/s  (0.162s,  791.00/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1464.638s\n",
      "Train: 18 [9100/10009 ( 91%)]  Loss: 2.91 (3.16)  Time: 0.167s,  765.22/s  (0.162s,  791.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1472.720s\n",
      "Train: 18 [9150/10009 ( 91%)]  Loss: 3.06 (3.16)  Time: 0.161s,  797.07/s  (0.162s,  790.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1480.837s\n",
      "Train: 18 [9200/10009 ( 92%)]  Loss: 3.14 (3.16)  Time: 0.166s,  773.23/s  (0.162s,  791.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1488.906s\n",
      "Train: 18 [9250/10009 ( 92%)]  Loss: 3.26 (3.16)  Time: 0.161s,  795.69/s  (0.162s,  790.98/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1497.038s\n",
      "Train: 18 [9300/10009 ( 93%)]  Loss: 2.98 (3.16)  Time: 0.162s,  788.35/s  (0.162s,  790.98/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1505.125s\n",
      "Train: 18 [9350/10009 ( 93%)]  Loss: 3.18 (3.16)  Time: 0.160s,  798.69/s  (0.162s,  791.00/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1513.170s\n",
      "Train: 18 [9400/10009 ( 94%)]  Loss: 3.29 (3.16)  Time: 0.162s,  788.52/s  (0.162s,  791.01/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1521.245s\n",
      "Train: 18 [9450/10009 ( 94%)]  Loss: 3.38 (3.16)  Time: 0.162s,  790.34/s  (0.162s,  791.03/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1529.303s\n",
      "Train: 18 [9500/10009 ( 95%)]  Loss: 3.24 (3.16)  Time: 0.161s,  795.63/s  (0.162s,  791.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1537.364s\n",
      "Train: 18 [9550/10009 ( 95%)]  Loss: 3.15 (3.16)  Time: 0.161s,  793.97/s  (0.162s,  791.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1545.414s\n",
      "Train: 18 [9600/10009 ( 96%)]  Loss: 3.13 (3.16)  Time: 0.161s,  795.97/s  (0.162s,  791.08/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1553.479s\n",
      "Train: 18 [9650/10009 ( 96%)]  Loss: 2.96 (3.16)  Time: 0.162s,  788.64/s  (0.162s,  791.07/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1561.579s\n",
      "Train: 18 [9700/10009 ( 97%)]  Loss: 3.18 (3.16)  Time: 0.161s,  795.21/s  (0.162s,  791.08/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1569.662s\n",
      "Train: 18 [9750/10009 ( 97%)]  Loss: 2.80 (3.16)  Time: 0.161s,  794.92/s  (0.162s,  791.10/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1577.717s\n",
      "Train: 18 [9800/10009 ( 98%)]  Loss: 3.09 (3.16)  Time: 0.163s,  783.65/s  (0.162s,  791.12/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1585.757s\n",
      "Train: 18 [9850/10009 ( 98%)]  Loss: 2.99 (3.16)  Time: 0.161s,  792.77/s  (0.162s,  791.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1593.797s\n",
      "Train: 18 [9900/10009 ( 99%)]  Loss: 3.33 (3.16)  Time: 0.161s,  797.04/s  (0.162s,  791.16/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1601.851s\n",
      "Train: 18 [9950/10009 ( 99%)]  Loss: 2.82 (3.16)  Time: 0.162s,  789.97/s  (0.162s,  791.16/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1609.937s\n",
      "Train: 18 [10000/10009 (100%)]  Loss: 2.77 (3.16)  Time: 0.205s,  625.59/s  (0.162s,  791.15/s)  LR: 0.000e+00  Data: 0.051 (0.007)Time: 1618.054s\n",
      "Test: [   0/390]  Time: 0.697 (0.697)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.054 (0.155)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.051 (0.141)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.053 (0.143)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.051 (0.143)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.052 (0.142)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.052 (0.139)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.051 (0.139)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.034 (0.139)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-18.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 19 [   0/10009 (  0%)]  Loss: 3.62 (3.62)  Time: 0.693s,  184.71/s  (0.693s,  184.71/s)  LR: 0.000e+00  Data: 0.542 (0.542)Time: 0.693s\n",
      "Train: 19 [  50/10009 (  0%)]  Loss: 3.37 (3.24)  Time: 0.159s,  803.41/s  (0.170s,  754.66/s)  LR: 0.000e+00  Data: 0.006 (0.017)Time: 8.651s\n",
      "Train: 19 [ 100/10009 (  1%)]  Loss: 3.30 (3.22)  Time: 0.160s,  801.85/s  (0.166s,  773.41/s)  LR: 0.000e+00  Data: 0.006 (0.012)Time: 16.716s\n",
      "Train: 19 [ 150/10009 (  1%)]  Loss: 3.05 (3.21)  Time: 0.159s,  805.82/s  (0.164s,  782.30/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 24.707s\n",
      "Train: 19 [ 200/10009 (  2%)]  Loss: 3.04 (3.20)  Time: 0.160s,  801.51/s  (0.163s,  785.42/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 32.757s\n",
      "Train: 19 [ 250/10009 (  2%)]  Loss: 3.16 (3.19)  Time: 0.159s,  803.60/s  (0.162s,  788.15/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 40.764s\n",
      "Train: 19 [ 300/10009 (  3%)]  Loss: 3.06 (3.19)  Time: 0.159s,  803.83/s  (0.162s,  789.95/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 48.773s\n",
      "Train: 19 [ 350/10009 (  3%)]  Loss: 3.18 (3.18)  Time: 0.160s,  798.15/s  (0.162s,  791.05/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 56.795s\n",
      "Train: 19 [ 400/10009 (  4%)]  Loss: 3.30 (3.18)  Time: 0.161s,  796.34/s  (0.162s,  791.88/s)  LR: 0.000e+00  Data: 0.007 (0.008)Time: 64.818s\n",
      "Train: 19 [ 450/10009 (  4%)]  Loss: 3.02 (3.18)  Time: 0.161s,  797.49/s  (0.162s,  792.30/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 72.862s\n",
      "Train: 19 [ 500/10009 (  5%)]  Loss: 3.29 (3.18)  Time: 0.162s,  791.82/s  (0.161s,  792.71/s)  LR: 0.000e+00  Data: 0.007 (0.008)Time: 80.898s\n",
      "Train: 19 [ 550/10009 (  5%)]  Loss: 3.05 (3.18)  Time: 0.161s,  794.06/s  (0.161s,  793.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 88.937s\n",
      "Train: 19 [ 600/10009 (  6%)]  Loss: 3.35 (3.18)  Time: 0.170s,  750.74/s  (0.163s,  787.04/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 97.744s\n",
      "Train: 19 [ 650/10009 (  6%)]  Loss: 3.16 (3.18)  Time: 0.172s,  742.98/s  (0.164s,  782.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 106.557s\n",
      "Train: 19 [ 700/10009 (  7%)]  Loss: 3.47 (3.18)  Time: 0.172s,  742.10/s  (0.164s,  778.89/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 115.199s\n",
      "Train: 19 [ 750/10009 (  7%)]  Loss: 3.13 (3.18)  Time: 0.173s,  738.85/s  (0.165s,  776.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 123.868s\n",
      "Train: 19 [ 800/10009 (  8%)]  Loss: 3.31 (3.17)  Time: 0.173s,  741.01/s  (0.165s,  773.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 132.547s\n",
      "Train: 19 [ 850/10009 (  8%)]  Loss: 3.28 (3.17)  Time: 0.174s,  736.26/s  (0.166s,  771.17/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 141.250s\n",
      "Train: 19 [ 900/10009 (  9%)]  Loss: 3.10 (3.17)  Time: 0.172s,  743.99/s  (0.166s,  769.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 149.894s\n",
      "Train: 19 [ 950/10009 (  9%)]  Loss: 3.26 (3.17)  Time: 0.172s,  746.23/s  (0.167s,  767.84/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 158.534s\n",
      "Train: 19 [1000/10009 ( 10%)]  Loss: 3.09 (3.17)  Time: 0.172s,  744.81/s  (0.167s,  766.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 167.183s\n",
      "Train: 19 [1050/10009 ( 10%)]  Loss: 2.90 (3.17)  Time: 0.173s,  738.53/s  (0.167s,  764.84/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 175.890s\n",
      "Train: 19 [1100/10009 ( 11%)]  Loss: 3.06 (3.17)  Time: 0.173s,  741.02/s  (0.168s,  763.55/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 184.568s\n",
      "Train: 19 [1150/10009 ( 11%)]  Loss: 3.21 (3.17)  Time: 0.173s,  739.79/s  (0.168s,  762.39/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 193.244s\n",
      "Train: 19 [1200/10009 ( 12%)]  Loss: 3.28 (3.17)  Time: 0.173s,  740.86/s  (0.168s,  761.47/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 201.883s\n",
      "Train: 19 [1250/10009 ( 12%)]  Loss: 3.12 (3.17)  Time: 0.174s,  735.90/s  (0.168s,  760.63/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 210.520s\n",
      "Train: 19 [1300/10009 ( 13%)]  Loss: 2.94 (3.18)  Time: 0.178s,  719.82/s  (0.169s,  759.56/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 219.243s\n",
      "Train: 19 [1350/10009 ( 13%)]  Loss: 3.21 (3.18)  Time: 0.185s,  691.81/s  (0.169s,  758.21/s)  LR: 0.000e+00  Data: 0.017 (0.007)Time: 228.073s\n",
      "Train: 19 [1400/10009 ( 14%)]  Loss: 3.27 (3.18)  Time: 0.175s,  730.82/s  (0.169s,  757.10/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 236.863s\n",
      "Train: 19 [1450/10009 ( 14%)]  Loss: 3.04 (3.17)  Time: 0.174s,  734.31/s  (0.169s,  756.08/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 245.645s\n",
      "Train: 19 [1500/10009 ( 15%)]  Loss: 3.02 (3.17)  Time: 0.177s,  724.37/s  (0.170s,  755.13/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 254.430s\n",
      "Train: 19 [1550/10009 ( 15%)]  Loss: 3.08 (3.17)  Time: 0.177s,  721.41/s  (0.170s,  754.29/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 263.199s\n",
      "Train: 19 [1600/10009 ( 16%)]  Loss: 3.05 (3.17)  Time: 0.178s,  719.98/s  (0.170s,  753.56/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 271.945s\n",
      "Train: 19 [1650/10009 ( 16%)]  Loss: 3.01 (3.17)  Time: 0.173s,  738.89/s  (0.170s,  752.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 280.653s\n",
      "Train: 19 [1700/10009 ( 17%)]  Loss: 3.42 (3.17)  Time: 0.173s,  741.50/s  (0.170s,  752.39/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 289.380s\n",
      "Train: 19 [1750/10009 ( 17%)]  Loss: 3.26 (3.17)  Time: 0.173s,  741.90/s  (0.170s,  751.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 298.075s\n",
      "Train: 19 [1800/10009 ( 18%)]  Loss: 3.04 (3.17)  Time: 0.173s,  741.06/s  (0.170s,  751.59/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 306.721s\n",
      "Train: 19 [1850/10009 ( 18%)]  Loss: 3.43 (3.17)  Time: 0.172s,  742.22/s  (0.170s,  751.28/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 315.366s\n",
      "Train: 19 [1900/10009 ( 19%)]  Loss: 3.13 (3.17)  Time: 0.176s,  728.05/s  (0.170s,  750.90/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 324.046s\n",
      "Train: 19 [1950/10009 ( 19%)]  Loss: 3.16 (3.17)  Time: 0.172s,  743.67/s  (0.171s,  750.53/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 332.735s\n",
      "Train: 19 [2000/10009 ( 20%)]  Loss: 3.20 (3.17)  Time: 0.175s,  732.60/s  (0.171s,  749.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 341.522s\n",
      "Train: 19 [2050/10009 ( 20%)]  Loss: 3.31 (3.17)  Time: 0.177s,  724.65/s  (0.171s,  749.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 350.307s\n",
      "Train: 19 [2100/10009 ( 21%)]  Loss: 3.01 (3.17)  Time: 0.174s,  737.15/s  (0.171s,  748.93/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 359.083s\n",
      "Train: 19 [2150/10009 ( 21%)]  Loss: 3.02 (3.17)  Time: 0.173s,  738.00/s  (0.171s,  748.58/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 367.801s\n",
      "Train: 19 [2200/10009 ( 22%)]  Loss: 3.18 (3.17)  Time: 0.175s,  730.41/s  (0.171s,  748.33/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 376.475s\n",
      "Train: 19 [2250/10009 ( 22%)]  Loss: 3.24 (3.17)  Time: 0.173s,  738.90/s  (0.171s,  748.13/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 385.132s\n",
      "Train: 19 [2300/10009 ( 23%)]  Loss: 3.21 (3.17)  Time: 0.172s,  744.39/s  (0.171s,  747.93/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 393.790s\n",
      "Train: 19 [2350/10009 ( 23%)]  Loss: 3.17 (3.17)  Time: 0.172s,  744.74/s  (0.171s,  747.68/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 402.484s\n",
      "Train: 19 [2400/10009 ( 24%)]  Loss: 3.13 (3.17)  Time: 0.178s,  718.44/s  (0.171s,  747.32/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 411.239s\n",
      "Train: 19 [2450/10009 ( 24%)]  Loss: 3.07 (3.17)  Time: 0.175s,  732.81/s  (0.171s,  746.95/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 420.013s\n",
      "Train: 19 [2500/10009 ( 25%)]  Loss: 3.07 (3.17)  Time: 0.173s,  738.29/s  (0.171s,  746.58/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 428.790s\n",
      "Train: 19 [2550/10009 ( 25%)]  Loss: 3.11 (3.16)  Time: 0.172s,  744.94/s  (0.171s,  746.49/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 437.417s\n",
      "Train: 19 [2600/10009 ( 26%)]  Loss: 3.24 (3.16)  Time: 0.175s,  729.86/s  (0.172s,  746.18/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 446.173s\n",
      "Train: 19 [2650/10009 ( 26%)]  Loss: 3.31 (3.16)  Time: 0.176s,  725.94/s  (0.172s,  745.83/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 454.968s\n",
      "Train: 19 [2700/10009 ( 27%)]  Loss: 3.02 (3.16)  Time: 0.176s,  728.05/s  (0.172s,  745.53/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 463.735s\n",
      "Train: 19 [2750/10009 ( 27%)]  Loss: 3.23 (3.16)  Time: 0.172s,  745.07/s  (0.172s,  745.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 472.459s\n",
      "Train: 19 [2800/10009 ( 28%)]  Loss: 3.09 (3.16)  Time: 0.176s,  728.55/s  (0.172s,  745.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 481.238s\n",
      "Train: 19 [2850/10009 ( 28%)]  Loss: 3.04 (3.16)  Time: 0.174s,  733.70/s  (0.172s,  744.71/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 490.024s\n",
      "Train: 19 [2900/10009 ( 29%)]  Loss: 3.11 (3.16)  Time: 0.174s,  733.80/s  (0.172s,  744.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 498.820s\n",
      "Train: 19 [2950/10009 ( 29%)]  Loss: 3.35 (3.16)  Time: 0.172s,  742.54/s  (0.172s,  744.23/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 507.539s\n",
      "Train: 19 [3000/10009 ( 30%)]  Loss: 3.04 (3.16)  Time: 0.171s,  747.16/s  (0.172s,  744.20/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 516.160s\n",
      "Train: 19 [3050/10009 ( 30%)]  Loss: 3.14 (3.16)  Time: 0.172s,  746.01/s  (0.172s,  744.14/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 524.803s\n",
      "Train: 19 [3100/10009 ( 31%)]  Loss: 3.26 (3.16)  Time: 0.173s,  740.79/s  (0.172s,  744.11/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 533.428s\n",
      "Train: 19 [3150/10009 ( 31%)]  Loss: 3.10 (3.16)  Time: 0.172s,  745.05/s  (0.172s,  744.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 542.067s\n",
      "Train: 19 [3200/10009 ( 32%)]  Loss: 2.93 (3.16)  Time: 0.171s,  746.46/s  (0.172s,  744.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 550.677s\n",
      "Train: 19 [3250/10009 ( 32%)]  Loss: 2.98 (3.16)  Time: 0.173s,  740.56/s  (0.172s,  743.96/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 559.339s\n",
      "Train: 19 [3300/10009 ( 33%)]  Loss: 3.11 (3.16)  Time: 0.174s,  737.51/s  (0.172s,  743.82/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 568.046s\n",
      "Train: 19 [3350/10009 ( 33%)]  Loss: 3.31 (3.16)  Time: 0.182s,  702.43/s  (0.172s,  743.69/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 576.759s\n",
      "Train: 19 [3400/10009 ( 34%)]  Loss: 3.37 (3.16)  Time: 0.173s,  740.37/s  (0.172s,  743.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 585.505s\n",
      "Train: 19 [3450/10009 ( 34%)]  Loss: 3.53 (3.16)  Time: 0.173s,  738.49/s  (0.172s,  743.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 594.204s\n",
      "Train: 19 [3500/10009 ( 35%)]  Loss: 3.03 (3.16)  Time: 0.176s,  726.21/s  (0.172s,  743.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 602.894s\n",
      "Train: 19 [3550/10009 ( 35%)]  Loss: 3.25 (3.16)  Time: 0.178s,  719.27/s  (0.172s,  743.05/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 611.707s\n",
      "Train: 19 [3600/10009 ( 36%)]  Loss: 3.12 (3.16)  Time: 0.173s,  741.12/s  (0.172s,  742.88/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 620.455s\n",
      "Train: 19 [3650/10009 ( 36%)]  Loss: 3.17 (3.16)  Time: 0.175s,  732.01/s  (0.172s,  742.87/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 629.080s\n",
      "Train: 19 [3700/10009 ( 37%)]  Loss: 3.35 (3.17)  Time: 0.173s,  739.21/s  (0.172s,  742.81/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 637.751s\n",
      "Train: 19 [3750/10009 ( 37%)]  Loss: 2.56 (3.16)  Time: 0.174s,  737.28/s  (0.172s,  742.66/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 646.492s\n",
      "Train: 19 [3800/10009 ( 38%)]  Loss: 3.20 (3.16)  Time: 0.173s,  741.82/s  (0.172s,  742.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 655.237s\n",
      "Train: 19 [3850/10009 ( 38%)]  Loss: 3.14 (3.16)  Time: 0.173s,  740.90/s  (0.172s,  742.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 663.867s\n",
      "Train: 19 [3900/10009 ( 39%)]  Loss: 3.30 (3.16)  Time: 0.172s,  744.44/s  (0.172s,  742.50/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 672.490s\n",
      "Train: 19 [3950/10009 ( 39%)]  Loss: 3.18 (3.16)  Time: 0.176s,  725.38/s  (0.172s,  742.49/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 681.120s\n",
      "Train: 19 [4000/10009 ( 40%)]  Loss: 2.80 (3.16)  Time: 0.176s,  728.44/s  (0.172s,  742.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 689.913s\n",
      "Train: 19 [4050/10009 ( 40%)]  Loss: 3.40 (3.16)  Time: 0.177s,  724.85/s  (0.172s,  742.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 698.691s\n",
      "Train: 19 [4100/10009 ( 41%)]  Loss: 3.06 (3.16)  Time: 0.174s,  734.47/s  (0.173s,  741.98/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 707.463s\n",
      "Train: 19 [4150/10009 ( 41%)]  Loss: 3.03 (3.16)  Time: 0.174s,  733.68/s  (0.173s,  741.86/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 716.209s\n",
      "Train: 19 [4200/10009 ( 42%)]  Loss: 3.09 (3.16)  Time: 0.174s,  736.16/s  (0.173s,  741.76/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 724.931s\n",
      "Train: 19 [4250/10009 ( 42%)]  Loss: 3.30 (3.16)  Time: 0.174s,  735.99/s  (0.173s,  741.69/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 733.634s\n",
      "Train: 19 [4300/10009 ( 43%)]  Loss: 3.14 (3.16)  Time: 0.172s,  742.14/s  (0.173s,  741.62/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 742.328s\n",
      "Train: 19 [4350/10009 ( 43%)]  Loss: 3.04 (3.16)  Time: 0.174s,  736.16/s  (0.173s,  741.62/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 750.962s\n",
      "Train: 19 [4400/10009 ( 44%)]  Loss: 3.12 (3.16)  Time: 0.175s,  729.99/s  (0.173s,  741.62/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 759.584s\n",
      "Train: 19 [4450/10009 ( 44%)]  Loss: 3.14 (3.16)  Time: 0.175s,  732.30/s  (0.173s,  741.49/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 768.351s\n",
      "Train: 19 [4500/10009 ( 45%)]  Loss: 3.28 (3.16)  Time: 0.175s,  732.21/s  (0.173s,  741.35/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 777.133s\n",
      "Train: 19 [4550/10009 ( 45%)]  Loss: 3.39 (3.16)  Time: 0.174s,  737.64/s  (0.173s,  741.21/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 785.915s\n",
      "Train: 19 [4600/10009 ( 46%)]  Loss: 3.17 (3.16)  Time: 0.178s,  717.63/s  (0.173s,  741.06/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 794.706s\n",
      "Train: 19 [4650/10009 ( 46%)]  Loss: 3.05 (3.16)  Time: 0.174s,  733.69/s  (0.173s,  740.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 803.501s\n",
      "Train: 19 [4700/10009 ( 47%)]  Loss: 3.28 (3.16)  Time: 0.177s,  721.61/s  (0.173s,  740.77/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 812.295s\n",
      "Train: 19 [4750/10009 ( 47%)]  Loss: 2.94 (3.16)  Time: 0.176s,  726.89/s  (0.173s,  740.64/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 821.087s\n",
      "Train: 19 [4800/10009 ( 48%)]  Loss: 3.15 (3.16)  Time: 0.174s,  734.27/s  (0.173s,  740.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 829.853s\n",
      "Train: 19 [4850/10009 ( 48%)]  Loss: 3.20 (3.16)  Time: 0.174s,  735.75/s  (0.173s,  740.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 838.621s\n",
      "Train: 19 [4900/10009 ( 49%)]  Loss: 3.00 (3.16)  Time: 0.175s,  730.75/s  (0.173s,  740.31/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 847.387s\n",
      "Train: 19 [4950/10009 ( 49%)]  Loss: 3.19 (3.16)  Time: 0.174s,  735.68/s  (0.173s,  740.23/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 856.116s\n",
      "Train: 19 [5000/10009 ( 50%)]  Loss: 2.96 (3.16)  Time: 0.173s,  740.97/s  (0.173s,  740.18/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 864.827s\n",
      "Train: 19 [5050/10009 ( 50%)]  Loss: 3.31 (3.16)  Time: 0.171s,  748.84/s  (0.173s,  740.19/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 873.461s\n",
      "Train: 19 [5100/10009 ( 51%)]  Loss: 3.07 (3.16)  Time: 0.160s,  797.92/s  (0.173s,  740.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 881.719s\n",
      "Train: 19 [5150/10009 ( 51%)]  Loss: 3.54 (3.16)  Time: 0.160s,  798.65/s  (0.173s,  741.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 889.773s\n",
      "Train: 19 [5200/10009 ( 52%)]  Loss: 2.98 (3.16)  Time: 0.160s,  798.22/s  (0.173s,  741.47/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 897.849s\n",
      "Train: 19 [5250/10009 ( 52%)]  Loss: 3.21 (3.16)  Time: 0.161s,  796.63/s  (0.173s,  741.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 905.924s\n",
      "Train: 19 [5300/10009 ( 53%)]  Loss: 3.51 (3.16)  Time: 0.161s,  794.02/s  (0.172s,  742.38/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 913.991s\n",
      "Train: 19 [5350/10009 ( 53%)]  Loss: 3.20 (3.16)  Time: 0.162s,  790.63/s  (0.172s,  742.83/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 922.049s\n",
      "Train: 19 [5400/10009 ( 54%)]  Loss: 3.10 (3.16)  Time: 0.167s,  766.56/s  (0.172s,  743.24/s)  LR: 0.000e+00  Data: 0.011 (0.007)Time: 930.151s\n",
      "Train: 19 [5450/10009 ( 54%)]  Loss: 3.43 (3.16)  Time: 0.161s,  796.66/s  (0.172s,  743.69/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 938.200s\n",
      "Train: 19 [5500/10009 ( 55%)]  Loss: 2.99 (3.16)  Time: 0.160s,  799.94/s  (0.172s,  744.12/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 946.250s\n",
      "Train: 19 [5550/10009 ( 55%)]  Loss: 3.40 (3.16)  Time: 0.162s,  792.45/s  (0.172s,  744.56/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 954.295s\n",
      "Train: 19 [5600/10009 ( 56%)]  Loss: 3.12 (3.16)  Time: 0.162s,  789.22/s  (0.172s,  744.94/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 962.392s\n",
      "Train: 19 [5650/10009 ( 56%)]  Loss: 3.49 (3.16)  Time: 0.161s,  797.46/s  (0.172s,  745.36/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 970.440s\n",
      "Train: 19 [5700/10009 ( 57%)]  Loss: 2.91 (3.16)  Time: 0.164s,  778.99/s  (0.172s,  745.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 978.508s\n",
      "Train: 19 [5750/10009 ( 57%)]  Loss: 3.12 (3.16)  Time: 0.161s,  795.02/s  (0.172s,  746.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 986.581s\n",
      "Train: 19 [5800/10009 ( 58%)]  Loss: 3.21 (3.16)  Time: 0.160s,  797.77/s  (0.171s,  746.50/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 994.670s\n",
      "Train: 19 [5850/10009 ( 58%)]  Loss: 3.46 (3.16)  Time: 0.161s,  794.99/s  (0.171s,  746.90/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1002.717s\n",
      "Train: 19 [5900/10009 ( 59%)]  Loss: 3.31 (3.16)  Time: 0.165s,  777.60/s  (0.171s,  747.27/s)  LR: 0.000e+00  Data: 0.010 (0.007)Time: 1010.774s\n",
      "Train: 19 [5950/10009 ( 59%)]  Loss: 3.29 (3.16)  Time: 0.161s,  797.41/s  (0.171s,  747.65/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1018.820s\n",
      "Train: 19 [6000/10009 ( 60%)]  Loss: 3.12 (3.16)  Time: 0.160s,  798.67/s  (0.171s,  748.03/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1026.868s\n",
      "Train: 19 [6050/10009 ( 60%)]  Loss: 3.08 (3.16)  Time: 0.160s,  797.73/s  (0.171s,  748.35/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1034.976s\n",
      "Train: 19 [6100/10009 ( 61%)]  Loss: 3.21 (3.16)  Time: 0.163s,  785.51/s  (0.171s,  748.70/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1043.043s\n",
      "Train: 19 [6150/10009 ( 61%)]  Loss: 3.05 (3.16)  Time: 0.160s,  799.94/s  (0.171s,  749.05/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1051.102s\n",
      "Train: 19 [6200/10009 ( 62%)]  Loss: 2.92 (3.16)  Time: 0.161s,  794.57/s  (0.171s,  749.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1059.156s\n",
      "Train: 19 [6250/10009 ( 62%)]  Loss: 3.13 (3.16)  Time: 0.160s,  799.96/s  (0.171s,  749.72/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1067.229s\n",
      "Train: 19 [6300/10009 ( 63%)]  Loss: 3.12 (3.16)  Time: 0.160s,  798.23/s  (0.171s,  750.07/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1075.266s\n",
      "Train: 19 [6350/10009 ( 63%)]  Loss: 2.95 (3.16)  Time: 0.160s,  798.77/s  (0.171s,  750.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1083.315s\n",
      "Train: 19 [6400/10009 ( 64%)]  Loss: 3.18 (3.16)  Time: 0.162s,  791.47/s  (0.171s,  750.73/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1091.373s\n",
      "Train: 19 [6450/10009 ( 64%)]  Loss: 3.12 (3.16)  Time: 0.163s,  786.94/s  (0.170s,  751.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1099.430s\n",
      "Train: 19 [6500/10009 ( 65%)]  Loss: 3.24 (3.16)  Time: 0.163s,  787.65/s  (0.170s,  751.35/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1107.503s\n",
      "Train: 19 [6550/10009 ( 65%)]  Loss: 3.26 (3.16)  Time: 0.161s,  793.95/s  (0.170s,  751.64/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1115.590s\n",
      "Train: 19 [6600/10009 ( 66%)]  Loss: 3.31 (3.16)  Time: 0.161s,  797.34/s  (0.170s,  751.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1123.638s\n",
      "Train: 19 [6650/10009 ( 66%)]  Loss: 3.05 (3.16)  Time: 0.161s,  797.41/s  (0.170s,  752.24/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1131.727s\n",
      "Train: 19 [6700/10009 ( 67%)]  Loss: 3.02 (3.16)  Time: 0.161s,  796.33/s  (0.170s,  752.54/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1139.781s\n",
      "Train: 19 [6750/10009 ( 67%)]  Loss: 3.21 (3.16)  Time: 0.161s,  793.76/s  (0.170s,  752.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1147.841s\n",
      "Train: 19 [6800/10009 ( 68%)]  Loss: 2.91 (3.16)  Time: 0.161s,  796.62/s  (0.170s,  753.12/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1155.900s\n",
      "Train: 19 [6850/10009 ( 68%)]  Loss: 3.25 (3.16)  Time: 0.160s,  798.28/s  (0.170s,  753.41/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1163.943s\n",
      "Train: 19 [6900/10009 ( 69%)]  Loss: 3.17 (3.16)  Time: 0.161s,  797.27/s  (0.170s,  753.69/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1171.996s\n",
      "Train: 19 [6950/10009 ( 69%)]  Loss: 2.99 (3.16)  Time: 0.160s,  798.56/s  (0.170s,  753.96/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1180.063s\n",
      "Train: 19 [7000/10009 ( 70%)]  Loss: 2.94 (3.16)  Time: 0.160s,  800.83/s  (0.170s,  754.23/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1188.138s\n",
      "Train: 19 [7050/10009 ( 70%)]  Loss: 3.34 (3.16)  Time: 0.161s,  797.42/s  (0.170s,  754.46/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1196.246s\n",
      "Train: 19 [7100/10009 ( 71%)]  Loss: 2.99 (3.16)  Time: 0.160s,  799.17/s  (0.170s,  754.73/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1204.308s\n",
      "Train: 19 [7150/10009 ( 71%)]  Loss: 3.12 (3.16)  Time: 0.160s,  799.48/s  (0.170s,  755.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1212.346s\n",
      "Train: 19 [7200/10009 ( 72%)]  Loss: 3.30 (3.16)  Time: 0.162s,  790.32/s  (0.169s,  755.27/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1220.396s\n",
      "Train: 19 [7250/10009 ( 72%)]  Loss: 3.09 (3.16)  Time: 0.161s,  796.17/s  (0.169s,  755.53/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1228.443s\n",
      "Train: 19 [7300/10009 ( 73%)]  Loss: 3.14 (3.16)  Time: 0.162s,  788.23/s  (0.169s,  755.79/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1236.485s\n",
      "Train: 19 [7350/10009 ( 73%)]  Loss: 3.19 (3.16)  Time: 0.163s,  784.09/s  (0.169s,  756.05/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 1244.522s\n",
      "Train: 19 [7400/10009 ( 74%)]  Loss: 3.04 (3.16)  Time: 0.160s,  797.54/s  (0.169s,  756.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1252.568s\n",
      "Train: 19 [7450/10009 ( 74%)]  Loss: 3.21 (3.17)  Time: 0.161s,  797.42/s  (0.169s,  756.56/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1260.602s\n",
      "Train: 19 [7500/10009 ( 75%)]  Loss: 3.03 (3.17)  Time: 0.160s,  797.87/s  (0.169s,  756.81/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1268.646s\n",
      "Train: 19 [7550/10009 ( 75%)]  Loss: 3.11 (3.17)  Time: 0.161s,  795.01/s  (0.169s,  757.05/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1276.696s\n",
      "Train: 19 [7600/10009 ( 76%)]  Loss: 3.04 (3.17)  Time: 0.160s,  798.36/s  (0.169s,  757.29/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1284.741s\n",
      "Train: 19 [7650/10009 ( 76%)]  Loss: 3.37 (3.16)  Time: 0.160s,  801.16/s  (0.169s,  757.52/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1292.808s\n",
      "Train: 19 [7700/10009 ( 77%)]  Loss: 3.41 (3.17)  Time: 0.160s,  798.66/s  (0.169s,  757.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1300.851s\n",
      "Train: 19 [7750/10009 ( 77%)]  Loss: 2.91 (3.17)  Time: 0.160s,  797.51/s  (0.169s,  757.99/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1308.887s\n",
      "Train: 19 [7800/10009 ( 78%)]  Loss: 3.46 (3.17)  Time: 0.160s,  799.58/s  (0.169s,  758.23/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1316.921s\n",
      "Train: 19 [7850/10009 ( 78%)]  Loss: 2.77 (3.17)  Time: 0.161s,  796.75/s  (0.169s,  758.46/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1324.960s\n",
      "Train: 19 [7900/10009 ( 79%)]  Loss: 2.91 (3.17)  Time: 0.161s,  795.82/s  (0.169s,  758.68/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1333.008s\n",
      "Train: 19 [7950/10009 ( 79%)]  Loss: 3.00 (3.17)  Time: 0.161s,  794.98/s  (0.169s,  758.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1341.048s\n",
      "Train: 19 [8000/10009 ( 80%)]  Loss: 3.14 (3.17)  Time: 0.160s,  798.15/s  (0.169s,  759.12/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1349.089s\n",
      "Train: 19 [8050/10009 ( 80%)]  Loss: 3.16 (3.17)  Time: 0.160s,  799.78/s  (0.169s,  759.34/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1357.140s\n",
      "Train: 19 [8100/10009 ( 81%)]  Loss: 3.33 (3.17)  Time: 0.160s,  800.68/s  (0.169s,  759.55/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1365.186s\n",
      "Train: 19 [8150/10009 ( 81%)]  Loss: 3.29 (3.17)  Time: 0.161s,  794.36/s  (0.168s,  759.76/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1373.232s\n",
      "Train: 19 [8200/10009 ( 82%)]  Loss: 3.33 (3.17)  Time: 0.160s,  798.87/s  (0.168s,  759.97/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1381.273s\n",
      "Train: 19 [8250/10009 ( 82%)]  Loss: 3.21 (3.17)  Time: 0.162s,  788.43/s  (0.168s,  760.18/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1389.318s\n",
      "Train: 19 [8300/10009 ( 83%)]  Loss: 3.17 (3.17)  Time: 0.161s,  794.50/s  (0.168s,  760.38/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1397.354s\n",
      "Train: 19 [8350/10009 ( 83%)]  Loss: 2.78 (3.17)  Time: 0.162s,  791.84/s  (0.168s,  760.59/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1405.395s\n",
      "Train: 19 [8400/10009 ( 84%)]  Loss: 3.07 (3.17)  Time: 0.161s,  796.67/s  (0.168s,  760.79/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1413.436s\n",
      "Train: 19 [8450/10009 ( 84%)]  Loss: 3.46 (3.17)  Time: 0.161s,  796.11/s  (0.168s,  760.99/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1421.479s\n",
      "Train: 19 [8500/10009 ( 85%)]  Loss: 3.33 (3.17)  Time: 0.162s,  790.73/s  (0.168s,  761.18/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1429.526s\n",
      "Train: 19 [8550/10009 ( 85%)]  Loss: 3.26 (3.17)  Time: 0.161s,  795.36/s  (0.168s,  761.37/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1437.578s\n",
      "Train: 19 [8600/10009 ( 86%)]  Loss: 3.12 (3.17)  Time: 0.161s,  797.33/s  (0.168s,  761.56/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1445.627s\n",
      "Train: 19 [8650/10009 ( 86%)]  Loss: 3.29 (3.17)  Time: 0.160s,  800.59/s  (0.168s,  761.75/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1453.662s\n",
      "Train: 19 [8700/10009 ( 87%)]  Loss: 2.98 (3.17)  Time: 0.160s,  797.85/s  (0.168s,  761.93/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1461.716s\n",
      "Train: 19 [8750/10009 ( 87%)]  Loss: 3.31 (3.17)  Time: 0.160s,  797.61/s  (0.168s,  762.12/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1469.756s\n",
      "Train: 19 [8800/10009 ( 88%)]  Loss: 3.11 (3.16)  Time: 0.160s,  798.77/s  (0.168s,  762.29/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1477.821s\n",
      "Train: 19 [8850/10009 ( 88%)]  Loss: 3.28 (3.17)  Time: 0.161s,  795.83/s  (0.168s,  762.47/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1485.870s\n",
      "Train: 19 [8900/10009 ( 89%)]  Loss: 2.90 (3.16)  Time: 0.162s,  792.18/s  (0.168s,  762.64/s)  LR: 0.000e+00  Data: 0.008 (0.006)Time: 1493.918s\n",
      "Train: 19 [8950/10009 ( 89%)]  Loss: 3.14 (3.16)  Time: 0.160s,  799.82/s  (0.168s,  762.82/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1501.951s\n",
      "Train: 19 [9000/10009 ( 90%)]  Loss: 3.33 (3.16)  Time: 0.160s,  798.51/s  (0.168s,  763.00/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1509.989s\n",
      "Train: 19 [9050/10009 ( 90%)]  Loss: 3.19 (3.16)  Time: 0.161s,  795.24/s  (0.168s,  763.17/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1518.034s\n",
      "Train: 19 [9100/10009 ( 91%)]  Loss: 3.43 (3.16)  Time: 0.160s,  799.91/s  (0.168s,  763.35/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1526.073s\n",
      "Train: 19 [9150/10009 ( 91%)]  Loss: 3.38 (3.16)  Time: 0.162s,  792.36/s  (0.168s,  763.51/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1534.130s\n",
      "Train: 19 [9200/10009 ( 92%)]  Loss: 3.43 (3.16)  Time: 0.161s,  797.48/s  (0.168s,  763.67/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1542.181s\n",
      "Train: 19 [9250/10009 ( 92%)]  Loss: 3.21 (3.16)  Time: 0.164s,  780.87/s  (0.168s,  763.84/s)  LR: 0.000e+00  Data: 0.010 (0.006)Time: 1550.228s\n",
      "Train: 19 [9300/10009 ( 93%)]  Loss: 3.07 (3.16)  Time: 0.161s,  796.57/s  (0.168s,  764.00/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1558.279s\n",
      "Train: 19 [9350/10009 ( 93%)]  Loss: 3.26 (3.17)  Time: 0.160s,  800.13/s  (0.168s,  764.16/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1566.327s\n",
      "Train: 19 [9400/10009 ( 94%)]  Loss: 3.24 (3.17)  Time: 0.161s,  793.72/s  (0.167s,  764.32/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1574.363s\n",
      "Train: 19 [9450/10009 ( 94%)]  Loss: 3.01 (3.17)  Time: 0.161s,  795.93/s  (0.167s,  764.48/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1582.404s\n",
      "Train: 19 [9500/10009 ( 95%)]  Loss: 3.37 (3.17)  Time: 0.162s,  789.88/s  (0.167s,  764.64/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1590.455s\n",
      "Train: 19 [9550/10009 ( 95%)]  Loss: 3.22 (3.17)  Time: 0.161s,  796.22/s  (0.167s,  764.79/s)  LR: 0.000e+00  Data: 0.005 (0.006)Time: 1598.500s\n",
      "Train: 19 [9600/10009 ( 96%)]  Loss: 3.13 (3.16)  Time: 0.161s,  795.95/s  (0.167s,  764.95/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1606.539s\n",
      "Train: 19 [9650/10009 ( 96%)]  Loss: 3.15 (3.17)  Time: 0.160s,  798.74/s  (0.167s,  765.11/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1614.574s\n",
      "Train: 19 [9700/10009 ( 97%)]  Loss: 2.93 (3.17)  Time: 0.160s,  797.59/s  (0.167s,  765.26/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1622.623s\n",
      "Train: 19 [9750/10009 ( 97%)]  Loss: 3.49 (3.17)  Time: 0.161s,  797.36/s  (0.167s,  765.41/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1630.668s\n",
      "Train: 19 [9800/10009 ( 98%)]  Loss: 3.16 (3.17)  Time: 0.162s,  791.94/s  (0.167s,  765.56/s)  LR: 0.000e+00  Data: 0.007 (0.006)Time: 1638.708s\n",
      "Train: 19 [9850/10009 ( 98%)]  Loss: 3.37 (3.17)  Time: 0.160s,  798.89/s  (0.167s,  765.71/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1646.748s\n",
      "Train: 19 [9900/10009 ( 99%)]  Loss: 3.00 (3.17)  Time: 0.161s,  797.26/s  (0.167s,  765.85/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1654.792s\n",
      "Train: 19 [9950/10009 ( 99%)]  Loss: 3.24 (3.17)  Time: 0.160s,  798.44/s  (0.167s,  765.99/s)  LR: 0.000e+00  Data: 0.006 (0.006)Time: 1662.841s\n",
      "Train: 19 [10000/10009 (100%)]  Loss: 3.33 (3.17)  Time: 0.204s,  627.49/s  (0.167s,  766.12/s)  LR: 0.000e+00  Data: 0.050 (0.006)Time: 1670.926s\n",
      "Test: [   0/390]  Time: 0.680 (0.680)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.140)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.187 (0.132)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.053 (0.136)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.051 (0.132)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.052 (0.132)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.428 (0.132)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.051 (0.130)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.035 (0.130)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-18.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-19.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 20 [   0/10009 (  0%)]  Loss: 3.13 (3.13)  Time: 0.794s,  161.22/s  (0.794s,  161.22/s)  LR: 0.000e+00  Data: 0.644 (0.644)Time: 0.794s\n",
      "Train: 20 [  50/10009 (  0%)]  Loss: 3.04 (3.16)  Time: 0.159s,  806.00/s  (0.172s,  745.44/s)  LR: 0.000e+00  Data: 0.006 (0.019)Time: 8.758s\n",
      "Train: 20 [ 100/10009 (  1%)]  Loss: 3.17 (3.16)  Time: 0.160s,  800.60/s  (0.166s,  772.31/s)  LR: 0.000e+00  Data: 0.006 (0.013)Time: 16.740s\n",
      "Train: 20 [ 150/10009 (  1%)]  Loss: 3.25 (3.17)  Time: 0.159s,  802.74/s  (0.164s,  781.32/s)  LR: 0.000e+00  Data: 0.006 (0.011)Time: 24.738s\n",
      "Train: 20 [ 200/10009 (  2%)]  Loss: 3.20 (3.17)  Time: 0.160s,  800.14/s  (0.163s,  786.23/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 32.723s\n",
      "Train: 20 [ 250/10009 (  2%)]  Loss: 3.05 (3.17)  Time: 0.160s,  800.17/s  (0.162s,  788.94/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 40.723s\n",
      "Train: 20 [ 300/10009 (  3%)]  Loss: 2.82 (3.17)  Time: 0.160s,  801.93/s  (0.162s,  790.56/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 48.736s\n",
      "Train: 20 [ 350/10009 (  3%)]  Loss: 3.18 (3.16)  Time: 0.160s,  801.26/s  (0.162s,  791.76/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 56.745s\n",
      "Train: 20 [ 400/10009 (  4%)]  Loss: 3.32 (3.16)  Time: 0.160s,  798.49/s  (0.162s,  792.41/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 64.775s\n",
      "Train: 20 [ 450/10009 (  4%)]  Loss: 2.89 (3.16)  Time: 0.159s,  802.54/s  (0.161s,  793.06/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 72.792s\n",
      "Train: 20 [ 500/10009 (  5%)]  Loss: 3.07 (3.16)  Time: 0.160s,  800.63/s  (0.161s,  793.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 80.825s\n",
      "Train: 20 [ 550/10009 (  5%)]  Loss: 3.06 (3.16)  Time: 0.160s,  797.77/s  (0.161s,  793.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 88.852s\n",
      "Train: 20 [ 600/10009 (  6%)]  Loss: 3.22 (3.16)  Time: 0.162s,  789.69/s  (0.161s,  793.90/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 96.899s\n",
      "Train: 20 [ 650/10009 (  6%)]  Loss: 3.07 (3.16)  Time: 0.161s,  792.66/s  (0.161s,  794.08/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 104.937s\n",
      "Train: 20 [ 700/10009 (  7%)]  Loss: 3.08 (3.16)  Time: 0.162s,  790.89/s  (0.161s,  794.05/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 113.000s\n",
      "Train: 20 [ 750/10009 (  7%)]  Loss: 3.13 (3.16)  Time: 0.160s,  800.29/s  (0.161s,  794.28/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 121.026s\n",
      "Train: 20 [ 800/10009 (  8%)]  Loss: 2.99 (3.16)  Time: 0.160s,  800.17/s  (0.161s,  794.47/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 129.052s\n",
      "Train: 20 [ 850/10009 (  8%)]  Loss: 3.19 (3.16)  Time: 0.160s,  797.83/s  (0.161s,  794.59/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 137.088s\n",
      "Train: 20 [ 900/10009 (  9%)]  Loss: 2.99 (3.16)  Time: 0.313s,  409.32/s  (0.164s,  778.20/s)  LR: 0.000e+00  Data: 0.160 (0.010)Time: 148.198s\n",
      "Train: 20 [ 950/10009 (  9%)]  Loss: 3.12 (3.16)  Time: 0.160s,  800.37/s  (0.165s,  775.74/s)  LR: 0.000e+00  Data: 0.006 (0.011)Time: 156.919s\n",
      "Train: 20 [1000/10009 ( 10%)]  Loss: 2.84 (3.16)  Time: 0.160s,  802.42/s  (0.165s,  776.82/s)  LR: 0.000e+00  Data: 0.005 (0.011)Time: 164.939s\n",
      "Train: 20 [1050/10009 ( 10%)]  Loss: 3.06 (3.16)  Time: 0.160s,  801.88/s  (0.165s,  777.82/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 172.954s\n",
      "Train: 20 [1100/10009 ( 11%)]  Loss: 3.27 (3.16)  Time: 0.161s,  794.38/s  (0.164s,  778.71/s)  LR: 0.000e+00  Data: 0.007 (0.010)Time: 180.976s\n",
      "Train: 20 [1150/10009 ( 11%)]  Loss: 3.10 (3.16)  Time: 0.162s,  792.44/s  (0.164s,  779.50/s)  LR: 0.000e+00  Data: 0.007 (0.010)Time: 189.002s\n",
      "Train: 20 [1200/10009 ( 12%)]  Loss: 3.06 (3.16)  Time: 0.161s,  795.38/s  (0.164s,  780.22/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 197.032s\n",
      "Train: 20 [1250/10009 ( 12%)]  Loss: 3.17 (3.16)  Time: 0.161s,  794.69/s  (0.164s,  780.85/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 205.068s\n",
      "Train: 20 [1300/10009 ( 13%)]  Loss: 3.30 (3.16)  Time: 0.160s,  798.02/s  (0.164s,  781.44/s)  LR: 0.000e+00  Data: 0.006 (0.010)Time: 213.104s\n",
      "Train: 20 [1350/10009 ( 13%)]  Loss: 3.07 (3.16)  Time: 0.161s,  795.20/s  (0.164s,  781.98/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 221.141s\n",
      "Train: 20 [1400/10009 ( 14%)]  Loss: 3.38 (3.16)  Time: 0.162s,  790.20/s  (0.164s,  782.48/s)  LR: 0.000e+00  Data: 0.007 (0.009)Time: 229.179s\n",
      "Train: 20 [1450/10009 ( 14%)]  Loss: 3.19 (3.16)  Time: 0.161s,  797.21/s  (0.163s,  782.95/s)  LR: 0.000e+00  Data: 0.005 (0.009)Time: 237.214s\n",
      "Train: 20 [1500/10009 ( 15%)]  Loss: 3.25 (3.17)  Time: 0.161s,  793.05/s  (0.163s,  783.40/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 245.248s\n",
      "Train: 20 [1550/10009 ( 15%)]  Loss: 3.01 (3.17)  Time: 0.160s,  798.37/s  (0.163s,  783.81/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 253.286s\n",
      "Train: 20 [1600/10009 ( 16%)]  Loss: 3.06 (3.17)  Time: 0.160s,  800.92/s  (0.163s,  784.19/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 261.323s\n",
      "Train: 20 [1650/10009 ( 16%)]  Loss: 3.34 (3.16)  Time: 0.160s,  799.95/s  (0.163s,  784.56/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 269.359s\n",
      "Train: 20 [1700/10009 ( 17%)]  Loss: 3.62 (3.16)  Time: 0.160s,  800.61/s  (0.163s,  784.88/s)  LR: 0.000e+00  Data: 0.005 (0.009)Time: 277.401s\n",
      "Train: 20 [1750/10009 ( 17%)]  Loss: 3.23 (3.16)  Time: 0.161s,  795.93/s  (0.163s,  785.21/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 285.436s\n",
      "Train: 20 [1800/10009 ( 18%)]  Loss: 3.04 (3.16)  Time: 0.161s,  795.89/s  (0.163s,  785.53/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 293.468s\n",
      "Train: 20 [1850/10009 ( 18%)]  Loss: 3.28 (3.16)  Time: 0.160s,  798.54/s  (0.163s,  785.77/s)  LR: 0.000e+00  Data: 0.006 (0.009)Time: 301.522s\n",
      "Train: 20 [1900/10009 ( 19%)]  Loss: 3.27 (3.16)  Time: 0.160s,  797.94/s  (0.163s,  786.06/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 309.555s\n",
      "Train: 20 [1950/10009 ( 19%)]  Loss: 3.43 (3.16)  Time: 0.161s,  795.48/s  (0.163s,  786.31/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 317.595s\n",
      "Train: 20 [2000/10009 ( 20%)]  Loss: 3.22 (3.16)  Time: 0.160s,  801.33/s  (0.163s,  786.56/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 325.630s\n",
      "Train: 20 [2050/10009 ( 20%)]  Loss: 3.00 (3.16)  Time: 0.160s,  798.13/s  (0.163s,  786.78/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 333.672s\n",
      "Train: 20 [2100/10009 ( 21%)]  Loss: 3.15 (3.16)  Time: 0.160s,  799.35/s  (0.163s,  787.01/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 341.710s\n",
      "Train: 20 [2150/10009 ( 21%)]  Loss: 2.99 (3.16)  Time: 0.160s,  800.76/s  (0.163s,  787.21/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 349.753s\n",
      "Train: 20 [2200/10009 ( 22%)]  Loss: 3.13 (3.16)  Time: 0.160s,  801.44/s  (0.163s,  787.42/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 357.786s\n",
      "Train: 20 [2250/10009 ( 22%)]  Loss: 3.41 (3.16)  Time: 0.163s,  784.96/s  (0.163s,  787.61/s)  LR: 0.000e+00  Data: 0.009 (0.008)Time: 365.826s\n",
      "Train: 20 [2300/10009 ( 23%)]  Loss: 2.89 (3.16)  Time: 0.160s,  799.50/s  (0.162s,  787.78/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 373.872s\n",
      "Train: 20 [2350/10009 ( 23%)]  Loss: 2.90 (3.16)  Time: 0.161s,  797.28/s  (0.162s,  787.98/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 381.896s\n",
      "Train: 20 [2400/10009 ( 24%)]  Loss: 3.10 (3.16)  Time: 0.160s,  799.23/s  (0.162s,  788.15/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 389.934s\n",
      "Train: 20 [2450/10009 ( 24%)]  Loss: 3.35 (3.16)  Time: 0.161s,  796.28/s  (0.162s,  788.30/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 397.978s\n",
      "Train: 20 [2500/10009 ( 25%)]  Loss: 3.25 (3.16)  Time: 0.160s,  798.81/s  (0.162s,  788.45/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 406.020s\n",
      "Train: 20 [2550/10009 ( 25%)]  Loss: 3.16 (3.16)  Time: 0.161s,  795.40/s  (0.162s,  788.60/s)  LR: 0.000e+00  Data: 0.007 (0.008)Time: 414.060s\n",
      "Train: 20 [2600/10009 ( 26%)]  Loss: 3.23 (3.16)  Time: 0.160s,  799.02/s  (0.162s,  788.75/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 422.096s\n",
      "Train: 20 [2650/10009 ( 26%)]  Loss: 3.26 (3.16)  Time: 0.161s,  795.16/s  (0.162s,  788.89/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 430.133s\n",
      "Train: 20 [2700/10009 ( 27%)]  Loss: 2.94 (3.16)  Time: 0.161s,  793.42/s  (0.162s,  789.03/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 438.166s\n",
      "Train: 20 [2750/10009 ( 27%)]  Loss: 3.11 (3.16)  Time: 0.161s,  793.44/s  (0.162s,  789.16/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 446.208s\n",
      "Train: 20 [2800/10009 ( 28%)]  Loss: 2.84 (3.16)  Time: 0.161s,  793.09/s  (0.162s,  789.30/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 454.233s\n",
      "Train: 20 [2850/10009 ( 28%)]  Loss: 3.14 (3.16)  Time: 0.161s,  795.23/s  (0.162s,  789.42/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 462.272s\n",
      "Train: 20 [2900/10009 ( 29%)]  Loss: 3.23 (3.16)  Time: 0.161s,  793.32/s  (0.162s,  789.52/s)  LR: 0.000e+00  Data: 0.007 (0.008)Time: 470.320s\n",
      "Train: 20 [2950/10009 ( 29%)]  Loss: 3.29 (3.16)  Time: 0.160s,  799.48/s  (0.162s,  789.64/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 478.353s\n",
      "Train: 20 [3000/10009 ( 30%)]  Loss: 3.18 (3.16)  Time: 0.160s,  800.24/s  (0.162s,  789.76/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 486.386s\n",
      "Train: 20 [3050/10009 ( 30%)]  Loss: 2.93 (3.16)  Time: 0.160s,  798.52/s  (0.162s,  789.87/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 494.417s\n",
      "Train: 20 [3100/10009 ( 31%)]  Loss: 3.03 (3.16)  Time: 0.160s,  799.26/s  (0.162s,  789.98/s)  LR: 0.000e+00  Data: 0.006 (0.008)Time: 502.453s\n",
      "Train: 20 [3150/10009 ( 31%)]  Loss: 3.13 (3.16)  Time: 0.160s,  801.93/s  (0.162s,  790.10/s)  LR: 0.000e+00  Data: 0.005 (0.008)Time: 510.479s\n",
      "Train: 20 [3200/10009 ( 32%)]  Loss: 3.35 (3.16)  Time: 0.160s,  798.92/s  (0.162s,  790.20/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 518.510s\n",
      "Train: 20 [3250/10009 ( 32%)]  Loss: 2.96 (3.16)  Time: 0.160s,  799.19/s  (0.162s,  790.30/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 526.542s\n",
      "Train: 20 [3300/10009 ( 33%)]  Loss: 3.23 (3.16)  Time: 0.160s,  797.66/s  (0.162s,  790.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 534.583s\n",
      "Train: 20 [3350/10009 ( 33%)]  Loss: 3.25 (3.16)  Time: 0.160s,  799.17/s  (0.162s,  790.48/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 542.613s\n",
      "Train: 20 [3400/10009 ( 34%)]  Loss: 3.09 (3.16)  Time: 0.161s,  794.98/s  (0.162s,  790.57/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 550.648s\n",
      "Train: 20 [3450/10009 ( 34%)]  Loss: 3.03 (3.16)  Time: 0.161s,  797.47/s  (0.162s,  790.65/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 558.686s\n",
      "Train: 20 [3500/10009 ( 35%)]  Loss: 2.99 (3.16)  Time: 0.163s,  783.38/s  (0.162s,  790.72/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 566.730s\n",
      "Train: 20 [3550/10009 ( 35%)]  Loss: 3.03 (3.16)  Time: 0.161s,  795.37/s  (0.162s,  790.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 574.771s\n",
      "Train: 20 [3600/10009 ( 36%)]  Loss: 3.32 (3.16)  Time: 0.160s,  798.25/s  (0.162s,  790.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 582.807s\n",
      "Train: 20 [3650/10009 ( 36%)]  Loss: 3.34 (3.16)  Time: 0.161s,  796.95/s  (0.162s,  790.94/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 590.851s\n",
      "Train: 20 [3700/10009 ( 37%)]  Loss: 3.40 (3.16)  Time: 0.161s,  794.98/s  (0.162s,  791.02/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 598.885s\n",
      "Train: 20 [3750/10009 ( 37%)]  Loss: 3.32 (3.16)  Time: 0.160s,  800.60/s  (0.162s,  791.09/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 606.918s\n",
      "Train: 20 [3800/10009 ( 38%)]  Loss: 3.12 (3.16)  Time: 0.161s,  795.97/s  (0.162s,  791.15/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 614.959s\n",
      "Train: 20 [3850/10009 ( 38%)]  Loss: 3.30 (3.16)  Time: 0.160s,  801.25/s  (0.162s,  791.23/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 622.986s\n",
      "Train: 20 [3900/10009 ( 39%)]  Loss: 3.07 (3.16)  Time: 0.160s,  798.11/s  (0.162s,  791.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 631.014s\n",
      "Train: 20 [3950/10009 ( 39%)]  Loss: 3.39 (3.16)  Time: 0.161s,  795.88/s  (0.162s,  791.37/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 639.051s\n",
      "Train: 20 [4000/10009 ( 40%)]  Loss: 3.47 (3.16)  Time: 0.164s,  780.49/s  (0.162s,  791.41/s)  LR: 0.000e+00  Data: 0.009 (0.007)Time: 647.108s\n",
      "Train: 20 [4050/10009 ( 40%)]  Loss: 3.11 (3.16)  Time: 0.161s,  793.22/s  (0.162s,  791.46/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 655.155s\n",
      "Train: 20 [4100/10009 ( 41%)]  Loss: 3.17 (3.16)  Time: 0.161s,  795.53/s  (0.162s,  791.51/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 663.199s\n",
      "Train: 20 [4150/10009 ( 41%)]  Loss: 3.16 (3.16)  Time: 0.161s,  795.79/s  (0.162s,  791.56/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 671.242s\n",
      "Train: 20 [4200/10009 ( 42%)]  Loss: 3.20 (3.16)  Time: 0.160s,  798.89/s  (0.162s,  791.62/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 679.277s\n",
      "Train: 20 [4250/10009 ( 42%)]  Loss: 3.03 (3.16)  Time: 0.161s,  797.43/s  (0.162s,  791.67/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 687.312s\n",
      "Train: 20 [4300/10009 ( 43%)]  Loss: 3.21 (3.16)  Time: 0.161s,  796.17/s  (0.162s,  791.71/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 695.362s\n",
      "Train: 20 [4350/10009 ( 43%)]  Loss: 3.07 (3.16)  Time: 0.160s,  800.44/s  (0.162s,  791.77/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 703.396s\n",
      "Train: 20 [4400/10009 ( 44%)]  Loss: 3.10 (3.16)  Time: 0.160s,  798.81/s  (0.162s,  791.82/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 711.433s\n",
      "Train: 20 [4450/10009 ( 44%)]  Loss: 3.24 (3.16)  Time: 0.160s,  798.00/s  (0.162s,  791.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 719.468s\n",
      "Train: 20 [4500/10009 ( 45%)]  Loss: 3.30 (3.16)  Time: 0.160s,  798.21/s  (0.162s,  791.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 727.503s\n",
      "Train: 20 [4550/10009 ( 45%)]  Loss: 3.10 (3.16)  Time: 0.160s,  797.88/s  (0.162s,  791.97/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 735.541s\n",
      "Train: 20 [4600/10009 ( 46%)]  Loss: 3.05 (3.16)  Time: 0.160s,  798.39/s  (0.162s,  792.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 743.586s\n",
      "Train: 20 [4650/10009 ( 46%)]  Loss: 3.26 (3.16)  Time: 0.162s,  791.49/s  (0.162s,  792.05/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 751.624s\n",
      "Train: 20 [4700/10009 ( 47%)]  Loss: 3.07 (3.16)  Time: 0.161s,  796.40/s  (0.162s,  792.10/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 759.660s\n",
      "Train: 20 [4750/10009 ( 47%)]  Loss: 3.26 (3.16)  Time: 0.163s,  785.20/s  (0.162s,  792.14/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 767.699s\n",
      "Train: 20 [4800/10009 ( 48%)]  Loss: 3.05 (3.16)  Time: 0.162s,  790.38/s  (0.162s,  792.17/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 775.747s\n",
      "Train: 20 [4850/10009 ( 48%)]  Loss: 3.03 (3.16)  Time: 0.161s,  797.43/s  (0.162s,  792.21/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 783.789s\n",
      "Train: 20 [4900/10009 ( 49%)]  Loss: 3.07 (3.16)  Time: 0.160s,  797.66/s  (0.162s,  792.24/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 791.835s\n",
      "Train: 20 [4950/10009 ( 49%)]  Loss: 2.95 (3.16)  Time: 0.161s,  796.92/s  (0.162s,  792.28/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 799.881s\n",
      "Train: 20 [5000/10009 ( 50%)]  Loss: 3.28 (3.16)  Time: 0.160s,  798.47/s  (0.162s,  792.31/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 807.920s\n",
      "Train: 20 [5050/10009 ( 50%)]  Loss: 2.88 (3.16)  Time: 0.161s,  796.13/s  (0.162s,  792.35/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 815.958s\n",
      "Train: 20 [5100/10009 ( 51%)]  Loss: 3.19 (3.16)  Time: 0.161s,  796.66/s  (0.162s,  792.39/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 823.996s\n",
      "Train: 20 [5150/10009 ( 51%)]  Loss: 2.95 (3.16)  Time: 0.160s,  800.76/s  (0.162s,  792.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 832.038s\n",
      "Train: 20 [5200/10009 ( 52%)]  Loss: 2.89 (3.16)  Time: 0.160s,  799.17/s  (0.162s,  792.46/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 840.079s\n",
      "Train: 20 [5250/10009 ( 52%)]  Loss: 3.03 (3.16)  Time: 0.161s,  793.43/s  (0.162s,  792.49/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 848.123s\n",
      "Train: 20 [5300/10009 ( 53%)]  Loss: 3.15 (3.16)  Time: 0.161s,  793.84/s  (0.162s,  792.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 856.159s\n",
      "Train: 20 [5350/10009 ( 53%)]  Loss: 3.11 (3.16)  Time: 0.161s,  796.60/s  (0.162s,  792.55/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 864.204s\n",
      "Train: 20 [5400/10009 ( 54%)]  Loss: 3.25 (3.16)  Time: 0.162s,  791.29/s  (0.161s,  792.59/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 872.239s\n",
      "Train: 20 [5450/10009 ( 54%)]  Loss: 3.02 (3.16)  Time: 0.160s,  798.67/s  (0.161s,  792.62/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 880.277s\n",
      "Train: 20 [5500/10009 ( 55%)]  Loss: 2.97 (3.16)  Time: 0.162s,  792.05/s  (0.161s,  792.64/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 888.331s\n",
      "Train: 20 [5550/10009 ( 55%)]  Loss: 3.22 (3.16)  Time: 0.161s,  796.75/s  (0.161s,  792.68/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 896.360s\n",
      "Train: 20 [5600/10009 ( 56%)]  Loss: 3.26 (3.16)  Time: 0.161s,  797.08/s  (0.161s,  792.71/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 904.399s\n",
      "Train: 20 [5650/10009 ( 56%)]  Loss: 3.51 (3.16)  Time: 0.162s,  790.24/s  (0.161s,  792.74/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 912.439s\n",
      "Train: 20 [5700/10009 ( 57%)]  Loss: 3.03 (3.16)  Time: 0.160s,  800.59/s  (0.161s,  792.77/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 920.474s\n",
      "Train: 20 [5750/10009 ( 57%)]  Loss: 3.36 (3.16)  Time: 0.160s,  800.97/s  (0.161s,  792.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 928.510s\n",
      "Train: 20 [5800/10009 ( 58%)]  Loss: 3.28 (3.16)  Time: 0.160s,  797.79/s  (0.161s,  792.84/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 936.541s\n",
      "Train: 20 [5850/10009 ( 58%)]  Loss: 2.96 (3.16)  Time: 0.160s,  800.21/s  (0.161s,  792.86/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 944.584s\n",
      "Train: 20 [5900/10009 ( 59%)]  Loss: 3.20 (3.16)  Time: 0.161s,  794.81/s  (0.161s,  792.90/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 952.615s\n",
      "Train: 20 [5950/10009 ( 59%)]  Loss: 2.90 (3.16)  Time: 0.161s,  793.72/s  (0.161s,  792.92/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 960.656s\n",
      "Train: 20 [6000/10009 ( 60%)]  Loss: 3.05 (3.16)  Time: 0.160s,  797.84/s  (0.161s,  792.95/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 968.700s\n",
      "Train: 20 [6050/10009 ( 60%)]  Loss: 3.20 (3.16)  Time: 0.161s,  794.02/s  (0.161s,  792.98/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 976.733s\n",
      "Train: 20 [6100/10009 ( 61%)]  Loss: 3.48 (3.16)  Time: 0.161s,  797.23/s  (0.161s,  793.00/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 984.771s\n",
      "Train: 20 [6150/10009 ( 61%)]  Loss: 3.20 (3.16)  Time: 0.161s,  797.31/s  (0.161s,  793.02/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 992.817s\n",
      "Train: 20 [6200/10009 ( 62%)]  Loss: 3.13 (3.16)  Time: 0.160s,  797.89/s  (0.161s,  793.04/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1000.867s\n",
      "Train: 20 [6250/10009 ( 62%)]  Loss: 3.13 (3.16)  Time: 0.161s,  794.92/s  (0.161s,  793.06/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1008.916s\n",
      "Train: 20 [6300/10009 ( 63%)]  Loss: 3.21 (3.16)  Time: 0.161s,  797.41/s  (0.161s,  793.07/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1016.962s\n",
      "Train: 20 [6350/10009 ( 63%)]  Loss: 3.10 (3.16)  Time: 0.161s,  797.30/s  (0.161s,  793.10/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1024.998s\n",
      "Train: 20 [6400/10009 ( 64%)]  Loss: 3.24 (3.16)  Time: 0.162s,  791.29/s  (0.161s,  793.12/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1033.041s\n",
      "Train: 20 [6450/10009 ( 64%)]  Loss: 3.19 (3.16)  Time: 0.161s,  793.28/s  (0.161s,  793.14/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1041.079s\n",
      "Train: 20 [6500/10009 ( 65%)]  Loss: 3.02 (3.16)  Time: 0.162s,  791.71/s  (0.161s,  793.17/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1049.121s\n",
      "Train: 20 [6550/10009 ( 65%)]  Loss: 3.26 (3.16)  Time: 0.161s,  795.24/s  (0.161s,  793.17/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1057.189s\n",
      "Train: 20 [6600/10009 ( 66%)]  Loss: 3.18 (3.16)  Time: 0.161s,  796.03/s  (0.161s,  793.19/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1065.228s\n",
      "Train: 20 [6650/10009 ( 66%)]  Loss: 3.28 (3.16)  Time: 0.160s,  798.38/s  (0.161s,  793.21/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1073.266s\n",
      "Train: 20 [6700/10009 ( 67%)]  Loss: 3.10 (3.16)  Time: 0.161s,  796.36/s  (0.161s,  793.22/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1081.318s\n",
      "Train: 20 [6750/10009 ( 67%)]  Loss: 2.99 (3.16)  Time: 0.160s,  798.72/s  (0.161s,  793.25/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1089.355s\n",
      "Train: 20 [6800/10009 ( 68%)]  Loss: 3.00 (3.16)  Time: 0.160s,  799.40/s  (0.161s,  793.27/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1097.394s\n",
      "Train: 20 [6850/10009 ( 68%)]  Loss: 3.05 (3.16)  Time: 0.160s,  798.07/s  (0.161s,  793.29/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1105.435s\n",
      "Train: 20 [6900/10009 ( 69%)]  Loss: 3.10 (3.16)  Time: 0.162s,  791.44/s  (0.161s,  793.31/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1113.472s\n",
      "Train: 20 [6950/10009 ( 69%)]  Loss: 3.25 (3.16)  Time: 0.162s,  790.22/s  (0.161s,  793.33/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1121.513s\n",
      "Train: 20 [7000/10009 ( 70%)]  Loss: 3.37 (3.16)  Time: 0.160s,  797.62/s  (0.161s,  793.34/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1129.558s\n",
      "Train: 20 [7050/10009 ( 70%)]  Loss: 3.16 (3.16)  Time: 0.160s,  799.88/s  (0.161s,  793.36/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1137.602s\n",
      "Train: 20 [7100/10009 ( 71%)]  Loss: 3.00 (3.16)  Time: 0.160s,  799.88/s  (0.161s,  793.38/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1145.642s\n",
      "Train: 20 [7150/10009 ( 71%)]  Loss: 3.21 (3.16)  Time: 0.162s,  791.49/s  (0.161s,  793.40/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1153.682s\n",
      "Train: 20 [7200/10009 ( 72%)]  Loss: 3.14 (3.16)  Time: 0.161s,  797.48/s  (0.161s,  793.42/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1161.711s\n",
      "Train: 20 [7250/10009 ( 72%)]  Loss: 3.40 (3.16)  Time: 0.161s,  795.80/s  (0.161s,  793.44/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1169.748s\n",
      "Train: 20 [7300/10009 ( 73%)]  Loss: 3.37 (3.16)  Time: 0.160s,  798.92/s  (0.161s,  793.46/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1177.789s\n",
      "Train: 20 [7350/10009 ( 73%)]  Loss: 3.23 (3.16)  Time: 0.160s,  801.84/s  (0.161s,  793.47/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1185.832s\n",
      "Train: 20 [7400/10009 ( 74%)]  Loss: 3.38 (3.16)  Time: 0.161s,  797.47/s  (0.161s,  793.50/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1193.864s\n",
      "Train: 20 [7450/10009 ( 74%)]  Loss: 3.08 (3.16)  Time: 0.161s,  795.36/s  (0.161s,  793.51/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1201.908s\n",
      "Train: 20 [7500/10009 ( 75%)]  Loss: 3.26 (3.16)  Time: 0.161s,  796.19/s  (0.161s,  793.52/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1209.954s\n",
      "Train: 20 [7550/10009 ( 75%)]  Loss: 3.26 (3.16)  Time: 0.162s,  787.77/s  (0.161s,  793.54/s)  LR: 0.000e+00  Data: 0.008 (0.007)Time: 1217.995s\n",
      "Train: 20 [7600/10009 ( 76%)]  Loss: 3.32 (3.16)  Time: 0.161s,  795.74/s  (0.161s,  793.55/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1226.039s\n",
      "Train: 20 [7650/10009 ( 76%)]  Loss: 3.27 (3.16)  Time: 0.160s,  798.21/s  (0.161s,  793.56/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1234.087s\n",
      "Train: 20 [7700/10009 ( 77%)]  Loss: 3.12 (3.16)  Time: 0.162s,  791.41/s  (0.161s,  793.59/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1242.113s\n",
      "Train: 20 [7750/10009 ( 77%)]  Loss: 3.08 (3.16)  Time: 0.163s,  784.04/s  (0.161s,  793.61/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1250.148s\n",
      "Train: 20 [7800/10009 ( 78%)]  Loss: 3.12 (3.16)  Time: 0.161s,  794.54/s  (0.161s,  793.63/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1258.180s\n",
      "Train: 20 [7850/10009 ( 78%)]  Loss: 3.40 (3.16)  Time: 0.161s,  794.17/s  (0.161s,  793.65/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1266.212s\n",
      "Train: 20 [7900/10009 ( 79%)]  Loss: 3.09 (3.16)  Time: 0.161s,  797.44/s  (0.161s,  793.66/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1274.250s\n",
      "Train: 20 [7950/10009 ( 79%)]  Loss: 2.95 (3.16)  Time: 0.161s,  797.11/s  (0.161s,  793.68/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1282.293s\n",
      "Train: 20 [8000/10009 ( 80%)]  Loss: 3.06 (3.16)  Time: 0.160s,  799.82/s  (0.161s,  793.69/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1290.334s\n",
      "Train: 20 [8050/10009 ( 80%)]  Loss: 3.02 (3.16)  Time: 0.161s,  796.48/s  (0.161s,  793.70/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1298.378s\n",
      "Train: 20 [8100/10009 ( 81%)]  Loss: 2.91 (3.16)  Time: 0.162s,  791.86/s  (0.161s,  793.72/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1306.416s\n",
      "Train: 20 [8150/10009 ( 81%)]  Loss: 3.06 (3.16)  Time: 0.160s,  799.06/s  (0.161s,  793.73/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1314.460s\n",
      "Train: 20 [8200/10009 ( 82%)]  Loss: 2.97 (3.16)  Time: 0.160s,  798.29/s  (0.161s,  793.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1322.493s\n",
      "Train: 20 [8250/10009 ( 82%)]  Loss: 2.89 (3.16)  Time: 0.160s,  800.22/s  (0.161s,  793.76/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1330.530s\n",
      "Train: 20 [8300/10009 ( 83%)]  Loss: 3.35 (3.16)  Time: 0.161s,  794.20/s  (0.161s,  793.77/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1338.573s\n",
      "Train: 20 [8350/10009 ( 83%)]  Loss: 3.10 (3.16)  Time: 0.161s,  793.20/s  (0.161s,  793.79/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1346.611s\n",
      "Train: 20 [8400/10009 ( 84%)]  Loss: 2.78 (3.16)  Time: 0.160s,  797.81/s  (0.161s,  793.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1354.651s\n",
      "Train: 20 [8450/10009 ( 84%)]  Loss: 3.06 (3.16)  Time: 0.162s,  791.99/s  (0.161s,  793.82/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1362.684s\n",
      "Train: 20 [8500/10009 ( 85%)]  Loss: 3.02 (3.16)  Time: 0.161s,  796.54/s  (0.161s,  793.83/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1370.731s\n",
      "Train: 20 [8550/10009 ( 85%)]  Loss: 2.84 (3.16)  Time: 0.159s,  803.10/s  (0.161s,  793.84/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1378.776s\n",
      "Train: 20 [8600/10009 ( 86%)]  Loss: 3.24 (3.16)  Time: 0.160s,  797.68/s  (0.161s,  793.85/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1386.825s\n",
      "Train: 20 [8650/10009 ( 86%)]  Loss: 2.99 (3.16)  Time: 0.160s,  800.09/s  (0.161s,  793.86/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1394.861s\n",
      "Train: 20 [8700/10009 ( 87%)]  Loss: 3.30 (3.16)  Time: 0.160s,  798.08/s  (0.161s,  793.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1402.901s\n",
      "Train: 20 [8750/10009 ( 87%)]  Loss: 3.20 (3.16)  Time: 0.160s,  799.19/s  (0.161s,  793.89/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1410.938s\n",
      "Train: 20 [8800/10009 ( 88%)]  Loss: 2.98 (3.16)  Time: 0.160s,  800.56/s  (0.161s,  793.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1418.984s\n",
      "Train: 20 [8850/10009 ( 88%)]  Loss: 3.04 (3.16)  Time: 0.160s,  799.81/s  (0.161s,  793.91/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1427.017s\n",
      "Train: 20 [8900/10009 ( 89%)]  Loss: 3.15 (3.16)  Time: 0.162s,  791.50/s  (0.161s,  793.92/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1435.058s\n",
      "Train: 20 [8950/10009 ( 89%)]  Loss: 3.20 (3.16)  Time: 0.161s,  794.12/s  (0.161s,  793.93/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1443.102s\n",
      "Train: 20 [9000/10009 ( 90%)]  Loss: 3.20 (3.16)  Time: 0.169s,  757.24/s  (0.161s,  793.58/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1451.798s\n",
      "Train: 20 [9050/10009 ( 90%)]  Loss: 3.11 (3.16)  Time: 0.173s,  741.47/s  (0.161s,  793.20/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1460.564s\n",
      "Train: 20 [9100/10009 ( 91%)]  Loss: 3.24 (3.16)  Time: 0.172s,  744.44/s  (0.161s,  792.87/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1469.249s\n",
      "Train: 20 [9150/10009 ( 91%)]  Loss: 3.01 (3.16)  Time: 0.176s,  725.92/s  (0.162s,  792.51/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1477.992s\n",
      "Train: 20 [9200/10009 ( 92%)]  Loss: 3.30 (3.16)  Time: 0.171s,  747.04/s  (0.162s,  792.17/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1486.701s\n",
      "Train: 20 [9250/10009 ( 92%)]  Loss: 3.15 (3.16)  Time: 0.172s,  745.63/s  (0.162s,  791.89/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1495.313s\n",
      "Train: 20 [9300/10009 ( 93%)]  Loss: 3.07 (3.16)  Time: 0.172s,  743.30/s  (0.162s,  791.61/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1503.920s\n",
      "Train: 20 [9350/10009 ( 93%)]  Loss: 3.25 (3.16)  Time: 0.173s,  740.66/s  (0.162s,  791.33/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1512.548s\n",
      "Train: 20 [9400/10009 ( 94%)]  Loss: 3.07 (3.16)  Time: 0.172s,  743.61/s  (0.162s,  791.06/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1521.147s\n",
      "Train: 20 [9450/10009 ( 94%)]  Loss: 3.28 (3.16)  Time: 0.170s,  750.95/s  (0.162s,  790.80/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1529.748s\n",
      "Train: 20 [9500/10009 ( 95%)]  Loss: 3.20 (3.16)  Time: 0.174s,  736.93/s  (0.162s,  790.48/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1538.458s\n",
      "Train: 20 [9550/10009 ( 95%)]  Loss: 3.11 (3.16)  Time: 0.171s,  747.25/s  (0.162s,  790.20/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1547.112s\n",
      "Train: 20 [9600/10009 ( 96%)]  Loss: 3.13 (3.16)  Time: 0.173s,  740.86/s  (0.162s,  789.90/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1555.791s\n",
      "Train: 20 [9650/10009 ( 96%)]  Loss: 3.26 (3.16)  Time: 0.175s,  732.64/s  (0.162s,  789.57/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1564.546s\n",
      "Train: 20 [9700/10009 ( 97%)]  Loss: 3.03 (3.16)  Time: 0.172s,  743.60/s  (0.162s,  789.27/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1573.256s\n",
      "Train: 20 [9750/10009 ( 97%)]  Loss: 3.26 (3.16)  Time: 0.172s,  745.35/s  (0.162s,  789.01/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1581.880s\n",
      "Train: 20 [9800/10009 ( 98%)]  Loss: 3.37 (3.16)  Time: 0.172s,  745.24/s  (0.162s,  788.75/s)  LR: 0.000e+00  Data: 0.006 (0.007)Time: 1590.521s\n",
      "Train: 20 [9850/10009 ( 98%)]  Loss: 3.12 (3.16)  Time: 0.173s,  740.73/s  (0.162s,  788.47/s)  LR: 0.000e+00  Data: 0.005 (0.007)Time: 1599.211s\n",
      "Train: 20 [9900/10009 ( 99%)]  Loss: 3.08 (3.16)  Time: 0.172s,  742.06/s  (0.162s,  788.22/s)  LR: 0.000e+00  Data: 0.007 (0.007)Time: 1607.821s\n",
      "Train: 20 [9950/10009 ( 99%)]  Loss: 3.28 (3.16)  Time: 0.176s,  726.40/s  (0.162s,  787.97/s)  LR: 0.000e+00  Data: 0.010 (0.007)Time: 1616.467s\n",
      "Train: 20 [10000/10009 (100%)]  Loss: 3.07 (3.16)  Time: 0.217s,  590.77/s  (0.162s,  787.71/s)  LR: 0.000e+00  Data: 0.049 (0.007)Time: 1625.130s\n",
      "Test: [   0/390]  Time: 0.692 (0.692)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.056 (0.144)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.150 (0.137)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.056 (0.141)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.055 (0.138)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.057 (0.137)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.103 (0.136)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.333 (0.135)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.038 (0.135)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-18.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-19.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-20.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      "\n",
      "*** Best metric: 48.84 (epoch 14)\n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        for epoch in range(15, 21):\n",
    "            if hasattr(dataset_train, 'set_epoch'):\n",
    "                dataset_train.set_epoch(epoch)\n",
    "            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n",
    "                loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                train_loss_fn,\n",
    "                args,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                saver=saver,\n",
    "                output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast,\n",
    "                loss_scaler=loss_scaler,\n",
    "                model_ema=model_ema,\n",
    "                mixup_fn=mixup_fn,\n",
    "                # fish: add preconditioner\n",
    "                preconditioner=preconditioner,\n",
    "            )\n",
    "\n",
    "            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                if utils.is_primary(args):\n",
    "                    _logger.info(\"Distributing BatchNorm running means and vars\")\n",
    "                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "            eval_metrics = validate(\n",
    "                model,\n",
    "                loader_eval,\n",
    "                validate_loss_fn,\n",
    "                args,\n",
    "                amp_autocast=amp_autocast,\n",
    "            )\n",
    "\n",
    "            if model_ema is not None and not args.model_ema_force_cpu:\n",
    "                if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                    utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "                ema_eval_metrics = validate(\n",
    "                    model_ema.module,\n",
    "                    loader_eval,\n",
    "                    validate_loss_fn,\n",
    "                    args,\n",
    "                    amp_autocast=amp_autocast,\n",
    "                    log_suffix=' (EMA)',\n",
    "                )\n",
    "                eval_metrics = ema_eval_metrics\n",
    "\n",
    "            if output_dir is not None:\n",
    "                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "                utils.update_summary(\n",
    "                    epoch,\n",
    "                    train_metrics,\n",
    "                    eval_metrics,\n",
    "                    filename=os.path.join(output_dir, 'summary.csv'),\n",
    "                    lr=sum(lrs) / len(lrs),\n",
    "                    write_header=best_metric is None,\n",
    "                    log_wandb=args.log_wandb and has_wandb,\n",
    "                )\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler\n",
    "args.epochs = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model bud_small_patch16_224_man created, param count:22050664\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n",
      "Using native Torch AMP. Training in mixed precision.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4p0mz90a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f74656e5864b6caddac3194a2957e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>eval_loss</td><td>▁</td></tr><tr><td>eval_top1</td><td>▁</td></tr><tr><td>eval_top5</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>eval_loss</td><td>2.65839</td></tr><tr><td>eval_top1</td><td>43.356</td></tr><tr><td>eval_top5</td><td>68.516</td></tr><tr><td>lr</td><td>3e-05</td></tr><tr><td>train_loss</td><td>3.59273</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">budgeted</strong> at: <a href='https://wandb.ai/fabfish/timm/runs/4p0mz90a' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/4p0mz90a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230828_115527-4p0mz90a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4p0mz90a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fe845d4f6b14febab2caa70b423dcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668225050671025, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/fish/Documents/GitHub/pytorch-image-models/wandb/run-20230828_122638-yq3t6v8g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fabfish/timm/runs/yq3t6v8g' target=\"_blank\">budgeted</a></strong> to <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fabfish/timm' target=\"_blank\">https://wandb.ai/fabfish/timm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fabfish/timm/runs/yq3t6v8g' target=\"_blank\">https://wandb.ai/fabfish/timm/runs/yq3t6v8g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scheduled epochs: 21. LR stepped per epoch.\n"
     ]
    }
   ],
   "source": [
    "    # old_model = model\n",
    "    # old_optimizer = optimizer\n",
    "    # old_loss_scaler = loss_scaler\n",
    "    # old_lr_scheduler = lr_scheduler\n",
    "    # if args.use_eva:\n",
    "    #     old_preconditioner = preconditioner\n",
    "\n",
    "    # args.model = args.model.replace('baby', 'child')\n",
    "    # args.model = args.model.replace('child', 'man')\n",
    "    # args.model = args.model.replace('man', 'baby')\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm',name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 10\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0117,  0.0170, -0.0500,  ..., -0.0457, -0.0069, -0.0012],\n",
      "        [-0.0065,  0.0319,  0.0395,  ...,  0.0188,  0.0278, -0.0423],\n",
      "        [-0.0169, -0.0090,  0.0215,  ..., -0.0005,  0.0468, -0.0011],\n",
      "        ...,\n",
      "        [-0.0097,  0.0373,  0.0293,  ..., -0.0073,  0.0088, -0.0163],\n",
      "        [-0.0138, -0.0104,  0.0100,  ...,  0.0055,  0.0143, -0.0381],\n",
      "        [-0.0040, -0.0056,  0.0030,  ..., -0.0136,  0.0052, -0.0294]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0384,  0.0526,  0.0231,  ...,  0.0080, -0.0127, -0.0318],\n",
      "        [ 0.0123,  0.0325, -0.0157,  ..., -0.0021, -0.1333, -0.1377],\n",
      "        [-0.0235, -0.0248, -0.1257,  ...,  0.0762, -0.0377, -0.0869],\n",
      "        ...,\n",
      "        [-0.0528,  0.0271,  0.0363,  ...,  0.0818,  0.0323, -0.0372],\n",
      "        [ 0.0458,  0.0301, -0.0118,  ...,  0.0388, -0.0197,  0.0305],\n",
      "        [ 0.1512,  0.0830, -0.0156,  ...,  0.0436,  0.0078,  0.0451]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0384,  0.0526,  0.0231,  ...,  0.0080, -0.0127, -0.0318],\n",
      "        [ 0.0123,  0.0325, -0.0157,  ..., -0.0021, -0.1333, -0.1377],\n",
      "        [-0.0235, -0.0248, -0.1257,  ...,  0.0762, -0.0377, -0.0869],\n",
      "        ...,\n",
      "        [-0.0528,  0.0271,  0.0363,  ...,  0.0818,  0.0323, -0.0372],\n",
      "        [ 0.0458,  0.0301, -0.0118,  ...,  0.0388, -0.0197,  0.0305],\n",
      "        [ 0.1512,  0.0830, -0.0156,  ...,  0.0436,  0.0078,  0.0451]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "print(old_model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    tmp = old_model.state_dict()\n",
    "    for param_tensor in old_model.state_dict(): # 字典的遍历默认是遍历 key，所以param_tensor实际上是键值\n",
    "        # print(param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "        # 如果不是 attn 或者 mlp 层\n",
    "        if 'attn' not in param_tensor and 'mlp' not in param_tensor:\n",
    "            continue\n",
    "        old_size = old_model.state_dict()[param_tensor].size()\n",
    "        new_size = model.state_dict()[param_tensor].size()\n",
    "        flag = False\n",
    "        if len(old_size) != len(new_size):\n",
    "            continue\n",
    "        elif len(old_size) == 1:\n",
    "            # 如果整除\n",
    "            if new_size[0] % old_size[0] == 0:\n",
    "                times = new_size[0] // old_size[0]\n",
    "                tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "            # if old_size[0]*2 == new_size[0]:\n",
    "            #     tmp[param_tensor] = torch.cat((tmp[param_tensor], tmp[param_tensor]), 0)\n",
    "                flag = True\n",
    "            # 如果是 2:3\n",
    "            elif old_size[0]*3 == new_size[0]*2:\n",
    "                # 拼接tensor和它自己的一半\n",
    "                if old_size[0] % 2 == 0:\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                    # tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][]), 0)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    # 未实现\n",
    "                    print('error')\n",
    "        elif len(old_size) == 2:\n",
    "            if old_size[0] == new_size[0]:\n",
    "                # if old_size[1]*2 == new_size[1]:\n",
    "                if new_size[1] % old_size[1] == 0:\n",
    "                    times = new_size[1] // old_size[1]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 1)\n",
    "                    flag = True\n",
    "                elif old_size[1]*3 == new_size[1]*2:\n",
    "                    if old_size[1] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:,old_size[1]//2]), 1)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "            elif old_size[1] == new_size[1]:\n",
    "                # if old_size[0]*2 == new_size[0]:\n",
    "                if new_size[0] % old_size[0] == 0:\n",
    "                    times = new_size[0] // old_size[0]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "                    flag = True\n",
    "                elif old_size[0]*3 == new_size[0]*2:    \n",
    "                    if old_size[0] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "        if not flag:\n",
    "            if param_tensor in model.state_dict():\n",
    "                tmp[param_tensor] = model.state_dict()[param_tensor]\n",
    "            else:\n",
    "                continue\n",
    "    model.load_state_dict(tmp, strict=False)\n",
    "    print(model.state_dict()['blocks.0.attn.qkv.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0206, device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor(0.0206, device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# print(old_optimizer.param_groups[0]['params'][0][0][0][0])\n",
    "# print(optimizer.param_groups[0]['params'][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(old_optimizer.param_groups[0]['params'])\n",
    "# 接下来，遍历原始模型的参数，把他们加入新的优化器中\n",
    "for param_group in range(len(old_optimizer.param_groups)):\n",
    "    for i, (old_p, new_p) in enumerate(zip(old_optimizer.param_groups[param_group]['params'], optimizer.param_groups[param_group]['params'])):\n",
    "        # 查看 old_p 和 new_p 的 shape\n",
    "        # print(old_p.shape, new_p.shape)\n",
    "        with torch.no_grad():\n",
    "            if old_p.shape == new_p.shape:\n",
    "                new_p = old_p\n",
    "            else:\n",
    "                # 否则，用 0 填补 new_p 的后半部分\n",
    "                # 如果 new_p 是 1 维\n",
    "                if len(new_p.shape) == 1:\n",
    "                    new_p[:old_p.shape[0]] = old_p\n",
    "                # 如果 new_p 是 2 维\n",
    "                elif len(new_p.shape) == 2:\n",
    "                    new_p[:old_p.shape[0], :old_p.shape[1]] = old_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 10 [   0/10009 (  0%)]  Loss: 7.29 (7.29)  Time: 0.832s,  153.79/s  (0.832s,  153.79/s)  LR: 2.687e-05  Data: 0.650 (0.650)Time: 0.833s\n",
      "Train: 10 [  50/10009 (  0%)]  Loss: 4.84 (5.42)  Time: 0.167s,  766.34/s  (0.181s,  707.81/s)  LR: 2.687e-05  Data: 0.006 (0.018)Time: 9.223s\n",
      "Train: 10 [ 100/10009 (  1%)]  Loss: 4.16 (4.90)  Time: 0.168s,  762.48/s  (0.174s,  735.11/s)  LR: 2.687e-05  Data: 0.005 (0.012)Time: 17.587s\n",
      "Train: 10 [ 150/10009 (  1%)]  Loss: 3.91 (4.63)  Time: 0.170s,  752.45/s  (0.172s,  742.46/s)  LR: 2.687e-05  Data: 0.006 (0.010)Time: 26.033s\n",
      "Train: 10 [ 200/10009 (  2%)]  Loss: 3.69 (4.47)  Time: 0.170s,  753.47/s  (0.172s,  744.53/s)  LR: 2.687e-05  Data: 0.006 (0.009)Time: 34.557s\n",
      "Train: 10 [ 250/10009 (  2%)]  Loss: 3.93 (4.35)  Time: 0.168s,  762.29/s  (0.172s,  742.54/s)  LR: 2.687e-05  Data: 0.007 (0.008)Time: 43.268s\n",
      "Train: 10 [ 300/10009 (  3%)]  Loss: 3.92 (4.26)  Time: 0.171s,  749.86/s  (0.172s,  743.03/s)  LR: 2.687e-05  Data: 0.006 (0.008)Time: 51.853s\n",
      "Train: 10 [ 350/10009 (  3%)]  Loss: 3.67 (4.19)  Time: 0.170s,  752.01/s  (0.172s,  744.06/s)  LR: 2.687e-05  Data: 0.005 (0.008)Time: 60.382s\n",
      "Train: 10 [ 400/10009 (  4%)]  Loss: 3.41 (4.13)  Time: 0.179s,  713.98/s  (0.172s,  742.97/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 69.085s\n",
      "Train: 10 [ 450/10009 (  4%)]  Loss: 3.83 (4.09)  Time: 0.183s,  700.44/s  (0.173s,  741.57/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 77.846s\n",
      "Train: 10 [ 500/10009 (  5%)]  Loss: 3.78 (4.05)  Time: 0.171s,  746.61/s  (0.173s,  740.83/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 86.563s\n",
      "Train: 10 [ 550/10009 (  5%)]  Loss: 3.70 (4.02)  Time: 0.185s,  692.97/s  (0.173s,  740.76/s)  LR: 2.687e-05  Data: 0.023 (0.007)Time: 95.211s\n",
      "Train: 10 [ 600/10009 (  6%)]  Loss: 3.93 (4.00)  Time: 0.172s,  745.23/s  (0.173s,  741.37/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 103.764s\n",
      "Train: 10 [ 650/10009 (  6%)]  Loss: 3.60 (3.97)  Time: 0.171s,  748.99/s  (0.173s,  741.06/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 112.444s\n",
      "Train: 10 [ 700/10009 (  7%)]  Loss: 3.72 (3.95)  Time: 0.165s,  776.10/s  (0.173s,  740.81/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 121.121s\n",
      "Train: 10 [ 750/10009 (  7%)]  Loss: 3.73 (3.93)  Time: 0.185s,  691.97/s  (0.172s,  742.21/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 129.516s\n",
      "Train: 10 [ 800/10009 (  8%)]  Loss: 3.87 (3.92)  Time: 0.183s,  698.85/s  (0.173s,  742.00/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 138.179s\n",
      "Train: 10 [ 850/10009 (  8%)]  Loss: 3.90 (3.90)  Time: 0.177s,  723.68/s  (0.173s,  741.44/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 146.915s\n",
      "Train: 10 [ 900/10009 (  9%)]  Loss: 4.03 (3.89)  Time: 0.168s,  762.59/s  (0.172s,  742.19/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 155.388s\n",
      "Train: 10 [ 950/10009 (  9%)]  Loss: 3.64 (3.88)  Time: 0.167s,  767.17/s  (0.172s,  743.20/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 163.789s\n",
      "Train: 10 [1000/10009 ( 10%)]  Loss: 3.73 (3.87)  Time: 0.169s,  759.15/s  (0.172s,  742.64/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 172.531s\n",
      "Train: 10 [1050/10009 ( 10%)]  Loss: 3.63 (3.86)  Time: 0.166s,  773.30/s  (0.172s,  743.45/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 180.951s\n",
      "Train: 10 [1100/10009 ( 11%)]  Loss: 3.72 (3.85)  Time: 0.180s,  710.48/s  (0.172s,  743.43/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 189.564s\n",
      "Train: 10 [1150/10009 ( 11%)]  Loss: 3.82 (3.84)  Time: 0.169s,  756.34/s  (0.172s,  743.16/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 198.246s\n",
      "Train: 10 [1200/10009 ( 12%)]  Loss: 3.62 (3.83)  Time: 0.167s,  766.71/s  (0.172s,  744.02/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 206.617s\n",
      "Train: 10 [1250/10009 ( 12%)]  Loss: 3.64 (3.82)  Time: 0.167s,  766.90/s  (0.172s,  744.65/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 215.038s\n",
      "Train: 10 [1300/10009 ( 13%)]  Loss: 3.64 (3.82)  Time: 0.171s,  747.78/s  (0.172s,  744.55/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 223.662s\n",
      "Train: 10 [1350/10009 ( 13%)]  Loss: 3.87 (3.81)  Time: 0.182s,  701.42/s  (0.172s,  743.79/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 232.494s\n",
      "Train: 10 [1400/10009 ( 14%)]  Loss: 3.77 (3.81)  Time: 0.171s,  748.64/s  (0.172s,  742.74/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 241.441s\n",
      "Train: 10 [1450/10009 ( 14%)]  Loss: 3.84 (3.80)  Time: 0.177s,  723.35/s  (0.172s,  742.22/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 250.234s\n",
      "Train: 10 [1500/10009 ( 15%)]  Loss: 3.41 (3.79)  Time: 0.180s,  709.81/s  (0.172s,  742.09/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 258.901s\n",
      "Train: 10 [1550/10009 ( 15%)]  Loss: 3.48 (3.79)  Time: 0.178s,  718.62/s  (0.173s,  741.51/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 267.734s\n",
      "Train: 10 [1600/10009 ( 16%)]  Loss: 3.90 (3.78)  Time: 0.169s,  757.93/s  (0.173s,  741.83/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 276.248s\n",
      "Train: 10 [1650/10009 ( 16%)]  Loss: 3.95 (3.78)  Time: 0.173s,  741.41/s  (0.173s,  741.74/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 284.908s\n",
      "Train: 10 [1700/10009 ( 17%)]  Loss: 3.56 (3.78)  Time: 0.168s,  762.61/s  (0.172s,  742.19/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 293.360s\n",
      "Train: 10 [1750/10009 ( 17%)]  Loss: 3.56 (3.77)  Time: 0.171s,  748.61/s  (0.172s,  742.65/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 301.796s\n",
      "Train: 10 [1800/10009 ( 18%)]  Loss: 3.44 (3.77)  Time: 0.169s,  755.83/s  (0.172s,  743.09/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 310.230s\n",
      "Train: 10 [1850/10009 ( 18%)]  Loss: 3.64 (3.76)  Time: 0.169s,  757.86/s  (0.172s,  743.55/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 318.642s\n",
      "Train: 10 [1900/10009 ( 19%)]  Loss: 3.57 (3.76)  Time: 0.167s,  768.06/s  (0.172s,  743.94/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 327.082s\n",
      "Train: 10 [1950/10009 ( 19%)]  Loss: 3.61 (3.76)  Time: 0.168s,  763.45/s  (0.172s,  744.42/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 335.467s\n",
      "Train: 10 [2000/10009 ( 20%)]  Loss: 3.61 (3.75)  Time: 0.168s,  764.10/s  (0.172s,  744.83/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 343.874s\n",
      "Train: 10 [2050/10009 ( 20%)]  Loss: 3.70 (3.75)  Time: 0.167s,  766.71/s  (0.172s,  745.17/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 352.306s\n",
      "Train: 10 [2100/10009 ( 21%)]  Loss: 3.51 (3.75)  Time: 0.171s,  750.16/s  (0.172s,  745.54/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 360.717s\n",
      "Train: 10 [2150/10009 ( 21%)]  Loss: 3.39 (3.74)  Time: 0.172s,  743.11/s  (0.172s,  745.78/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 369.181s\n",
      "Train: 10 [2200/10009 ( 22%)]  Loss: 3.47 (3.74)  Time: 0.171s,  748.62/s  (0.172s,  746.16/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 377.569s\n",
      "Train: 10 [2250/10009 ( 22%)]  Loss: 3.62 (3.74)  Time: 0.167s,  765.98/s  (0.171s,  746.48/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 385.983s\n",
      "Train: 10 [2300/10009 ( 23%)]  Loss: 3.53 (3.73)  Time: 0.170s,  754.17/s  (0.171s,  746.81/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 394.379s\n",
      "Train: 10 [2350/10009 ( 23%)]  Loss: 3.58 (3.73)  Time: 0.167s,  766.86/s  (0.171s,  747.13/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 402.781s\n",
      "Train: 10 [2400/10009 ( 24%)]  Loss: 3.63 (3.73)  Time: 0.167s,  768.74/s  (0.171s,  747.30/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 411.249s\n",
      "Train: 10 [2450/10009 ( 24%)]  Loss: 3.62 (3.73)  Time: 0.170s,  753.71/s  (0.171s,  747.57/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 419.664s\n",
      "Train: 10 [2500/10009 ( 25%)]  Loss: 3.60 (3.72)  Time: 0.170s,  750.86/s  (0.171s,  747.74/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 428.125s\n",
      "Train: 10 [2550/10009 ( 25%)]  Loss: 3.45 (3.72)  Time: 0.167s,  765.81/s  (0.171s,  748.02/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 436.521s\n",
      "Train: 10 [2600/10009 ( 26%)]  Loss: 3.54 (3.72)  Time: 0.168s,  761.42/s  (0.171s,  748.26/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 444.935s\n",
      "Train: 10 [2650/10009 ( 26%)]  Loss: 3.85 (3.72)  Time: 0.166s,  770.27/s  (0.171s,  748.51/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 453.339s\n",
      "Train: 10 [2700/10009 ( 27%)]  Loss: 3.72 (3.71)  Time: 0.168s,  761.06/s  (0.171s,  748.65/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 461.799s\n",
      "Train: 10 [2750/10009 ( 27%)]  Loss: 3.50 (3.71)  Time: 0.170s,  753.95/s  (0.171s,  748.83/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 470.238s\n",
      "Train: 10 [2800/10009 ( 28%)]  Loss: 3.37 (3.71)  Time: 0.171s,  748.44/s  (0.171s,  749.06/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 478.635s\n",
      "Train: 10 [2850/10009 ( 28%)]  Loss: 3.78 (3.71)  Time: 0.167s,  768.18/s  (0.171s,  749.22/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 487.076s\n",
      "Train: 10 [2900/10009 ( 29%)]  Loss: 3.73 (3.71)  Time: 0.167s,  765.10/s  (0.171s,  749.43/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 495.480s\n",
      "Train: 10 [2950/10009 ( 29%)]  Loss: 3.81 (3.70)  Time: 0.170s,  754.25/s  (0.171s,  749.58/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 503.921s\n",
      "Train: 10 [3000/10009 ( 30%)]  Loss: 3.29 (3.70)  Time: 0.168s,  760.43/s  (0.171s,  749.74/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 512.345s\n",
      "Train: 10 [3050/10009 ( 30%)]  Loss: 3.53 (3.70)  Time: 0.168s,  762.36/s  (0.171s,  749.93/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 520.753s\n",
      "Train: 10 [3100/10009 ( 31%)]  Loss: 3.71 (3.70)  Time: 0.174s,  736.02/s  (0.171s,  750.09/s)  LR: 2.687e-05  Data: 0.010 (0.006)Time: 529.171s\n",
      "Train: 10 [3150/10009 ( 31%)]  Loss: 3.54 (3.70)  Time: 0.169s,  755.17/s  (0.171s,  750.26/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 537.580s\n",
      "Train: 10 [3200/10009 ( 32%)]  Loss: 3.42 (3.69)  Time: 0.168s,  762.54/s  (0.171s,  750.38/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 546.026s\n",
      "Train: 10 [3250/10009 ( 32%)]  Loss: 3.34 (3.69)  Time: 0.168s,  761.82/s  (0.171s,  750.54/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 554.439s\n",
      "Train: 10 [3300/10009 ( 33%)]  Loss: 3.69 (3.69)  Time: 0.169s,  758.05/s  (0.171s,  750.69/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 562.855s\n",
      "Train: 10 [3350/10009 ( 33%)]  Loss: 3.70 (3.69)  Time: 0.167s,  765.70/s  (0.171s,  750.69/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 571.379s\n",
      "Train: 10 [3400/10009 ( 34%)]  Loss: 3.28 (3.69)  Time: 0.168s,  759.89/s  (0.170s,  750.86/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 579.774s\n",
      "Train: 10 [3450/10009 ( 34%)]  Loss: 3.70 (3.69)  Time: 0.168s,  759.90/s  (0.170s,  750.85/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 588.301s\n",
      "Train: 10 [3500/10009 ( 35%)]  Loss: 3.70 (3.69)  Time: 0.182s,  704.23/s  (0.171s,  750.63/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 597.001s\n",
      "Train: 10 [3550/10009 ( 35%)]  Loss: 3.49 (3.68)  Time: 0.185s,  692.88/s  (0.171s,  750.36/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 605.749s\n",
      "Train: 10 [3600/10009 ( 36%)]  Loss: 3.54 (3.68)  Time: 0.167s,  765.18/s  (0.171s,  749.95/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 614.609s\n",
      "Train: 10 [3650/10009 ( 36%)]  Loss: 3.40 (3.68)  Time: 0.169s,  757.26/s  (0.171s,  749.64/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 623.400s\n",
      "Train: 10 [3700/10009 ( 37%)]  Loss: 3.26 (3.68)  Time: 0.185s,  693.24/s  (0.171s,  749.29/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 632.232s\n",
      "Train: 10 [3750/10009 ( 37%)]  Loss: 3.61 (3.68)  Time: 0.166s,  772.00/s  (0.171s,  749.34/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 640.735s\n",
      "Train: 10 [3800/10009 ( 38%)]  Loss: 3.59 (3.68)  Time: 0.170s,  754.67/s  (0.171s,  749.33/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 649.279s\n",
      "Train: 10 [3850/10009 ( 38%)]  Loss: 3.32 (3.68)  Time: 0.167s,  765.24/s  (0.171s,  749.37/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 657.785s\n",
      "Train: 10 [3900/10009 ( 39%)]  Loss: 3.55 (3.67)  Time: 0.168s,  760.47/s  (0.171s,  749.45/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 666.256s\n",
      "Train: 10 [3950/10009 ( 39%)]  Loss: 3.78 (3.67)  Time: 0.170s,  752.95/s  (0.171s,  749.42/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 674.820s\n",
      "Train: 10 [4000/10009 ( 40%)]  Loss: 3.69 (3.67)  Time: 0.173s,  740.62/s  (0.171s,  749.31/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 683.460s\n",
      "Train: 10 [4050/10009 ( 40%)]  Loss: 3.36 (3.67)  Time: 0.169s,  758.44/s  (0.171s,  749.23/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 692.078s\n",
      "Train: 10 [4100/10009 ( 41%)]  Loss: 3.78 (3.67)  Time: 0.176s,  728.18/s  (0.171s,  749.18/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 700.672s\n",
      "Train: 10 [4150/10009 ( 41%)]  Loss: 3.44 (3.67)  Time: 0.186s,  686.89/s  (0.171s,  748.87/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 709.502s\n",
      "Train: 10 [4200/10009 ( 42%)]  Loss: 3.09 (3.67)  Time: 0.170s,  752.45/s  (0.171s,  748.43/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 718.469s\n",
      "Train: 10 [4250/10009 ( 42%)]  Loss: 3.44 (3.67)  Time: 0.169s,  756.91/s  (0.171s,  748.43/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 727.029s\n",
      "Train: 10 [4300/10009 ( 43%)]  Loss: 3.52 (3.67)  Time: 0.186s,  687.90/s  (0.171s,  748.25/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 735.750s\n",
      "Train: 10 [4350/10009 ( 43%)]  Loss: 3.79 (3.66)  Time: 0.172s,  746.31/s  (0.171s,  748.26/s)  LR: 2.687e-05  Data: 0.009 (0.007)Time: 744.300s\n",
      "Train: 10 [4400/10009 ( 44%)]  Loss: 3.33 (3.66)  Time: 0.171s,  748.49/s  (0.171s,  748.36/s)  LR: 2.687e-05  Data: 0.008 (0.007)Time: 752.749s\n",
      "Train: 10 [4450/10009 ( 44%)]  Loss: 3.44 (3.66)  Time: 0.170s,  753.72/s  (0.171s,  748.47/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 761.185s\n",
      "Train: 10 [4500/10009 ( 45%)]  Loss: 3.12 (3.66)  Time: 0.167s,  768.05/s  (0.171s,  748.61/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 769.590s\n",
      "Train: 10 [4550/10009 ( 45%)]  Loss: 3.44 (3.66)  Time: 0.169s,  758.17/s  (0.171s,  748.73/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 778.019s\n",
      "Train: 10 [4600/10009 ( 46%)]  Loss: 3.35 (3.66)  Time: 0.167s,  768.56/s  (0.171s,  748.85/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 786.441s\n",
      "Train: 10 [4650/10009 ( 46%)]  Loss: 3.53 (3.66)  Time: 0.167s,  766.49/s  (0.171s,  748.99/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 794.843s\n",
      "Train: 10 [4700/10009 ( 47%)]  Loss: 3.87 (3.66)  Time: 0.174s,  736.00/s  (0.171s,  749.07/s)  LR: 2.687e-05  Data: 0.009 (0.007)Time: 803.299s\n",
      "Train: 10 [4750/10009 ( 47%)]  Loss: 3.44 (3.65)  Time: 0.169s,  758.91/s  (0.171s,  749.20/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 811.705s\n",
      "Train: 10 [4800/10009 ( 48%)]  Loss: 3.50 (3.65)  Time: 0.169s,  757.63/s  (0.171s,  749.25/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 820.185s\n",
      "Train: 10 [4850/10009 ( 48%)]  Loss: 3.60 (3.65)  Time: 0.166s,  772.51/s  (0.171s,  749.35/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 828.621s\n",
      "Train: 10 [4900/10009 ( 49%)]  Loss: 3.40 (3.65)  Time: 0.167s,  766.04/s  (0.171s,  749.46/s)  LR: 2.687e-05  Data: 0.005 (0.007)Time: 837.037s\n",
      "Train: 10 [4950/10009 ( 49%)]  Loss: 3.89 (3.65)  Time: 0.169s,  758.14/s  (0.171s,  749.58/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 845.445s\n",
      "Train: 10 [5000/10009 ( 50%)]  Loss: 3.53 (3.65)  Time: 0.167s,  765.06/s  (0.171s,  749.71/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 853.835s\n",
      "Train: 10 [5050/10009 ( 50%)]  Loss: 3.39 (3.65)  Time: 0.177s,  721.20/s  (0.171s,  749.68/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 862.402s\n",
      "Train: 10 [5100/10009 ( 51%)]  Loss: 3.59 (3.65)  Time: 0.166s,  770.15/s  (0.171s,  749.42/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 871.246s\n",
      "Train: 10 [5150/10009 ( 51%)]  Loss: 3.72 (3.65)  Time: 0.168s,  761.56/s  (0.171s,  749.21/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 880.028s\n",
      "Train: 10 [5200/10009 ( 52%)]  Loss: 3.79 (3.65)  Time: 0.185s,  693.50/s  (0.171s,  749.08/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 888.720s\n",
      "Train: 10 [5250/10009 ( 52%)]  Loss: 3.71 (3.65)  Time: 0.170s,  754.46/s  (0.171s,  748.89/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 897.493s\n",
      "Train: 10 [5300/10009 ( 53%)]  Loss: 3.55 (3.64)  Time: 0.176s,  727.27/s  (0.171s,  748.80/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 906.147s\n",
      "Train: 10 [5350/10009 ( 53%)]  Loss: 3.44 (3.64)  Time: 0.171s,  748.89/s  (0.171s,  748.58/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 914.967s\n",
      "Train: 10 [5400/10009 ( 54%)]  Loss: 3.59 (3.64)  Time: 0.170s,  752.68/s  (0.171s,  748.61/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 923.476s\n",
      "Train: 10 [5450/10009 ( 54%)]  Loss: 3.83 (3.64)  Time: 0.170s,  753.06/s  (0.171s,  748.66/s)  LR: 2.687e-05  Data: 0.007 (0.007)Time: 931.967s\n",
      "Train: 10 [5500/10009 ( 55%)]  Loss: 3.48 (3.64)  Time: 0.174s,  737.17/s  (0.171s,  748.63/s)  LR: 2.687e-05  Data: 0.009 (0.007)Time: 940.549s\n",
      "Train: 10 [5550/10009 ( 55%)]  Loss: 3.53 (3.64)  Time: 0.174s,  735.89/s  (0.171s,  748.62/s)  LR: 2.687e-05  Data: 0.009 (0.007)Time: 949.118s\n",
      "Train: 10 [5600/10009 ( 56%)]  Loss: 3.50 (3.64)  Time: 0.171s,  746.83/s  (0.171s,  748.65/s)  LR: 2.687e-05  Data: 0.006 (0.007)Time: 957.631s\n",
      "Train: 10 [5650/10009 ( 56%)]  Loss: 3.33 (3.64)  Time: 0.169s,  757.28/s  (0.171s,  748.68/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 966.140s\n",
      "Train: 10 [5700/10009 ( 57%)]  Loss: 3.58 (3.64)  Time: 0.168s,  762.61/s  (0.171s,  748.69/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 974.667s\n",
      "Train: 10 [5750/10009 ( 57%)]  Loss: 3.76 (3.64)  Time: 0.176s,  725.46/s  (0.171s,  748.64/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 983.288s\n",
      "Train: 10 [5800/10009 ( 58%)]  Loss: 3.65 (3.64)  Time: 0.171s,  749.55/s  (0.171s,  748.59/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 991.898s\n",
      "Train: 10 [5850/10009 ( 58%)]  Loss: 3.44 (3.64)  Time: 0.167s,  764.83/s  (0.171s,  748.48/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1000.598s\n",
      "Train: 10 [5900/10009 ( 59%)]  Loss: 3.65 (3.64)  Time: 0.169s,  757.94/s  (0.171s,  748.51/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1009.108s\n",
      "Train: 10 [5950/10009 ( 59%)]  Loss: 3.60 (3.63)  Time: 0.167s,  764.69/s  (0.171s,  748.42/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1017.784s\n",
      "Train: 10 [6000/10009 ( 60%)]  Loss: 3.43 (3.63)  Time: 0.169s,  756.61/s  (0.171s,  748.42/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1026.329s\n",
      "Train: 10 [6050/10009 ( 60%)]  Loss: 3.34 (3.63)  Time: 0.180s,  711.67/s  (0.171s,  748.47/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1034.809s\n",
      "Train: 10 [6100/10009 ( 61%)]  Loss: 3.30 (3.63)  Time: 0.172s,  745.15/s  (0.171s,  748.32/s)  LR: 2.687e-05  Data: 0.009 (0.006)Time: 1043.573s\n",
      "Train: 10 [6150/10009 ( 61%)]  Loss: 3.83 (3.63)  Time: 0.169s,  755.85/s  (0.171s,  748.31/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1052.134s\n",
      "Train: 10 [6200/10009 ( 62%)]  Loss: 3.33 (3.63)  Time: 0.169s,  759.07/s  (0.171s,  748.36/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1060.623s\n",
      "Train: 10 [6250/10009 ( 62%)]  Loss: 3.40 (3.63)  Time: 0.169s,  759.22/s  (0.171s,  748.36/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1069.179s\n",
      "Train: 10 [6300/10009 ( 63%)]  Loss: 3.64 (3.63)  Time: 0.171s,  747.08/s  (0.171s,  748.41/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1077.648s\n",
      "Train: 10 [6350/10009 ( 63%)]  Loss: 3.18 (3.63)  Time: 0.170s,  752.47/s  (0.171s,  748.37/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1086.263s\n",
      "Train: 10 [6400/10009 ( 64%)]  Loss: 3.77 (3.63)  Time: 0.174s,  737.04/s  (0.171s,  748.36/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1094.831s\n",
      "Train: 10 [6450/10009 ( 64%)]  Loss: 3.38 (3.63)  Time: 0.174s,  735.73/s  (0.171s,  748.34/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1103.411s\n",
      "Train: 10 [6500/10009 ( 65%)]  Loss: 3.48 (3.63)  Time: 0.186s,  689.61/s  (0.171s,  748.32/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 1111.993s\n",
      "Train: 10 [6550/10009 ( 65%)]  Loss: 3.40 (3.63)  Time: 0.172s,  743.50/s  (0.171s,  748.29/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1120.589s\n",
      "Train: 10 [6600/10009 ( 66%)]  Loss: 3.72 (3.63)  Time: 0.167s,  768.62/s  (0.171s,  748.36/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1129.031s\n",
      "Train: 10 [6650/10009 ( 66%)]  Loss: 3.24 (3.62)  Time: 0.187s,  686.30/s  (0.171s,  748.39/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1137.538s\n",
      "Train: 10 [6700/10009 ( 67%)]  Loss: 3.58 (3.62)  Time: 0.187s,  682.69/s  (0.171s,  748.25/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 1146.315s\n",
      "Train: 10 [6750/10009 ( 67%)]  Loss: 3.50 (3.62)  Time: 0.170s,  753.90/s  (0.171s,  748.11/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1155.082s\n",
      "Train: 10 [6800/10009 ( 68%)]  Loss: 3.55 (3.62)  Time: 0.186s,  686.69/s  (0.171s,  748.12/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1163.625s\n",
      "Train: 10 [6850/10009 ( 68%)]  Loss: 3.63 (3.62)  Time: 0.168s,  761.52/s  (0.171s,  747.91/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1172.499s\n",
      "Train: 10 [6900/10009 ( 69%)]  Loss: 3.80 (3.62)  Time: 0.165s,  775.97/s  (0.171s,  747.81/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1181.216s\n",
      "Train: 10 [6950/10009 ( 69%)]  Loss: 3.80 (3.62)  Time: 0.186s,  688.36/s  (0.171s,  747.55/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1190.192s\n",
      "Train: 10 [7000/10009 ( 70%)]  Loss: 3.40 (3.62)  Time: 0.168s,  760.48/s  (0.171s,  747.35/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1199.071s\n",
      "Train: 10 [7050/10009 ( 70%)]  Loss: 3.76 (3.62)  Time: 0.188s,  681.98/s  (0.171s,  747.29/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1207.728s\n",
      "Train: 10 [7100/10009 ( 71%)]  Loss: 3.84 (3.62)  Time: 0.168s,  763.37/s  (0.171s,  747.32/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1216.253s\n",
      "Train: 10 [7150/10009 ( 71%)]  Loss: 3.72 (3.62)  Time: 0.166s,  770.86/s  (0.171s,  747.38/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1224.719s\n",
      "Train: 10 [7200/10009 ( 72%)]  Loss: 3.72 (3.62)  Time: 0.169s,  759.12/s  (0.171s,  747.47/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1233.131s\n",
      "Train: 10 [7250/10009 ( 72%)]  Loss: 3.48 (3.62)  Time: 0.169s,  755.69/s  (0.171s,  747.55/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1241.563s\n",
      "Train: 10 [7300/10009 ( 73%)]  Loss: 3.50 (3.62)  Time: 0.169s,  756.59/s  (0.171s,  747.64/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1249.962s\n",
      "Train: 10 [7350/10009 ( 73%)]  Loss: 3.62 (3.62)  Time: 0.167s,  767.33/s  (0.171s,  747.73/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1258.382s\n",
      "Train: 10 [7400/10009 ( 74%)]  Loss: 3.42 (3.62)  Time: 0.168s,  761.74/s  (0.171s,  747.80/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 1266.822s\n",
      "Train: 10 [7450/10009 ( 74%)]  Loss: 3.42 (3.62)  Time: 0.189s,  676.94/s  (0.171s,  747.59/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 1275.733s\n",
      "Train: 10 [7500/10009 ( 75%)]  Loss: 3.39 (3.61)  Time: 0.168s,  763.56/s  (0.171s,  747.62/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1284.236s\n",
      "Train: 10 [7550/10009 ( 75%)]  Loss: 3.72 (3.61)  Time: 0.172s,  744.24/s  (0.171s,  747.71/s)  LR: 2.687e-05  Data: 0.008 (0.006)Time: 1292.651s\n",
      "Train: 10 [7600/10009 ( 76%)]  Loss: 3.41 (3.61)  Time: 0.171s,  746.70/s  (0.171s,  747.73/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1301.175s\n",
      "Train: 10 [7650/10009 ( 76%)]  Loss: 3.49 (3.61)  Time: 0.168s,  760.41/s  (0.171s,  747.65/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1309.880s\n",
      "Train: 10 [7700/10009 ( 77%)]  Loss: 3.34 (3.61)  Time: 0.187s,  685.80/s  (0.171s,  747.66/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1318.408s\n",
      "Train: 10 [7750/10009 ( 77%)]  Loss: 3.36 (3.61)  Time: 0.167s,  768.44/s  (0.171s,  747.52/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1327.222s\n",
      "Train: 10 [7800/10009 ( 78%)]  Loss: 3.76 (3.61)  Time: 0.171s,  750.47/s  (0.171s,  747.53/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1335.765s\n",
      "Train: 10 [7850/10009 ( 78%)]  Loss: 3.50 (3.61)  Time: 0.169s,  757.88/s  (0.171s,  747.59/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1344.218s\n",
      "Train: 10 [7900/10009 ( 79%)]  Loss: 3.74 (3.61)  Time: 0.186s,  688.96/s  (0.171s,  747.56/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1352.841s\n",
      "Train: 10 [7950/10009 ( 79%)]  Loss: 3.59 (3.61)  Time: 0.171s,  747.52/s  (0.171s,  747.56/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1361.393s\n",
      "Train: 10 [8000/10009 ( 80%)]  Loss: 3.63 (3.61)  Time: 0.166s,  769.55/s  (0.171s,  747.59/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1369.896s\n",
      "Train: 10 [8050/10009 ( 80%)]  Loss: 3.46 (3.61)  Time: 0.186s,  689.32/s  (0.171s,  747.60/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1378.447s\n",
      "Train: 10 [8100/10009 ( 81%)]  Loss: 3.48 (3.61)  Time: 0.169s,  758.52/s  (0.171s,  747.57/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1387.052s\n",
      "Train: 10 [8150/10009 ( 81%)]  Loss: 3.52 (3.61)  Time: 0.169s,  757.03/s  (0.171s,  747.66/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1395.457s\n",
      "Train: 10 [8200/10009 ( 82%)]  Loss: 3.35 (3.61)  Time: 0.181s,  708.90/s  (0.171s,  747.61/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1404.113s\n",
      "Train: 10 [8250/10009 ( 82%)]  Loss: 3.86 (3.61)  Time: 0.185s,  692.19/s  (0.171s,  747.59/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1412.715s\n",
      "Train: 10 [8300/10009 ( 83%)]  Loss: 3.88 (3.61)  Time: 0.167s,  766.99/s  (0.171s,  747.54/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1421.368s\n",
      "Train: 10 [8350/10009 ( 83%)]  Loss: 3.51 (3.61)  Time: 0.168s,  761.43/s  (0.171s,  747.54/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1429.930s\n",
      "Train: 10 [8400/10009 ( 84%)]  Loss: 3.64 (3.61)  Time: 0.184s,  696.02/s  (0.171s,  747.39/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1438.766s\n",
      "Train: 10 [8450/10009 ( 84%)]  Loss: 3.44 (3.61)  Time: 0.168s,  759.73/s  (0.171s,  747.14/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1447.814s\n",
      "Train: 10 [8500/10009 ( 85%)]  Loss: 3.63 (3.61)  Time: 0.168s,  761.49/s  (0.171s,  747.20/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1456.272s\n",
      "Train: 10 [8550/10009 ( 85%)]  Loss: 3.89 (3.60)  Time: 0.167s,  767.40/s  (0.171s,  747.13/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1464.974s\n",
      "Train: 10 [8600/10009 ( 86%)]  Loss: 3.73 (3.60)  Time: 0.171s,  749.78/s  (0.171s,  747.17/s)  LR: 2.687e-05  Data: 0.009 (0.006)Time: 1473.466s\n",
      "Train: 10 [8650/10009 ( 86%)]  Loss: 3.43 (3.60)  Time: 0.170s,  752.32/s  (0.171s,  747.20/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1481.961s\n",
      "Train: 10 [8700/10009 ( 87%)]  Loss: 3.44 (3.60)  Time: 0.182s,  702.20/s  (0.171s,  747.08/s)  LR: 2.687e-05  Data: 0.009 (0.006)Time: 1490.762s\n",
      "Train: 10 [8750/10009 ( 87%)]  Loss: 3.90 (3.60)  Time: 0.169s,  757.03/s  (0.171s,  746.83/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1499.832s\n",
      "Train: 10 [8800/10009 ( 88%)]  Loss: 3.31 (3.60)  Time: 0.169s,  756.02/s  (0.171s,  746.89/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1508.282s\n",
      "Train: 10 [8850/10009 ( 88%)]  Loss: 3.53 (3.60)  Time: 0.166s,  769.04/s  (0.171s,  746.74/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1517.160s\n",
      "Train: 10 [8900/10009 ( 89%)]  Loss: 3.42 (3.60)  Time: 0.170s,  752.02/s  (0.171s,  746.78/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1525.657s\n",
      "Train: 10 [8950/10009 ( 89%)]  Loss: 3.22 (3.60)  Time: 0.166s,  769.59/s  (0.171s,  746.83/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1534.118s\n",
      "Train: 10 [9000/10009 ( 90%)]  Loss: 3.67 (3.60)  Time: 0.167s,  764.48/s  (0.171s,  746.91/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1542.521s\n",
      "Train: 10 [9050/10009 ( 90%)]  Loss: 3.54 (3.60)  Time: 0.168s,  763.22/s  (0.171s,  746.97/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1550.969s\n",
      "Train: 10 [9100/10009 ( 91%)]  Loss: 3.52 (3.60)  Time: 0.167s,  767.97/s  (0.171s,  747.06/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1559.347s\n",
      "Train: 10 [9150/10009 ( 91%)]  Loss: 3.60 (3.60)  Time: 0.169s,  755.93/s  (0.171s,  747.13/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1567.767s\n",
      "Train: 10 [9200/10009 ( 92%)]  Loss: 3.65 (3.60)  Time: 0.176s,  728.55/s  (0.171s,  747.19/s)  LR: 2.687e-05  Data: 0.011 (0.006)Time: 1576.205s\n",
      "Train: 10 [9250/10009 ( 92%)]  Loss: 3.58 (3.60)  Time: 0.169s,  757.27/s  (0.171s,  747.23/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1584.682s\n",
      "Train: 10 [9300/10009 ( 93%)]  Loss: 3.61 (3.60)  Time: 0.168s,  760.64/s  (0.171s,  747.29/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1593.118s\n",
      "Train: 10 [9350/10009 ( 93%)]  Loss: 3.48 (3.60)  Time: 0.167s,  768.49/s  (0.171s,  747.35/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1601.550s\n",
      "Train: 10 [9400/10009 ( 94%)]  Loss: 3.53 (3.60)  Time: 0.167s,  767.12/s  (0.171s,  747.42/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1609.974s\n",
      "Train: 10 [9450/10009 ( 94%)]  Loss: 3.63 (3.60)  Time: 0.168s,  763.81/s  (0.171s,  747.48/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1618.398s\n",
      "Train: 10 [9500/10009 ( 95%)]  Loss: 3.65 (3.60)  Time: 0.168s,  759.90/s  (0.171s,  747.55/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1626.804s\n",
      "Train: 10 [9550/10009 ( 95%)]  Loss: 3.21 (3.60)  Time: 0.170s,  753.98/s  (0.171s,  747.61/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1635.235s\n",
      "Train: 10 [9600/10009 ( 96%)]  Loss: 3.83 (3.60)  Time: 0.167s,  765.69/s  (0.171s,  747.70/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1643.617s\n",
      "Train: 10 [9650/10009 ( 96%)]  Loss: 3.46 (3.59)  Time: 0.168s,  762.35/s  (0.171s,  747.77/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1652.018s\n",
      "Train: 10 [9700/10009 ( 97%)]  Loss: 3.39 (3.59)  Time: 0.165s,  776.70/s  (0.171s,  747.82/s)  LR: 2.687e-05  Data: 0.005 (0.006)Time: 1660.454s\n",
      "Train: 10 [9750/10009 ( 97%)]  Loss: 3.39 (3.59)  Time: 0.167s,  764.53/s  (0.171s,  747.89/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1668.872s\n",
      "Train: 10 [9800/10009 ( 98%)]  Loss: 3.43 (3.59)  Time: 0.169s,  756.23/s  (0.171s,  747.92/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1677.345s\n",
      "Train: 10 [9850/10009 ( 98%)]  Loss: 3.65 (3.59)  Time: 0.170s,  754.46/s  (0.171s,  747.99/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1685.752s\n",
      "Train: 10 [9900/10009 ( 99%)]  Loss: 3.66 (3.59)  Time: 0.168s,  761.18/s  (0.171s,  748.05/s)  LR: 2.687e-05  Data: 0.007 (0.006)Time: 1694.165s\n",
      "Train: 10 [9950/10009 ( 99%)]  Loss: 3.75 (3.59)  Time: 0.166s,  769.78/s  (0.171s,  748.11/s)  LR: 2.687e-05  Data: 0.006 (0.006)Time: 1702.593s\n",
      "Train: 10 [10000/10009 (100%)]  Loss: 3.54 (3.59)  Time: 0.211s,  607.92/s  (0.171s,  748.16/s)  LR: 2.687e-05  Data: 0.050 (0.006)Time: 1711.038s\n",
      "Test: [   0/390]  Time: 0.701 (0.701)  Loss:   1.182 ( 1.182)  Acc@1:  75.781 ( 75.781)  Acc@5:  88.281 ( 88.281)\n",
      "Test: [  50/390]  Time: 0.054 (0.152)  Loss:   1.412 ( 2.198)  Acc@1:  67.188 ( 53.079)  Acc@5:  85.938 ( 75.337)\n",
      "Test: [ 100/390]  Time: 0.120 (0.142)  Loss:   2.373 ( 2.235)  Acc@1:  46.094 ( 49.822)  Acc@5:  79.688 ( 75.975)\n",
      "Test: [ 150/390]  Time: 0.053 (0.146)  Loss:   1.895 ( 2.208)  Acc@1:  53.906 ( 50.440)  Acc@5:  81.250 ( 76.304)\n",
      "Test: [ 200/390]  Time: 0.055 (0.141)  Loss:   3.564 ( 2.391)  Acc@1:  21.094 ( 47.579)  Acc@5:  54.688 ( 73.115)\n",
      "Test: [ 250/390]  Time: 0.053 (0.140)  Loss:   2.589 ( 2.516)  Acc@1:  47.656 ( 45.708)  Acc@5:  67.969 ( 70.975)\n",
      "Test: [ 300/390]  Time: 0.406 (0.140)  Loss:   2.971 ( 2.615)  Acc@1:  43.750 ( 44.145)  Acc@5:  64.844 ( 69.085)\n",
      "Test: [ 350/390]  Time: 0.053 (0.138)  Loss:   2.834 ( 2.687)  Acc@1:  42.188 ( 42.995)  Acc@5:  66.406 ( 67.853)\n",
      "Test: [ 390/390]  Time: 0.035 (0.138)  Loss:   3.869 ( 2.647)  Acc@1:  17.500 ( 43.770)  Acc@5:  51.250 ( 68.514)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 11 [   0/10009 (  0%)]  Loss: 3.41 (3.41)  Time: 0.744s,  172.09/s  (0.744s,  172.09/s)  LR: 2.313e-05  Data: 0.587 (0.587)Time: 0.744s\n",
      "Train: 11 [  50/10009 (  0%)]  Loss: 3.30 (3.40)  Time: 0.166s,  769.70/s  (0.181s,  706.83/s)  LR: 2.313e-05  Data: 0.007 (0.021)Time: 9.236s\n",
      "Train: 11 [ 100/10009 (  1%)]  Loss: 3.31 (3.41)  Time: 0.166s,  770.79/s  (0.174s,  735.44/s)  LR: 2.313e-05  Data: 0.006 (0.014)Time: 17.579s\n",
      "Train: 11 [ 150/10009 (  1%)]  Loss: 3.26 (3.42)  Time: 0.168s,  761.92/s  (0.172s,  745.71/s)  LR: 2.313e-05  Data: 0.006 (0.011)Time: 25.919s\n",
      "Train: 11 [ 200/10009 (  2%)]  Loss: 3.52 (3.42)  Time: 0.166s,  770.26/s  (0.171s,  749.87/s)  LR: 2.313e-05  Data: 0.006 (0.010)Time: 34.310s\n",
      "Train: 11 [ 250/10009 (  2%)]  Loss: 3.53 (3.42)  Time: 0.167s,  764.59/s  (0.170s,  752.89/s)  LR: 2.313e-05  Data: 0.006 (0.009)Time: 42.673s\n",
      "Train: 11 [ 300/10009 (  3%)]  Loss: 3.29 (3.43)  Time: 0.168s,  762.45/s  (0.170s,  754.38/s)  LR: 2.313e-05  Data: 0.007 (0.009)Time: 51.073s\n",
      "Train: 11 [ 350/10009 (  3%)]  Loss: 3.40 (3.43)  Time: 0.169s,  759.07/s  (0.169s,  755.32/s)  LR: 2.313e-05  Data: 0.006 (0.008)Time: 59.483s\n",
      "Train: 11 [ 400/10009 (  4%)]  Loss: 3.41 (3.42)  Time: 0.167s,  766.73/s  (0.169s,  756.31/s)  LR: 2.313e-05  Data: 0.006 (0.008)Time: 67.867s\n",
      "Train: 11 [ 450/10009 (  4%)]  Loss: 3.41 (3.42)  Time: 0.167s,  764.80/s  (0.169s,  756.90/s)  LR: 2.313e-05  Data: 0.006 (0.008)Time: 76.270s\n",
      "Train: 11 [ 500/10009 (  5%)]  Loss: 3.58 (3.42)  Time: 0.167s,  767.21/s  (0.169s,  757.41/s)  LR: 2.313e-05  Data: 0.006 (0.008)Time: 84.668s\n",
      "Train: 11 [ 550/10009 (  5%)]  Loss: 3.48 (3.42)  Time: 0.167s,  768.37/s  (0.169s,  757.79/s)  LR: 2.313e-05  Data: 0.005 (0.008)Time: 93.071s\n",
      "Train: 11 [ 600/10009 (  6%)]  Loss: 3.46 (3.42)  Time: 0.170s,  754.33/s  (0.169s,  757.98/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 101.491s\n",
      "Train: 11 [ 650/10009 (  6%)]  Loss: 3.70 (3.42)  Time: 0.169s,  757.16/s  (0.169s,  758.35/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 109.881s\n",
      "Train: 11 [ 700/10009 (  7%)]  Loss: 3.38 (3.42)  Time: 0.167s,  764.61/s  (0.169s,  758.53/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 118.292s\n",
      "Train: 11 [ 750/10009 (  7%)]  Loss: 3.60 (3.42)  Time: 0.166s,  770.46/s  (0.169s,  758.63/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 126.712s\n",
      "Train: 11 [ 800/10009 (  8%)]  Loss: 3.42 (3.42)  Time: 0.167s,  765.02/s  (0.169s,  758.79/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 135.121s\n",
      "Train: 11 [ 850/10009 (  8%)]  Loss: 3.48 (3.42)  Time: 0.167s,  764.30/s  (0.169s,  758.81/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 143.551s\n",
      "Train: 11 [ 900/10009 (  9%)]  Loss: 3.26 (3.42)  Time: 0.171s,  749.98/s  (0.169s,  758.99/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 151.949s\n",
      "Train: 11 [ 950/10009 (  9%)]  Loss: 3.56 (3.42)  Time: 0.169s,  757.96/s  (0.169s,  759.11/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 160.357s\n",
      "Train: 11 [1000/10009 ( 10%)]  Loss: 3.38 (3.42)  Time: 0.166s,  768.85/s  (0.169s,  759.31/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 168.742s\n",
      "Train: 11 [1050/10009 ( 10%)]  Loss: 3.43 (3.43)  Time: 0.168s,  762.79/s  (0.169s,  759.35/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 177.161s\n",
      "Train: 11 [1100/10009 ( 11%)]  Loss: 3.30 (3.43)  Time: 0.168s,  762.07/s  (0.169s,  759.32/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 185.598s\n",
      "Train: 11 [1150/10009 ( 11%)]  Loss: 3.16 (3.43)  Time: 0.169s,  756.66/s  (0.169s,  759.45/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 193.992s\n",
      "Train: 11 [1200/10009 ( 12%)]  Loss: 3.77 (3.43)  Time: 0.169s,  755.81/s  (0.169s,  759.57/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 202.387s\n",
      "Train: 11 [1250/10009 ( 12%)]  Loss: 3.32 (3.43)  Time: 0.168s,  761.73/s  (0.168s,  759.65/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 210.791s\n",
      "Train: 11 [1300/10009 ( 13%)]  Loss: 3.37 (3.43)  Time: 0.166s,  769.61/s  (0.169s,  759.63/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 219.222s\n",
      "Train: 11 [1350/10009 ( 13%)]  Loss: 3.29 (3.43)  Time: 0.165s,  775.77/s  (0.169s,  759.62/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 227.651s\n",
      "Train: 11 [1400/10009 ( 14%)]  Loss: 3.39 (3.43)  Time: 0.167s,  765.70/s  (0.169s,  759.64/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 236.069s\n",
      "Train: 11 [1450/10009 ( 14%)]  Loss: 3.47 (3.43)  Time: 0.170s,  754.70/s  (0.168s,  759.67/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 244.485s\n",
      "Train: 11 [1500/10009 ( 15%)]  Loss: 3.59 (3.43)  Time: 0.168s,  761.84/s  (0.168s,  759.76/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 252.878s\n",
      "Train: 11 [1550/10009 ( 15%)]  Loss: 3.61 (3.43)  Time: 0.167s,  766.83/s  (0.168s,  759.75/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 261.306s\n",
      "Train: 11 [1600/10009 ( 16%)]  Loss: 3.34 (3.43)  Time: 0.168s,  763.81/s  (0.168s,  759.77/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 269.725s\n",
      "Train: 11 [1650/10009 ( 16%)]  Loss: 3.26 (3.43)  Time: 0.167s,  766.03/s  (0.168s,  759.79/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 278.138s\n",
      "Train: 11 [1700/10009 ( 17%)]  Loss: 3.35 (3.43)  Time: 0.170s,  754.46/s  (0.168s,  759.83/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 286.549s\n",
      "Train: 11 [1750/10009 ( 17%)]  Loss: 3.49 (3.43)  Time: 0.170s,  754.01/s  (0.168s,  759.85/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 294.961s\n",
      "Train: 11 [1800/10009 ( 18%)]  Loss: 3.73 (3.43)  Time: 0.169s,  756.12/s  (0.168s,  759.83/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 303.395s\n",
      "Train: 11 [1850/10009 ( 18%)]  Loss: 3.73 (3.43)  Time: 0.167s,  765.57/s  (0.168s,  759.84/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 311.811s\n",
      "Train: 11 [1900/10009 ( 19%)]  Loss: 3.33 (3.43)  Time: 0.171s,  748.49/s  (0.168s,  759.92/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 320.202s\n",
      "Train: 11 [1950/10009 ( 19%)]  Loss: 3.69 (3.43)  Time: 0.167s,  764.53/s  (0.168s,  759.94/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 328.615s\n",
      "Train: 11 [2000/10009 ( 20%)]  Loss: 3.71 (3.43)  Time: 0.169s,  758.54/s  (0.168s,  759.96/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 337.026s\n",
      "Train: 11 [2050/10009 ( 20%)]  Loss: 3.42 (3.43)  Time: 0.170s,  751.81/s  (0.168s,  759.99/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 345.436s\n",
      "Train: 11 [2100/10009 ( 21%)]  Loss: 3.18 (3.43)  Time: 0.166s,  770.68/s  (0.168s,  760.01/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 353.846s\n",
      "Train: 11 [2150/10009 ( 21%)]  Loss: 3.41 (3.43)  Time: 0.168s,  762.41/s  (0.168s,  760.05/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 362.247s\n",
      "Train: 11 [2200/10009 ( 22%)]  Loss: 3.31 (3.43)  Time: 0.161s,  794.50/s  (0.168s,  760.63/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 370.387s\n",
      "Train: 11 [2250/10009 ( 22%)]  Loss: 3.31 (3.43)  Time: 0.162s,  789.71/s  (0.168s,  761.33/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 378.453s\n",
      "Train: 11 [2300/10009 ( 23%)]  Loss: 3.25 (3.43)  Time: 0.161s,  793.70/s  (0.168s,  761.98/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 386.527s\n",
      "Train: 11 [2350/10009 ( 23%)]  Loss: 3.47 (3.43)  Time: 0.160s,  798.09/s  (0.168s,  762.64/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 394.584s\n",
      "Train: 11 [2400/10009 ( 24%)]  Loss: 3.85 (3.43)  Time: 0.162s,  788.35/s  (0.168s,  763.23/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 402.665s\n",
      "Train: 11 [2450/10009 ( 24%)]  Loss: 3.41 (3.43)  Time: 0.161s,  792.67/s  (0.168s,  763.86/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 410.714s\n",
      "Train: 11 [2500/10009 ( 25%)]  Loss: 3.42 (3.43)  Time: 0.162s,  789.69/s  (0.167s,  764.47/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 418.759s\n",
      "Train: 11 [2550/10009 ( 25%)]  Loss: 3.85 (3.43)  Time: 0.161s,  792.89/s  (0.167s,  765.02/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 426.819s\n",
      "Train: 11 [2600/10009 ( 26%)]  Loss: 3.32 (3.43)  Time: 0.161s,  792.68/s  (0.167s,  765.57/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 434.874s\n",
      "Train: 11 [2650/10009 ( 26%)]  Loss: 3.28 (3.43)  Time: 0.160s,  799.13/s  (0.167s,  766.10/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 442.926s\n",
      "Train: 11 [2700/10009 ( 27%)]  Loss: 3.38 (3.43)  Time: 0.161s,  797.39/s  (0.167s,  766.61/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 450.979s\n",
      "Train: 11 [2750/10009 ( 27%)]  Loss: 3.42 (3.43)  Time: 0.161s,  794.16/s  (0.167s,  767.06/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 459.058s\n",
      "Train: 11 [2800/10009 ( 28%)]  Loss: 3.55 (3.43)  Time: 0.160s,  799.09/s  (0.167s,  767.55/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 467.105s\n",
      "Train: 11 [2850/10009 ( 28%)]  Loss: 3.30 (3.43)  Time: 0.161s,  796.86/s  (0.167s,  767.99/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 475.172s\n",
      "Train: 11 [2900/10009 ( 29%)]  Loss: 3.31 (3.43)  Time: 0.161s,  797.05/s  (0.167s,  768.40/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 483.245s\n",
      "Train: 11 [2950/10009 ( 29%)]  Loss: 3.35 (3.43)  Time: 0.161s,  796.96/s  (0.166s,  768.84/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 491.295s\n",
      "Train: 11 [3000/10009 ( 30%)]  Loss: 3.57 (3.43)  Time: 0.161s,  794.76/s  (0.166s,  769.25/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 499.354s\n",
      "Train: 11 [3050/10009 ( 30%)]  Loss: 3.53 (3.43)  Time: 0.160s,  799.19/s  (0.166s,  769.65/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 507.406s\n",
      "Train: 11 [3100/10009 ( 31%)]  Loss: 3.28 (3.43)  Time: 0.160s,  799.03/s  (0.166s,  770.04/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 515.460s\n",
      "Train: 11 [3150/10009 ( 31%)]  Loss: 3.34 (3.43)  Time: 0.161s,  795.59/s  (0.166s,  770.44/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 523.502s\n",
      "Train: 11 [3200/10009 ( 32%)]  Loss: 3.51 (3.43)  Time: 0.163s,  787.06/s  (0.166s,  770.79/s)  LR: 2.313e-05  Data: 0.008 (0.006)Time: 531.567s\n",
      "Train: 11 [3250/10009 ( 32%)]  Loss: 3.48 (3.43)  Time: 0.164s,  780.03/s  (0.166s,  771.12/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 539.638s\n",
      "Train: 11 [3300/10009 ( 33%)]  Loss: 3.33 (3.43)  Time: 0.163s,  787.45/s  (0.166s,  771.45/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 547.707s\n",
      "Train: 11 [3350/10009 ( 33%)]  Loss: 3.11 (3.43)  Time: 0.163s,  785.74/s  (0.166s,  771.78/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 555.764s\n",
      "Train: 11 [3400/10009 ( 34%)]  Loss: 3.37 (3.43)  Time: 0.160s,  800.21/s  (0.166s,  772.09/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 563.832s\n",
      "Train: 11 [3450/10009 ( 34%)]  Loss: 3.41 (3.43)  Time: 0.164s,  782.69/s  (0.166s,  772.39/s)  LR: 2.313e-05  Data: 0.009 (0.006)Time: 571.897s\n",
      "Train: 11 [3500/10009 ( 35%)]  Loss: 3.63 (3.43)  Time: 0.161s,  797.04/s  (0.166s,  772.69/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 579.958s\n",
      "Train: 11 [3550/10009 ( 35%)]  Loss: 3.21 (3.43)  Time: 0.161s,  793.79/s  (0.166s,  772.94/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 588.052s\n",
      "Train: 11 [3600/10009 ( 36%)]  Loss: 3.09 (3.43)  Time: 0.161s,  793.23/s  (0.166s,  773.23/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 596.110s\n",
      "Train: 11 [3650/10009 ( 36%)]  Loss: 3.48 (3.43)  Time: 0.160s,  797.54/s  (0.165s,  773.51/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 604.162s\n",
      "Train: 11 [3700/10009 ( 37%)]  Loss: 3.14 (3.43)  Time: 0.162s,  789.61/s  (0.165s,  773.77/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 612.233s\n",
      "Train: 11 [3750/10009 ( 37%)]  Loss: 3.18 (3.43)  Time: 0.162s,  791.82/s  (0.165s,  774.03/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 620.292s\n",
      "Train: 11 [3800/10009 ( 38%)]  Loss: 3.52 (3.43)  Time: 0.162s,  791.32/s  (0.165s,  774.30/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 628.341s\n",
      "Train: 11 [3850/10009 ( 38%)]  Loss: 3.31 (3.43)  Time: 0.161s,  797.44/s  (0.165s,  774.54/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 636.414s\n",
      "Train: 11 [3900/10009 ( 39%)]  Loss: 3.29 (3.43)  Time: 0.160s,  798.57/s  (0.165s,  774.77/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 644.482s\n",
      "Train: 11 [3950/10009 ( 39%)]  Loss: 3.49 (3.43)  Time: 0.160s,  797.87/s  (0.165s,  775.00/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 652.553s\n",
      "Train: 11 [4000/10009 ( 40%)]  Loss: 3.30 (3.43)  Time: 0.163s,  783.90/s  (0.165s,  775.21/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 660.633s\n",
      "Train: 11 [4050/10009 ( 40%)]  Loss: 3.38 (3.43)  Time: 0.164s,  781.07/s  (0.165s,  775.41/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 668.711s\n",
      "Train: 11 [4100/10009 ( 41%)]  Loss: 3.38 (3.43)  Time: 0.189s,  676.88/s  (0.165s,  775.55/s)  LR: 2.313e-05  Data: 0.008 (0.006)Time: 676.840s\n",
      "Train: 11 [4150/10009 ( 41%)]  Loss: 3.52 (3.43)  Time: 0.160s,  799.70/s  (0.165s,  775.75/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 684.924s\n",
      "Train: 11 [4200/10009 ( 42%)]  Loss: 3.55 (3.43)  Time: 0.161s,  797.24/s  (0.165s,  775.98/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 692.967s\n",
      "Train: 11 [4250/10009 ( 42%)]  Loss: 3.25 (3.43)  Time: 0.162s,  792.09/s  (0.165s,  776.15/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 701.061s\n",
      "Train: 11 [4300/10009 ( 43%)]  Loss: 3.63 (3.43)  Time: 0.161s,  794.93/s  (0.165s,  776.36/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 709.109s\n",
      "Train: 11 [4350/10009 ( 43%)]  Loss: 3.69 (3.43)  Time: 0.160s,  799.67/s  (0.165s,  776.52/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 717.206s\n",
      "Train: 11 [4400/10009 ( 44%)]  Loss: 3.67 (3.43)  Time: 0.161s,  795.80/s  (0.165s,  776.70/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 725.281s\n",
      "Train: 11 [4450/10009 ( 44%)]  Loss: 3.42 (3.43)  Time: 0.160s,  799.91/s  (0.165s,  776.91/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 733.324s\n",
      "Train: 11 [4500/10009 ( 45%)]  Loss: 3.45 (3.43)  Time: 0.160s,  798.77/s  (0.165s,  777.12/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 741.365s\n",
      "Train: 11 [4550/10009 ( 45%)]  Loss: 3.48 (3.43)  Time: 0.160s,  800.22/s  (0.165s,  777.27/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 749.455s\n",
      "Train: 11 [4600/10009 ( 46%)]  Loss: 3.38 (3.43)  Time: 0.161s,  792.57/s  (0.165s,  777.47/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 757.495s\n",
      "Train: 11 [4650/10009 ( 46%)]  Loss: 3.55 (3.43)  Time: 0.161s,  794.30/s  (0.165s,  777.66/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 765.536s\n",
      "Train: 11 [4700/10009 ( 47%)]  Loss: 3.31 (3.43)  Time: 0.161s,  794.93/s  (0.165s,  777.84/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 773.587s\n",
      "Train: 11 [4750/10009 ( 47%)]  Loss: 3.41 (3.43)  Time: 0.162s,  791.72/s  (0.165s,  778.00/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 781.649s\n",
      "Train: 11 [4800/10009 ( 48%)]  Loss: 3.67 (3.43)  Time: 0.162s,  792.46/s  (0.164s,  778.17/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 789.712s\n",
      "Train: 11 [4850/10009 ( 48%)]  Loss: 3.61 (3.43)  Time: 0.162s,  789.99/s  (0.164s,  778.33/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 797.767s\n",
      "Train: 11 [4900/10009 ( 49%)]  Loss: 3.29 (3.43)  Time: 0.160s,  799.41/s  (0.164s,  778.48/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 805.832s\n",
      "Train: 11 [4950/10009 ( 49%)]  Loss: 3.34 (3.43)  Time: 0.160s,  802.09/s  (0.164s,  778.62/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 813.905s\n",
      "Train: 11 [5000/10009 ( 50%)]  Loss: 3.51 (3.43)  Time: 0.160s,  797.70/s  (0.164s,  778.78/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 821.962s\n",
      "Train: 11 [5050/10009 ( 50%)]  Loss: 3.27 (3.43)  Time: 0.161s,  795.82/s  (0.164s,  778.92/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 830.029s\n",
      "Train: 11 [5100/10009 ( 51%)]  Loss: 3.36 (3.43)  Time: 0.163s,  786.14/s  (0.164s,  779.06/s)  LR: 2.313e-05  Data: 0.009 (0.006)Time: 838.092s\n",
      "Train: 11 [5150/10009 ( 51%)]  Loss: 3.58 (3.43)  Time: 0.162s,  789.32/s  (0.164s,  779.20/s)  LR: 2.313e-05  Data: 0.007 (0.006)Time: 846.156s\n",
      "Train: 11 [5200/10009 ( 52%)]  Loss: 3.52 (3.43)  Time: 0.160s,  800.61/s  (0.164s,  779.35/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 854.206s\n",
      "Train: 11 [5250/10009 ( 52%)]  Loss: 3.46 (3.43)  Time: 0.161s,  797.14/s  (0.164s,  779.48/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 862.273s\n",
      "Train: 11 [5300/10009 ( 53%)]  Loss: 3.34 (3.43)  Time: 0.161s,  795.59/s  (0.164s,  779.63/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 870.321s\n",
      "Train: 11 [5350/10009 ( 53%)]  Loss: 3.51 (3.42)  Time: 0.160s,  801.35/s  (0.164s,  779.74/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 878.407s\n",
      "Train: 11 [5400/10009 ( 54%)]  Loss: 3.36 (3.42)  Time: 0.159s,  803.57/s  (0.164s,  779.87/s)  LR: 2.313e-05  Data: 0.005 (0.006)Time: 886.469s\n",
      "Train: 11 [5450/10009 ( 54%)]  Loss: 3.61 (3.42)  Time: 0.163s,  786.33/s  (0.164s,  779.98/s)  LR: 2.313e-05  Data: 0.008 (0.006)Time: 894.541s\n",
      "Train: 11 [5500/10009 ( 55%)]  Loss: 3.33 (3.42)  Time: 0.161s,  797.50/s  (0.164s,  780.11/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 902.596s\n",
      "Train: 11 [5550/10009 ( 55%)]  Loss: 3.59 (3.42)  Time: 0.161s,  794.62/s  (0.164s,  780.24/s)  LR: 2.313e-05  Data: 0.006 (0.006)Time: 910.650s\n",
      "Train: 11 [5600/10009 ( 56%)]  Loss: 3.42 (3.42)  Time: 0.160s,  799.94/s  (0.165s,  776.18/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 923.657s\n",
      "Train: 11 [5650/10009 ( 56%)]  Loss: 3.45 (3.43)  Time: 0.161s,  797.50/s  (0.165s,  775.62/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 932.581s\n",
      "Train: 11 [5700/10009 ( 57%)]  Loss: 3.56 (3.42)  Time: 0.160s,  798.39/s  (0.165s,  775.80/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 940.616s\n",
      "Train: 11 [5750/10009 ( 57%)]  Loss: 3.46 (3.42)  Time: 0.162s,  789.31/s  (0.165s,  775.95/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 948.676s\n",
      "Train: 11 [5800/10009 ( 58%)]  Loss: 3.27 (3.42)  Time: 0.161s,  793.83/s  (0.165s,  776.11/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 956.729s\n",
      "Train: 11 [5850/10009 ( 58%)]  Loss: 3.25 (3.42)  Time: 0.160s,  802.05/s  (0.165s,  776.24/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 964.808s\n",
      "Train: 11 [5900/10009 ( 59%)]  Loss: 3.40 (3.42)  Time: 0.161s,  796.96/s  (0.165s,  776.41/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 972.847s\n",
      "Train: 11 [5950/10009 ( 59%)]  Loss: 3.52 (3.42)  Time: 0.164s,  782.27/s  (0.165s,  776.51/s)  LR: 2.313e-05  Data: 0.009 (0.007)Time: 980.958s\n",
      "Train: 11 [6000/10009 ( 60%)]  Loss: 3.42 (3.42)  Time: 0.161s,  794.98/s  (0.165s,  776.66/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 989.013s\n",
      "Train: 11 [6050/10009 ( 60%)]  Loss: 3.46 (3.42)  Time: 0.161s,  796.39/s  (0.165s,  776.80/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 997.075s\n",
      "Train: 11 [6100/10009 ( 61%)]  Loss: 3.53 (3.42)  Time: 0.162s,  791.54/s  (0.165s,  776.95/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1005.117s\n",
      "Train: 11 [6150/10009 ( 61%)]  Loss: 3.75 (3.42)  Time: 0.160s,  798.12/s  (0.165s,  777.05/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1013.219s\n",
      "Train: 11 [6200/10009 ( 62%)]  Loss: 3.20 (3.42)  Time: 0.161s,  796.20/s  (0.165s,  777.19/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1021.279s\n",
      "Train: 11 [6250/10009 ( 62%)]  Loss: 3.52 (3.42)  Time: 0.161s,  796.32/s  (0.165s,  777.32/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1029.344s\n",
      "Train: 11 [6300/10009 ( 63%)]  Loss: 3.38 (3.42)  Time: 0.161s,  797.07/s  (0.165s,  777.44/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1037.406s\n",
      "Train: 11 [6350/10009 ( 63%)]  Loss: 3.25 (3.43)  Time: 0.161s,  796.77/s  (0.165s,  777.58/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1045.457s\n",
      "Train: 11 [6400/10009 ( 64%)]  Loss: 3.36 (3.43)  Time: 0.162s,  789.11/s  (0.165s,  777.70/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1053.527s\n",
      "Train: 11 [6450/10009 ( 64%)]  Loss: 3.20 (3.43)  Time: 0.161s,  793.37/s  (0.165s,  777.80/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1061.621s\n",
      "Train: 11 [6500/10009 ( 65%)]  Loss: 3.04 (3.43)  Time: 0.161s,  792.74/s  (0.165s,  777.93/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1069.667s\n",
      "Train: 11 [6550/10009 ( 65%)]  Loss: 3.26 (3.43)  Time: 0.163s,  784.36/s  (0.165s,  778.06/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1077.713s\n",
      "Train: 11 [6600/10009 ( 66%)]  Loss: 3.58 (3.43)  Time: 0.161s,  794.11/s  (0.164s,  778.18/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1085.770s\n",
      "Train: 11 [6650/10009 ( 66%)]  Loss: 3.49 (3.42)  Time: 0.160s,  799.48/s  (0.164s,  778.31/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1093.819s\n",
      "Train: 11 [6700/10009 ( 67%)]  Loss: 3.27 (3.42)  Time: 0.160s,  800.10/s  (0.164s,  778.43/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1101.865s\n",
      "Train: 11 [6750/10009 ( 67%)]  Loss: 3.30 (3.42)  Time: 0.160s,  798.45/s  (0.164s,  778.55/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1109.913s\n",
      "Train: 11 [6800/10009 ( 68%)]  Loss: 3.49 (3.42)  Time: 0.160s,  799.45/s  (0.164s,  778.67/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1117.970s\n",
      "Train: 11 [6850/10009 ( 68%)]  Loss: 3.47 (3.42)  Time: 0.160s,  798.59/s  (0.164s,  778.78/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1126.030s\n",
      "Train: 11 [6900/10009 ( 69%)]  Loss: 3.64 (3.42)  Time: 0.162s,  791.58/s  (0.164s,  778.90/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1134.067s\n",
      "Train: 11 [6950/10009 ( 69%)]  Loss: 3.63 (3.42)  Time: 0.161s,  797.35/s  (0.164s,  779.00/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1142.136s\n",
      "Train: 11 [7000/10009 ( 70%)]  Loss: 3.29 (3.42)  Time: 0.161s,  796.10/s  (0.164s,  779.12/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1150.183s\n",
      "Train: 11 [7050/10009 ( 70%)]  Loss: 3.29 (3.42)  Time: 0.163s,  786.81/s  (0.164s,  779.23/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 1158.229s\n",
      "Train: 11 [7100/10009 ( 71%)]  Loss: 3.82 (3.42)  Time: 0.160s,  800.16/s  (0.164s,  779.33/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1166.292s\n",
      "Train: 11 [7150/10009 ( 71%)]  Loss: 3.80 (3.42)  Time: 0.162s,  790.03/s  (0.164s,  779.40/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1174.391s\n",
      "Train: 11 [7200/10009 ( 72%)]  Loss: 3.41 (3.42)  Time: 0.161s,  793.58/s  (0.164s,  779.48/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1182.495s\n",
      "Train: 11 [7250/10009 ( 72%)]  Loss: 3.32 (3.42)  Time: 0.161s,  796.97/s  (0.164s,  779.55/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1190.590s\n",
      "Train: 11 [7300/10009 ( 73%)]  Loss: 3.43 (3.42)  Time: 0.162s,  791.99/s  (0.164s,  779.65/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1198.641s\n",
      "Train: 11 [7350/10009 ( 73%)]  Loss: 3.50 (3.42)  Time: 0.161s,  793.86/s  (0.164s,  779.76/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1206.693s\n",
      "Train: 11 [7400/10009 ( 74%)]  Loss: 3.41 (3.42)  Time: 0.162s,  791.25/s  (0.164s,  779.86/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1214.731s\n",
      "Train: 11 [7450/10009 ( 74%)]  Loss: 3.65 (3.42)  Time: 0.162s,  792.56/s  (0.164s,  779.97/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1222.775s\n",
      "Train: 11 [7500/10009 ( 75%)]  Loss: 3.44 (3.42)  Time: 0.165s,  776.84/s  (0.164s,  780.03/s)  LR: 2.313e-05  Data: 0.010 (0.007)Time: 1230.883s\n",
      "Train: 11 [7550/10009 ( 75%)]  Loss: 3.34 (3.42)  Time: 0.161s,  796.08/s  (0.164s,  780.10/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1238.981s\n",
      "Train: 11 [7600/10009 ( 76%)]  Loss: 3.56 (3.42)  Time: 0.161s,  797.35/s  (0.164s,  780.19/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1247.034s\n",
      "Train: 11 [7650/10009 ( 76%)]  Loss: 3.41 (3.42)  Time: 0.161s,  796.77/s  (0.164s,  780.29/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1255.079s\n",
      "Train: 11 [7700/10009 ( 77%)]  Loss: 3.47 (3.42)  Time: 0.160s,  799.27/s  (0.164s,  780.39/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1263.125s\n",
      "Train: 11 [7750/10009 ( 77%)]  Loss: 3.10 (3.42)  Time: 0.162s,  788.88/s  (0.164s,  780.46/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 1271.211s\n",
      "Train: 11 [7800/10009 ( 78%)]  Loss: 3.43 (3.42)  Time: 0.161s,  797.36/s  (0.164s,  780.54/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1279.269s\n",
      "Train: 11 [7850/10009 ( 78%)]  Loss: 3.24 (3.42)  Time: 0.166s,  769.07/s  (0.164s,  780.61/s)  LR: 2.313e-05  Data: 0.011 (0.007)Time: 1287.358s\n",
      "Train: 11 [7900/10009 ( 79%)]  Loss: 3.35 (3.42)  Time: 0.161s,  793.29/s  (0.164s,  780.63/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1295.520s\n",
      "Train: 11 [7950/10009 ( 79%)]  Loss: 3.48 (3.42)  Time: 0.161s,  794.53/s  (0.164s,  780.70/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1303.599s\n",
      "Train: 11 [8000/10009 ( 80%)]  Loss: 3.37 (3.42)  Time: 0.161s,  796.24/s  (0.164s,  780.79/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1311.655s\n",
      "Train: 11 [8050/10009 ( 80%)]  Loss: 3.37 (3.42)  Time: 0.161s,  794.28/s  (0.164s,  780.88/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1319.705s\n",
      "Train: 11 [8100/10009 ( 81%)]  Loss: 3.45 (3.42)  Time: 0.161s,  793.60/s  (0.164s,  780.96/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1327.753s\n",
      "Train: 11 [8150/10009 ( 81%)]  Loss: 3.67 (3.42)  Time: 0.160s,  800.54/s  (0.164s,  781.05/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1335.791s\n",
      "Train: 11 [8200/10009 ( 82%)]  Loss: 3.70 (3.42)  Time: 0.164s,  780.72/s  (0.164s,  781.10/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 1343.901s\n",
      "Train: 11 [8250/10009 ( 82%)]  Loss: 3.44 (3.42)  Time: 0.166s,  769.14/s  (0.164s,  781.13/s)  LR: 2.313e-05  Data: 0.010 (0.007)Time: 1352.044s\n",
      "Train: 11 [8300/10009 ( 83%)]  Loss: 3.22 (3.42)  Time: 0.161s,  797.31/s  (0.164s,  781.15/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1360.213s\n",
      "Train: 11 [8350/10009 ( 83%)]  Loss: 3.48 (3.42)  Time: 0.162s,  791.31/s  (0.164s,  781.23/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1368.256s\n",
      "Train: 11 [8400/10009 ( 84%)]  Loss: 3.42 (3.42)  Time: 0.160s,  801.03/s  (0.164s,  781.32/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1376.300s\n",
      "Train: 11 [8450/10009 ( 84%)]  Loss: 3.39 (3.42)  Time: 0.165s,  773.99/s  (0.164s,  781.38/s)  LR: 2.313e-05  Data: 0.010 (0.007)Time: 1384.383s\n",
      "Train: 11 [8500/10009 ( 85%)]  Loss: 3.26 (3.42)  Time: 0.161s,  793.00/s  (0.164s,  781.45/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1392.448s\n",
      "Train: 11 [8550/10009 ( 85%)]  Loss: 3.36 (3.42)  Time: 0.160s,  799.97/s  (0.164s,  781.31/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1400.882s\n",
      "Train: 11 [8600/10009 ( 86%)]  Loss: 3.27 (3.42)  Time: 0.163s,  784.87/s  (0.164s,  781.39/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1408.929s\n",
      "Train: 11 [8650/10009 ( 86%)]  Loss: 3.34 (3.42)  Time: 0.161s,  795.72/s  (0.164s,  781.44/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1417.037s\n",
      "Train: 11 [8700/10009 ( 87%)]  Loss: 3.38 (3.42)  Time: 0.161s,  796.72/s  (0.164s,  781.48/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1425.149s\n",
      "Train: 11 [8750/10009 ( 87%)]  Loss: 3.37 (3.42)  Time: 0.161s,  796.78/s  (0.164s,  781.55/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1433.203s\n",
      "Train: 11 [8800/10009 ( 88%)]  Loss: 3.48 (3.42)  Time: 0.161s,  795.61/s  (0.164s,  781.58/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1441.337s\n",
      "Train: 11 [8850/10009 ( 88%)]  Loss: 3.22 (3.42)  Time: 0.162s,  788.81/s  (0.164s,  781.64/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1449.423s\n",
      "Train: 11 [8900/10009 ( 89%)]  Loss: 3.29 (3.42)  Time: 0.161s,  794.35/s  (0.164s,  781.67/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1457.561s\n",
      "Train: 11 [8950/10009 ( 89%)]  Loss: 3.66 (3.42)  Time: 0.159s,  803.29/s  (0.164s,  781.74/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1465.610s\n",
      "Train: 11 [9000/10009 ( 90%)]  Loss: 3.49 (3.42)  Time: 0.162s,  792.55/s  (0.164s,  781.77/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1473.746s\n",
      "Train: 11 [9050/10009 ( 90%)]  Loss: 3.44 (3.42)  Time: 0.160s,  800.13/s  (0.164s,  781.83/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1481.806s\n",
      "Train: 11 [9100/10009 ( 91%)]  Loss: 3.64 (3.42)  Time: 0.165s,  778.08/s  (0.164s,  781.90/s)  LR: 2.313e-05  Data: 0.010 (0.007)Time: 1489.860s\n",
      "Train: 11 [9150/10009 ( 91%)]  Loss: 3.64 (3.42)  Time: 0.161s,  794.39/s  (0.164s,  781.97/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1497.915s\n",
      "Train: 11 [9200/10009 ( 92%)]  Loss: 3.22 (3.42)  Time: 0.161s,  794.49/s  (0.164s,  782.04/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1505.958s\n",
      "Train: 11 [9250/10009 ( 92%)]  Loss: 3.29 (3.42)  Time: 0.160s,  800.23/s  (0.164s,  782.02/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1514.184s\n",
      "Train: 11 [9300/10009 ( 93%)]  Loss: 3.26 (3.42)  Time: 0.161s,  797.16/s  (0.164s,  782.05/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1522.317s\n",
      "Train: 11 [9350/10009 ( 93%)]  Loss: 3.25 (3.42)  Time: 0.162s,  791.37/s  (0.164s,  782.11/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1530.370s\n",
      "Train: 11 [9400/10009 ( 94%)]  Loss: 3.33 (3.42)  Time: 0.162s,  788.92/s  (0.164s,  782.12/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1538.548s\n",
      "Train: 11 [9450/10009 ( 94%)]  Loss: 3.31 (3.42)  Time: 0.161s,  794.06/s  (0.164s,  782.16/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1546.637s\n",
      "Train: 11 [9500/10009 ( 95%)]  Loss: 3.63 (3.42)  Time: 0.163s,  786.83/s  (0.164s,  782.22/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1554.701s\n",
      "Train: 11 [9550/10009 ( 95%)]  Loss: 3.49 (3.42)  Time: 0.164s,  782.43/s  (0.164s,  782.29/s)  LR: 2.313e-05  Data: 0.009 (0.007)Time: 1562.751s\n",
      "Train: 11 [9600/10009 ( 96%)]  Loss: 3.69 (3.42)  Time: 0.160s,  800.41/s  (0.164s,  782.35/s)  LR: 2.313e-05  Data: 0.005 (0.007)Time: 1570.808s\n",
      "Train: 11 [9650/10009 ( 96%)]  Loss: 3.12 (3.42)  Time: 0.161s,  795.00/s  (0.164s,  782.06/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1579.571s\n",
      "Train: 11 [9700/10009 ( 97%)]  Loss: 3.18 (3.42)  Time: 0.163s,  787.32/s  (0.164s,  782.13/s)  LR: 2.313e-05  Data: 0.007 (0.007)Time: 1587.625s\n",
      "Train: 11 [9750/10009 ( 97%)]  Loss: 3.34 (3.42)  Time: 0.161s,  792.71/s  (0.164s,  782.19/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1595.674s\n",
      "Train: 11 [9800/10009 ( 98%)]  Loss: 3.29 (3.42)  Time: 0.162s,  791.25/s  (0.164s,  782.25/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1603.728s\n",
      "Train: 11 [9850/10009 ( 98%)]  Loss: 3.10 (3.42)  Time: 0.162s,  789.63/s  (0.164s,  782.31/s)  LR: 2.313e-05  Data: 0.008 (0.007)Time: 1611.806s\n",
      "Train: 11 [9900/10009 ( 99%)]  Loss: 3.22 (3.42)  Time: 0.160s,  798.34/s  (0.164s,  782.34/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1619.906s\n",
      "Train: 11 [9950/10009 ( 99%)]  Loss: 3.49 (3.42)  Time: 0.161s,  793.97/s  (0.164s,  782.37/s)  LR: 2.313e-05  Data: 0.006 (0.007)Time: 1628.038s\n",
      "Train: 11 [10000/10009 (100%)]  Loss: 3.45 (3.42)  Time: 0.223s,  572.77/s  (0.164s,  782.38/s)  LR: 2.313e-05  Data: 0.068 (0.007)Time: 1636.187s\n",
      "Test: [   0/390]  Time: 0.702 (0.702)  Loss:   1.430 ( 1.430)  Acc@1:  72.656 ( 72.656)  Acc@5:  85.156 ( 85.156)\n",
      "Test: [  50/390]  Time: 0.052 (0.158)  Loss:   1.354 ( 2.090)  Acc@1:  67.188 ( 55.959)  Acc@5:  87.500 ( 77.191)\n",
      "Test: [ 100/390]  Time: 0.053 (0.145)  Loss:   2.122 ( 2.139)  Acc@1:  45.312 ( 51.965)  Acc@5:  85.156 ( 77.174)\n",
      "Test: [ 150/390]  Time: 0.118 (0.145)  Loss:   1.892 ( 2.109)  Acc@1:  50.781 ( 52.489)  Acc@5:  82.812 ( 77.809)\n",
      "Test: [ 200/390]  Time: 0.230 (0.143)  Loss:   3.122 ( 2.293)  Acc@1:  32.812 ( 49.223)  Acc@5:  61.719 ( 74.732)\n",
      "Test: [ 250/390]  Time: 0.051 (0.144)  Loss:   2.442 ( 2.400)  Acc@1:  49.219 ( 47.641)  Acc@5:  71.875 ( 72.871)\n",
      "Test: [ 300/390]  Time: 0.398 (0.146)  Loss:   2.718 ( 2.489)  Acc@1:  47.656 ( 46.200)  Acc@5:  64.062 ( 71.195)\n",
      "Test: [ 350/390]  Time: 0.052 (0.145)  Loss:   2.965 ( 2.563)  Acc@1:  37.500 ( 44.965)  Acc@5:  63.281 ( 69.956)\n",
      "Test: [ 390/390]  Time: 0.034 (0.143)  Loss:   3.779 ( 2.528)  Acc@1:  18.750 ( 45.678)  Acc@5:  57.500 ( 70.512)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 12 [   0/10009 (  0%)]  Loss: 3.33 (3.33)  Time: 0.731s,  175.16/s  (0.731s,  175.16/s)  LR: 1.944e-05  Data: 0.580 (0.580)Time: 0.731s\n",
      "Train: 12 [  50/10009 (  0%)]  Loss: 3.10 (3.32)  Time: 0.158s,  810.26/s  (0.171s,  747.56/s)  LR: 1.944e-05  Data: 0.005 (0.018)Time: 8.733s\n",
      "Train: 12 [ 100/10009 (  1%)]  Loss: 3.16 (3.34)  Time: 0.160s,  802.26/s  (0.165s,  774.29/s)  LR: 1.944e-05  Data: 0.006 (0.012)Time: 16.697s\n",
      "Train: 12 [ 150/10009 (  1%)]  Loss: 3.45 (3.34)  Time: 0.160s,  799.94/s  (0.164s,  780.39/s)  LR: 1.944e-05  Data: 0.006 (0.011)Time: 24.767s\n",
      "Train: 12 [ 200/10009 (  2%)]  Loss: 3.07 (3.33)  Time: 0.160s,  797.67/s  (0.163s,  784.89/s)  LR: 1.944e-05  Data: 0.007 (0.010)Time: 32.780s\n",
      "Train: 12 [ 250/10009 (  2%)]  Loss: 3.44 (3.32)  Time: 0.160s,  798.16/s  (0.163s,  787.07/s)  LR: 1.944e-05  Data: 0.006 (0.009)Time: 40.820s\n",
      "Train: 12 [ 300/10009 (  3%)]  Loss: 3.08 (3.33)  Time: 0.159s,  802.73/s  (0.162s,  788.74/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 48.848s\n",
      "Train: 12 [ 350/10009 (  3%)]  Loss: 3.22 (3.33)  Time: 0.161s,  796.09/s  (0.162s,  790.00/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 56.871s\n",
      "Train: 12 [ 400/10009 (  4%)]  Loss: 3.20 (3.32)  Time: 0.250s,  512.99/s  (0.162s,  788.90/s)  LR: 1.944e-05  Data: 0.096 (0.008)Time: 65.063s\n",
      "Train: 12 [ 450/10009 (  4%)]  Loss: 3.24 (3.32)  Time: 0.163s,  786.62/s  (0.162s,  788.67/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 73.197s\n",
      "Train: 12 [ 500/10009 (  5%)]  Loss: 3.43 (3.32)  Time: 0.161s,  797.14/s  (0.162s,  789.12/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 81.266s\n",
      "Train: 12 [ 550/10009 (  5%)]  Loss: 3.04 (3.32)  Time: 0.160s,  800.83/s  (0.162s,  789.82/s)  LR: 1.944e-05  Data: 0.005 (0.008)Time: 89.297s\n",
      "Train: 12 [ 600/10009 (  6%)]  Loss: 3.29 (3.32)  Time: 0.160s,  798.44/s  (0.162s,  790.40/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 97.328s\n",
      "Train: 12 [ 650/10009 (  6%)]  Loss: 3.15 (3.32)  Time: 0.159s,  803.03/s  (0.162s,  790.77/s)  LR: 1.944e-05  Data: 0.005 (0.008)Time: 105.376s\n",
      "Train: 12 [ 700/10009 (  7%)]  Loss: 3.33 (3.32)  Time: 0.160s,  797.99/s  (0.162s,  790.99/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 113.437s\n",
      "Train: 12 [ 750/10009 (  7%)]  Loss: 3.26 (3.31)  Time: 0.158s,  811.87/s  (0.162s,  791.40/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 121.466s\n",
      "Train: 12 [ 800/10009 (  8%)]  Loss: 3.33 (3.32)  Time: 0.161s,  796.69/s  (0.162s,  791.69/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 129.505s\n",
      "Train: 12 [ 850/10009 (  8%)]  Loss: 3.36 (3.32)  Time: 0.160s,  799.44/s  (0.162s,  791.89/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 137.555s\n",
      "Train: 12 [ 900/10009 (  9%)]  Loss: 3.46 (3.32)  Time: 0.161s,  797.30/s  (0.162s,  792.11/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 145.595s\n",
      "Train: 12 [ 950/10009 (  9%)]  Loss: 3.36 (3.32)  Time: 0.161s,  795.80/s  (0.162s,  792.17/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 153.664s\n",
      "Train: 12 [1000/10009 ( 10%)]  Loss: 3.24 (3.32)  Time: 0.159s,  802.66/s  (0.162s,  792.40/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 161.697s\n",
      "Train: 12 [1050/10009 ( 10%)]  Loss: 3.31 (3.32)  Time: 0.160s,  797.60/s  (0.162s,  792.41/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 169.770s\n",
      "Train: 12 [1100/10009 ( 11%)]  Loss: 3.09 (3.31)  Time: 0.160s,  798.29/s  (0.162s,  792.52/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 177.822s\n",
      "Train: 12 [1150/10009 ( 11%)]  Loss: 3.13 (3.32)  Time: 0.161s,  794.49/s  (0.161s,  792.59/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 185.882s\n",
      "Train: 12 [1200/10009 ( 12%)]  Loss: 3.27 (3.32)  Time: 0.160s,  802.09/s  (0.161s,  792.67/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 193.937s\n",
      "Train: 12 [1250/10009 ( 12%)]  Loss: 3.69 (3.32)  Time: 0.161s,  793.89/s  (0.161s,  792.77/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 201.984s\n",
      "Train: 12 [1300/10009 ( 13%)]  Loss: 3.36 (3.32)  Time: 0.161s,  795.06/s  (0.161s,  792.90/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 210.024s\n",
      "Train: 12 [1350/10009 ( 13%)]  Loss: 2.88 (3.32)  Time: 0.162s,  792.34/s  (0.161s,  792.94/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 218.084s\n",
      "Train: 12 [1400/10009 ( 14%)]  Loss: 3.32 (3.32)  Time: 0.161s,  792.72/s  (0.161s,  793.05/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 226.126s\n",
      "Train: 12 [1450/10009 ( 14%)]  Loss: 3.29 (3.32)  Time: 0.165s,  777.35/s  (0.161s,  793.07/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 234.190s\n",
      "Train: 12 [1500/10009 ( 15%)]  Loss: 3.28 (3.32)  Time: 0.160s,  799.18/s  (0.161s,  793.14/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 242.237s\n",
      "Train: 12 [1550/10009 ( 15%)]  Loss: 3.25 (3.32)  Time: 0.161s,  797.04/s  (0.161s,  793.15/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 250.301s\n",
      "Train: 12 [1600/10009 ( 16%)]  Loss: 3.25 (3.32)  Time: 0.163s,  786.75/s  (0.161s,  793.23/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 258.346s\n",
      "Train: 12 [1650/10009 ( 16%)]  Loss: 3.05 (3.32)  Time: 0.161s,  794.24/s  (0.161s,  793.21/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 266.420s\n",
      "Train: 12 [1700/10009 ( 17%)]  Loss: 3.41 (3.32)  Time: 0.160s,  801.28/s  (0.161s,  793.07/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 274.538s\n",
      "Train: 12 [1750/10009 ( 17%)]  Loss: 3.28 (3.32)  Time: 0.160s,  801.29/s  (0.161s,  793.00/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 282.632s\n",
      "Train: 12 [1800/10009 ( 18%)]  Loss: 3.44 (3.32)  Time: 0.164s,  782.66/s  (0.161s,  793.00/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 290.704s\n",
      "Train: 12 [1850/10009 ( 18%)]  Loss: 3.54 (3.31)  Time: 0.160s,  797.84/s  (0.161s,  792.96/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 298.788s\n",
      "Train: 12 [1900/10009 ( 19%)]  Loss: 3.05 (3.32)  Time: 0.163s,  785.88/s  (0.161s,  793.10/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 306.805s\n",
      "Train: 12 [1950/10009 ( 19%)]  Loss: 3.37 (3.32)  Time: 0.161s,  795.01/s  (0.161s,  793.12/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 314.868s\n",
      "Train: 12 [2000/10009 ( 20%)]  Loss: 3.43 (3.32)  Time: 0.161s,  796.43/s  (0.161s,  793.15/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 322.922s\n",
      "Train: 12 [2050/10009 ( 20%)]  Loss: 3.20 (3.31)  Time: 0.160s,  800.29/s  (0.161s,  793.23/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 330.961s\n",
      "Train: 12 [2100/10009 ( 21%)]  Loss: 3.67 (3.31)  Time: 0.164s,  780.00/s  (0.161s,  793.09/s)  LR: 1.944e-05  Data: 0.009 (0.007)Time: 339.090s\n",
      "Train: 12 [2150/10009 ( 21%)]  Loss: 3.53 (3.31)  Time: 0.172s,  742.92/s  (0.161s,  792.81/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 347.280s\n",
      "Train: 12 [2200/10009 ( 22%)]  Loss: 3.10 (3.31)  Time: 0.170s,  751.03/s  (0.162s,  791.73/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 355.837s\n",
      "Train: 12 [2250/10009 ( 22%)]  Loss: 3.08 (3.31)  Time: 0.172s,  744.95/s  (0.162s,  790.84/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 364.332s\n",
      "Train: 12 [2300/10009 ( 23%)]  Loss: 3.29 (3.31)  Time: 0.173s,  740.87/s  (0.162s,  790.03/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 372.804s\n",
      "Train: 12 [2350/10009 ( 23%)]  Loss: 3.23 (3.31)  Time: 0.169s,  755.57/s  (0.162s,  789.29/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 381.263s\n",
      "Train: 12 [2400/10009 ( 24%)]  Loss: 3.39 (3.31)  Time: 0.169s,  759.02/s  (0.162s,  788.56/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 389.730s\n",
      "Train: 12 [2450/10009 ( 24%)]  Loss: 3.23 (3.31)  Time: 0.168s,  760.23/s  (0.162s,  787.73/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 398.268s\n",
      "Train: 12 [2500/10009 ( 25%)]  Loss: 3.66 (3.31)  Time: 0.168s,  760.30/s  (0.163s,  787.00/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 406.770s\n",
      "Train: 12 [2550/10009 ( 25%)]  Loss: 3.19 (3.31)  Time: 0.171s,  749.85/s  (0.163s,  786.36/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 415.241s\n",
      "Train: 12 [2600/10009 ( 26%)]  Loss: 3.33 (3.31)  Time: 0.170s,  751.01/s  (0.163s,  785.78/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 423.688s\n",
      "Train: 12 [2650/10009 ( 26%)]  Loss: 3.28 (3.31)  Time: 0.172s,  742.86/s  (0.163s,  785.07/s)  LR: 1.944e-05  Data: 0.010 (0.007)Time: 432.227s\n",
      "Train: 12 [2700/10009 ( 27%)]  Loss: 3.29 (3.31)  Time: 0.171s,  749.29/s  (0.163s,  784.45/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 440.728s\n",
      "Train: 12 [2750/10009 ( 27%)]  Loss: 3.13 (3.31)  Time: 0.169s,  759.27/s  (0.163s,  783.78/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 449.267s\n",
      "Train: 12 [2800/10009 ( 28%)]  Loss: 3.27 (3.31)  Time: 0.171s,  747.60/s  (0.163s,  783.22/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 457.761s\n",
      "Train: 12 [2850/10009 ( 28%)]  Loss: 3.30 (3.31)  Time: 0.170s,  752.48/s  (0.164s,  782.56/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 466.323s\n",
      "Train: 12 [2900/10009 ( 29%)]  Loss: 3.20 (3.31)  Time: 0.170s,  752.64/s  (0.164s,  782.01/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 474.836s\n",
      "Train: 12 [2950/10009 ( 29%)]  Loss: 3.22 (3.31)  Time: 0.172s,  746.30/s  (0.164s,  781.47/s)  LR: 1.944e-05  Data: 0.009 (0.007)Time: 483.353s\n",
      "Train: 12 [3000/10009 ( 30%)]  Loss: 3.07 (3.31)  Time: 0.211s,  605.93/s  (0.164s,  780.35/s)  LR: 1.944e-05  Data: 0.048 (0.007)Time: 492.251s\n",
      "Train: 12 [3050/10009 ( 30%)]  Loss: 3.42 (3.31)  Time: 0.167s,  767.57/s  (0.165s,  775.49/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 503.588s\n",
      "Train: 12 [3100/10009 ( 31%)]  Loss: 3.30 (3.31)  Time: 0.167s,  765.25/s  (0.165s,  774.61/s)  LR: 1.944e-05  Data: 0.005 (0.008)Time: 512.423s\n",
      "Train: 12 [3150/10009 ( 31%)]  Loss: 3.65 (3.31)  Time: 0.170s,  753.47/s  (0.165s,  774.23/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 520.939s\n",
      "Train: 12 [3200/10009 ( 32%)]  Loss: 3.28 (3.31)  Time: 0.170s,  751.06/s  (0.165s,  773.94/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 529.402s\n",
      "Train: 12 [3250/10009 ( 32%)]  Loss: 3.33 (3.31)  Time: 0.169s,  756.65/s  (0.165s,  773.69/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 537.848s\n",
      "Train: 12 [3300/10009 ( 33%)]  Loss: 3.43 (3.31)  Time: 0.168s,  759.96/s  (0.166s,  773.41/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 546.318s\n",
      "Train: 12 [3350/10009 ( 33%)]  Loss: 3.23 (3.31)  Time: 0.169s,  758.53/s  (0.166s,  773.14/s)  LR: 1.944e-05  Data: 0.005 (0.008)Time: 554.788s\n",
      "Train: 12 [3400/10009 ( 34%)]  Loss: 3.53 (3.31)  Time: 0.171s,  750.62/s  (0.166s,  772.91/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 563.234s\n",
      "Train: 12 [3450/10009 ( 34%)]  Loss: 3.22 (3.31)  Time: 0.169s,  756.09/s  (0.166s,  772.67/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 571.688s\n",
      "Train: 12 [3500/10009 ( 35%)]  Loss: 3.13 (3.31)  Time: 0.168s,  759.68/s  (0.166s,  772.48/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 580.112s\n",
      "Train: 12 [3550/10009 ( 35%)]  Loss: 3.31 (3.31)  Time: 0.168s,  761.93/s  (0.166s,  772.28/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 588.553s\n",
      "Train: 12 [3600/10009 ( 36%)]  Loss: 2.95 (3.31)  Time: 0.168s,  763.26/s  (0.166s,  772.06/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 597.007s\n",
      "Train: 12 [3650/10009 ( 36%)]  Loss: 3.31 (3.31)  Time: 0.170s,  751.00/s  (0.166s,  771.88/s)  LR: 1.944e-05  Data: 0.007 (0.008)Time: 605.437s\n",
      "Train: 12 [3700/10009 ( 37%)]  Loss: 3.39 (3.31)  Time: 0.168s,  761.75/s  (0.166s,  771.55/s)  LR: 1.944e-05  Data: 0.005 (0.008)Time: 613.990s\n",
      "Train: 12 [3750/10009 ( 37%)]  Loss: 3.24 (3.31)  Time: 0.170s,  754.93/s  (0.166s,  771.41/s)  LR: 1.944e-05  Data: 0.008 (0.008)Time: 622.400s\n",
      "Train: 12 [3800/10009 ( 38%)]  Loss: 3.62 (3.31)  Time: 0.168s,  760.83/s  (0.166s,  771.23/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 630.846s\n",
      "Train: 12 [3850/10009 ( 38%)]  Loss: 3.08 (3.31)  Time: 0.167s,  765.66/s  (0.166s,  771.08/s)  LR: 1.944e-05  Data: 0.006 (0.008)Time: 639.269s\n",
      "Train: 12 [3900/10009 ( 39%)]  Loss: 3.26 (3.31)  Time: 0.168s,  761.20/s  (0.166s,  770.96/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 647.671s\n",
      "Train: 12 [3950/10009 ( 39%)]  Loss: 3.46 (3.31)  Time: 0.169s,  756.31/s  (0.166s,  770.83/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 656.078s\n",
      "Train: 12 [4000/10009 ( 40%)]  Loss: 3.37 (3.31)  Time: 0.168s,  762.91/s  (0.166s,  770.70/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 664.498s\n",
      "Train: 12 [4050/10009 ( 40%)]  Loss: 3.51 (3.31)  Time: 0.168s,  762.21/s  (0.166s,  770.58/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 672.905s\n",
      "Train: 12 [4100/10009 ( 41%)]  Loss: 3.50 (3.31)  Time: 0.168s,  760.95/s  (0.166s,  770.46/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 681.319s\n",
      "Train: 12 [4150/10009 ( 41%)]  Loss: 3.16 (3.31)  Time: 0.167s,  764.40/s  (0.166s,  770.04/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 690.002s\n",
      "Train: 12 [4200/10009 ( 42%)]  Loss: 3.42 (3.31)  Time: 0.167s,  765.50/s  (0.166s,  769.76/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 698.562s\n",
      "Train: 12 [4250/10009 ( 42%)]  Loss: 3.44 (3.31)  Time: 0.188s,  682.52/s  (0.166s,  769.45/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 707.163s\n",
      "Train: 12 [4300/10009 ( 43%)]  Loss: 3.38 (3.31)  Time: 0.183s,  699.37/s  (0.166s,  769.01/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 715.886s\n",
      "Train: 12 [4350/10009 ( 43%)]  Loss: 3.26 (3.31)  Time: 0.183s,  701.18/s  (0.167s,  768.59/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 724.612s\n",
      "Train: 12 [4400/10009 ( 44%)]  Loss: 3.48 (3.31)  Time: 0.179s,  715.03/s  (0.167s,  768.09/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 733.417s\n",
      "Train: 12 [4450/10009 ( 44%)]  Loss: 3.11 (3.31)  Time: 0.181s,  709.00/s  (0.167s,  767.53/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 742.286s\n",
      "Train: 12 [4500/10009 ( 45%)]  Loss: 3.74 (3.31)  Time: 0.172s,  746.07/s  (0.167s,  767.25/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 750.901s\n",
      "Train: 12 [4550/10009 ( 45%)]  Loss: 3.16 (3.31)  Time: 0.170s,  755.05/s  (0.167s,  766.83/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 759.652s\n",
      "Train: 12 [4600/10009 ( 46%)]  Loss: 3.20 (3.31)  Time: 0.182s,  703.24/s  (0.167s,  766.69/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 768.140s\n",
      "Train: 12 [4650/10009 ( 46%)]  Loss: 3.61 (3.31)  Time: 0.176s,  727.57/s  (0.167s,  766.10/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 777.090s\n",
      "Train: 12 [4700/10009 ( 47%)]  Loss: 3.53 (3.31)  Time: 0.167s,  767.13/s  (0.167s,  765.64/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 785.912s\n",
      "Train: 12 [4750/10009 ( 47%)]  Loss: 3.47 (3.31)  Time: 0.168s,  761.41/s  (0.167s,  765.55/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 794.361s\n",
      "Train: 12 [4800/10009 ( 48%)]  Loss: 3.26 (3.31)  Time: 0.186s,  687.92/s  (0.167s,  765.32/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 802.970s\n",
      "Train: 12 [4850/10009 ( 48%)]  Loss: 3.22 (3.31)  Time: 0.178s,  719.08/s  (0.167s,  765.15/s)  LR: 1.944e-05  Data: 0.010 (0.007)Time: 811.511s\n",
      "Train: 12 [4900/10009 ( 49%)]  Loss: 3.41 (3.31)  Time: 0.167s,  766.21/s  (0.167s,  765.03/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 820.006s\n",
      "Train: 12 [4950/10009 ( 49%)]  Loss: 3.41 (3.31)  Time: 0.165s,  774.48/s  (0.167s,  764.87/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 828.538s\n",
      "Train: 12 [5000/10009 ( 50%)]  Loss: 3.15 (3.31)  Time: 0.172s,  743.20/s  (0.167s,  764.66/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 837.139s\n",
      "Train: 12 [5050/10009 ( 50%)]  Loss: 3.38 (3.31)  Time: 0.174s,  736.92/s  (0.167s,  764.35/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 845.848s\n",
      "Train: 12 [5100/10009 ( 51%)]  Loss: 3.17 (3.31)  Time: 0.172s,  745.59/s  (0.168s,  764.08/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 854.527s\n",
      "Train: 12 [5150/10009 ( 51%)]  Loss: 3.41 (3.31)  Time: 0.177s,  721.55/s  (0.168s,  763.90/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 863.102s\n",
      "Train: 12 [5200/10009 ( 52%)]  Loss: 3.35 (3.31)  Time: 0.168s,  759.65/s  (0.168s,  763.75/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 871.655s\n",
      "Train: 12 [5250/10009 ( 52%)]  Loss: 3.45 (3.31)  Time: 0.181s,  708.62/s  (0.168s,  763.46/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 880.364s\n",
      "Train: 12 [5300/10009 ( 53%)]  Loss: 3.37 (3.31)  Time: 0.175s,  730.03/s  (0.168s,  763.23/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 889.017s\n",
      "Train: 12 [5350/10009 ( 53%)]  Loss: 3.58 (3.31)  Time: 0.168s,  762.48/s  (0.168s,  763.05/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 897.613s\n",
      "Train: 12 [5400/10009 ( 54%)]  Loss: 3.50 (3.31)  Time: 0.168s,  761.40/s  (0.168s,  762.99/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 906.081s\n",
      "Train: 12 [5450/10009 ( 54%)]  Loss: 3.92 (3.31)  Time: 0.171s,  749.56/s  (0.168s,  762.69/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 914.820s\n",
      "Train: 12 [5500/10009 ( 55%)]  Loss: 3.31 (3.31)  Time: 0.169s,  756.29/s  (0.168s,  762.67/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 923.232s\n",
      "Train: 12 [5550/10009 ( 55%)]  Loss: 3.18 (3.31)  Time: 0.169s,  758.89/s  (0.168s,  762.64/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 931.664s\n",
      "Train: 12 [5600/10009 ( 56%)]  Loss: 3.39 (3.31)  Time: 0.169s,  759.30/s  (0.168s,  762.63/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 940.077s\n",
      "Train: 12 [5650/10009 ( 56%)]  Loss: 3.15 (3.31)  Time: 0.169s,  757.17/s  (0.168s,  762.59/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 948.516s\n",
      "Train: 12 [5700/10009 ( 57%)]  Loss: 3.54 (3.31)  Time: 0.169s,  757.27/s  (0.168s,  762.57/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 956.927s\n",
      "Train: 12 [5750/10009 ( 57%)]  Loss: 3.29 (3.31)  Time: 0.172s,  745.30/s  (0.168s,  762.56/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 965.330s\n",
      "Train: 12 [5800/10009 ( 58%)]  Loss: 3.71 (3.31)  Time: 0.171s,  747.66/s  (0.168s,  762.53/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 973.760s\n",
      "Train: 12 [5850/10009 ( 58%)]  Loss: 3.22 (3.31)  Time: 0.168s,  763.90/s  (0.168s,  762.50/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 982.200s\n",
      "Train: 12 [5900/10009 ( 59%)]  Loss: 3.14 (3.31)  Time: 0.172s,  744.40/s  (0.168s,  762.47/s)  LR: 1.944e-05  Data: 0.011 (0.007)Time: 990.626s\n",
      "Train: 12 [5950/10009 ( 59%)]  Loss: 3.45 (3.31)  Time: 0.167s,  766.05/s  (0.168s,  762.43/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 999.070s\n",
      "Train: 12 [6000/10009 ( 60%)]  Loss: 3.16 (3.31)  Time: 0.168s,  759.78/s  (0.168s,  762.42/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1007.478s\n",
      "Train: 12 [6050/10009 ( 60%)]  Loss: 3.25 (3.31)  Time: 0.167s,  765.11/s  (0.168s,  762.39/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1015.915s\n",
      "Train: 12 [6100/10009 ( 61%)]  Loss: 3.21 (3.31)  Time: 0.167s,  767.66/s  (0.168s,  762.39/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1024.310s\n",
      "Train: 12 [6150/10009 ( 61%)]  Loss: 3.72 (3.31)  Time: 0.167s,  766.88/s  (0.168s,  762.37/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1032.729s\n",
      "Train: 12 [6200/10009 ( 62%)]  Loss: 3.46 (3.31)  Time: 0.172s,  745.80/s  (0.168s,  762.34/s)  LR: 1.944e-05  Data: 0.010 (0.007)Time: 1041.173s\n",
      "Train: 12 [6250/10009 ( 62%)]  Loss: 3.29 (3.31)  Time: 0.168s,  762.43/s  (0.168s,  762.33/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1049.582s\n",
      "Train: 12 [6300/10009 ( 63%)]  Loss: 3.39 (3.31)  Time: 0.166s,  772.42/s  (0.168s,  762.29/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1058.029s\n",
      "Train: 12 [6350/10009 ( 63%)]  Loss: 3.19 (3.31)  Time: 0.166s,  771.20/s  (0.168s,  762.27/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1066.453s\n",
      "Train: 12 [6400/10009 ( 64%)]  Loss: 3.36 (3.31)  Time: 0.170s,  750.99/s  (0.168s,  762.24/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1074.889s\n",
      "Train: 12 [6450/10009 ( 64%)]  Loss: 3.47 (3.31)  Time: 0.170s,  751.28/s  (0.168s,  762.22/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1083.312s\n",
      "Train: 12 [6500/10009 ( 65%)]  Loss: 3.12 (3.31)  Time: 0.169s,  759.31/s  (0.168s,  762.21/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1091.734s\n",
      "Train: 12 [6550/10009 ( 65%)]  Loss: 3.19 (3.31)  Time: 0.165s,  773.50/s  (0.168s,  762.18/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1100.162s\n",
      "Train: 12 [6600/10009 ( 66%)]  Loss: 3.42 (3.31)  Time: 0.166s,  769.95/s  (0.168s,  762.17/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1108.581s\n",
      "Train: 12 [6650/10009 ( 66%)]  Loss: 3.29 (3.31)  Time: 0.169s,  756.18/s  (0.168s,  762.14/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1117.019s\n",
      "Train: 12 [6700/10009 ( 67%)]  Loss: 3.38 (3.31)  Time: 0.169s,  757.23/s  (0.168s,  762.13/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1125.430s\n",
      "Train: 12 [6750/10009 ( 67%)]  Loss: 2.97 (3.31)  Time: 0.173s,  740.76/s  (0.168s,  762.07/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 1133.913s\n",
      "Train: 12 [6800/10009 ( 68%)]  Loss: 3.47 (3.31)  Time: 0.169s,  758.52/s  (0.168s,  762.03/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1142.377s\n",
      "Train: 12 [6850/10009 ( 68%)]  Loss: 3.35 (3.31)  Time: 0.168s,  761.77/s  (0.168s,  762.01/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1150.807s\n",
      "Train: 12 [6900/10009 ( 69%)]  Loss: 3.19 (3.31)  Time: 0.169s,  757.94/s  (0.168s,  761.97/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1159.262s\n",
      "Train: 12 [6950/10009 ( 69%)]  Loss: 3.18 (3.31)  Time: 0.168s,  760.46/s  (0.168s,  761.96/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1167.683s\n",
      "Train: 12 [7000/10009 ( 70%)]  Loss: 3.30 (3.31)  Time: 0.168s,  763.41/s  (0.168s,  761.95/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1176.098s\n",
      "Train: 12 [7050/10009 ( 70%)]  Loss: 3.40 (3.31)  Time: 0.166s,  769.71/s  (0.168s,  761.92/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1184.539s\n",
      "Train: 12 [7100/10009 ( 71%)]  Loss: 3.51 (3.31)  Time: 0.167s,  766.20/s  (0.168s,  761.91/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1192.958s\n",
      "Train: 12 [7150/10009 ( 71%)]  Loss: 3.58 (3.31)  Time: 0.170s,  755.11/s  (0.168s,  761.88/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1201.397s\n",
      "Train: 12 [7200/10009 ( 72%)]  Loss: 3.53 (3.31)  Time: 0.169s,  759.52/s  (0.168s,  761.88/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1209.798s\n",
      "Train: 12 [7250/10009 ( 72%)]  Loss: 3.23 (3.31)  Time: 0.167s,  765.74/s  (0.168s,  761.88/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1218.209s\n",
      "Train: 12 [7300/10009 ( 73%)]  Loss: 3.45 (3.31)  Time: 0.167s,  768.37/s  (0.168s,  761.84/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1226.666s\n",
      "Train: 12 [7350/10009 ( 73%)]  Loss: 3.05 (3.31)  Time: 0.166s,  770.46/s  (0.168s,  761.83/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1235.089s\n",
      "Train: 12 [7400/10009 ( 74%)]  Loss: 2.94 (3.31)  Time: 0.166s,  768.88/s  (0.168s,  761.80/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1243.537s\n",
      "Train: 12 [7450/10009 ( 74%)]  Loss: 3.50 (3.31)  Time: 0.168s,  761.97/s  (0.168s,  761.79/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1251.946s\n",
      "Train: 12 [7500/10009 ( 75%)]  Loss: 3.39 (3.31)  Time: 0.169s,  757.07/s  (0.168s,  761.78/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1260.369s\n",
      "Train: 12 [7550/10009 ( 75%)]  Loss: 3.38 (3.31)  Time: 0.170s,  752.59/s  (0.168s,  761.76/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1268.805s\n",
      "Train: 12 [7600/10009 ( 76%)]  Loss: 3.39 (3.31)  Time: 0.170s,  753.05/s  (0.168s,  761.75/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1277.224s\n",
      "Train: 12 [7650/10009 ( 76%)]  Loss: 3.63 (3.31)  Time: 0.169s,  757.37/s  (0.168s,  761.72/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1285.680s\n",
      "Train: 12 [7700/10009 ( 77%)]  Loss: 3.11 (3.31)  Time: 0.170s,  751.26/s  (0.168s,  761.70/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1294.114s\n",
      "Train: 12 [7750/10009 ( 77%)]  Loss: 3.17 (3.31)  Time: 0.168s,  760.14/s  (0.168s,  761.69/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1302.527s\n",
      "Train: 12 [7800/10009 ( 78%)]  Loss: 3.37 (3.31)  Time: 0.167s,  764.79/s  (0.168s,  761.68/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1310.951s\n",
      "Train: 12 [7850/10009 ( 78%)]  Loss: 3.05 (3.31)  Time: 0.170s,  754.28/s  (0.168s,  761.67/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1319.366s\n",
      "Train: 12 [7900/10009 ( 79%)]  Loss: 3.21 (3.31)  Time: 0.169s,  755.21/s  (0.168s,  761.66/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1327.795s\n",
      "Train: 12 [7950/10009 ( 79%)]  Loss: 3.26 (3.31)  Time: 0.166s,  769.41/s  (0.168s,  761.65/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1336.207s\n",
      "Train: 12 [8000/10009 ( 80%)]  Loss: 3.31 (3.31)  Time: 0.167s,  766.84/s  (0.168s,  761.65/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1344.616s\n",
      "Train: 12 [8050/10009 ( 80%)]  Loss: 3.42 (3.31)  Time: 0.168s,  763.16/s  (0.168s,  761.65/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1353.023s\n",
      "Train: 12 [8100/10009 ( 81%)]  Loss: 3.36 (3.31)  Time: 0.167s,  767.15/s  (0.168s,  761.65/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1361.425s\n",
      "Train: 12 [8150/10009 ( 81%)]  Loss: 2.90 (3.31)  Time: 0.166s,  769.71/s  (0.168s,  761.63/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1369.864s\n",
      "Train: 12 [8200/10009 ( 82%)]  Loss: 3.31 (3.31)  Time: 0.170s,  750.77/s  (0.168s,  761.62/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 1378.276s\n",
      "Train: 12 [8250/10009 ( 82%)]  Loss: 3.21 (3.31)  Time: 0.169s,  756.41/s  (0.168s,  761.59/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1386.729s\n",
      "Train: 12 [8300/10009 ( 83%)]  Loss: 3.06 (3.31)  Time: 0.168s,  761.25/s  (0.168s,  761.59/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1395.149s\n",
      "Train: 12 [8350/10009 ( 83%)]  Loss: 3.56 (3.31)  Time: 0.168s,  763.82/s  (0.168s,  761.58/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1403.558s\n",
      "Train: 12 [8400/10009 ( 84%)]  Loss: 3.14 (3.31)  Time: 0.169s,  758.27/s  (0.168s,  761.57/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 1411.991s\n",
      "Train: 12 [8450/10009 ( 84%)]  Loss: 3.09 (3.31)  Time: 0.166s,  769.27/s  (0.168s,  761.56/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1420.402s\n",
      "Train: 12 [8500/10009 ( 85%)]  Loss: 3.01 (3.31)  Time: 0.169s,  755.95/s  (0.168s,  761.56/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1428.817s\n",
      "Train: 12 [8550/10009 ( 85%)]  Loss: 3.30 (3.31)  Time: 0.169s,  759.48/s  (0.168s,  761.55/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1437.228s\n",
      "Train: 12 [8600/10009 ( 86%)]  Loss: 3.25 (3.31)  Time: 0.170s,  754.64/s  (0.168s,  761.55/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 1445.628s\n",
      "Train: 12 [8650/10009 ( 86%)]  Loss: 3.26 (3.31)  Time: 0.167s,  765.25/s  (0.168s,  761.53/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1454.082s\n",
      "Train: 12 [8700/10009 ( 87%)]  Loss: 3.64 (3.31)  Time: 0.166s,  772.91/s  (0.168s,  761.52/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1462.499s\n",
      "Train: 12 [8750/10009 ( 87%)]  Loss: 3.29 (3.31)  Time: 0.167s,  767.28/s  (0.168s,  761.51/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1470.934s\n",
      "Train: 12 [8800/10009 ( 88%)]  Loss: 3.26 (3.31)  Time: 0.441s,  290.31/s  (0.168s,  760.78/s)  LR: 1.944e-05  Data: 0.278 (0.007)Time: 1480.750s\n",
      "Train: 12 [8850/10009 ( 88%)]  Loss: 3.07 (3.31)  Time: 0.167s,  768.48/s  (0.168s,  760.63/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1489.450s\n",
      "Train: 12 [8900/10009 ( 89%)]  Loss: 3.75 (3.31)  Time: 0.169s,  758.08/s  (0.168s,  760.63/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1497.873s\n",
      "Train: 12 [8950/10009 ( 89%)]  Loss: 3.00 (3.31)  Time: 0.167s,  764.29/s  (0.168s,  760.63/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1506.279s\n",
      "Train: 12 [9000/10009 ( 90%)]  Loss: 3.53 (3.31)  Time: 0.165s,  777.56/s  (0.168s,  760.62/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1514.717s\n",
      "Train: 12 [9050/10009 ( 90%)]  Loss: 3.10 (3.31)  Time: 0.160s,  802.16/s  (0.168s,  760.80/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1522.777s\n",
      "Train: 12 [9100/10009 ( 91%)]  Loss: 3.39 (3.31)  Time: 0.160s,  800.34/s  (0.168s,  760.98/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1530.824s\n",
      "Train: 12 [9150/10009 ( 91%)]  Loss: 3.36 (3.31)  Time: 0.160s,  801.11/s  (0.168s,  761.16/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1538.877s\n",
      "Train: 12 [9200/10009 ( 92%)]  Loss: 3.36 (3.31)  Time: 0.160s,  799.28/s  (0.168s,  761.34/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1546.915s\n",
      "Train: 12 [9250/10009 ( 92%)]  Loss: 3.41 (3.31)  Time: 0.160s,  800.13/s  (0.168s,  761.51/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1554.966s\n",
      "Train: 12 [9300/10009 ( 93%)]  Loss: 3.52 (3.31)  Time: 0.161s,  796.32/s  (0.168s,  761.69/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1563.011s\n",
      "Train: 12 [9350/10009 ( 93%)]  Loss: 3.01 (3.31)  Time: 0.159s,  802.73/s  (0.168s,  761.86/s)  LR: 1.944e-05  Data: 0.005 (0.007)Time: 1571.059s\n",
      "Train: 12 [9400/10009 ( 94%)]  Loss: 3.17 (3.31)  Time: 0.161s,  797.22/s  (0.168s,  762.03/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1579.110s\n",
      "Train: 12 [9450/10009 ( 94%)]  Loss: 3.43 (3.31)  Time: 0.161s,  795.39/s  (0.168s,  762.20/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1587.154s\n",
      "Train: 12 [9500/10009 ( 95%)]  Loss: 3.24 (3.31)  Time: 0.161s,  793.25/s  (0.168s,  762.37/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1595.193s\n",
      "Train: 12 [9550/10009 ( 95%)]  Loss: 3.22 (3.31)  Time: 0.162s,  789.67/s  (0.168s,  762.53/s)  LR: 1.944e-05  Data: 0.008 (0.007)Time: 1603.241s\n",
      "Train: 12 [9600/10009 ( 96%)]  Loss: 3.18 (3.31)  Time: 0.161s,  796.98/s  (0.168s,  762.69/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1611.299s\n",
      "Train: 12 [9650/10009 ( 96%)]  Loss: 3.07 (3.31)  Time: 0.162s,  789.46/s  (0.168s,  762.84/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1619.378s\n",
      "Train: 12 [9700/10009 ( 97%)]  Loss: 3.19 (3.31)  Time: 0.160s,  799.37/s  (0.168s,  762.98/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1627.457s\n",
      "Train: 12 [9750/10009 ( 97%)]  Loss: 3.25 (3.31)  Time: 0.161s,  796.73/s  (0.168s,  763.14/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1635.519s\n",
      "Train: 12 [9800/10009 ( 98%)]  Loss: 3.23 (3.31)  Time: 0.163s,  785.48/s  (0.168s,  763.29/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1643.579s\n",
      "Train: 12 [9850/10009 ( 98%)]  Loss: 3.31 (3.31)  Time: 0.163s,  784.64/s  (0.168s,  763.45/s)  LR: 1.944e-05  Data: 0.007 (0.007)Time: 1651.615s\n",
      "Train: 12 [9900/10009 ( 99%)]  Loss: 3.32 (3.31)  Time: 0.162s,  788.42/s  (0.168s,  763.60/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1659.661s\n",
      "Train: 12 [9950/10009 ( 99%)]  Loss: 3.36 (3.31)  Time: 0.161s,  796.82/s  (0.168s,  763.76/s)  LR: 1.944e-05  Data: 0.006 (0.007)Time: 1667.712s\n",
      "Train: 12 [10000/10009 (100%)]  Loss: 3.32 (3.31)  Time: 0.208s,  614.73/s  (0.168s,  763.88/s)  LR: 1.944e-05  Data: 0.055 (0.007)Time: 1675.811s\n",
      "Test: [   0/390]  Time: 0.713 (0.713)  Loss:   1.184 ( 1.184)  Acc@1:  76.562 ( 76.562)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.145)  Loss:   1.069 ( 1.982)  Acc@1:  74.219 ( 57.246)  Acc@5:  92.969 ( 78.814)\n",
      "Test: [ 100/390]  Time: 0.308 (0.139)  Loss:   1.858 ( 2.011)  Acc@1:  59.375 ( 53.899)  Acc@5:  83.594 ( 79.409)\n",
      "Test: [ 150/390]  Time: 0.053 (0.138)  Loss:   1.785 ( 1.978)  Acc@1:  59.375 ( 54.957)  Acc@5:  83.594 ( 79.988)\n",
      "Test: [ 200/390]  Time: 0.052 (0.137)  Loss:   2.890 ( 2.165)  Acc@1:  32.812 ( 51.765)  Acc@5:  70.312 ( 76.866)\n",
      "Test: [ 250/390]  Time: 0.362 (0.138)  Loss:   2.402 ( 2.283)  Acc@1:  52.344 ( 49.997)  Acc@5:  69.531 ( 74.770)\n",
      "Test: [ 300/390]  Time: 0.281 (0.136)  Loss:   2.459 ( 2.376)  Acc@1:  53.906 ( 48.518)  Acc@5:  70.312 ( 73.064)\n",
      "Test: [ 350/390]  Time: 0.443 (0.137)  Loss:   2.481 ( 2.448)  Acc@1:  50.000 ( 47.409)  Acc@5:  71.875 ( 71.873)\n",
      "Test: [ 390/390]  Time: 0.034 (0.135)  Loss:   3.327 ( 2.418)  Acc@1:  25.000 ( 47.984)  Acc@5:  60.000 ( 72.372)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 13 [   0/10009 (  0%)]  Loss: 2.77 (2.77)  Time: 0.920s,  139.15/s  (0.920s,  139.15/s)  LR: 1.587e-05  Data: 0.769 (0.769)Time: 0.920s\n",
      "Train: 13 [  50/10009 (  0%)]  Loss: 3.07 (3.20)  Time: 0.158s,  809.71/s  (0.174s,  734.76/s)  LR: 1.587e-05  Data: 0.005 (0.021)Time: 8.885s\n",
      "Train: 13 [ 100/10009 (  1%)]  Loss: 3.36 (3.22)  Time: 0.160s,  799.16/s  (0.167s,  767.17/s)  LR: 1.587e-05  Data: 0.007 (0.014)Time: 16.852s\n",
      "Train: 13 [ 150/10009 (  1%)]  Loss: 3.18 (3.24)  Time: 0.159s,  802.66/s  (0.164s,  778.54/s)  LR: 1.587e-05  Data: 0.006 (0.011)Time: 24.826s\n",
      "Train: 13 [ 200/10009 (  2%)]  Loss: 3.20 (3.22)  Time: 0.160s,  800.55/s  (0.164s,  782.85/s)  LR: 1.587e-05  Data: 0.006 (0.010)Time: 32.865s\n",
      "Train: 13 [ 250/10009 (  2%)]  Loss: 3.31 (3.22)  Time: 0.160s,  800.30/s  (0.163s,  785.89/s)  LR: 1.587e-05  Data: 0.006 (0.009)Time: 40.881s\n",
      "Train: 13 [ 300/10009 (  3%)]  Loss: 3.20 (3.22)  Time: 0.161s,  796.92/s  (0.162s,  788.08/s)  LR: 1.587e-05  Data: 0.007 (0.009)Time: 48.889s\n",
      "Train: 13 [ 350/10009 (  3%)]  Loss: 3.48 (3.22)  Time: 0.160s,  798.70/s  (0.162s,  789.37/s)  LR: 1.587e-05  Data: 0.006 (0.008)Time: 56.916s\n",
      "Train: 13 [ 400/10009 (  4%)]  Loss: 3.12 (3.22)  Time: 0.160s,  801.56/s  (0.162s,  790.36/s)  LR: 1.587e-05  Data: 0.006 (0.008)Time: 64.943s\n",
      "Train: 13 [ 450/10009 (  4%)]  Loss: 3.21 (3.22)  Time: 0.160s,  799.27/s  (0.162s,  791.07/s)  LR: 1.587e-05  Data: 0.006 (0.008)Time: 72.975s\n",
      "Train: 13 [ 500/10009 (  5%)]  Loss: 3.37 (3.22)  Time: 0.161s,  796.47/s  (0.162s,  791.66/s)  LR: 1.587e-05  Data: 0.006 (0.008)Time: 81.004s\n",
      "Train: 13 [ 550/10009 (  5%)]  Loss: 3.17 (3.22)  Time: 0.160s,  799.40/s  (0.162s,  792.01/s)  LR: 1.587e-05  Data: 0.006 (0.008)Time: 89.050s\n",
      "Train: 13 [ 600/10009 (  6%)]  Loss: 3.15 (3.22)  Time: 0.161s,  792.85/s  (0.162s,  791.71/s)  LR: 1.587e-05  Data: 0.007 (0.007)Time: 97.167s\n",
      "Train: 13 [ 650/10009 (  6%)]  Loss: 3.51 (3.22)  Time: 0.161s,  793.77/s  (0.162s,  791.88/s)  LR: 1.587e-05  Data: 0.007 (0.007)Time: 105.228s\n",
      "Train: 13 [ 700/10009 (  7%)]  Loss: 3.41 (3.22)  Time: 0.160s,  798.46/s  (0.162s,  791.98/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 113.296s\n",
      "Train: 13 [ 750/10009 (  7%)]  Loss: 3.38 (3.21)  Time: 0.161s,  797.31/s  (0.162s,  792.34/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 121.322s\n",
      "Train: 13 [ 800/10009 (  8%)]  Loss: 3.28 (3.22)  Time: 0.159s,  802.92/s  (0.161s,  792.67/s)  LR: 1.587e-05  Data: 0.005 (0.007)Time: 129.345s\n",
      "Train: 13 [ 850/10009 (  8%)]  Loss: 3.30 (3.22)  Time: 0.160s,  797.59/s  (0.161s,  792.90/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 137.379s\n",
      "Train: 13 [ 900/10009 (  9%)]  Loss: 3.12 (3.22)  Time: 0.161s,  792.67/s  (0.161s,  793.09/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 145.415s\n",
      "Train: 13 [ 950/10009 (  9%)]  Loss: 3.58 (3.22)  Time: 0.161s,  794.98/s  (0.161s,  793.29/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 153.447s\n",
      "Train: 13 [1000/10009 ( 10%)]  Loss: 3.16 (3.21)  Time: 0.161s,  794.29/s  (0.161s,  793.45/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 161.482s\n",
      "Train: 13 [1050/10009 ( 10%)]  Loss: 3.13 (3.21)  Time: 0.161s,  796.57/s  (0.161s,  793.62/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 169.512s\n",
      "Train: 13 [1100/10009 ( 11%)]  Loss: 3.11 (3.21)  Time: 0.161s,  792.86/s  (0.161s,  793.80/s)  LR: 1.587e-05  Data: 0.007 (0.007)Time: 177.536s\n",
      "Train: 13 [1150/10009 ( 11%)]  Loss: 3.31 (3.21)  Time: 0.160s,  799.12/s  (0.161s,  793.96/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 185.562s\n",
      "Train: 13 [1200/10009 ( 12%)]  Loss: 3.14 (3.21)  Time: 0.161s,  793.67/s  (0.161s,  794.06/s)  LR: 1.587e-05  Data: 0.007 (0.007)Time: 193.597s\n",
      "Train: 13 [1250/10009 ( 12%)]  Loss: 3.20 (3.21)  Time: 0.161s,  794.65/s  (0.161s,  794.14/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 201.637s\n",
      "Train: 13 [1300/10009 ( 13%)]  Loss: 3.19 (3.22)  Time: 0.160s,  800.41/s  (0.161s,  794.24/s)  LR: 1.587e-05  Data: 0.005 (0.007)Time: 209.671s\n",
      "Train: 13 [1350/10009 ( 13%)]  Loss: 3.33 (3.22)  Time: 0.160s,  801.09/s  (0.161s,  794.30/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 217.712s\n",
      "Train: 13 [1400/10009 ( 14%)]  Loss: 3.16 (3.22)  Time: 0.163s,  786.79/s  (0.161s,  794.33/s)  LR: 1.587e-05  Data: 0.008 (0.007)Time: 225.760s\n",
      "Train: 13 [1450/10009 ( 14%)]  Loss: 3.25 (3.22)  Time: 0.160s,  800.53/s  (0.161s,  794.41/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 233.793s\n",
      "Train: 13 [1500/10009 ( 15%)]  Loss: 3.07 (3.22)  Time: 0.160s,  800.25/s  (0.161s,  794.52/s)  LR: 1.587e-05  Data: 0.005 (0.007)Time: 241.816s\n",
      "Train: 13 [1550/10009 ( 15%)]  Loss: 3.06 (3.22)  Time: 0.160s,  802.14/s  (0.161s,  794.61/s)  LR: 1.587e-05  Data: 0.005 (0.007)Time: 249.842s\n",
      "Train: 13 [1600/10009 ( 16%)]  Loss: 3.05 (3.22)  Time: 0.160s,  798.84/s  (0.161s,  794.65/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 257.885s\n",
      "Train: 13 [1650/10009 ( 16%)]  Loss: 3.01 (3.22)  Time: 0.160s,  798.54/s  (0.161s,  794.70/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 265.921s\n",
      "Train: 13 [1700/10009 ( 17%)]  Loss: 3.35 (3.22)  Time: 0.161s,  794.73/s  (0.161s,  794.75/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 273.956s\n",
      "Train: 13 [1750/10009 ( 17%)]  Loss: 3.18 (3.21)  Time: 0.160s,  800.44/s  (0.161s,  794.78/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 282.001s\n",
      "Train: 13 [1800/10009 ( 18%)]  Loss: 3.23 (3.21)  Time: 0.160s,  799.23/s  (0.161s,  794.85/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 290.027s\n",
      "Train: 13 [1850/10009 ( 18%)]  Loss: 3.25 (3.22)  Time: 0.161s,  797.13/s  (0.161s,  794.93/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 298.050s\n",
      "Train: 13 [1900/10009 ( 19%)]  Loss: 3.15 (3.22)  Time: 0.159s,  802.68/s  (0.161s,  795.00/s)  LR: 1.587e-05  Data: 0.005 (0.007)Time: 306.074s\n",
      "Train: 13 [1950/10009 ( 19%)]  Loss: 3.16 (3.22)  Time: 0.160s,  799.37/s  (0.161s,  795.05/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 314.102s\n",
      "Train: 13 [2000/10009 ( 20%)]  Loss: 3.25 (3.22)  Time: 0.160s,  801.19/s  (0.161s,  795.10/s)  LR: 1.587e-05  Data: 0.005 (0.007)Time: 322.131s\n",
      "Train: 13 [2050/10009 ( 20%)]  Loss: 3.10 (3.21)  Time: 0.160s,  800.19/s  (0.161s,  795.16/s)  LR: 1.587e-05  Data: 0.006 (0.007)Time: 330.157s\n",
      "Train: 13 [2100/10009 ( 21%)]  Loss: 3.26 (3.21)  Time: 0.162s,  791.19/s  (0.161s,  795.20/s)  LR: 1.587e-05  Data: 0.007 (0.007)Time: 338.187s\n",
      "Train: 13 [2150/10009 ( 21%)]  Loss: 3.27 (3.22)  Time: 0.161s,  797.10/s  (0.161s,  795.25/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 346.217s\n",
      "Train: 13 [2200/10009 ( 22%)]  Loss: 2.97 (3.21)  Time: 0.160s,  798.32/s  (0.161s,  795.27/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 354.254s\n",
      "Train: 13 [2250/10009 ( 22%)]  Loss: 3.14 (3.21)  Time: 0.161s,  792.68/s  (0.161s,  795.32/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 362.280s\n",
      "Train: 13 [2300/10009 ( 23%)]  Loss: 3.05 (3.21)  Time: 0.161s,  793.67/s  (0.161s,  795.36/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 370.306s\n",
      "Train: 13 [2350/10009 ( 23%)]  Loss: 3.29 (3.21)  Time: 0.164s,  781.31/s  (0.161s,  795.39/s)  LR: 1.587e-05  Data: 0.010 (0.006)Time: 378.342s\n",
      "Train: 13 [2400/10009 ( 24%)]  Loss: 3.51 (3.22)  Time: 0.161s,  792.87/s  (0.161s,  795.40/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 386.379s\n",
      "Train: 13 [2450/10009 ( 24%)]  Loss: 3.41 (3.22)  Time: 0.162s,  787.88/s  (0.161s,  795.42/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 394.417s\n",
      "Train: 13 [2500/10009 ( 25%)]  Loss: 3.26 (3.21)  Time: 0.161s,  795.29/s  (0.161s,  795.44/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 402.454s\n",
      "Train: 13 [2550/10009 ( 25%)]  Loss: 3.05 (3.21)  Time: 0.161s,  797.05/s  (0.161s,  795.49/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 410.473s\n",
      "Train: 13 [2600/10009 ( 26%)]  Loss: 3.44 (3.21)  Time: 0.161s,  795.58/s  (0.161s,  795.53/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 418.499s\n",
      "Train: 13 [2650/10009 ( 26%)]  Loss: 3.27 (3.22)  Time: 0.161s,  797.15/s  (0.161s,  795.55/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 426.530s\n",
      "Train: 13 [2700/10009 ( 27%)]  Loss: 3.43 (3.22)  Time: 0.160s,  798.63/s  (0.161s,  795.60/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 434.550s\n",
      "Train: 13 [2750/10009 ( 27%)]  Loss: 3.09 (3.22)  Time: 0.161s,  796.18/s  (0.161s,  795.62/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 442.583s\n",
      "Train: 13 [2800/10009 ( 28%)]  Loss: 3.08 (3.22)  Time: 0.160s,  800.09/s  (0.161s,  795.65/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 450.611s\n",
      "Train: 13 [2850/10009 ( 28%)]  Loss: 2.91 (3.22)  Time: 0.160s,  798.12/s  (0.161s,  795.66/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 458.649s\n",
      "Train: 13 [2900/10009 ( 29%)]  Loss: 3.06 (3.22)  Time: 0.163s,  785.33/s  (0.161s,  795.66/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 466.693s\n",
      "Train: 13 [2950/10009 ( 29%)]  Loss: 3.18 (3.22)  Time: 0.160s,  797.80/s  (0.161s,  795.69/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 474.715s\n",
      "Train: 13 [3000/10009 ( 30%)]  Loss: 3.45 (3.22)  Time: 0.162s,  791.88/s  (0.161s,  795.71/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 482.747s\n",
      "Train: 13 [3050/10009 ( 30%)]  Loss: 3.28 (3.22)  Time: 0.160s,  797.66/s  (0.161s,  795.74/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 490.772s\n",
      "Train: 13 [3100/10009 ( 31%)]  Loss: 3.28 (3.22)  Time: 0.161s,  797.35/s  (0.161s,  795.77/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 498.796s\n",
      "Train: 13 [3150/10009 ( 31%)]  Loss: 3.07 (3.22)  Time: 0.162s,  791.96/s  (0.161s,  795.77/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 506.840s\n",
      "Train: 13 [3200/10009 ( 32%)]  Loss: 3.30 (3.22)  Time: 0.160s,  801.24/s  (0.161s,  795.80/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 514.865s\n",
      "Train: 13 [3250/10009 ( 32%)]  Loss: 3.26 (3.22)  Time: 0.160s,  799.19/s  (0.161s,  795.81/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 522.898s\n",
      "Train: 13 [3300/10009 ( 33%)]  Loss: 3.36 (3.22)  Time: 0.160s,  800.93/s  (0.161s,  795.84/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 530.918s\n",
      "Train: 13 [3350/10009 ( 33%)]  Loss: 3.75 (3.22)  Time: 0.160s,  799.71/s  (0.161s,  795.86/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 538.950s\n",
      "Train: 13 [3400/10009 ( 34%)]  Loss: 3.15 (3.22)  Time: 0.160s,  799.12/s  (0.161s,  795.88/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 546.979s\n",
      "Train: 13 [3450/10009 ( 34%)]  Loss: 3.50 (3.22)  Time: 0.161s,  794.90/s  (0.161s,  795.88/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 555.015s\n",
      "Train: 13 [3500/10009 ( 35%)]  Loss: 3.06 (3.22)  Time: 0.161s,  793.94/s  (0.161s,  795.88/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 563.056s\n",
      "Train: 13 [3550/10009 ( 35%)]  Loss: 3.30 (3.22)  Time: 0.160s,  801.51/s  (0.161s,  795.90/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 571.084s\n",
      "Train: 13 [3600/10009 ( 36%)]  Loss: 3.37 (3.22)  Time: 0.160s,  802.16/s  (0.161s,  795.91/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 579.121s\n",
      "Train: 13 [3650/10009 ( 36%)]  Loss: 3.16 (3.22)  Time: 0.160s,  797.94/s  (0.161s,  795.93/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 587.148s\n",
      "Train: 13 [3700/10009 ( 37%)]  Loss: 3.15 (3.22)  Time: 0.160s,  797.74/s  (0.161s,  795.95/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 595.173s\n",
      "Train: 13 [3750/10009 ( 37%)]  Loss: 3.35 (3.22)  Time: 0.161s,  795.68/s  (0.161s,  795.98/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 603.192s\n",
      "Train: 13 [3800/10009 ( 38%)]  Loss: 3.07 (3.22)  Time: 0.160s,  801.63/s  (0.161s,  795.97/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 611.240s\n",
      "Train: 13 [3850/10009 ( 38%)]  Loss: 3.30 (3.22)  Time: 0.160s,  799.28/s  (0.161s,  795.97/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 619.281s\n",
      "Train: 13 [3900/10009 ( 39%)]  Loss: 3.12 (3.22)  Time: 0.160s,  797.89/s  (0.161s,  795.98/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 627.309s\n",
      "Train: 13 [3950/10009 ( 39%)]  Loss: 3.11 (3.22)  Time: 0.160s,  799.84/s  (0.161s,  796.00/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 635.333s\n",
      "Train: 13 [4000/10009 ( 40%)]  Loss: 2.96 (3.22)  Time: 0.161s,  795.10/s  (0.161s,  796.02/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 643.359s\n",
      "Train: 13 [4050/10009 ( 40%)]  Loss: 3.11 (3.22)  Time: 0.160s,  801.46/s  (0.161s,  796.04/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 651.381s\n",
      "Train: 13 [4100/10009 ( 41%)]  Loss: 3.38 (3.22)  Time: 0.161s,  795.27/s  (0.161s,  796.05/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 659.415s\n",
      "Train: 13 [4150/10009 ( 41%)]  Loss: 3.02 (3.22)  Time: 0.161s,  797.32/s  (0.161s,  796.06/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 667.446s\n",
      "Train: 13 [4200/10009 ( 42%)]  Loss: 3.45 (3.22)  Time: 0.163s,  785.24/s  (0.161s,  796.07/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 675.476s\n",
      "Train: 13 [4250/10009 ( 42%)]  Loss: 3.11 (3.22)  Time: 0.161s,  794.16/s  (0.161s,  796.09/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 683.497s\n",
      "Train: 13 [4300/10009 ( 43%)]  Loss: 3.17 (3.22)  Time: 0.162s,  792.39/s  (0.161s,  796.11/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 691.521s\n",
      "Train: 13 [4350/10009 ( 43%)]  Loss: 2.99 (3.22)  Time: 0.160s,  799.98/s  (0.161s,  796.13/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 699.546s\n",
      "Train: 13 [4400/10009 ( 44%)]  Loss: 3.14 (3.22)  Time: 0.161s,  794.27/s  (0.161s,  796.14/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 707.572s\n",
      "Train: 13 [4450/10009 ( 44%)]  Loss: 3.34 (3.22)  Time: 0.160s,  801.18/s  (0.161s,  796.15/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 715.603s\n",
      "Train: 13 [4500/10009 ( 45%)]  Loss: 2.91 (3.22)  Time: 0.161s,  792.85/s  (0.161s,  796.16/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 723.631s\n",
      "Train: 13 [4550/10009 ( 45%)]  Loss: 3.35 (3.22)  Time: 0.159s,  805.81/s  (0.161s,  796.17/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 731.662s\n",
      "Train: 13 [4600/10009 ( 46%)]  Loss: 3.13 (3.22)  Time: 0.160s,  799.25/s  (0.161s,  796.19/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 739.681s\n",
      "Train: 13 [4650/10009 ( 46%)]  Loss: 3.23 (3.22)  Time: 0.160s,  802.49/s  (0.161s,  796.21/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 747.702s\n",
      "Train: 13 [4700/10009 ( 47%)]  Loss: 2.98 (3.22)  Time: 0.160s,  800.32/s  (0.161s,  796.22/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 755.729s\n",
      "Train: 13 [4750/10009 ( 47%)]  Loss: 3.33 (3.22)  Time: 0.160s,  799.23/s  (0.161s,  796.24/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 763.745s\n",
      "Train: 13 [4800/10009 ( 48%)]  Loss: 3.34 (3.22)  Time: 0.160s,  799.16/s  (0.161s,  796.26/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 771.764s\n",
      "Train: 13 [4850/10009 ( 48%)]  Loss: 3.08 (3.22)  Time: 0.162s,  792.18/s  (0.161s,  796.27/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 779.796s\n",
      "Train: 13 [4900/10009 ( 49%)]  Loss: 3.48 (3.22)  Time: 0.160s,  798.89/s  (0.161s,  796.29/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 787.814s\n",
      "Train: 13 [4950/10009 ( 49%)]  Loss: 3.05 (3.22)  Time: 0.161s,  795.39/s  (0.161s,  796.30/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 795.837s\n",
      "Train: 13 [5000/10009 ( 50%)]  Loss: 3.05 (3.22)  Time: 0.161s,  797.22/s  (0.161s,  796.31/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 803.864s\n",
      "Train: 13 [5050/10009 ( 50%)]  Loss: 3.39 (3.22)  Time: 0.160s,  801.20/s  (0.161s,  796.32/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 811.894s\n",
      "Train: 13 [5100/10009 ( 51%)]  Loss: 2.98 (3.22)  Time: 0.161s,  797.42/s  (0.161s,  796.34/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 819.913s\n",
      "Train: 13 [5150/10009 ( 51%)]  Loss: 3.01 (3.22)  Time: 0.160s,  799.95/s  (0.161s,  796.34/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 827.947s\n",
      "Train: 13 [5200/10009 ( 52%)]  Loss: 3.11 (3.22)  Time: 0.161s,  794.50/s  (0.161s,  796.35/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 835.975s\n",
      "Train: 13 [5250/10009 ( 52%)]  Loss: 3.12 (3.22)  Time: 0.160s,  800.58/s  (0.161s,  796.36/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 843.999s\n",
      "Train: 13 [5300/10009 ( 53%)]  Loss: 3.38 (3.22)  Time: 0.160s,  797.70/s  (0.161s,  796.37/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 852.025s\n",
      "Train: 13 [5350/10009 ( 53%)]  Loss: 3.37 (3.22)  Time: 0.162s,  789.91/s  (0.161s,  796.37/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 860.060s\n",
      "Train: 13 [5400/10009 ( 54%)]  Loss: 3.38 (3.22)  Time: 0.160s,  799.77/s  (0.161s,  796.38/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 868.086s\n",
      "Train: 13 [5450/10009 ( 54%)]  Loss: 3.36 (3.22)  Time: 0.160s,  798.01/s  (0.161s,  796.39/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 876.109s\n",
      "Train: 13 [5500/10009 ( 55%)]  Loss: 3.26 (3.22)  Time: 0.162s,  792.14/s  (0.161s,  796.39/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 884.148s\n",
      "Train: 13 [5550/10009 ( 55%)]  Loss: 3.28 (3.22)  Time: 0.161s,  793.66/s  (0.161s,  796.40/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 892.178s\n",
      "Train: 13 [5600/10009 ( 56%)]  Loss: 3.36 (3.22)  Time: 0.161s,  794.18/s  (0.161s,  796.41/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 900.197s\n",
      "Train: 13 [5650/10009 ( 56%)]  Loss: 3.45 (3.22)  Time: 0.161s,  794.23/s  (0.161s,  796.42/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 908.223s\n",
      "Train: 13 [5700/10009 ( 57%)]  Loss: 3.40 (3.22)  Time: 0.160s,  801.73/s  (0.161s,  796.44/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 916.237s\n",
      "Train: 13 [5750/10009 ( 57%)]  Loss: 3.58 (3.22)  Time: 0.160s,  800.75/s  (0.161s,  796.45/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 924.260s\n",
      "Train: 13 [5800/10009 ( 58%)]  Loss: 3.23 (3.22)  Time: 0.160s,  800.65/s  (0.161s,  796.45/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 932.290s\n",
      "Train: 13 [5850/10009 ( 58%)]  Loss: 3.11 (3.22)  Time: 0.160s,  801.49/s  (0.161s,  796.47/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 940.312s\n",
      "Train: 13 [5900/10009 ( 59%)]  Loss: 2.95 (3.22)  Time: 0.160s,  797.72/s  (0.161s,  796.48/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 948.332s\n",
      "Train: 13 [5950/10009 ( 59%)]  Loss: 3.20 (3.22)  Time: 0.160s,  800.17/s  (0.161s,  796.50/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 956.347s\n",
      "Train: 13 [6000/10009 ( 60%)]  Loss: 3.27 (3.22)  Time: 0.161s,  796.67/s  (0.161s,  796.50/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 964.379s\n",
      "Train: 13 [6050/10009 ( 60%)]  Loss: 3.42 (3.22)  Time: 0.161s,  795.49/s  (0.161s,  796.50/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 972.406s\n",
      "Train: 13 [6100/10009 ( 61%)]  Loss: 3.04 (3.22)  Time: 0.160s,  797.92/s  (0.161s,  796.51/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 980.433s\n",
      "Train: 13 [6150/10009 ( 61%)]  Loss: 3.42 (3.22)  Time: 0.160s,  798.09/s  (0.161s,  796.51/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 988.468s\n",
      "Train: 13 [6200/10009 ( 62%)]  Loss: 3.42 (3.22)  Time: 0.160s,  798.10/s  (0.161s,  796.51/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 996.502s\n",
      "Train: 13 [6250/10009 ( 62%)]  Loss: 3.25 (3.22)  Time: 0.161s,  792.87/s  (0.161s,  796.52/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1004.531s\n",
      "Train: 13 [6300/10009 ( 63%)]  Loss: 3.38 (3.22)  Time: 0.159s,  803.36/s  (0.161s,  796.53/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1012.554s\n",
      "Train: 13 [6350/10009 ( 63%)]  Loss: 2.91 (3.22)  Time: 0.161s,  796.29/s  (0.161s,  796.54/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1020.577s\n",
      "Train: 13 [6400/10009 ( 64%)]  Loss: 3.10 (3.22)  Time: 0.160s,  798.61/s  (0.161s,  796.55/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1028.599s\n",
      "Train: 13 [6450/10009 ( 64%)]  Loss: 3.15 (3.22)  Time: 0.161s,  796.52/s  (0.161s,  796.55/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1036.628s\n",
      "Train: 13 [6500/10009 ( 65%)]  Loss: 3.20 (3.22)  Time: 0.160s,  800.03/s  (0.161s,  796.56/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1044.646s\n",
      "Train: 13 [6550/10009 ( 65%)]  Loss: 3.53 (3.22)  Time: 0.160s,  800.86/s  (0.161s,  796.56/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1052.684s\n",
      "Train: 13 [6600/10009 ( 66%)]  Loss: 3.24 (3.22)  Time: 0.161s,  792.65/s  (0.161s,  796.57/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1060.705s\n",
      "Train: 13 [6650/10009 ( 66%)]  Loss: 3.16 (3.22)  Time: 0.160s,  798.82/s  (0.161s,  796.58/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1068.723s\n",
      "Train: 13 [6700/10009 ( 67%)]  Loss: 3.11 (3.22)  Time: 0.160s,  801.91/s  (0.161s,  796.58/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1076.758s\n",
      "Train: 13 [6750/10009 ( 67%)]  Loss: 3.11 (3.22)  Time: 0.159s,  803.54/s  (0.161s,  796.59/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1084.775s\n",
      "Train: 13 [6800/10009 ( 68%)]  Loss: 3.11 (3.22)  Time: 0.162s,  790.36/s  (0.161s,  796.60/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 1092.806s\n",
      "Train: 13 [6850/10009 ( 68%)]  Loss: 3.06 (3.22)  Time: 0.161s,  795.37/s  (0.161s,  796.60/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1100.835s\n",
      "Train: 13 [6900/10009 ( 69%)]  Loss: 3.20 (3.22)  Time: 0.159s,  802.54/s  (0.161s,  796.60/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1108.873s\n",
      "Train: 13 [6950/10009 ( 69%)]  Loss: 3.23 (3.22)  Time: 0.160s,  798.13/s  (0.161s,  796.60/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1116.902s\n",
      "Train: 13 [7000/10009 ( 70%)]  Loss: 3.27 (3.22)  Time: 0.161s,  795.08/s  (0.161s,  796.61/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1124.930s\n",
      "Train: 13 [7050/10009 ( 70%)]  Loss: 3.44 (3.22)  Time: 0.161s,  793.92/s  (0.161s,  796.61/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1132.965s\n",
      "Train: 13 [7100/10009 ( 71%)]  Loss: 2.79 (3.22)  Time: 0.161s,  796.57/s  (0.161s,  796.61/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1140.994s\n",
      "Train: 13 [7150/10009 ( 71%)]  Loss: 2.99 (3.22)  Time: 0.162s,  787.78/s  (0.161s,  796.61/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 1149.026s\n",
      "Train: 13 [7200/10009 ( 72%)]  Loss: 3.04 (3.22)  Time: 0.160s,  797.84/s  (0.161s,  796.62/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1157.051s\n",
      "Train: 13 [7250/10009 ( 72%)]  Loss: 3.13 (3.22)  Time: 0.160s,  799.59/s  (0.161s,  796.62/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1165.084s\n",
      "Train: 13 [7300/10009 ( 73%)]  Loss: 2.99 (3.22)  Time: 0.161s,  792.60/s  (0.161s,  796.62/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1173.108s\n",
      "Train: 13 [7350/10009 ( 73%)]  Loss: 3.04 (3.22)  Time: 0.161s,  797.27/s  (0.161s,  796.63/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1181.136s\n",
      "Train: 13 [7400/10009 ( 74%)]  Loss: 3.34 (3.22)  Time: 0.160s,  799.37/s  (0.161s,  796.63/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1189.161s\n",
      "Train: 13 [7450/10009 ( 74%)]  Loss: 3.03 (3.22)  Time: 0.167s,  764.74/s  (0.161s,  796.32/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1197.670s\n",
      "Train: 13 [7500/10009 ( 75%)]  Loss: 3.27 (3.22)  Time: 0.168s,  761.95/s  (0.161s,  795.93/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1206.299s\n",
      "Train: 13 [7550/10009 ( 75%)]  Loss: 3.25 (3.22)  Time: 0.170s,  753.09/s  (0.161s,  795.69/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 1214.699s\n",
      "Train: 13 [7600/10009 ( 76%)]  Loss: 3.15 (3.22)  Time: 0.167s,  766.18/s  (0.161s,  795.45/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1223.116s\n",
      "Train: 13 [7650/10009 ( 76%)]  Loss: 3.37 (3.22)  Time: 0.168s,  762.23/s  (0.161s,  795.22/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1231.519s\n",
      "Train: 13 [7700/10009 ( 77%)]  Loss: 3.10 (3.22)  Time: 0.167s,  766.35/s  (0.161s,  795.00/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1239.910s\n",
      "Train: 13 [7750/10009 ( 77%)]  Loss: 3.60 (3.22)  Time: 0.168s,  762.47/s  (0.161s,  794.78/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1248.307s\n",
      "Train: 13 [7800/10009 ( 78%)]  Loss: 3.11 (3.22)  Time: 0.168s,  762.55/s  (0.161s,  794.56/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1256.700s\n",
      "Train: 13 [7850/10009 ( 78%)]  Loss: 3.31 (3.22)  Time: 0.167s,  764.53/s  (0.161s,  794.33/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 1265.122s\n",
      "Train: 13 [7900/10009 ( 79%)]  Loss: 3.00 (3.22)  Time: 0.167s,  764.54/s  (0.161s,  794.12/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1273.520s\n",
      "Train: 13 [7950/10009 ( 79%)]  Loss: 3.28 (3.22)  Time: 0.169s,  758.14/s  (0.161s,  793.90/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1281.936s\n",
      "Train: 13 [8000/10009 ( 80%)]  Loss: 3.33 (3.22)  Time: 0.168s,  763.26/s  (0.161s,  793.69/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1290.329s\n",
      "Train: 13 [8050/10009 ( 80%)]  Loss: 3.47 (3.22)  Time: 0.169s,  755.53/s  (0.161s,  793.48/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1298.746s\n",
      "Train: 13 [8100/10009 ( 81%)]  Loss: 3.19 (3.22)  Time: 0.169s,  756.86/s  (0.161s,  793.24/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1307.200s\n",
      "Train: 13 [8150/10009 ( 81%)]  Loss: 3.48 (3.22)  Time: 0.169s,  755.30/s  (0.161s,  793.04/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1315.602s\n",
      "Train: 13 [8200/10009 ( 82%)]  Loss: 3.29 (3.22)  Time: 0.171s,  749.70/s  (0.161s,  792.84/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1324.011s\n",
      "Train: 13 [8250/10009 ( 82%)]  Loss: 3.27 (3.22)  Time: 0.171s,  749.05/s  (0.161s,  792.64/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 1332.416s\n",
      "Train: 13 [8300/10009 ( 83%)]  Loss: 3.39 (3.22)  Time: 0.169s,  756.34/s  (0.162s,  792.44/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1340.826s\n",
      "Train: 13 [8350/10009 ( 83%)]  Loss: 3.08 (3.22)  Time: 0.169s,  757.69/s  (0.162s,  792.24/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1349.249s\n",
      "Train: 13 [8400/10009 ( 84%)]  Loss: 3.38 (3.22)  Time: 0.171s,  750.09/s  (0.162s,  792.03/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1357.680s\n",
      "Train: 13 [8450/10009 ( 84%)]  Loss: 3.29 (3.22)  Time: 0.171s,  750.09/s  (0.162s,  791.83/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1366.101s\n",
      "Train: 13 [8500/10009 ( 85%)]  Loss: 3.32 (3.22)  Time: 0.169s,  757.84/s  (0.162s,  791.65/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1374.498s\n",
      "Train: 13 [8550/10009 ( 85%)]  Loss: 3.17 (3.22)  Time: 0.168s,  762.46/s  (0.162s,  791.45/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1382.929s\n",
      "Train: 13 [8600/10009 ( 86%)]  Loss: 3.43 (3.22)  Time: 0.166s,  771.65/s  (0.162s,  791.26/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1391.357s\n",
      "Train: 13 [8650/10009 ( 86%)]  Loss: 3.29 (3.22)  Time: 0.172s,  746.25/s  (0.162s,  791.07/s)  LR: 1.587e-05  Data: 0.009 (0.006)Time: 1399.786s\n",
      "Train: 13 [8700/10009 ( 87%)]  Loss: 3.41 (3.22)  Time: 0.171s,  748.32/s  (0.162s,  790.86/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1408.243s\n",
      "Train: 13 [8750/10009 ( 87%)]  Loss: 3.31 (3.22)  Time: 0.168s,  760.51/s  (0.162s,  790.67/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1416.671s\n",
      "Train: 13 [8800/10009 ( 88%)]  Loss: 3.23 (3.22)  Time: 0.168s,  762.68/s  (0.162s,  790.48/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1425.109s\n",
      "Train: 13 [8850/10009 ( 88%)]  Loss: 3.35 (3.22)  Time: 0.167s,  766.95/s  (0.162s,  790.30/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1433.537s\n",
      "Train: 13 [8900/10009 ( 89%)]  Loss: 3.61 (3.22)  Time: 0.167s,  767.34/s  (0.162s,  790.13/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1441.953s\n",
      "Train: 13 [8950/10009 ( 89%)]  Loss: 3.20 (3.22)  Time: 0.167s,  766.69/s  (0.162s,  789.95/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1450.379s\n",
      "Train: 13 [9000/10009 ( 90%)]  Loss: 3.30 (3.22)  Time: 0.170s,  753.10/s  (0.162s,  789.78/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1458.798s\n",
      "Train: 13 [9050/10009 ( 90%)]  Loss: 2.99 (3.22)  Time: 0.178s,  718.72/s  (0.162s,  789.56/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1467.297s\n",
      "Train: 13 [9100/10009 ( 91%)]  Loss: 3.41 (3.22)  Time: 0.175s,  733.02/s  (0.162s,  789.22/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1476.038s\n",
      "Train: 13 [9150/10009 ( 91%)]  Loss: 3.06 (3.22)  Time: 0.162s,  790.13/s  (0.162s,  789.02/s)  LR: 1.587e-05  Data: 0.007 (0.006)Time: 1484.523s\n",
      "Train: 13 [9200/10009 ( 92%)]  Loss: 3.20 (3.22)  Time: 0.178s,  718.64/s  (0.162s,  788.83/s)  LR: 1.587e-05  Data: 0.008 (0.006)Time: 1493.002s\n",
      "Train: 13 [9250/10009 ( 92%)]  Loss: 3.03 (3.22)  Time: 0.168s,  762.33/s  (0.162s,  788.56/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1501.632s\n",
      "Train: 13 [9300/10009 ( 93%)]  Loss: 3.33 (3.22)  Time: 0.194s,  660.76/s  (0.162s,  788.30/s)  LR: 1.587e-05  Data: 0.019 (0.006)Time: 1510.251s\n",
      "Train: 13 [9350/10009 ( 93%)]  Loss: 3.27 (3.22)  Time: 0.178s,  719.06/s  (0.162s,  788.06/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1518.826s\n",
      "Train: 13 [9400/10009 ( 94%)]  Loss: 3.24 (3.22)  Time: 0.163s,  784.86/s  (0.162s,  787.91/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1527.227s\n",
      "Train: 13 [9450/10009 ( 94%)]  Loss: 3.25 (3.22)  Time: 0.162s,  789.82/s  (0.162s,  787.85/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1535.478s\n",
      "Train: 13 [9500/10009 ( 95%)]  Loss: 2.91 (3.22)  Time: 0.161s,  796.41/s  (0.162s,  787.81/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1543.672s\n",
      "Train: 13 [9550/10009 ( 95%)]  Loss: 3.01 (3.22)  Time: 0.161s,  797.49/s  (0.162s,  787.81/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1551.808s\n",
      "Train: 13 [9600/10009 ( 96%)]  Loss: 3.25 (3.22)  Time: 0.160s,  798.74/s  (0.162s,  787.83/s)  LR: 1.587e-05  Data: 0.005 (0.006)Time: 1559.894s\n",
      "Train: 13 [9650/10009 ( 96%)]  Loss: 3.04 (3.22)  Time: 0.160s,  799.28/s  (0.162s,  787.77/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1568.120s\n",
      "Train: 13 [9700/10009 ( 97%)]  Loss: 3.39 (3.22)  Time: 0.161s,  796.40/s  (0.162s,  787.79/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1576.210s\n",
      "Train: 13 [9750/10009 ( 97%)]  Loss: 2.92 (3.22)  Time: 0.161s,  795.83/s  (0.162s,  787.69/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1584.530s\n",
      "Train: 13 [9800/10009 ( 98%)]  Loss: 3.35 (3.22)  Time: 0.163s,  784.45/s  (0.163s,  787.61/s)  LR: 1.587e-05  Data: 0.009 (0.006)Time: 1592.829s\n",
      "Train: 13 [9850/10009 ( 98%)]  Loss: 3.21 (3.22)  Time: 0.168s,  761.22/s  (0.163s,  787.52/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1601.135s\n",
      "Train: 13 [9900/10009 ( 99%)]  Loss: 3.01 (3.22)  Time: 0.161s,  796.87/s  (0.163s,  787.41/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1609.479s\n",
      "Train: 13 [9950/10009 ( 99%)]  Loss: 2.87 (3.22)  Time: 0.160s,  799.82/s  (0.163s,  787.40/s)  LR: 1.587e-05  Data: 0.006 (0.006)Time: 1617.629s\n",
      "Train: 13 [10000/10009 (100%)]  Loss: 3.37 (3.22)  Time: 0.207s,  618.64/s  (0.163s,  787.41/s)  LR: 1.587e-05  Data: 0.053 (0.006)Time: 1625.734s\n",
      "Test: [   0/390]  Time: 0.827 (0.827)  Loss:   1.031 ( 1.031)  Acc@1:  77.344 ( 77.344)  Acc@5:  92.188 ( 92.188)\n",
      "Test: [  50/390]  Time: 0.051 (0.185)  Loss:   1.159 ( 1.918)  Acc@1:  75.781 ( 58.471)  Acc@5:  89.844 ( 79.841)\n",
      "Test: [ 100/390]  Time: 0.054 (0.163)  Loss:   1.772 ( 1.956)  Acc@1:  55.469 ( 55.090)  Acc@5:  89.062 ( 80.438)\n",
      "Test: [ 150/390]  Time: 0.053 (0.155)  Loss:   1.657 ( 1.922)  Acc@1:  56.250 ( 56.074)  Acc@5:  87.500 ( 81.074)\n",
      "Test: [ 200/390]  Time: 0.474 (0.158)  Loss:   3.316 ( 2.116)  Acc@1:  27.344 ( 52.740)  Acc@5:  57.812 ( 77.822)\n",
      "Test: [ 250/390]  Time: 0.051 (0.154)  Loss:   2.311 ( 2.221)  Acc@1:  57.031 ( 51.261)  Acc@5:  70.312 ( 76.018)\n",
      "Test: [ 300/390]  Time: 0.067 (0.150)  Loss:   2.390 ( 2.312)  Acc@1:  55.469 ( 49.660)  Acc@5:  70.312 ( 74.323)\n",
      "Test: [ 350/390]  Time: 0.248 (0.152)  Loss:   2.694 ( 2.386)  Acc@1:  42.969 ( 48.411)  Acc@5:  70.312 ( 73.148)\n",
      "Test: [ 390/390]  Time: 0.034 (0.151)  Loss:   3.371 ( 2.352)  Acc@1:  31.250 ( 49.058)  Acc@5:  58.750 ( 73.676)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 14 [   0/10009 (  0%)]  Loss: 2.98 (2.98)  Time: 0.904s,  141.63/s  (0.904s,  141.63/s)  LR: 1.250e-05  Data: 0.753 (0.753)Time: 0.904s\n",
      "Train: 14 [  50/10009 (  0%)]  Loss: 3.36 (3.16)  Time: 0.160s,  801.81/s  (0.177s,  723.09/s)  LR: 1.250e-05  Data: 0.007 (0.023)Time: 9.028s\n",
      "Train: 14 [ 100/10009 (  1%)]  Loss: 3.57 (3.14)  Time: 0.159s,  806.65/s  (0.168s,  760.81/s)  LR: 1.250e-05  Data: 0.005 (0.015)Time: 16.993s\n",
      "Train: 14 [ 150/10009 (  1%)]  Loss: 3.20 (3.16)  Time: 0.161s,  793.65/s  (0.165s,  773.86/s)  LR: 1.250e-05  Data: 0.006 (0.012)Time: 24.976s\n",
      "Train: 14 [ 200/10009 (  2%)]  Loss: 3.30 (3.16)  Time: 0.159s,  802.69/s  (0.164s,  780.48/s)  LR: 1.250e-05  Data: 0.006 (0.011)Time: 32.965s\n",
      "Train: 14 [ 250/10009 (  2%)]  Loss: 3.14 (3.15)  Time: 0.162s,  789.91/s  (0.164s,  782.44/s)  LR: 1.250e-05  Data: 0.008 (0.010)Time: 41.062s\n",
      "Train: 14 [ 300/10009 (  3%)]  Loss: 3.16 (3.15)  Time: 0.160s,  800.21/s  (0.163s,  784.87/s)  LR: 1.250e-05  Data: 0.006 (0.009)Time: 49.089s\n",
      "Train: 14 [ 350/10009 (  3%)]  Loss: 3.20 (3.14)  Time: 0.159s,  806.14/s  (0.163s,  785.60/s)  LR: 1.250e-05  Data: 0.005 (0.009)Time: 57.190s\n",
      "Train: 14 [ 400/10009 (  4%)]  Loss: 3.25 (3.15)  Time: 0.160s,  798.76/s  (0.163s,  786.68/s)  LR: 1.250e-05  Data: 0.006 (0.009)Time: 65.246s\n",
      "Train: 14 [ 450/10009 (  4%)]  Loss: 3.03 (3.14)  Time: 0.161s,  794.67/s  (0.162s,  787.71/s)  LR: 1.250e-05  Data: 0.007 (0.009)Time: 73.286s\n",
      "Train: 14 [ 500/10009 (  5%)]  Loss: 2.88 (3.14)  Time: 0.160s,  798.35/s  (0.162s,  788.09/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 81.371s\n",
      "Train: 14 [ 550/10009 (  5%)]  Loss: 3.21 (3.14)  Time: 0.160s,  800.69/s  (0.162s,  788.34/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 89.464s\n",
      "Train: 14 [ 600/10009 (  6%)]  Loss: 3.06 (3.14)  Time: 0.160s,  802.04/s  (0.162s,  788.70/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 97.537s\n",
      "Train: 14 [ 650/10009 (  6%)]  Loss: 3.54 (3.14)  Time: 0.160s,  801.42/s  (0.162s,  788.80/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 105.640s\n",
      "Train: 14 [ 700/10009 (  7%)]  Loss: 3.02 (3.14)  Time: 0.161s,  797.27/s  (0.162s,  789.22/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 113.693s\n",
      "Train: 14 [ 750/10009 (  7%)]  Loss: 3.63 (3.14)  Time: 0.191s,  671.62/s  (0.162s,  788.68/s)  LR: 1.250e-05  Data: 0.027 (0.008)Time: 121.885s\n",
      "Train: 14 [ 800/10009 (  8%)]  Loss: 3.28 (3.14)  Time: 0.161s,  797.39/s  (0.162s,  788.93/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 129.958s\n",
      "Train: 14 [ 850/10009 (  8%)]  Loss: 2.93 (3.14)  Time: 0.161s,  797.48/s  (0.162s,  789.26/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 138.012s\n",
      "Train: 14 [ 900/10009 (  9%)]  Loss: 3.13 (3.14)  Time: 0.160s,  798.81/s  (0.162s,  789.54/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 146.070s\n",
      "Train: 14 [ 950/10009 (  9%)]  Loss: 3.31 (3.14)  Time: 0.163s,  785.84/s  (0.162s,  789.70/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 154.145s\n",
      "Train: 14 [1000/10009 ( 10%)]  Loss: 3.21 (3.14)  Time: 0.163s,  787.48/s  (0.162s,  789.79/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 162.230s\n",
      "Train: 14 [1050/10009 ( 10%)]  Loss: 3.51 (3.14)  Time: 0.162s,  789.46/s  (0.162s,  789.66/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 170.361s\n",
      "Train: 14 [1100/10009 ( 11%)]  Loss: 2.90 (3.14)  Time: 0.162s,  790.24/s  (0.162s,  789.82/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 178.430s\n",
      "Train: 14 [1150/10009 ( 11%)]  Loss: 3.11 (3.14)  Time: 0.160s,  799.40/s  (0.162s,  790.03/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 186.485s\n",
      "Train: 14 [1200/10009 ( 12%)]  Loss: 3.27 (3.14)  Time: 0.162s,  792.47/s  (0.162s,  790.07/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 194.574s\n",
      "Train: 14 [1250/10009 ( 12%)]  Loss: 3.14 (3.14)  Time: 0.165s,  773.52/s  (0.162s,  790.03/s)  LR: 1.250e-05  Data: 0.011 (0.007)Time: 202.687s\n",
      "Train: 14 [1300/10009 ( 13%)]  Loss: 3.34 (3.14)  Time: 0.161s,  793.53/s  (0.162s,  790.11/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 210.765s\n",
      "Train: 14 [1350/10009 ( 13%)]  Loss: 3.10 (3.14)  Time: 0.161s,  796.47/s  (0.162s,  789.76/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 218.963s\n",
      "Train: 14 [1400/10009 ( 14%)]  Loss: 2.92 (3.14)  Time: 0.160s,  800.48/s  (0.162s,  789.32/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 227.192s\n",
      "Train: 14 [1450/10009 ( 14%)]  Loss: 3.14 (3.14)  Time: 0.166s,  772.18/s  (0.162s,  789.27/s)  LR: 1.250e-05  Data: 0.010 (0.007)Time: 235.317s\n",
      "Train: 14 [1500/10009 ( 15%)]  Loss: 3.34 (3.14)  Time: 0.161s,  795.95/s  (0.162s,  789.14/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 243.465s\n",
      "Train: 14 [1550/10009 ( 15%)]  Loss: 2.95 (3.14)  Time: 0.161s,  794.11/s  (0.162s,  788.92/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 251.646s\n",
      "Train: 14 [1600/10009 ( 16%)]  Loss: 2.97 (3.13)  Time: 0.161s,  793.38/s  (0.163s,  785.39/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 260.926s\n",
      "Train: 14 [1650/10009 ( 16%)]  Loss: 3.18 (3.13)  Time: 0.168s,  762.12/s  (0.163s,  784.90/s)  LR: 1.250e-05  Data: 0.011 (0.008)Time: 269.243s\n",
      "Train: 14 [1700/10009 ( 17%)]  Loss: 3.18 (3.13)  Time: 0.162s,  791.03/s  (0.163s,  784.99/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 277.364s\n",
      "Train: 14 [1750/10009 ( 17%)]  Loss: 3.29 (3.13)  Time: 0.167s,  767.92/s  (0.163s,  785.20/s)  LR: 1.250e-05  Data: 0.013 (0.008)Time: 285.442s\n",
      "Train: 14 [1800/10009 ( 18%)]  Loss: 2.99 (3.13)  Time: 0.162s,  788.86/s  (0.163s,  785.42/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 293.509s\n",
      "Train: 14 [1850/10009 ( 18%)]  Loss: 3.10 (3.13)  Time: 0.160s,  799.67/s  (0.163s,  785.63/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 301.577s\n",
      "Train: 14 [1900/10009 ( 19%)]  Loss: 3.01 (3.14)  Time: 0.163s,  787.60/s  (0.163s,  785.44/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 309.800s\n",
      "Train: 14 [1950/10009 ( 19%)]  Loss: 3.19 (3.14)  Time: 0.160s,  797.56/s  (0.163s,  785.41/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 317.959s\n",
      "Train: 14 [2000/10009 ( 20%)]  Loss: 3.17 (3.14)  Time: 0.172s,  743.65/s  (0.163s,  785.54/s)  LR: 1.250e-05  Data: 0.016 (0.008)Time: 326.054s\n",
      "Train: 14 [2050/10009 ( 20%)]  Loss: 3.02 (3.14)  Time: 0.161s,  792.72/s  (0.163s,  785.73/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 334.118s\n",
      "Train: 14 [2100/10009 ( 21%)]  Loss: 2.81 (3.14)  Time: 0.163s,  785.05/s  (0.163s,  785.81/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 342.230s\n",
      "Train: 14 [2150/10009 ( 21%)]  Loss: 3.07 (3.14)  Time: 0.161s,  794.29/s  (0.163s,  785.83/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 350.363s\n",
      "Train: 14 [2200/10009 ( 22%)]  Loss: 3.05 (3.14)  Time: 0.165s,  773.65/s  (0.163s,  785.88/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 358.486s\n",
      "Train: 14 [2250/10009 ( 22%)]  Loss: 2.98 (3.14)  Time: 0.161s,  797.49/s  (0.163s,  786.00/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 366.573s\n",
      "Train: 14 [2300/10009 ( 23%)]  Loss: 3.27 (3.14)  Time: 0.162s,  791.38/s  (0.163s,  786.01/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 374.713s\n",
      "Train: 14 [2350/10009 ( 23%)]  Loss: 3.35 (3.14)  Time: 0.161s,  797.44/s  (0.163s,  786.13/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 382.796s\n",
      "Train: 14 [2400/10009 ( 24%)]  Loss: 3.25 (3.14)  Time: 0.161s,  795.75/s  (0.163s,  786.28/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 390.862s\n",
      "Train: 14 [2450/10009 ( 24%)]  Loss: 3.16 (3.14)  Time: 0.161s,  795.75/s  (0.163s,  786.44/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 398.919s\n",
      "Train: 14 [2500/10009 ( 25%)]  Loss: 3.11 (3.14)  Time: 0.164s,  779.77/s  (0.163s,  786.59/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 406.982s\n",
      "Train: 14 [2550/10009 ( 25%)]  Loss: 2.97 (3.14)  Time: 0.162s,  791.19/s  (0.163s,  785.71/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 415.584s\n",
      "Train: 14 [2600/10009 ( 26%)]  Loss: 3.02 (3.14)  Time: 0.160s,  800.90/s  (0.163s,  785.88/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 423.637s\n",
      "Train: 14 [2650/10009 ( 26%)]  Loss: 3.06 (3.14)  Time: 0.161s,  792.91/s  (0.163s,  786.02/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 431.705s\n",
      "Train: 14 [2700/10009 ( 27%)]  Loss: 2.91 (3.14)  Time: 0.160s,  798.17/s  (0.163s,  786.16/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 439.769s\n",
      "Train: 14 [2750/10009 ( 27%)]  Loss: 3.03 (3.14)  Time: 0.161s,  795.06/s  (0.163s,  786.27/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 447.844s\n",
      "Train: 14 [2800/10009 ( 28%)]  Loss: 3.17 (3.14)  Time: 0.161s,  796.68/s  (0.163s,  786.41/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 455.906s\n",
      "Train: 14 [2850/10009 ( 28%)]  Loss: 3.22 (3.14)  Time: 0.162s,  792.21/s  (0.163s,  786.42/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 464.034s\n",
      "Train: 14 [2900/10009 ( 29%)]  Loss: 3.14 (3.14)  Time: 0.160s,  802.00/s  (0.163s,  786.54/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 472.100s\n",
      "Train: 14 [2950/10009 ( 29%)]  Loss: 2.93 (3.14)  Time: 0.162s,  788.69/s  (0.163s,  786.65/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 480.174s\n",
      "Train: 14 [3000/10009 ( 30%)]  Loss: 2.97 (3.14)  Time: 0.159s,  803.47/s  (0.163s,  786.75/s)  LR: 1.250e-05  Data: 0.004 (0.008)Time: 488.247s\n",
      "Train: 14 [3050/10009 ( 30%)]  Loss: 3.36 (3.14)  Time: 0.164s,  779.34/s  (0.163s,  786.76/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 496.376s\n",
      "Train: 14 [3100/10009 ( 31%)]  Loss: 3.09 (3.14)  Time: 0.162s,  788.41/s  (0.163s,  786.79/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 504.490s\n",
      "Train: 14 [3150/10009 ( 31%)]  Loss: 3.00 (3.14)  Time: 0.163s,  787.41/s  (0.163s,  786.88/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 512.566s\n",
      "Train: 14 [3200/10009 ( 32%)]  Loss: 3.13 (3.14)  Time: 0.159s,  802.74/s  (0.163s,  786.97/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 520.639s\n",
      "Train: 14 [3250/10009 ( 32%)]  Loss: 3.01 (3.14)  Time: 0.168s,  762.10/s  (0.163s,  786.91/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 528.808s\n",
      "Train: 14 [3300/10009 ( 33%)]  Loss: 3.24 (3.14)  Time: 0.161s,  793.63/s  (0.163s,  786.87/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 536.975s\n",
      "Train: 14 [3350/10009 ( 33%)]  Loss: 3.13 (3.14)  Time: 0.160s,  800.42/s  (0.163s,  786.74/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 545.197s\n",
      "Train: 14 [3400/10009 ( 34%)]  Loss: 2.97 (3.14)  Time: 0.161s,  795.08/s  (0.163s,  786.81/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 553.284s\n",
      "Train: 14 [3450/10009 ( 34%)]  Loss: 2.76 (3.14)  Time: 0.176s,  728.64/s  (0.163s,  786.59/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 561.570s\n",
      "Train: 14 [3500/10009 ( 35%)]  Loss: 3.01 (3.14)  Time: 0.161s,  793.41/s  (0.163s,  786.49/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 569.778s\n",
      "Train: 14 [3550/10009 ( 35%)]  Loss: 3.39 (3.14)  Time: 0.172s,  744.74/s  (0.163s,  786.27/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 578.076s\n",
      "Train: 14 [3600/10009 ( 36%)]  Loss: 3.44 (3.14)  Time: 0.171s,  748.59/s  (0.163s,  785.44/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 586.842s\n",
      "Train: 14 [3650/10009 ( 36%)]  Loss: 3.02 (3.14)  Time: 0.168s,  764.17/s  (0.163s,  784.74/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 595.520s\n",
      "Train: 14 [3700/10009 ( 37%)]  Loss: 3.01 (3.14)  Time: 0.185s,  693.53/s  (0.163s,  784.40/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 603.935s\n",
      "Train: 14 [3750/10009 ( 37%)]  Loss: 3.20 (3.14)  Time: 0.166s,  771.26/s  (0.163s,  783.84/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 612.533s\n",
      "Train: 14 [3800/10009 ( 38%)]  Loss: 2.80 (3.14)  Time: 0.177s,  725.07/s  (0.163s,  783.54/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 620.937s\n",
      "Train: 14 [3850/10009 ( 38%)]  Loss: 3.14 (3.14)  Time: 0.166s,  769.10/s  (0.163s,  782.98/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 629.551s\n",
      "Train: 14 [3900/10009 ( 39%)]  Loss: 3.15 (3.14)  Time: 0.169s,  757.30/s  (0.164s,  782.61/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 638.028s\n",
      "Train: 14 [3950/10009 ( 39%)]  Loss: 3.13 (3.14)  Time: 0.182s,  701.85/s  (0.164s,  781.96/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 646.744s\n",
      "Train: 14 [4000/10009 ( 40%)]  Loss: 3.16 (3.14)  Time: 0.169s,  757.13/s  (0.164s,  781.53/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 655.289s\n",
      "Train: 14 [4050/10009 ( 40%)]  Loss: 2.97 (3.14)  Time: 0.172s,  743.50/s  (0.164s,  781.12/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 663.823s\n",
      "Train: 14 [4100/10009 ( 41%)]  Loss: 3.03 (3.14)  Time: 0.172s,  742.96/s  (0.164s,  780.67/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 672.404s\n",
      "Train: 14 [4150/10009 ( 41%)]  Loss: 3.39 (3.14)  Time: 0.167s,  768.57/s  (0.164s,  780.20/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 681.013s\n",
      "Train: 14 [4200/10009 ( 42%)]  Loss: 3.18 (3.14)  Time: 0.169s,  758.42/s  (0.164s,  779.96/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 689.426s\n",
      "Train: 14 [4250/10009 ( 42%)]  Loss: 3.07 (3.14)  Time: 0.175s,  732.74/s  (0.164s,  779.67/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 697.897s\n",
      "Train: 14 [4300/10009 ( 43%)]  Loss: 3.21 (3.14)  Time: 0.168s,  763.94/s  (0.164s,  779.35/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 706.392s\n",
      "Train: 14 [4350/10009 ( 43%)]  Loss: 3.38 (3.14)  Time: 0.167s,  767.44/s  (0.164s,  779.09/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 714.842s\n",
      "Train: 14 [4400/10009 ( 44%)]  Loss: 3.21 (3.14)  Time: 0.167s,  764.54/s  (0.164s,  778.84/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 723.287s\n",
      "Train: 14 [4450/10009 ( 44%)]  Loss: 3.30 (3.14)  Time: 0.168s,  762.68/s  (0.164s,  778.26/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 732.050s\n",
      "Train: 14 [4500/10009 ( 45%)]  Loss: 3.22 (3.14)  Time: 0.173s,  738.32/s  (0.165s,  777.79/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 740.722s\n",
      "Train: 14 [4550/10009 ( 45%)]  Loss: 3.20 (3.14)  Time: 0.168s,  762.08/s  (0.165s,  777.55/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 749.178s\n",
      "Train: 14 [4600/10009 ( 46%)]  Loss: 3.01 (3.14)  Time: 0.171s,  749.83/s  (0.165s,  777.28/s)  LR: 1.250e-05  Data: 0.009 (0.007)Time: 757.676s\n",
      "Train: 14 [4650/10009 ( 46%)]  Loss: 3.16 (3.14)  Time: 0.171s,  746.77/s  (0.165s,  777.09/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 766.093s\n",
      "Train: 14 [4700/10009 ( 47%)]  Loss: 3.36 (3.14)  Time: 0.175s,  730.37/s  (0.165s,  776.84/s)  LR: 1.250e-05  Data: 0.011 (0.007)Time: 774.581s\n",
      "Train: 14 [4750/10009 ( 47%)]  Loss: 2.96 (3.14)  Time: 0.175s,  731.86/s  (0.165s,  776.49/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 783.179s\n",
      "Train: 14 [4800/10009 ( 48%)]  Loss: 3.12 (3.14)  Time: 0.168s,  760.52/s  (0.165s,  776.03/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 791.881s\n",
      "Train: 14 [4850/10009 ( 48%)]  Loss: 2.91 (3.14)  Time: 0.179s,  713.41/s  (0.165s,  773.54/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 802.705s\n",
      "Train: 14 [4900/10009 ( 49%)]  Loss: 3.20 (3.14)  Time: 0.184s,  695.83/s  (0.166s,  771.16/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 813.485s\n",
      "Train: 14 [4950/10009 ( 49%)]  Loss: 3.54 (3.14)  Time: 0.180s,  711.65/s  (0.166s,  770.75/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 822.223s\n",
      "Train: 14 [5000/10009 ( 50%)]  Loss: 3.12 (3.14)  Time: 0.174s,  737.37/s  (0.166s,  770.19/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 831.133s\n",
      "Train: 14 [5050/10009 ( 50%)]  Loss: 3.04 (3.14)  Time: 0.170s,  754.12/s  (0.166s,  769.84/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 839.814s\n",
      "Train: 14 [5100/10009 ( 51%)]  Loss: 3.22 (3.14)  Time: 0.171s,  748.39/s  (0.166s,  769.59/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 848.409s\n",
      "Train: 14 [5150/10009 ( 51%)]  Loss: 3.14 (3.14)  Time: 0.182s,  702.92/s  (0.166s,  769.29/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 857.054s\n",
      "Train: 14 [5200/10009 ( 52%)]  Loss: 3.36 (3.14)  Time: 0.177s,  722.72/s  (0.166s,  769.02/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 865.681s\n",
      "Train: 14 [5250/10009 ( 52%)]  Loss: 3.05 (3.14)  Time: 0.168s,  762.13/s  (0.166s,  768.89/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 874.155s\n",
      "Train: 14 [5300/10009 ( 53%)]  Loss: 3.13 (3.14)  Time: 0.167s,  767.45/s  (0.167s,  768.65/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 882.750s\n",
      "Train: 14 [5350/10009 ( 53%)]  Loss: 3.15 (3.14)  Time: 0.172s,  745.24/s  (0.167s,  768.52/s)  LR: 1.250e-05  Data: 0.011 (0.008)Time: 891.233s\n",
      "Train: 14 [5400/10009 ( 54%)]  Loss: 3.29 (3.14)  Time: 0.167s,  764.45/s  (0.167s,  768.36/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 899.740s\n",
      "Train: 14 [5450/10009 ( 54%)]  Loss: 3.37 (3.14)  Time: 0.178s,  721.10/s  (0.167s,  768.02/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 908.475s\n",
      "Train: 14 [5500/10009 ( 55%)]  Loss: 3.23 (3.14)  Time: 0.166s,  769.54/s  (0.167s,  767.75/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 917.133s\n",
      "Train: 14 [5550/10009 ( 55%)]  Loss: 3.31 (3.14)  Time: 0.168s,  761.23/s  (0.167s,  767.56/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 925.694s\n",
      "Train: 14 [5600/10009 ( 56%)]  Loss: 3.15 (3.14)  Time: 0.168s,  763.44/s  (0.167s,  767.36/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 934.270s\n",
      "Train: 14 [5650/10009 ( 56%)]  Loss: 3.06 (3.14)  Time: 0.170s,  752.18/s  (0.167s,  767.22/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 942.793s\n",
      "Train: 14 [5700/10009 ( 57%)]  Loss: 3.08 (3.14)  Time: 0.169s,  756.25/s  (0.167s,  767.15/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 951.219s\n",
      "Train: 14 [5750/10009 ( 57%)]  Loss: 3.28 (3.14)  Time: 0.166s,  771.60/s  (0.167s,  767.09/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 959.632s\n",
      "Train: 14 [5800/10009 ( 58%)]  Loss: 3.06 (3.14)  Time: 0.169s,  758.87/s  (0.167s,  767.02/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 968.064s\n",
      "Train: 14 [5850/10009 ( 58%)]  Loss: 3.16 (3.14)  Time: 0.167s,  765.05/s  (0.167s,  766.96/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 976.482s\n",
      "Train: 14 [5900/10009 ( 59%)]  Loss: 3.20 (3.14)  Time: 0.169s,  757.07/s  (0.167s,  766.92/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 984.887s\n",
      "Train: 14 [5950/10009 ( 59%)]  Loss: 2.99 (3.14)  Time: 0.168s,  760.90/s  (0.167s,  766.84/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 993.336s\n",
      "Train: 14 [6000/10009 ( 60%)]  Loss: 2.98 (3.14)  Time: 0.169s,  755.73/s  (0.167s,  766.78/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1001.755s\n",
      "Train: 14 [6050/10009 ( 60%)]  Loss: 3.08 (3.14)  Time: 0.170s,  754.80/s  (0.167s,  766.65/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1010.273s\n",
      "Train: 14 [6100/10009 ( 61%)]  Loss: 3.10 (3.14)  Time: 0.170s,  753.48/s  (0.167s,  766.57/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 1018.730s\n",
      "Train: 14 [6150/10009 ( 61%)]  Loss: 2.96 (3.14)  Time: 0.171s,  749.55/s  (0.167s,  766.51/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 1027.151s\n",
      "Train: 14 [6200/10009 ( 62%)]  Loss: 3.19 (3.14)  Time: 0.166s,  771.02/s  (0.167s,  766.43/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1035.609s\n",
      "Train: 14 [6250/10009 ( 62%)]  Loss: 3.33 (3.14)  Time: 0.170s,  753.26/s  (0.167s,  766.35/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1044.072s\n",
      "Train: 14 [6300/10009 ( 63%)]  Loss: 2.93 (3.14)  Time: 0.170s,  754.95/s  (0.167s,  766.26/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1052.544s\n",
      "Train: 14 [6350/10009 ( 63%)]  Loss: 3.42 (3.14)  Time: 0.169s,  758.89/s  (0.167s,  766.15/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1061.059s\n",
      "Train: 14 [6400/10009 ( 64%)]  Loss: 3.03 (3.14)  Time: 0.171s,  749.51/s  (0.167s,  766.00/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1069.609s\n",
      "Train: 14 [6450/10009 ( 64%)]  Loss: 3.12 (3.14)  Time: 0.169s,  758.18/s  (0.167s,  765.91/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1078.097s\n",
      "Train: 14 [6500/10009 ( 65%)]  Loss: 3.15 (3.14)  Time: 0.175s,  731.10/s  (0.167s,  765.81/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1086.597s\n",
      "Train: 14 [6550/10009 ( 65%)]  Loss: 3.02 (3.14)  Time: 0.174s,  735.76/s  (0.167s,  765.64/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 1095.193s\n",
      "Train: 14 [6600/10009 ( 66%)]  Loss: 3.33 (3.14)  Time: 0.168s,  761.05/s  (0.167s,  765.57/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1103.654s\n",
      "Train: 14 [6650/10009 ( 66%)]  Loss: 3.27 (3.14)  Time: 0.169s,  758.18/s  (0.167s,  765.50/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1112.117s\n",
      "Train: 14 [6700/10009 ( 67%)]  Loss: 2.71 (3.14)  Time: 0.167s,  768.36/s  (0.167s,  765.43/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1120.581s\n",
      "Train: 14 [6750/10009 ( 67%)]  Loss: 2.90 (3.14)  Time: 0.169s,  756.75/s  (0.167s,  765.36/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1129.044s\n",
      "Train: 14 [6800/10009 ( 68%)]  Loss: 3.11 (3.14)  Time: 0.170s,  754.15/s  (0.167s,  765.27/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1137.534s\n",
      "Train: 14 [6850/10009 ( 68%)]  Loss: 3.13 (3.14)  Time: 0.168s,  764.02/s  (0.167s,  765.19/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1146.023s\n",
      "Train: 14 [6900/10009 ( 69%)]  Loss: 3.39 (3.14)  Time: 0.168s,  760.75/s  (0.167s,  765.13/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1154.484s\n",
      "Train: 14 [6950/10009 ( 69%)]  Loss: 3.16 (3.14)  Time: 0.168s,  761.23/s  (0.167s,  765.06/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1162.943s\n",
      "Train: 14 [7000/10009 ( 70%)]  Loss: 3.26 (3.14)  Time: 0.171s,  749.36/s  (0.167s,  765.00/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1171.405s\n",
      "Train: 14 [7050/10009 ( 70%)]  Loss: 2.86 (3.14)  Time: 0.172s,  744.35/s  (0.167s,  764.93/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1179.877s\n",
      "Train: 14 [7100/10009 ( 71%)]  Loss: 2.87 (3.14)  Time: 0.169s,  759.10/s  (0.167s,  764.89/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1188.304s\n",
      "Train: 14 [7150/10009 ( 71%)]  Loss: 3.15 (3.14)  Time: 0.174s,  737.68/s  (0.167s,  764.79/s)  LR: 1.250e-05  Data: 0.010 (0.008)Time: 1196.834s\n",
      "Train: 14 [7200/10009 ( 72%)]  Loss: 3.24 (3.14)  Time: 0.168s,  761.82/s  (0.167s,  764.72/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1205.315s\n",
      "Train: 14 [7250/10009 ( 72%)]  Loss: 3.16 (3.14)  Time: 0.170s,  754.79/s  (0.167s,  764.58/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1213.909s\n",
      "Train: 14 [7300/10009 ( 73%)]  Loss: 3.49 (3.14)  Time: 0.169s,  755.38/s  (0.167s,  764.52/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1222.362s\n",
      "Train: 14 [7350/10009 ( 73%)]  Loss: 2.99 (3.14)  Time: 0.170s,  751.34/s  (0.167s,  764.47/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 1230.826s\n",
      "Train: 14 [7400/10009 ( 74%)]  Loss: 3.19 (3.14)  Time: 0.170s,  754.62/s  (0.167s,  764.40/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1239.304s\n",
      "Train: 14 [7450/10009 ( 74%)]  Loss: 3.03 (3.14)  Time: 0.168s,  761.67/s  (0.167s,  764.32/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1247.814s\n",
      "Train: 14 [7500/10009 ( 75%)]  Loss: 3.12 (3.14)  Time: 0.170s,  751.42/s  (0.167s,  764.25/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1256.299s\n",
      "Train: 14 [7550/10009 ( 75%)]  Loss: 3.03 (3.14)  Time: 0.167s,  767.18/s  (0.168s,  763.82/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1265.381s\n",
      "Train: 14 [7600/10009 ( 76%)]  Loss: 3.07 (3.14)  Time: 0.171s,  750.61/s  (0.168s,  763.53/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1274.248s\n",
      "Train: 14 [7650/10009 ( 76%)]  Loss: 2.99 (3.14)  Time: 0.171s,  746.39/s  (0.168s,  763.46/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1282.742s\n",
      "Train: 14 [7700/10009 ( 77%)]  Loss: 3.09 (3.14)  Time: 0.170s,  753.03/s  (0.168s,  763.44/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1291.162s\n",
      "Train: 14 [7750/10009 ( 77%)]  Loss: 3.09 (3.14)  Time: 0.168s,  760.96/s  (0.168s,  763.39/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1299.624s\n",
      "Train: 14 [7800/10009 ( 78%)]  Loss: 3.31 (3.14)  Time: 0.168s,  761.31/s  (0.168s,  763.35/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1308.080s\n",
      "Train: 14 [7850/10009 ( 78%)]  Loss: 3.03 (3.14)  Time: 0.169s,  758.73/s  (0.168s,  763.31/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1316.538s\n",
      "Train: 14 [7900/10009 ( 79%)]  Loss: 3.15 (3.14)  Time: 0.169s,  759.53/s  (0.168s,  763.26/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1324.999s\n",
      "Train: 14 [7950/10009 ( 79%)]  Loss: 3.29 (3.14)  Time: 0.173s,  738.82/s  (0.168s,  763.24/s)  LR: 1.250e-05  Data: 0.011 (0.008)Time: 1333.435s\n",
      "Train: 14 [8000/10009 ( 80%)]  Loss: 3.12 (3.14)  Time: 0.170s,  753.81/s  (0.168s,  763.21/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1341.872s\n",
      "Train: 14 [8050/10009 ( 80%)]  Loss: 3.15 (3.14)  Time: 0.168s,  761.05/s  (0.168s,  763.19/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1350.287s\n",
      "Train: 14 [8100/10009 ( 81%)]  Loss: 3.13 (3.14)  Time: 0.168s,  760.87/s  (0.168s,  763.15/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1358.741s\n",
      "Train: 14 [8150/10009 ( 81%)]  Loss: 2.96 (3.14)  Time: 0.169s,  758.86/s  (0.168s,  763.12/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1367.191s\n",
      "Train: 14 [8200/10009 ( 82%)]  Loss: 3.02 (3.14)  Time: 0.172s,  746.20/s  (0.168s,  763.06/s)  LR: 1.250e-05  Data: 0.010 (0.008)Time: 1375.672s\n",
      "Train: 14 [8250/10009 ( 82%)]  Loss: 3.21 (3.14)  Time: 0.167s,  766.76/s  (0.168s,  763.05/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1384.085s\n",
      "Train: 14 [8300/10009 ( 83%)]  Loss: 3.36 (3.14)  Time: 0.165s,  773.85/s  (0.168s,  763.04/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1392.491s\n",
      "Train: 14 [8350/10009 ( 83%)]  Loss: 2.91 (3.14)  Time: 0.170s,  752.31/s  (0.168s,  763.04/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1400.885s\n",
      "Train: 14 [8400/10009 ( 84%)]  Loss: 3.31 (3.14)  Time: 0.168s,  763.25/s  (0.168s,  763.01/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1409.313s\n",
      "Train: 14 [8450/10009 ( 84%)]  Loss: 3.23 (3.14)  Time: 0.165s,  774.29/s  (0.168s,  762.95/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1417.810s\n",
      "Train: 14 [8500/10009 ( 85%)]  Loss: 3.30 (3.14)  Time: 0.167s,  766.24/s  (0.168s,  762.86/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1426.367s\n",
      "Train: 14 [8550/10009 ( 85%)]  Loss: 3.02 (3.14)  Time: 0.168s,  762.97/s  (0.168s,  762.86/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1434.771s\n",
      "Train: 14 [8600/10009 ( 86%)]  Loss: 3.08 (3.14)  Time: 0.168s,  763.70/s  (0.168s,  762.81/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1443.258s\n",
      "Train: 14 [8650/10009 ( 86%)]  Loss: 2.92 (3.14)  Time: 0.169s,  758.74/s  (0.168s,  762.80/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1451.652s\n",
      "Train: 14 [8700/10009 ( 87%)]  Loss: 3.03 (3.14)  Time: 0.168s,  762.83/s  (0.168s,  762.81/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1460.034s\n",
      "Train: 14 [8750/10009 ( 87%)]  Loss: 3.09 (3.14)  Time: 0.167s,  766.87/s  (0.168s,  762.56/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1468.904s\n",
      "Train: 14 [8800/10009 ( 88%)]  Loss: 3.30 (3.14)  Time: 0.167s,  764.32/s  (0.168s,  762.51/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1477.393s\n",
      "Train: 14 [8850/10009 ( 88%)]  Loss: 2.89 (3.14)  Time: 0.170s,  752.90/s  (0.168s,  762.49/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 1485.825s\n",
      "Train: 14 [8900/10009 ( 89%)]  Loss: 3.47 (3.14)  Time: 0.168s,  762.64/s  (0.168s,  762.49/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1494.211s\n",
      "Train: 14 [8950/10009 ( 89%)]  Loss: 3.30 (3.14)  Time: 0.168s,  762.56/s  (0.168s,  762.46/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1502.672s\n",
      "Train: 14 [9000/10009 ( 90%)]  Loss: 3.05 (3.14)  Time: 0.168s,  763.34/s  (0.168s,  762.45/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1511.079s\n",
      "Train: 14 [9050/10009 ( 90%)]  Loss: 2.88 (3.14)  Time: 0.169s,  758.84/s  (0.168s,  762.44/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1519.495s\n",
      "Train: 14 [9100/10009 ( 91%)]  Loss: 3.18 (3.14)  Time: 0.168s,  763.56/s  (0.168s,  762.43/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1527.903s\n",
      "Train: 14 [9150/10009 ( 91%)]  Loss: 2.94 (3.14)  Time: 0.168s,  761.23/s  (0.168s,  762.43/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1536.314s\n",
      "Train: 14 [9200/10009 ( 92%)]  Loss: 2.89 (3.14)  Time: 0.163s,  785.54/s  (0.168s,  762.46/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 1544.630s\n",
      "Train: 14 [9250/10009 ( 92%)]  Loss: 3.33 (3.14)  Time: 0.163s,  786.46/s  (0.168s,  762.62/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1552.713s\n",
      "Train: 14 [9300/10009 ( 93%)]  Loss: 3.06 (3.14)  Time: 0.161s,  796.22/s  (0.168s,  762.76/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1560.802s\n",
      "Train: 14 [9350/10009 ( 93%)]  Loss: 3.01 (3.14)  Time: 0.166s,  770.02/s  (0.168s,  762.91/s)  LR: 1.250e-05  Data: 0.011 (0.008)Time: 1568.902s\n",
      "Train: 14 [9400/10009 ( 94%)]  Loss: 3.08 (3.14)  Time: 0.163s,  786.66/s  (0.168s,  763.07/s)  LR: 1.250e-05  Data: 0.008 (0.008)Time: 1576.942s\n",
      "Train: 14 [9450/10009 ( 94%)]  Loss: 3.27 (3.14)  Time: 0.160s,  799.99/s  (0.168s,  763.24/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1584.982s\n",
      "Train: 14 [9500/10009 ( 95%)]  Loss: 2.97 (3.14)  Time: 0.161s,  795.85/s  (0.168s,  763.40/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1593.034s\n",
      "Train: 14 [9550/10009 ( 95%)]  Loss: 3.15 (3.14)  Time: 0.159s,  803.14/s  (0.168s,  763.30/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1601.629s\n",
      "Train: 14 [9600/10009 ( 96%)]  Loss: 3.31 (3.14)  Time: 0.159s,  802.66/s  (0.168s,  763.46/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1609.688s\n",
      "Train: 14 [9650/10009 ( 96%)]  Loss: 3.14 (3.14)  Time: 0.163s,  785.60/s  (0.168s,  763.55/s)  LR: 1.250e-05  Data: 0.009 (0.008)Time: 1617.869s\n",
      "Train: 14 [9700/10009 ( 97%)]  Loss: 3.24 (3.14)  Time: 0.161s,  794.79/s  (0.168s,  763.71/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1625.917s\n",
      "Train: 14 [9750/10009 ( 97%)]  Loss: 3.11 (3.14)  Time: 0.160s,  799.12/s  (0.168s,  763.84/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1634.012s\n",
      "Train: 14 [9800/10009 ( 98%)]  Loss: 2.93 (3.14)  Time: 0.161s,  795.42/s  (0.168s,  763.90/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 1642.273s\n",
      "Train: 14 [9850/10009 ( 98%)]  Loss: 3.16 (3.14)  Time: 0.159s,  803.07/s  (0.168s,  764.02/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1650.382s\n",
      "Train: 14 [9900/10009 ( 99%)]  Loss: 3.32 (3.14)  Time: 0.160s,  800.48/s  (0.168s,  763.98/s)  LR: 1.250e-05  Data: 0.005 (0.008)Time: 1658.851s\n",
      "Train: 14 [9950/10009 ( 99%)]  Loss: 2.98 (3.14)  Time: 0.163s,  786.72/s  (0.168s,  764.13/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 1666.887s\n",
      "Train: 14 [10000/10009 (100%)]  Loss: 3.33 (3.14)  Time: 0.203s,  631.96/s  (0.168s,  764.18/s)  LR: 1.250e-05  Data: 0.048 (0.008)Time: 1675.165s\n",
      "Test: [   0/390]  Time: 0.907 (0.907)  Loss:   1.283 ( 1.283)  Acc@1:  71.094 ( 71.094)  Acc@5:  89.844 ( 89.844)\n",
      "Test: [  50/390]  Time: 0.340 (0.180)  Loss:   1.106 ( 1.852)  Acc@1:  74.219 ( 60.110)  Acc@5:  89.844 ( 80.622)\n",
      "Test: [ 100/390]  Time: 0.053 (0.156)  Loss:   2.047 ( 1.863)  Acc@1:  46.094 ( 57.642)  Acc@5:  86.719 ( 81.730)\n",
      "Test: [ 150/390]  Time: 0.052 (0.156)  Loss:   1.571 ( 1.846)  Acc@1:  60.156 ( 58.061)  Acc@5:  88.281 ( 82.140)\n",
      "Test: [ 200/390]  Time: 0.052 (0.154)  Loss:   2.954 ( 2.035)  Acc@1:  32.031 ( 54.656)  Acc@5:  64.844 ( 79.097)\n",
      "Test: [ 250/390]  Time: 0.053 (0.152)  Loss:   2.137 ( 2.147)  Acc@1:  58.594 ( 52.988)  Acc@5:  76.562 ( 77.129)\n",
      "Test: [ 300/390]  Time: 0.389 (0.159)  Loss:   2.393 ( 2.238)  Acc@1:  53.125 ( 51.402)  Acc@5:  70.312 ( 75.517)\n",
      "Test: [ 350/390]  Time: 0.053 (0.155)  Loss:   2.510 ( 2.313)  Acc@1:  53.125 ( 50.098)  Acc@5:  70.312 ( 74.301)\n",
      "Test: [ 390/390]  Time: 0.034 (0.159)  Loss:   3.241 ( 2.281)  Acc@1:  31.250 ( 50.648)  Acc@5:  63.750 ( 74.840)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 15 [   0/10009 (  0%)]  Loss: 3.01 (3.01)  Time: 0.720s,  177.76/s  (0.720s,  177.76/s)  LR: 9.413e-06  Data: 0.570 (0.570)Time: 0.721s\n",
      "Train: 15 [  50/10009 (  0%)]  Loss: 3.08 (3.08)  Time: 0.159s,  807.34/s  (0.176s,  727.61/s)  LR: 9.413e-06  Data: 0.007 (0.024)Time: 8.972s\n",
      "Train: 15 [ 100/10009 (  1%)]  Loss: 3.38 (3.09)  Time: 0.158s,  809.97/s  (0.168s,  763.31/s)  LR: 9.413e-06  Data: 0.005 (0.015)Time: 16.937s\n",
      "Train: 15 [ 150/10009 (  1%)]  Loss: 3.32 (3.08)  Time: 0.159s,  805.88/s  (0.165s,  776.54/s)  LR: 9.413e-06  Data: 0.006 (0.012)Time: 24.890s\n",
      "Train: 15 [ 200/10009 (  2%)]  Loss: 3.30 (3.09)  Time: 0.159s,  804.89/s  (0.164s,  780.91/s)  LR: 9.413e-06  Data: 0.006 (0.011)Time: 32.946s\n",
      "Train: 15 [ 250/10009 (  2%)]  Loss: 2.85 (3.08)  Time: 0.158s,  809.24/s  (0.171s,  748.22/s)  LR: 9.413e-06  Data: 0.005 (0.018)Time: 42.939s\n",
      "Train: 15 [ 300/10009 (  3%)]  Loss: 3.08 (3.08)  Time: 0.161s,  792.65/s  (0.173s,  739.79/s)  LR: 9.413e-06  Data: 0.008 (0.020)Time: 52.080s\n",
      "Train: 15 [ 350/10009 (  3%)]  Loss: 2.71 (3.08)  Time: 0.160s,  800.45/s  (0.171s,  746.96/s)  LR: 9.413e-06  Data: 0.006 (0.018)Time: 60.148s\n",
      "Train: 15 [ 400/10009 (  4%)]  Loss: 3.28 (3.08)  Time: 0.162s,  790.88/s  (0.171s,  750.50/s)  LR: 9.413e-06  Data: 0.009 (0.018)Time: 68.392s\n",
      "Train: 15 [ 450/10009 (  4%)]  Loss: 2.74 (3.09)  Time: 0.159s,  804.66/s  (0.169s,  755.49/s)  LR: 9.413e-06  Data: 0.006 (0.016)Time: 76.411s\n",
      "Train: 15 [ 500/10009 (  5%)]  Loss: 3.07 (3.08)  Time: 0.160s,  802.34/s  (0.169s,  755.55/s)  LR: 9.413e-06  Data: 0.005 (0.016)Time: 84.876s\n",
      "Train: 15 [ 550/10009 (  5%)]  Loss: 3.14 (3.08)  Time: 0.161s,  793.92/s  (0.169s,  756.01/s)  LR: 9.413e-06  Data: 0.007 (0.016)Time: 93.290s\n",
      "Train: 15 [ 600/10009 (  6%)]  Loss: 3.07 (3.09)  Time: 0.160s,  801.84/s  (0.169s,  759.39/s)  LR: 9.413e-06  Data: 0.005 (0.015)Time: 101.302s\n",
      "Train: 15 [ 650/10009 (  6%)]  Loss: 3.03 (3.08)  Time: 0.160s,  799.97/s  (0.168s,  762.14/s)  LR: 9.413e-06  Data: 0.006 (0.014)Time: 109.334s\n",
      "Train: 15 [ 700/10009 (  7%)]  Loss: 2.99 (3.08)  Time: 0.165s,  777.81/s  (0.168s,  764.08/s)  LR: 9.413e-06  Data: 0.009 (0.014)Time: 117.433s\n",
      "Train: 15 [ 750/10009 (  7%)]  Loss: 3.25 (3.08)  Time: 0.160s,  800.54/s  (0.167s,  765.67/s)  LR: 9.413e-06  Data: 0.006 (0.013)Time: 125.547s\n",
      "Train: 15 [ 800/10009 (  8%)]  Loss: 2.93 (3.08)  Time: 0.160s,  800.48/s  (0.167s,  766.68/s)  LR: 9.413e-06  Data: 0.006 (0.013)Time: 133.731s\n",
      "Train: 15 [ 850/10009 (  8%)]  Loss: 3.06 (3.08)  Time: 0.170s,  751.14/s  (0.167s,  765.14/s)  LR: 9.413e-06  Data: 0.005 (0.013)Time: 142.363s\n",
      "Train: 15 [ 900/10009 (  9%)]  Loss: 3.19 (3.08)  Time: 0.185s,  690.41/s  (0.168s,  763.06/s)  LR: 9.413e-06  Data: 0.007 (0.013)Time: 151.138s\n",
      "Train: 15 [ 950/10009 (  9%)]  Loss: 3.01 (3.08)  Time: 0.174s,  734.97/s  (0.168s,  760.86/s)  LR: 9.413e-06  Data: 0.006 (0.012)Time: 159.988s\n",
      "Train: 15 [1000/10009 ( 10%)]  Loss: 3.25 (3.08)  Time: 0.172s,  744.65/s  (0.169s,  759.21/s)  LR: 9.413e-06  Data: 0.006 (0.012)Time: 168.764s\n",
      "Train: 15 [1050/10009 ( 10%)]  Loss: 2.98 (3.08)  Time: 0.180s,  710.73/s  (0.169s,  758.26/s)  LR: 9.413e-06  Data: 0.014 (0.012)Time: 177.416s\n",
      "Train: 15 [1100/10009 ( 11%)]  Loss: 2.84 (3.08)  Time: 0.178s,  718.93/s  (0.169s,  756.49/s)  LR: 9.413e-06  Data: 0.007 (0.012)Time: 186.292s\n",
      "Train: 15 [1150/10009 ( 11%)]  Loss: 2.83 (3.08)  Time: 0.172s,  745.48/s  (0.169s,  755.24/s)  LR: 9.413e-06  Data: 0.007 (0.011)Time: 195.075s\n",
      "Train: 15 [1200/10009 ( 12%)]  Loss: 3.25 (3.08)  Time: 0.170s,  754.99/s  (0.170s,  753.99/s)  LR: 9.413e-06  Data: 0.006 (0.011)Time: 203.887s\n",
      "Train: 15 [1250/10009 ( 12%)]  Loss: 3.42 (3.08)  Time: 0.176s,  727.01/s  (0.170s,  753.24/s)  LR: 9.413e-06  Data: 0.011 (0.011)Time: 212.585s\n",
      "Train: 15 [1300/10009 ( 13%)]  Loss: 2.95 (3.08)  Time: 0.193s,  662.60/s  (0.170s,  751.35/s)  LR: 9.413e-06  Data: 0.016 (0.011)Time: 221.637s\n",
      "Train: 15 [1350/10009 ( 13%)]  Loss: 3.28 (3.08)  Time: 0.175s,  733.30/s  (0.171s,  750.07/s)  LR: 9.413e-06  Data: 0.005 (0.011)Time: 230.548s\n",
      "Train: 15 [1400/10009 ( 14%)]  Loss: 2.84 (3.08)  Time: 0.170s,  752.90/s  (0.171s,  749.35/s)  LR: 9.413e-06  Data: 0.007 (0.011)Time: 239.311s\n",
      "Train: 15 [1450/10009 ( 14%)]  Loss: 3.33 (3.08)  Time: 0.170s,  755.06/s  (0.171s,  748.86/s)  LR: 9.413e-06  Data: 0.008 (0.011)Time: 248.014s\n",
      "Train: 15 [1500/10009 ( 15%)]  Loss: 3.07 (3.08)  Time: 0.171s,  748.11/s  (0.171s,  748.39/s)  LR: 9.413e-06  Data: 0.011 (0.011)Time: 256.721s\n",
      "Train: 15 [1550/10009 ( 15%)]  Loss: 3.25 (3.08)  Time: 0.169s,  757.54/s  (0.171s,  748.29/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 265.308s\n",
      "Train: 15 [1600/10009 ( 16%)]  Loss: 2.94 (3.08)  Time: 0.168s,  764.12/s  (0.171s,  747.94/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 273.991s\n",
      "Train: 15 [1650/10009 ( 16%)]  Loss: 3.04 (3.08)  Time: 0.170s,  752.36/s  (0.171s,  747.96/s)  LR: 9.413e-06  Data: 0.007 (0.010)Time: 282.539s\n",
      "Train: 15 [1700/10009 ( 17%)]  Loss: 3.18 (3.08)  Time: 0.167s,  766.24/s  (0.171s,  748.05/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 291.059s\n",
      "Train: 15 [1750/10009 ( 17%)]  Loss: 3.02 (3.08)  Time: 0.172s,  742.25/s  (0.171s,  748.24/s)  LR: 9.413e-06  Data: 0.010 (0.010)Time: 299.539s\n",
      "Train: 15 [1800/10009 ( 18%)]  Loss: 3.17 (3.08)  Time: 0.168s,  763.21/s  (0.171s,  748.07/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 308.163s\n",
      "Train: 15 [1850/10009 ( 18%)]  Loss: 3.11 (3.08)  Time: 0.178s,  719.69/s  (0.171s,  747.51/s)  LR: 9.413e-06  Data: 0.010 (0.010)Time: 316.956s\n",
      "Train: 15 [1900/10009 ( 19%)]  Loss: 3.17 (3.08)  Time: 0.172s,  745.20/s  (0.171s,  747.02/s)  LR: 9.413e-06  Data: 0.009 (0.010)Time: 325.729s\n",
      "Train: 15 [1950/10009 ( 19%)]  Loss: 3.08 (3.08)  Time: 0.182s,  703.63/s  (0.171s,  746.77/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 334.410s\n",
      "Train: 15 [2000/10009 ( 20%)]  Loss: 3.25 (3.07)  Time: 0.172s,  743.99/s  (0.172s,  746.28/s)  LR: 9.413e-06  Data: 0.007 (0.010)Time: 343.206s\n",
      "Train: 15 [2050/10009 ( 20%)]  Loss: 3.13 (3.07)  Time: 0.169s,  758.83/s  (0.172s,  746.19/s)  LR: 9.413e-06  Data: 0.007 (0.010)Time: 351.826s\n",
      "Train: 15 [2100/10009 ( 21%)]  Loss: 3.06 (3.07)  Time: 0.177s,  722.15/s  (0.172s,  745.83/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 360.575s\n",
      "Train: 15 [2150/10009 ( 21%)]  Loss: 2.96 (3.07)  Time: 0.187s,  684.96/s  (0.172s,  745.79/s)  LR: 9.413e-06  Data: 0.010 (0.010)Time: 369.177s\n",
      "Train: 15 [2200/10009 ( 22%)]  Loss: 2.83 (3.07)  Time: 0.167s,  764.22/s  (0.172s,  745.77/s)  LR: 9.413e-06  Data: 0.006 (0.010)Time: 377.765s\n",
      "Train: 15 [2250/10009 ( 22%)]  Loss: 3.29 (3.07)  Time: 0.178s,  720.38/s  (0.172s,  745.44/s)  LR: 9.413e-06  Data: 0.009 (0.009)Time: 386.520s\n",
      "Train: 15 [2300/10009 ( 23%)]  Loss: 2.92 (3.07)  Time: 0.180s,  712.84/s  (0.172s,  745.42/s)  LR: 9.413e-06  Data: 0.009 (0.009)Time: 395.116s\n",
      "Train: 15 [2350/10009 ( 23%)]  Loss: 3.29 (3.07)  Time: 0.170s,  752.70/s  (0.172s,  745.64/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 403.584s\n",
      "Train: 15 [2400/10009 ( 24%)]  Loss: 3.11 (3.07)  Time: 0.167s,  765.52/s  (0.172s,  745.74/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 412.109s\n",
      "Train: 15 [2450/10009 ( 24%)]  Loss: 3.19 (3.07)  Time: 0.174s,  734.05/s  (0.172s,  745.36/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 420.907s\n",
      "Train: 15 [2500/10009 ( 25%)]  Loss: 3.29 (3.07)  Time: 0.178s,  718.27/s  (0.172s,  745.02/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 429.689s\n",
      "Train: 15 [2550/10009 ( 25%)]  Loss: 3.26 (3.07)  Time: 0.179s,  716.37/s  (0.172s,  744.99/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 438.296s\n",
      "Train: 15 [2600/10009 ( 26%)]  Loss: 3.27 (3.07)  Time: 0.168s,  761.85/s  (0.172s,  744.85/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 446.971s\n",
      "Train: 15 [2650/10009 ( 26%)]  Loss: 3.45 (3.07)  Time: 0.175s,  733.43/s  (0.172s,  744.75/s)  LR: 9.413e-06  Data: 0.010 (0.009)Time: 455.623s\n",
      "Train: 15 [2700/10009 ( 27%)]  Loss: 3.04 (3.07)  Time: 0.168s,  761.67/s  (0.172s,  744.64/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 464.286s\n",
      "Train: 15 [2750/10009 ( 27%)]  Loss: 2.91 (3.07)  Time: 0.175s,  729.57/s  (0.172s,  744.56/s)  LR: 9.413e-06  Data: 0.011 (0.009)Time: 472.934s\n",
      "Train: 15 [2800/10009 ( 28%)]  Loss: 3.39 (3.07)  Time: 0.171s,  747.08/s  (0.172s,  744.48/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 481.580s\n",
      "Train: 15 [2850/10009 ( 28%)]  Loss: 2.95 (3.07)  Time: 0.170s,  752.95/s  (0.172s,  744.72/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 490.020s\n",
      "Train: 15 [2900/10009 ( 29%)]  Loss: 2.87 (3.07)  Time: 0.179s,  713.53/s  (0.172s,  744.70/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 498.627s\n",
      "Train: 15 [2950/10009 ( 29%)]  Loss: 3.26 (3.07)  Time: 0.173s,  737.79/s  (0.172s,  744.49/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 507.362s\n",
      "Train: 15 [3000/10009 ( 30%)]  Loss: 2.92 (3.07)  Time: 0.170s,  751.39/s  (0.172s,  744.48/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 515.964s\n",
      "Train: 15 [3050/10009 ( 30%)]  Loss: 3.09 (3.07)  Time: 0.179s,  715.44/s  (0.172s,  744.02/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 524.885s\n",
      "Train: 15 [3100/10009 ( 31%)]  Loss: 3.34 (3.07)  Time: 0.168s,  761.47/s  (0.172s,  743.92/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 533.561s\n",
      "Train: 15 [3150/10009 ( 31%)]  Loss: 3.09 (3.07)  Time: 0.184s,  695.49/s  (0.172s,  743.71/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 542.319s\n",
      "Train: 15 [3200/10009 ( 32%)]  Loss: 3.01 (3.07)  Time: 0.170s,  751.62/s  (0.172s,  743.64/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 550.976s\n",
      "Train: 15 [3250/10009 ( 32%)]  Loss: 3.10 (3.07)  Time: 0.171s,  750.18/s  (0.172s,  743.67/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 559.562s\n",
      "Train: 15 [3300/10009 ( 33%)]  Loss: 2.86 (3.07)  Time: 0.167s,  766.95/s  (0.172s,  743.62/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 568.204s\n",
      "Train: 15 [3350/10009 ( 33%)]  Loss: 3.00 (3.07)  Time: 0.177s,  722.01/s  (0.172s,  743.54/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 576.868s\n",
      "Train: 15 [3400/10009 ( 34%)]  Loss: 3.22 (3.07)  Time: 0.181s,  707.62/s  (0.172s,  743.33/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 585.640s\n",
      "Train: 15 [3450/10009 ( 34%)]  Loss: 2.90 (3.07)  Time: 0.181s,  708.47/s  (0.172s,  742.96/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 594.552s\n",
      "Train: 15 [3500/10009 ( 35%)]  Loss: 3.04 (3.07)  Time: 0.183s,  700.69/s  (0.172s,  742.82/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 603.274s\n",
      "Train: 15 [3550/10009 ( 35%)]  Loss: 2.89 (3.07)  Time: 0.171s,  750.31/s  (0.172s,  742.60/s)  LR: 9.413e-06  Data: 0.009 (0.009)Time: 612.076s\n",
      "Train: 15 [3600/10009 ( 36%)]  Loss: 3.31 (3.07)  Time: 0.170s,  754.68/s  (0.172s,  742.66/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 620.642s\n",
      "Train: 15 [3650/10009 ( 36%)]  Loss: 2.90 (3.07)  Time: 0.178s,  718.00/s  (0.172s,  742.63/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 629.284s\n",
      "Train: 15 [3700/10009 ( 37%)]  Loss: 3.07 (3.07)  Time: 0.170s,  753.02/s  (0.172s,  742.57/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 637.959s\n",
      "Train: 15 [3750/10009 ( 37%)]  Loss: 3.18 (3.07)  Time: 0.171s,  747.81/s  (0.172s,  742.62/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 646.527s\n",
      "Train: 15 [3800/10009 ( 38%)]  Loss: 2.95 (3.07)  Time: 0.167s,  765.40/s  (0.172s,  742.70/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 655.082s\n",
      "Train: 15 [3850/10009 ( 38%)]  Loss: 3.35 (3.07)  Time: 0.169s,  758.87/s  (0.172s,  742.83/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 663.578s\n",
      "Train: 15 [3900/10009 ( 39%)]  Loss: 3.05 (3.07)  Time: 0.173s,  739.99/s  (0.172s,  742.96/s)  LR: 9.413e-06  Data: 0.009 (0.008)Time: 672.076s\n",
      "Train: 15 [3950/10009 ( 39%)]  Loss: 3.38 (3.07)  Time: 0.178s,  719.97/s  (0.172s,  742.97/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 680.686s\n",
      "Train: 15 [4000/10009 ( 40%)]  Loss: 2.92 (3.07)  Time: 0.170s,  754.85/s  (0.172s,  743.09/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 689.181s\n",
      "Train: 15 [4050/10009 ( 40%)]  Loss: 2.93 (3.07)  Time: 0.171s,  750.57/s  (0.172s,  743.22/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 697.680s\n",
      "Train: 15 [4100/10009 ( 41%)]  Loss: 3.12 (3.07)  Time: 0.168s,  764.04/s  (0.172s,  743.10/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 706.399s\n",
      "Train: 15 [4150/10009 ( 41%)]  Loss: 2.97 (3.07)  Time: 0.168s,  761.44/s  (0.172s,  743.11/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 715.005s\n",
      "Train: 15 [4200/10009 ( 42%)]  Loss: 2.90 (3.07)  Time: 0.172s,  745.33/s  (0.172s,  742.95/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 723.777s\n",
      "Train: 15 [4250/10009 ( 42%)]  Loss: 3.24 (3.07)  Time: 0.168s,  760.37/s  (0.172s,  742.79/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 732.540s\n",
      "Train: 15 [4300/10009 ( 43%)]  Loss: 3.26 (3.07)  Time: 0.168s,  761.75/s  (0.172s,  742.81/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 741.136s\n",
      "Train: 15 [4350/10009 ( 43%)]  Loss: 3.16 (3.07)  Time: 0.171s,  747.64/s  (0.172s,  742.77/s)  LR: 9.413e-06  Data: 0.008 (0.008)Time: 749.800s\n",
      "Train: 15 [4400/10009 ( 44%)]  Loss: 2.99 (3.07)  Time: 0.169s,  755.38/s  (0.172s,  742.65/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 758.539s\n",
      "Train: 15 [4450/10009 ( 44%)]  Loss: 3.17 (3.08)  Time: 0.173s,  741.49/s  (0.172s,  742.40/s)  LR: 9.413e-06  Data: 0.010 (0.008)Time: 767.412s\n",
      "Train: 15 [4500/10009 ( 45%)]  Loss: 3.23 (3.08)  Time: 0.169s,  758.71/s  (0.172s,  742.62/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 775.806s\n",
      "Train: 15 [4550/10009 ( 45%)]  Loss: 2.91 (3.08)  Time: 0.170s,  751.16/s  (0.172s,  742.83/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 784.196s\n",
      "Train: 15 [4600/10009 ( 46%)]  Loss: 3.02 (3.08)  Time: 0.167s,  765.85/s  (0.172s,  742.85/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 792.793s\n",
      "Train: 15 [4650/10009 ( 46%)]  Loss: 2.88 (3.08)  Time: 0.165s,  773.71/s  (0.172s,  743.03/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 801.219s\n",
      "Train: 15 [4700/10009 ( 47%)]  Loss: 3.08 (3.08)  Time: 0.166s,  770.24/s  (0.172s,  743.24/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 809.602s\n",
      "Train: 15 [4750/10009 ( 47%)]  Loss: 3.08 (3.08)  Time: 0.170s,  751.43/s  (0.172s,  743.42/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 818.016s\n",
      "Train: 15 [4800/10009 ( 48%)]  Loss: 3.00 (3.08)  Time: 0.169s,  755.85/s  (0.172s,  743.50/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 826.534s\n",
      "Train: 15 [4850/10009 ( 48%)]  Loss: 3.06 (3.08)  Time: 0.170s,  753.34/s  (0.172s,  743.39/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 835.268s\n",
      "Train: 15 [4900/10009 ( 49%)]  Loss: 2.94 (3.08)  Time: 0.175s,  731.06/s  (0.172s,  743.28/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 843.996s\n",
      "Train: 15 [4950/10009 ( 49%)]  Loss: 3.10 (3.08)  Time: 0.184s,  697.43/s  (0.172s,  743.11/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 852.799s\n",
      "Train: 15 [5000/10009 ( 50%)]  Loss: 3.20 (3.08)  Time: 0.172s,  745.76/s  (0.172s,  742.46/s)  LR: 9.413e-06  Data: 0.009 (0.008)Time: 862.175s\n",
      "Train: 15 [5050/10009 ( 50%)]  Loss: 2.99 (3.08)  Time: 0.170s,  752.02/s  (0.172s,  742.59/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 870.638s\n",
      "Train: 15 [5100/10009 ( 51%)]  Loss: 3.13 (3.08)  Time: 0.169s,  758.19/s  (0.172s,  742.59/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 879.252s\n",
      "Train: 15 [5150/10009 ( 51%)]  Loss: 2.99 (3.08)  Time: 0.181s,  706.46/s  (0.172s,  742.57/s)  LR: 9.413e-06  Data: 0.014 (0.008)Time: 887.897s\n",
      "Train: 15 [5200/10009 ( 52%)]  Loss: 2.90 (3.08)  Time: 0.170s,  754.60/s  (0.172s,  742.56/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 896.523s\n",
      "Train: 15 [5250/10009 ( 52%)]  Loss: 2.89 (3.08)  Time: 0.167s,  766.57/s  (0.172s,  742.60/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 905.104s\n",
      "Train: 15 [5300/10009 ( 53%)]  Loss: 2.89 (3.08)  Time: 0.170s,  751.45/s  (0.172s,  742.63/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 913.679s\n",
      "Train: 15 [5350/10009 ( 53%)]  Loss: 3.12 (3.08)  Time: 0.169s,  755.46/s  (0.172s,  742.62/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 922.305s\n",
      "Train: 15 [5400/10009 ( 54%)]  Loss: 3.07 (3.08)  Time: 0.169s,  758.73/s  (0.172s,  742.48/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 931.106s\n",
      "Train: 15 [5450/10009 ( 54%)]  Loss: 3.00 (3.07)  Time: 0.169s,  758.93/s  (0.172s,  742.31/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 939.938s\n",
      "Train: 15 [5500/10009 ( 55%)]  Loss: 3.07 (3.07)  Time: 0.177s,  724.25/s  (0.172s,  742.26/s)  LR: 9.413e-06  Data: 0.014 (0.008)Time: 948.624s\n",
      "Train: 15 [5550/10009 ( 55%)]  Loss: 3.12 (3.07)  Time: 0.177s,  723.35/s  (0.172s,  742.32/s)  LR: 9.413e-06  Data: 0.017 (0.008)Time: 957.171s\n",
      "Train: 15 [5600/10009 ( 56%)]  Loss: 3.33 (3.07)  Time: 0.167s,  767.66/s  (0.172s,  742.03/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 966.168s\n",
      "Train: 15 [5650/10009 ( 56%)]  Loss: 3.06 (3.07)  Time: 0.168s,  760.90/s  (0.172s,  742.18/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 974.595s\n",
      "Train: 15 [5700/10009 ( 57%)]  Loss: 3.35 (3.07)  Time: 0.169s,  758.21/s  (0.172s,  742.17/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 983.228s\n",
      "Train: 15 [5750/10009 ( 57%)]  Loss: 3.09 (3.07)  Time: 0.173s,  738.84/s  (0.172s,  742.11/s)  LR: 9.413e-06  Data: 0.011 (0.008)Time: 991.937s\n",
      "Train: 15 [5800/10009 ( 58%)]  Loss: 3.10 (3.07)  Time: 0.167s,  767.59/s  (0.172s,  742.14/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1000.514s\n",
      "Train: 15 [5850/10009 ( 58%)]  Loss: 3.05 (3.07)  Time: 0.166s,  769.41/s  (0.172s,  742.29/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1008.934s\n",
      "Train: 15 [5900/10009 ( 59%)]  Loss: 3.01 (3.07)  Time: 0.166s,  769.90/s  (0.172s,  742.38/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1017.439s\n",
      "Train: 15 [5950/10009 ( 59%)]  Loss: 3.53 (3.07)  Time: 0.169s,  756.91/s  (0.172s,  742.50/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1025.888s\n",
      "Train: 15 [6000/10009 ( 60%)]  Loss: 3.27 (3.07)  Time: 0.165s,  776.11/s  (0.172s,  742.64/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1034.315s\n",
      "Train: 15 [6050/10009 ( 60%)]  Loss: 3.15 (3.07)  Time: 0.169s,  759.24/s  (0.172s,  742.79/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1042.726s\n",
      "Train: 15 [6100/10009 ( 61%)]  Loss: 3.19 (3.07)  Time: 0.167s,  764.66/s  (0.172s,  742.90/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1051.182s\n",
      "Train: 15 [6150/10009 ( 61%)]  Loss: 3.01 (3.07)  Time: 0.168s,  762.39/s  (0.172s,  743.04/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1059.607s\n",
      "Train: 15 [6200/10009 ( 62%)]  Loss: 3.12 (3.07)  Time: 0.169s,  756.70/s  (0.172s,  743.15/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1068.062s\n",
      "Train: 15 [6250/10009 ( 62%)]  Loss: 3.40 (3.07)  Time: 0.169s,  755.49/s  (0.172s,  743.28/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1076.479s\n",
      "Train: 15 [6300/10009 ( 63%)]  Loss: 3.04 (3.08)  Time: 0.169s,  757.40/s  (0.172s,  743.35/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1084.994s\n",
      "Train: 15 [6350/10009 ( 63%)]  Loss: 3.07 (3.07)  Time: 0.169s,  758.22/s  (0.172s,  743.39/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1093.545s\n",
      "Train: 15 [6400/10009 ( 64%)]  Loss: 3.22 (3.08)  Time: 0.169s,  757.69/s  (0.172s,  743.49/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1101.994s\n",
      "Train: 15 [6450/10009 ( 64%)]  Loss: 3.09 (3.08)  Time: 0.169s,  756.46/s  (0.172s,  743.51/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1110.572s\n",
      "Train: 15 [6500/10009 ( 65%)]  Loss: 3.03 (3.08)  Time: 0.169s,  756.83/s  (0.172s,  743.63/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1119.009s\n",
      "Train: 15 [6550/10009 ( 65%)]  Loss: 3.32 (3.07)  Time: 0.172s,  745.82/s  (0.172s,  743.50/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1127.808s\n",
      "Train: 15 [6600/10009 ( 66%)]  Loss: 3.03 (3.07)  Time: 0.171s,  747.27/s  (0.172s,  743.60/s)  LR: 9.413e-06  Data: 0.009 (0.008)Time: 1136.259s\n",
      "Train: 15 [6650/10009 ( 66%)]  Loss: 2.99 (3.08)  Time: 0.170s,  751.29/s  (0.172s,  743.63/s)  LR: 9.413e-06  Data: 0.009 (0.008)Time: 1144.828s\n",
      "Train: 15 [6700/10009 ( 67%)]  Loss: 3.30 (3.08)  Time: 0.170s,  752.57/s  (0.172s,  743.56/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1153.546s\n",
      "Train: 15 [6750/10009 ( 67%)]  Loss: 3.15 (3.07)  Time: 0.169s,  759.43/s  (0.172s,  743.15/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1162.791s\n",
      "Train: 15 [6800/10009 ( 68%)]  Loss: 3.21 (3.07)  Time: 0.169s,  757.87/s  (0.172s,  743.22/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1171.297s\n",
      "Train: 15 [6850/10009 ( 68%)]  Loss: 3.16 (3.07)  Time: 0.169s,  758.09/s  (0.172s,  743.17/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1179.986s\n",
      "Train: 15 [6900/10009 ( 69%)]  Loss: 2.94 (3.07)  Time: 0.169s,  756.36/s  (0.172s,  743.24/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1188.477s\n",
      "Train: 15 [6950/10009 ( 69%)]  Loss: 3.03 (3.07)  Time: 0.167s,  765.92/s  (0.172s,  743.11/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1197.305s\n",
      "Train: 15 [7000/10009 ( 70%)]  Loss: 3.02 (3.07)  Time: 0.168s,  763.83/s  (0.172s,  742.88/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1206.290s\n",
      "Train: 15 [7050/10009 ( 70%)]  Loss: 2.87 (3.07)  Time: 0.167s,  766.46/s  (0.172s,  742.95/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1214.788s\n",
      "Train: 15 [7100/10009 ( 71%)]  Loss: 2.92 (3.07)  Time: 0.169s,  756.77/s  (0.172s,  743.06/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1223.226s\n",
      "Train: 15 [7150/10009 ( 71%)]  Loss: 2.98 (3.07)  Time: 0.170s,  753.48/s  (0.172s,  742.93/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1232.048s\n",
      "Train: 15 [7200/10009 ( 72%)]  Loss: 2.86 (3.07)  Time: 0.170s,  751.89/s  (0.172s,  743.04/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1240.482s\n",
      "Train: 15 [7250/10009 ( 72%)]  Loss: 2.94 (3.07)  Time: 0.167s,  764.70/s  (0.172s,  743.11/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1248.982s\n",
      "Train: 15 [7300/10009 ( 73%)]  Loss: 3.22 (3.07)  Time: 0.166s,  769.18/s  (0.172s,  743.11/s)  LR: 9.413e-06  Data: 0.005 (0.008)Time: 1257.583s\n",
      "Train: 15 [7350/10009 ( 73%)]  Loss: 3.40 (3.07)  Time: 0.170s,  753.01/s  (0.172s,  743.12/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1266.184s\n",
      "Train: 15 [7400/10009 ( 74%)]  Loss: 2.86 (3.07)  Time: 0.175s,  731.98/s  (0.172s,  743.20/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1274.658s\n",
      "Train: 15 [7450/10009 ( 74%)]  Loss: 2.98 (3.07)  Time: 0.168s,  759.90/s  (0.172s,  743.27/s)  LR: 9.413e-06  Data: 0.006 (0.008)Time: 1283.152s\n",
      "Train: 15 [7500/10009 ( 75%)]  Loss: 3.05 (3.07)  Time: 0.175s,  730.86/s  (0.172s,  743.25/s)  LR: 9.413e-06  Data: 0.012 (0.008)Time: 1291.786s\n",
      "Train: 15 [7550/10009 ( 75%)]  Loss: 2.80 (3.07)  Time: 0.167s,  767.75/s  (0.172s,  743.33/s)  LR: 9.413e-06  Data: 0.004 (0.008)Time: 1300.273s\n",
      "Train: 15 [7600/10009 ( 76%)]  Loss: 2.94 (3.07)  Time: 0.175s,  729.91/s  (0.172s,  743.18/s)  LR: 9.413e-06  Data: 0.010 (0.008)Time: 1309.139s\n",
      "Train: 15 [7650/10009 ( 76%)]  Loss: 3.31 (3.07)  Time: 0.179s,  714.68/s  (0.172s,  743.22/s)  LR: 9.413e-06  Data: 0.014 (0.008)Time: 1317.687s\n",
      "Train: 15 [7700/10009 ( 77%)]  Loss: 3.14 (3.07)  Time: 0.171s,  747.94/s  (0.172s,  743.23/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1326.269s\n",
      "Train: 15 [7750/10009 ( 77%)]  Loss: 3.16 (3.07)  Time: 0.168s,  762.12/s  (0.172s,  743.27/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1334.817s\n",
      "Train: 15 [7800/10009 ( 78%)]  Loss: 3.00 (3.07)  Time: 0.189s,  677.03/s  (0.172s,  743.07/s)  LR: 9.413e-06  Data: 0.007 (0.008)Time: 1343.781s\n",
      "Train: 15 [7850/10009 ( 78%)]  Loss: 2.98 (3.07)  Time: 0.265s,  482.28/s  (0.172s,  742.61/s)  LR: 9.413e-06  Data: 0.105 (0.008)Time: 1353.233s\n",
      "Train: 15 [7900/10009 ( 79%)]  Loss: 3.09 (3.07)  Time: 0.167s,  765.52/s  (0.173s,  740.86/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1365.060s\n",
      "Train: 15 [7950/10009 ( 79%)]  Loss: 2.92 (3.07)  Time: 0.168s,  760.66/s  (0.173s,  740.86/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1373.711s\n",
      "Train: 15 [8000/10009 ( 80%)]  Loss: 2.98 (3.07)  Time: 0.175s,  730.37/s  (0.173s,  740.35/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 1383.298s\n",
      "Train: 15 [8050/10009 ( 80%)]  Loss: 3.12 (3.07)  Time: 0.170s,  754.55/s  (0.173s,  740.29/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1392.047s\n",
      "Train: 15 [8100/10009 ( 81%)]  Loss: 3.23 (3.07)  Time: 0.167s,  766.85/s  (0.173s,  740.21/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1400.862s\n",
      "Train: 15 [8150/10009 ( 81%)]  Loss: 3.07 (3.07)  Time: 0.170s,  751.60/s  (0.173s,  740.31/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 1409.318s\n",
      "Train: 15 [8200/10009 ( 82%)]  Loss: 2.91 (3.07)  Time: 0.173s,  740.41/s  (0.173s,  740.42/s)  LR: 9.413e-06  Data: 0.011 (0.009)Time: 1417.752s\n",
      "Train: 15 [8250/10009 ( 82%)]  Loss: 2.89 (3.07)  Time: 0.165s,  775.42/s  (0.173s,  740.53/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1426.174s\n",
      "Train: 15 [8300/10009 ( 83%)]  Loss: 3.10 (3.07)  Time: 0.167s,  764.48/s  (0.173s,  740.65/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1434.592s\n",
      "Train: 15 [8350/10009 ( 83%)]  Loss: 3.08 (3.07)  Time: 0.168s,  763.44/s  (0.173s,  740.74/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1443.054s\n",
      "Train: 15 [8400/10009 ( 84%)]  Loss: 3.04 (3.07)  Time: 0.168s,  761.61/s  (0.173s,  740.86/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 1451.464s\n",
      "Train: 15 [8450/10009 ( 84%)]  Loss: 3.04 (3.07)  Time: 0.165s,  773.61/s  (0.173s,  740.92/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1459.979s\n",
      "Train: 15 [8500/10009 ( 85%)]  Loss: 3.29 (3.07)  Time: 0.166s,  770.10/s  (0.173s,  740.83/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1468.786s\n",
      "Train: 15 [8550/10009 ( 85%)]  Loss: 3.02 (3.07)  Time: 0.165s,  774.21/s  (0.173s,  740.83/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1477.436s\n",
      "Train: 15 [8600/10009 ( 86%)]  Loss: 2.94 (3.07)  Time: 0.165s,  774.63/s  (0.173s,  740.79/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1486.154s\n",
      "Train: 15 [8650/10009 ( 86%)]  Loss: 3.03 (3.07)  Time: 0.167s,  766.27/s  (0.173s,  740.81/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1494.754s\n",
      "Train: 15 [8700/10009 ( 87%)]  Loss: 3.10 (3.07)  Time: 0.174s,  737.26/s  (0.173s,  740.75/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 1503.504s\n",
      "Train: 15 [8750/10009 ( 87%)]  Loss: 3.39 (3.07)  Time: 0.169s,  758.55/s  (0.173s,  740.68/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 1512.293s\n",
      "Train: 15 [8800/10009 ( 88%)]  Loss: 2.71 (3.07)  Time: 0.183s,  699.22/s  (0.173s,  740.53/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 1521.242s\n",
      "Train: 15 [8850/10009 ( 88%)]  Loss: 3.48 (3.07)  Time: 0.169s,  758.32/s  (0.173s,  740.46/s)  LR: 9.413e-06  Data: 0.008 (0.009)Time: 1530.023s\n",
      "Train: 15 [8900/10009 ( 89%)]  Loss: 3.30 (3.07)  Time: 0.178s,  719.55/s  (0.173s,  740.52/s)  LR: 9.413e-06  Data: 0.013 (0.009)Time: 1538.552s\n",
      "Train: 15 [8950/10009 ( 89%)]  Loss: 3.30 (3.07)  Time: 0.172s,  742.44/s  (0.173s,  740.58/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1547.057s\n",
      "Train: 15 [9000/10009 ( 90%)]  Loss: 3.03 (3.07)  Time: 0.174s,  737.33/s  (0.173s,  740.61/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1555.640s\n",
      "Train: 15 [9050/10009 ( 90%)]  Loss: 3.13 (3.07)  Time: 0.166s,  768.89/s  (0.173s,  740.67/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 1564.151s\n",
      "Train: 15 [9100/10009 ( 91%)]  Loss: 2.95 (3.07)  Time: 0.182s,  702.43/s  (0.173s,  740.60/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 1572.945s\n",
      "Train: 15 [9150/10009 ( 91%)]  Loss: 3.02 (3.07)  Time: 0.174s,  737.53/s  (0.173s,  740.62/s)  LR: 9.413e-06  Data: 0.012 (0.009)Time: 1581.541s\n",
      "Train: 15 [9200/10009 ( 92%)]  Loss: 2.91 (3.07)  Time: 0.165s,  776.31/s  (0.173s,  740.74/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1589.937s\n",
      "Train: 15 [9250/10009 ( 92%)]  Loss: 2.93 (3.07)  Time: 0.167s,  768.65/s  (0.173s,  740.86/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1598.315s\n",
      "Train: 15 [9300/10009 ( 93%)]  Loss: 3.11 (3.07)  Time: 0.167s,  764.36/s  (0.173s,  740.54/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1607.634s\n",
      "Train: 15 [9350/10009 ( 93%)]  Loss: 2.98 (3.07)  Time: 0.171s,  747.22/s  (0.173s,  740.55/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1616.259s\n",
      "Train: 15 [9400/10009 ( 94%)]  Loss: 3.09 (3.07)  Time: 0.173s,  739.88/s  (0.173s,  740.49/s)  LR: 9.413e-06  Data: 0.007 (0.009)Time: 1625.035s\n",
      "Train: 15 [9450/10009 ( 94%)]  Loss: 2.95 (3.07)  Time: 0.171s,  747.21/s  (0.173s,  740.52/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1633.624s\n",
      "Train: 15 [9500/10009 ( 95%)]  Loss: 3.15 (3.07)  Time: 0.174s,  734.41/s  (0.173s,  740.45/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1642.412s\n",
      "Train: 15 [9550/10009 ( 95%)]  Loss: 2.97 (3.07)  Time: 0.169s,  755.37/s  (0.173s,  740.37/s)  LR: 9.413e-06  Data: 0.004 (0.009)Time: 1651.235s\n",
      "Train: 15 [9600/10009 ( 96%)]  Loss: 3.10 (3.07)  Time: 0.171s,  748.76/s  (0.173s,  740.38/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1659.847s\n",
      "Train: 15 [9650/10009 ( 96%)]  Loss: 2.85 (3.07)  Time: 0.174s,  737.06/s  (0.173s,  740.37/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1668.526s\n",
      "Train: 15 [9700/10009 ( 97%)]  Loss: 2.97 (3.07)  Time: 0.174s,  736.72/s  (0.173s,  740.33/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1677.260s\n",
      "Train: 15 [9750/10009 ( 97%)]  Loss: 3.19 (3.07)  Time: 0.173s,  741.84/s  (0.173s,  740.34/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1685.890s\n",
      "Train: 15 [9800/10009 ( 98%)]  Loss: 3.05 (3.07)  Time: 0.171s,  750.03/s  (0.173s,  740.31/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1694.585s\n",
      "Train: 15 [9850/10009 ( 98%)]  Loss: 3.13 (3.07)  Time: 0.173s,  737.96/s  (0.173s,  740.30/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1703.259s\n",
      "Train: 15 [9900/10009 ( 99%)]  Loss: 2.93 (3.07)  Time: 0.170s,  751.58/s  (0.173s,  740.28/s)  LR: 9.413e-06  Data: 0.005 (0.009)Time: 1711.963s\n",
      "Train: 15 [9950/10009 ( 99%)]  Loss: 3.38 (3.07)  Time: 0.172s,  744.86/s  (0.173s,  740.29/s)  LR: 9.413e-06  Data: 0.006 (0.009)Time: 1720.585s\n",
      "Train: 15 [10000/10009 (100%)]  Loss: 3.18 (3.07)  Time: 0.220s,  582.33/s  (0.173s,  740.22/s)  LR: 9.413e-06  Data: 0.049 (0.009)Time: 1729.374s\n",
      "Test: [   0/390]  Time: 0.696 (0.696)  Loss:   0.996 ( 0.996)  Acc@1:  82.812 ( 82.812)  Acc@5:  92.188 ( 92.188)\n",
      "Test: [  50/390]  Time: 0.512 (0.167)  Loss:   1.094 ( 1.801)  Acc@1:  75.781 ( 61.106)  Acc@5:  92.969 ( 81.327)\n",
      "Test: [ 100/390]  Time: 0.306 (0.163)  Loss:   1.963 ( 1.817)  Acc@1:  54.688 ( 58.547)  Acc@5:  85.938 ( 82.364)\n",
      "Test: [ 150/390]  Time: 0.054 (0.158)  Loss:   1.436 ( 1.792)  Acc@1:  63.281 ( 59.127)  Acc@5:  86.719 ( 82.709)\n",
      "Test: [ 200/390]  Time: 0.059 (0.155)  Loss:   3.238 ( 1.976)  Acc@1:  28.906 ( 55.760)  Acc@5:  60.156 ( 79.870)\n",
      "Test: [ 250/390]  Time: 0.055 (0.153)  Loss:   2.053 ( 2.085)  Acc@1:  60.938 ( 54.053)  Acc@5:  75.781 ( 77.923)\n",
      "Test: [ 300/390]  Time: 0.055 (0.153)  Loss:   2.401 ( 2.176)  Acc@1:  56.250 ( 52.333)  Acc@5:  71.094 ( 76.326)\n",
      "Test: [ 350/390]  Time: 0.055 (0.151)  Loss:   2.748 ( 2.249)  Acc@1:  41.406 ( 51.100)  Acc@5:  71.875 ( 75.149)\n",
      "Test: [ 390/390]  Time: 0.037 (0.152)  Loss:   3.499 ( 2.223)  Acc@1:  26.250 ( 51.510)  Acc@5:  57.500 ( 75.642)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 51.51)\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 16 [   0/10009 (  0%)]  Loss: 3.17 (3.17)  Time: 0.878s,  145.85/s  (0.878s,  145.85/s)  LR: 6.674e-06  Data: 0.715 (0.715)Time: 0.878s\n",
      "Train: 16 [  50/10009 (  0%)]  Loss: 2.87 (3.05)  Time: 0.170s,  754.49/s  (0.191s,  669.98/s)  LR: 6.674e-06  Data: 0.006 (0.027)Time: 9.744s\n",
      "Train: 16 [ 100/10009 (  1%)]  Loss: 3.06 (3.02)  Time: 0.172s,  746.31/s  (0.181s,  705.43/s)  LR: 6.674e-06  Data: 0.006 (0.017)Time: 18.327s\n",
      "Train: 16 [ 150/10009 (  1%)]  Loss: 2.92 (3.02)  Time: 0.169s,  758.78/s  (0.178s,  717.30/s)  LR: 6.674e-06  Data: 0.005 (0.014)Time: 26.946s\n",
      "Train: 16 [ 200/10009 (  2%)]  Loss: 3.34 (3.01)  Time: 0.170s,  751.78/s  (0.177s,  724.07/s)  LR: 6.674e-06  Data: 0.006 (0.012)Time: 35.533s\n",
      "Train: 16 [ 250/10009 (  2%)]  Loss: 3.08 (3.01)  Time: 0.173s,  741.68/s  (0.176s,  728.52/s)  LR: 6.674e-06  Data: 0.006 (0.011)Time: 44.101s\n",
      "Train: 16 [ 300/10009 (  3%)]  Loss: 2.86 (3.01)  Time: 0.177s,  723.11/s  (0.175s,  730.48/s)  LR: 6.674e-06  Data: 0.008 (0.010)Time: 52.744s\n",
      "Train: 16 [ 350/10009 (  3%)]  Loss: 2.99 (3.01)  Time: 0.168s,  764.12/s  (0.174s,  737.64/s)  LR: 6.674e-06  Data: 0.005 (0.010)Time: 60.908s\n",
      "Train: 16 [ 400/10009 (  4%)]  Loss: 2.82 (3.01)  Time: 0.161s,  795.62/s  (0.173s,  741.50/s)  LR: 6.674e-06  Data: 0.007 (0.009)Time: 69.222s\n",
      "Train: 16 [ 450/10009 (  4%)]  Loss: 3.08 (3.02)  Time: 0.163s,  783.33/s  (0.172s,  743.84/s)  LR: 6.674e-06  Data: 0.009 (0.009)Time: 77.609s\n",
      "Train: 16 [ 500/10009 (  5%)]  Loss: 3.16 (3.01)  Time: 0.161s,  797.11/s  (0.171s,  747.72/s)  LR: 6.674e-06  Data: 0.006 (0.009)Time: 85.765s\n",
      "Train: 16 [ 550/10009 (  5%)]  Loss: 3.19 (3.01)  Time: 0.160s,  799.50/s  (0.170s,  751.41/s)  LR: 6.674e-06  Data: 0.006 (0.008)Time: 93.861s\n",
      "Train: 16 [ 600/10009 (  6%)]  Loss: 3.12 (3.01)  Time: 0.165s,  777.97/s  (0.170s,  754.31/s)  LR: 6.674e-06  Data: 0.009 (0.008)Time: 101.985s\n",
      "Train: 16 [ 650/10009 (  6%)]  Loss: 3.38 (3.01)  Time: 0.162s,  790.54/s  (0.169s,  755.28/s)  LR: 6.674e-06  Data: 0.007 (0.008)Time: 110.327s\n",
      "Train: 16 [ 700/10009 (  7%)]  Loss: 3.08 (3.01)  Time: 0.161s,  792.87/s  (0.169s,  757.69/s)  LR: 6.674e-06  Data: 0.007 (0.008)Time: 118.423s\n",
      "Train: 16 [ 750/10009 (  7%)]  Loss: 3.22 (3.01)  Time: 0.160s,  799.12/s  (0.168s,  760.05/s)  LR: 6.674e-06  Data: 0.006 (0.008)Time: 126.477s\n",
      "Train: 16 [ 800/10009 (  8%)]  Loss: 3.15 (3.01)  Time: 0.166s,  769.07/s  (0.168s,  761.51/s)  LR: 6.674e-06  Data: 0.007 (0.008)Time: 134.638s\n",
      "Train: 16 [ 850/10009 (  8%)]  Loss: 2.59 (3.01)  Time: 0.162s,  788.31/s  (0.168s,  761.51/s)  LR: 6.674e-06  Data: 0.006 (0.008)Time: 143.043s\n",
      "Train: 16 [ 900/10009 (  9%)]  Loss: 3.17 (3.01)  Time: 0.174s,  734.83/s  (0.168s,  761.59/s)  LR: 6.674e-06  Data: 0.008 (0.008)Time: 151.431s\n",
      "Train: 16 [ 950/10009 (  9%)]  Loss: 2.78 (3.01)  Time: 0.161s,  795.47/s  (0.168s,  760.96/s)  LR: 6.674e-06  Data: 0.007 (0.008)Time: 159.967s\n",
      "Train: 16 [1000/10009 ( 10%)]  Loss: 2.86 (3.01)  Time: 0.184s,  697.29/s  (0.168s,  760.51/s)  LR: 6.674e-06  Data: 0.009 (0.008)Time: 168.477s\n",
      "Train: 16 [1050/10009 ( 10%)]  Loss: 2.99 (3.01)  Time: 0.176s,  728.83/s  (0.169s,  758.96/s)  LR: 6.674e-06  Data: 0.006 (0.008)Time: 177.253s\n",
      "Train: 16 [1100/10009 ( 11%)]  Loss: 3.39 (3.01)  Time: 0.175s,  731.83/s  (0.169s,  757.46/s)  LR: 6.674e-06  Data: 0.006 (0.008)Time: 186.052s\n",
      "Train: 16 [1150/10009 ( 11%)]  Loss: 2.87 (3.01)  Time: 0.176s,  726.62/s  (0.169s,  756.29/s)  LR: 6.674e-06  Data: 0.007 (0.008)Time: 194.804s\n",
      "Train: 16 [1200/10009 ( 12%)]  Loss: 3.30 (3.01)  Time: 0.174s,  735.03/s  (0.170s,  755.13/s)  LR: 6.674e-06  Data: 0.005 (0.008)Time: 203.579s\n",
      "Train: 16 [1250/10009 ( 12%)]  Loss: 2.93 (3.01)  Time: 0.174s,  737.56/s  (0.170s,  754.31/s)  LR: 6.674e-06  Data: 0.007 (0.008)Time: 212.283s\n",
      "Train: 16 [1300/10009 ( 13%)]  Loss: 2.74 (3.01)  Time: 0.173s,  740.71/s  (0.170s,  753.77/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 220.925s\n",
      "Train: 16 [1350/10009 ( 13%)]  Loss: 2.93 (3.01)  Time: 0.176s,  725.51/s  (0.170s,  752.86/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 229.694s\n",
      "Train: 16 [1400/10009 ( 14%)]  Loss: 3.18 (3.01)  Time: 0.173s,  739.81/s  (0.170s,  752.27/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 238.382s\n",
      "Train: 16 [1450/10009 ( 14%)]  Loss: 2.82 (3.01)  Time: 0.175s,  731.45/s  (0.170s,  751.67/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 247.088s\n",
      "Train: 16 [1500/10009 ( 15%)]  Loss: 2.81 (3.02)  Time: 0.172s,  743.88/s  (0.170s,  752.66/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 255.264s\n",
      "Train: 16 [1550/10009 ( 15%)]  Loss: 2.93 (3.02)  Time: 0.160s,  800.53/s  (0.170s,  752.65/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 263.770s\n",
      "Train: 16 [1600/10009 ( 16%)]  Loss: 2.93 (3.02)  Time: 0.161s,  796.82/s  (0.170s,  753.68/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 271.905s\n",
      "Train: 16 [1650/10009 ( 16%)]  Loss: 3.03 (3.02)  Time: 0.164s,  780.96/s  (0.170s,  754.78/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 279.985s\n",
      "Train: 16 [1700/10009 ( 17%)]  Loss: 2.92 (3.02)  Time: 0.161s,  795.91/s  (0.169s,  755.87/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 288.047s\n",
      "Train: 16 [1750/10009 ( 17%)]  Loss: 3.07 (3.02)  Time: 0.165s,  776.68/s  (0.169s,  756.40/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 296.309s\n",
      "Train: 16 [1800/10009 ( 18%)]  Loss: 2.69 (3.02)  Time: 0.160s,  802.14/s  (0.169s,  756.90/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 304.570s\n",
      "Train: 16 [1850/10009 ( 18%)]  Loss: 2.83 (3.02)  Time: 0.173s,  741.63/s  (0.169s,  756.69/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 313.112s\n",
      "Train: 16 [1900/10009 ( 19%)]  Loss: 3.06 (3.02)  Time: 0.164s,  780.67/s  (0.169s,  756.09/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 321.822s\n",
      "Train: 16 [1950/10009 ( 19%)]  Loss: 2.99 (3.02)  Time: 0.160s,  799.04/s  (0.169s,  756.62/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 330.059s\n",
      "Train: 16 [2000/10009 ( 20%)]  Loss: 2.83 (3.02)  Time: 0.164s,  781.51/s  (0.169s,  757.39/s)  LR: 6.674e-06  Data: 0.010 (0.007)Time: 338.170s\n",
      "Train: 16 [2050/10009 ( 20%)]  Loss: 3.04 (3.02)  Time: 0.163s,  787.29/s  (0.169s,  757.75/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 346.458s\n",
      "Train: 16 [2100/10009 ( 21%)]  Loss: 3.11 (3.02)  Time: 0.161s,  797.30/s  (0.169s,  757.93/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 354.820s\n",
      "Train: 16 [2150/10009 ( 21%)]  Loss: 3.01 (3.02)  Time: 0.160s,  797.92/s  (0.169s,  758.21/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 363.127s\n",
      "Train: 16 [2200/10009 ( 22%)]  Loss: 2.87 (3.02)  Time: 0.161s,  793.79/s  (0.169s,  758.77/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 371.294s\n",
      "Train: 16 [2250/10009 ( 22%)]  Loss: 3.07 (3.02)  Time: 0.161s,  795.53/s  (0.169s,  759.40/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 379.412s\n",
      "Train: 16 [2300/10009 ( 23%)]  Loss: 2.93 (3.02)  Time: 0.178s,  720.45/s  (0.169s,  758.94/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 388.078s\n",
      "Train: 16 [2350/10009 ( 23%)]  Loss: 2.78 (3.02)  Time: 0.162s,  791.20/s  (0.169s,  758.88/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 396.539s\n",
      "Train: 16 [2400/10009 ( 24%)]  Loss: 2.82 (3.02)  Time: 0.162s,  790.28/s  (0.169s,  758.74/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 405.051s\n",
      "Train: 16 [2450/10009 ( 24%)]  Loss: 3.07 (3.02)  Time: 0.177s,  724.27/s  (0.169s,  758.31/s)  LR: 6.674e-06  Data: 0.009 (0.007)Time: 413.720s\n",
      "Train: 16 [2500/10009 ( 25%)]  Loss: 3.01 (3.02)  Time: 0.172s,  746.31/s  (0.169s,  757.78/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 422.456s\n",
      "Train: 16 [2550/10009 ( 25%)]  Loss: 2.83 (3.01)  Time: 0.172s,  743.35/s  (0.169s,  757.28/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 431.184s\n",
      "Train: 16 [2600/10009 ( 26%)]  Loss: 3.02 (3.01)  Time: 0.172s,  743.10/s  (0.169s,  756.88/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 439.870s\n",
      "Train: 16 [2650/10009 ( 26%)]  Loss: 3.23 (3.02)  Time: 0.173s,  741.02/s  (0.169s,  756.53/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 448.531s\n",
      "Train: 16 [2700/10009 ( 27%)]  Loss: 2.97 (3.02)  Time: 0.174s,  734.80/s  (0.169s,  756.21/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 457.185s\n",
      "Train: 16 [2750/10009 ( 27%)]  Loss: 3.02 (3.02)  Time: 0.174s,  737.11/s  (0.169s,  755.83/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 465.882s\n",
      "Train: 16 [2800/10009 ( 28%)]  Loss: 2.83 (3.01)  Time: 0.175s,  731.57/s  (0.169s,  755.42/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 474.605s\n",
      "Train: 16 [2850/10009 ( 28%)]  Loss: 3.03 (3.02)  Time: 0.172s,  746.12/s  (0.169s,  755.20/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 483.219s\n",
      "Train: 16 [2900/10009 ( 29%)]  Loss: 3.12 (3.01)  Time: 0.161s,  794.28/s  (0.169s,  755.48/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 491.514s\n",
      "Train: 16 [2950/10009 ( 29%)]  Loss: 3.01 (3.01)  Time: 0.162s,  788.40/s  (0.169s,  755.77/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 499.788s\n",
      "Train: 16 [3000/10009 ( 30%)]  Loss: 3.16 (3.01)  Time: 0.160s,  799.52/s  (0.169s,  756.00/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 508.104s\n",
      "Train: 16 [3050/10009 ( 30%)]  Loss: 2.96 (3.01)  Time: 0.171s,  750.56/s  (0.169s,  756.28/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 516.380s\n",
      "Train: 16 [3100/10009 ( 31%)]  Loss: 3.00 (3.01)  Time: 0.170s,  755.03/s  (0.169s,  756.36/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 524.783s\n",
      "Train: 16 [3150/10009 ( 31%)]  Loss: 3.01 (3.01)  Time: 0.169s,  757.25/s  (0.169s,  756.52/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 533.133s\n",
      "Train: 16 [3200/10009 ( 32%)]  Loss: 3.01 (3.01)  Time: 0.160s,  800.21/s  (0.169s,  756.74/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 541.433s\n",
      "Train: 16 [3250/10009 ( 32%)]  Loss: 3.05 (3.01)  Time: 0.161s,  795.64/s  (0.169s,  757.23/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 549.537s\n",
      "Train: 16 [3300/10009 ( 33%)]  Loss: 2.91 (3.01)  Time: 0.160s,  799.91/s  (0.169s,  757.69/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 557.650s\n",
      "Train: 16 [3350/10009 ( 33%)]  Loss: 2.78 (3.01)  Time: 0.176s,  725.66/s  (0.169s,  757.66/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 566.117s\n",
      "Train: 16 [3400/10009 ( 34%)]  Loss: 2.68 (3.01)  Time: 0.164s,  780.82/s  (0.169s,  757.70/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 574.534s\n",
      "Train: 16 [3450/10009 ( 34%)]  Loss: 3.20 (3.01)  Time: 0.180s,  711.14/s  (0.169s,  757.80/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 582.908s\n",
      "Train: 16 [3500/10009 ( 35%)]  Loss: 2.89 (3.01)  Time: 0.166s,  770.54/s  (0.169s,  757.85/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 591.316s\n",
      "Train: 16 [3550/10009 ( 35%)]  Loss: 3.13 (3.01)  Time: 0.161s,  796.80/s  (0.169s,  758.32/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 599.390s\n",
      "Train: 16 [3600/10009 ( 36%)]  Loss: 2.77 (3.01)  Time: 0.174s,  734.08/s  (0.169s,  758.52/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 607.666s\n",
      "Train: 16 [3650/10009 ( 36%)]  Loss: 3.36 (3.01)  Time: 0.163s,  785.18/s  (0.169s,  758.49/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 616.124s\n",
      "Train: 16 [3700/10009 ( 37%)]  Loss: 2.95 (3.01)  Time: 0.163s,  786.79/s  (0.169s,  758.58/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 624.490s\n",
      "Train: 16 [3750/10009 ( 37%)]  Loss: 3.09 (3.01)  Time: 0.162s,  790.15/s  (0.169s,  758.61/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 632.905s\n",
      "Train: 16 [3800/10009 ( 38%)]  Loss: 2.93 (3.01)  Time: 0.160s,  800.87/s  (0.169s,  759.03/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 640.988s\n",
      "Train: 16 [3850/10009 ( 38%)]  Loss: 3.23 (3.01)  Time: 0.163s,  785.78/s  (0.169s,  759.48/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 649.036s\n",
      "Train: 16 [3900/10009 ( 39%)]  Loss: 3.13 (3.01)  Time: 0.161s,  795.18/s  (0.168s,  759.91/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 657.088s\n",
      "Train: 16 [3950/10009 ( 39%)]  Loss: 3.11 (3.01)  Time: 0.162s,  789.73/s  (0.168s,  760.32/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 665.152s\n",
      "Train: 16 [4000/10009 ( 40%)]  Loss: 3.22 (3.01)  Time: 0.163s,  786.36/s  (0.168s,  760.73/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 673.204s\n",
      "Train: 16 [4050/10009 ( 40%)]  Loss: 3.01 (3.01)  Time: 0.160s,  800.26/s  (0.168s,  761.12/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 681.271s\n",
      "Train: 16 [4100/10009 ( 41%)]  Loss: 3.11 (3.01)  Time: 0.161s,  793.97/s  (0.168s,  761.53/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 689.309s\n",
      "Train: 16 [4150/10009 ( 41%)]  Loss: 3.19 (3.01)  Time: 0.161s,  796.35/s  (0.168s,  761.92/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 697.351s\n",
      "Train: 16 [4200/10009 ( 42%)]  Loss: 3.35 (3.01)  Time: 0.160s,  798.34/s  (0.168s,  762.30/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 705.397s\n",
      "Train: 16 [4250/10009 ( 42%)]  Loss: 2.73 (3.01)  Time: 0.159s,  803.69/s  (0.168s,  762.69/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 713.427s\n",
      "Train: 16 [4300/10009 ( 43%)]  Loss: 3.20 (3.01)  Time: 0.160s,  799.52/s  (0.168s,  763.07/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 721.467s\n",
      "Train: 16 [4350/10009 ( 43%)]  Loss: 3.03 (3.01)  Time: 0.160s,  799.03/s  (0.168s,  763.41/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 729.528s\n",
      "Train: 16 [4400/10009 ( 44%)]  Loss: 3.04 (3.01)  Time: 0.160s,  798.48/s  (0.168s,  763.74/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 737.585s\n",
      "Train: 16 [4450/10009 ( 44%)]  Loss: 3.14 (3.01)  Time: 0.160s,  798.47/s  (0.168s,  764.09/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 745.629s\n",
      "Train: 16 [4500/10009 ( 45%)]  Loss: 2.85 (3.01)  Time: 0.160s,  800.72/s  (0.167s,  764.42/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 753.677s\n",
      "Train: 16 [4550/10009 ( 45%)]  Loss: 3.07 (3.01)  Time: 0.162s,  791.58/s  (0.167s,  764.74/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 761.730s\n",
      "Train: 16 [4600/10009 ( 46%)]  Loss: 3.10 (3.02)  Time: 0.160s,  798.12/s  (0.167s,  765.06/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 769.781s\n",
      "Train: 16 [4650/10009 ( 46%)]  Loss: 2.78 (3.01)  Time: 0.160s,  800.52/s  (0.167s,  765.39/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 777.812s\n",
      "Train: 16 [4700/10009 ( 47%)]  Loss: 3.06 (3.01)  Time: 0.160s,  800.20/s  (0.167s,  765.69/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 785.861s\n",
      "Train: 16 [4750/10009 ( 47%)]  Loss: 3.18 (3.01)  Time: 0.161s,  795.69/s  (0.167s,  765.97/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 793.931s\n",
      "Train: 16 [4800/10009 ( 48%)]  Loss: 3.12 (3.01)  Time: 0.161s,  796.83/s  (0.167s,  766.26/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 801.979s\n",
      "Train: 16 [4850/10009 ( 48%)]  Loss: 3.30 (3.01)  Time: 0.160s,  801.31/s  (0.167s,  766.55/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 810.029s\n",
      "Train: 16 [4900/10009 ( 49%)]  Loss: 2.96 (3.01)  Time: 0.162s,  791.06/s  (0.167s,  766.84/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 818.067s\n",
      "Train: 16 [4950/10009 ( 49%)]  Loss: 2.96 (3.01)  Time: 0.160s,  799.34/s  (0.167s,  767.10/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 826.136s\n",
      "Train: 16 [5000/10009 ( 50%)]  Loss: 3.05 (3.01)  Time: 0.160s,  800.02/s  (0.167s,  767.36/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 834.188s\n",
      "Train: 16 [5050/10009 ( 50%)]  Loss: 2.96 (3.01)  Time: 0.160s,  798.45/s  (0.167s,  767.63/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 842.239s\n",
      "Train: 16 [5100/10009 ( 51%)]  Loss: 3.13 (3.01)  Time: 0.161s,  796.21/s  (0.167s,  767.88/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 850.299s\n",
      "Train: 16 [5150/10009 ( 51%)]  Loss: 2.86 (3.01)  Time: 0.161s,  796.23/s  (0.167s,  768.14/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 858.336s\n",
      "Train: 16 [5200/10009 ( 52%)]  Loss: 2.84 (3.01)  Time: 0.163s,  786.15/s  (0.167s,  768.39/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 866.394s\n",
      "Train: 16 [5250/10009 ( 52%)]  Loss: 2.95 (3.01)  Time: 0.161s,  794.65/s  (0.167s,  768.63/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 874.446s\n",
      "Train: 16 [5300/10009 ( 53%)]  Loss: 3.20 (3.01)  Time: 0.161s,  796.82/s  (0.166s,  768.87/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 882.501s\n",
      "Train: 16 [5350/10009 ( 53%)]  Loss: 3.09 (3.01)  Time: 0.160s,  799.24/s  (0.166s,  769.09/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 890.564s\n",
      "Train: 16 [5400/10009 ( 54%)]  Loss: 3.02 (3.01)  Time: 0.161s,  796.28/s  (0.166s,  769.33/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 898.604s\n",
      "Train: 16 [5450/10009 ( 54%)]  Loss: 3.01 (3.01)  Time: 0.161s,  796.60/s  (0.166s,  769.56/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 906.652s\n",
      "Train: 16 [5500/10009 ( 55%)]  Loss: 3.03 (3.01)  Time: 0.161s,  796.69/s  (0.166s,  769.77/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 914.722s\n",
      "Train: 16 [5550/10009 ( 55%)]  Loss: 3.03 (3.01)  Time: 0.160s,  799.77/s  (0.166s,  770.00/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 922.762s\n",
      "Train: 16 [5600/10009 ( 56%)]  Loss: 3.16 (3.01)  Time: 0.160s,  799.53/s  (0.166s,  770.22/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 930.807s\n",
      "Train: 16 [5650/10009 ( 56%)]  Loss: 3.09 (3.01)  Time: 0.160s,  802.42/s  (0.166s,  770.44/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 938.853s\n",
      "Train: 16 [5700/10009 ( 57%)]  Loss: 2.80 (3.01)  Time: 0.159s,  802.65/s  (0.166s,  770.65/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 946.892s\n",
      "Train: 16 [5750/10009 ( 57%)]  Loss: 3.24 (3.01)  Time: 0.160s,  797.65/s  (0.166s,  770.87/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 954.934s\n",
      "Train: 16 [5800/10009 ( 58%)]  Loss: 2.92 (3.01)  Time: 0.160s,  801.70/s  (0.166s,  769.95/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 964.379s\n",
      "Train: 16 [5850/10009 ( 58%)]  Loss: 3.02 (3.01)  Time: 0.161s,  797.41/s  (0.166s,  769.15/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 973.712s\n",
      "Train: 16 [5900/10009 ( 59%)]  Loss: 2.98 (3.01)  Time: 0.161s,  796.14/s  (0.166s,  769.18/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 981.989s\n",
      "Train: 16 [5950/10009 ( 59%)]  Loss: 3.04 (3.01)  Time: 0.174s,  737.16/s  (0.166s,  769.17/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 990.325s\n",
      "Train: 16 [6000/10009 ( 60%)]  Loss: 2.92 (3.01)  Time: 0.177s,  722.34/s  (0.166s,  769.09/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 998.750s\n",
      "Train: 16 [6050/10009 ( 60%)]  Loss: 3.24 (3.01)  Time: 0.173s,  739.82/s  (0.166s,  769.05/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1007.118s\n",
      "Train: 16 [6100/10009 ( 61%)]  Loss: 2.97 (3.01)  Time: 0.175s,  730.30/s  (0.166s,  768.94/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1015.586s\n",
      "Train: 16 [6150/10009 ( 61%)]  Loss: 2.93 (3.01)  Time: 0.163s,  787.05/s  (0.166s,  768.86/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 1024.019s\n",
      "Train: 16 [6200/10009 ( 62%)]  Loss: 2.88 (3.01)  Time: 0.172s,  745.42/s  (0.167s,  768.74/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1032.509s\n",
      "Train: 16 [6250/10009 ( 62%)]  Loss: 2.95 (3.01)  Time: 0.172s,  745.91/s  (0.167s,  768.44/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1041.230s\n",
      "Train: 16 [6300/10009 ( 63%)]  Loss: 3.11 (3.01)  Time: 0.162s,  789.25/s  (0.167s,  768.35/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 1049.688s\n",
      "Train: 16 [6350/10009 ( 63%)]  Loss: 3.27 (3.01)  Time: 0.160s,  801.58/s  (0.167s,  768.55/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1057.741s\n",
      "Train: 16 [6400/10009 ( 64%)]  Loss: 3.18 (3.01)  Time: 0.160s,  800.33/s  (0.167s,  768.75/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1065.785s\n",
      "Train: 16 [6450/10009 ( 64%)]  Loss: 2.98 (3.01)  Time: 0.161s,  796.06/s  (0.166s,  768.96/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1073.822s\n",
      "Train: 16 [6500/10009 ( 65%)]  Loss: 2.87 (3.01)  Time: 0.160s,  799.48/s  (0.166s,  769.16/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1081.862s\n",
      "Train: 16 [6550/10009 ( 65%)]  Loss: 3.17 (3.01)  Time: 0.160s,  799.06/s  (0.166s,  769.37/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1089.889s\n",
      "Train: 16 [6600/10009 ( 66%)]  Loss: 3.18 (3.01)  Time: 0.161s,  792.86/s  (0.166s,  769.57/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1097.920s\n",
      "Train: 16 [6650/10009 ( 66%)]  Loss: 3.03 (3.01)  Time: 0.160s,  798.45/s  (0.166s,  769.76/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1105.959s\n",
      "Train: 16 [6700/10009 ( 67%)]  Loss: 3.10 (3.01)  Time: 0.160s,  797.86/s  (0.166s,  769.94/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1114.012s\n",
      "Train: 16 [6750/10009 ( 67%)]  Loss: 2.87 (3.02)  Time: 0.160s,  800.50/s  (0.166s,  770.13/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1122.048s\n",
      "Train: 16 [6800/10009 ( 68%)]  Loss: 3.28 (3.02)  Time: 0.160s,  800.16/s  (0.166s,  770.32/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1130.085s\n",
      "Train: 16 [6850/10009 ( 68%)]  Loss: 3.02 (3.02)  Time: 0.160s,  798.14/s  (0.166s,  770.31/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1138.400s\n",
      "Train: 16 [6900/10009 ( 69%)]  Loss: 3.07 (3.02)  Time: 0.161s,  796.61/s  (0.166s,  770.24/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1146.819s\n",
      "Train: 16 [6950/10009 ( 69%)]  Loss: 3.20 (3.02)  Time: 0.161s,  796.94/s  (0.166s,  770.38/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1154.923s\n",
      "Train: 16 [7000/10009 ( 70%)]  Loss: 3.10 (3.02)  Time: 0.169s,  757.61/s  (0.166s,  770.52/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1163.016s\n",
      "Train: 16 [7050/10009 ( 70%)]  Loss: 3.28 (3.02)  Time: 0.166s,  770.73/s  (0.166s,  770.59/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 1171.219s\n",
      "Train: 16 [7100/10009 ( 71%)]  Loss: 3.00 (3.02)  Time: 0.161s,  795.70/s  (0.166s,  770.59/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1179.519s\n",
      "Train: 16 [7150/10009 ( 71%)]  Loss: 2.87 (3.02)  Time: 0.164s,  778.18/s  (0.166s,  770.67/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1187.708s\n",
      "Train: 16 [7200/10009 ( 72%)]  Loss: 2.98 (3.02)  Time: 0.160s,  802.00/s  (0.166s,  770.56/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1196.178s\n",
      "Train: 16 [7250/10009 ( 72%)]  Loss: 2.56 (3.02)  Time: 0.167s,  766.98/s  (0.166s,  770.60/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1204.423s\n",
      "Train: 16 [7300/10009 ( 73%)]  Loss: 2.86 (3.02)  Time: 0.168s,  764.02/s  (0.166s,  770.67/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1212.614s\n",
      "Train: 16 [7350/10009 ( 73%)]  Loss: 3.17 (3.02)  Time: 0.175s,  731.67/s  (0.166s,  770.75/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1220.791s\n",
      "Train: 16 [7400/10009 ( 74%)]  Loss: 2.96 (3.02)  Time: 0.166s,  769.57/s  (0.166s,  770.82/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1228.982s\n",
      "Train: 16 [7450/10009 ( 74%)]  Loss: 3.02 (3.02)  Time: 0.160s,  798.10/s  (0.166s,  770.90/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1237.161s\n",
      "Train: 16 [7500/10009 ( 75%)]  Loss: 3.15 (3.02)  Time: 0.160s,  800.30/s  (0.166s,  770.85/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1245.545s\n",
      "Train: 16 [7550/10009 ( 75%)]  Loss: 2.86 (3.02)  Time: 0.180s,  709.54/s  (0.166s,  770.78/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1253.951s\n",
      "Train: 16 [7600/10009 ( 76%)]  Loss: 3.20 (3.02)  Time: 0.160s,  798.06/s  (0.166s,  770.71/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1262.375s\n",
      "Train: 16 [7650/10009 ( 76%)]  Loss: 2.86 (3.02)  Time: 0.180s,  711.81/s  (0.166s,  770.77/s)  LR: 6.674e-06  Data: 0.009 (0.007)Time: 1270.588s\n",
      "Train: 16 [7700/10009 ( 77%)]  Loss: 3.24 (3.02)  Time: 0.169s,  756.07/s  (0.166s,  770.72/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1278.967s\n",
      "Train: 16 [7750/10009 ( 77%)]  Loss: 3.26 (3.02)  Time: 0.163s,  785.28/s  (0.166s,  770.72/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1287.275s\n",
      "Train: 16 [7800/10009 ( 78%)]  Loss: 3.04 (3.02)  Time: 0.161s,  795.02/s  (0.166s,  770.68/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1295.649s\n",
      "Train: 16 [7850/10009 ( 78%)]  Loss: 2.92 (3.02)  Time: 0.160s,  800.19/s  (0.166s,  770.73/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1303.857s\n",
      "Train: 16 [7900/10009 ( 79%)]  Loss: 3.15 (3.02)  Time: 0.160s,  799.37/s  (0.166s,  770.85/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1311.967s\n",
      "Train: 16 [7950/10009 ( 79%)]  Loss: 2.71 (3.02)  Time: 0.163s,  784.99/s  (0.166s,  770.89/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1320.198s\n",
      "Train: 16 [8000/10009 ( 80%)]  Loss: 3.00 (3.02)  Time: 0.160s,  799.88/s  (0.166s,  770.94/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1328.414s\n",
      "Train: 16 [8050/10009 ( 80%)]  Loss: 2.84 (3.02)  Time: 0.163s,  784.81/s  (0.166s,  771.07/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1336.484s\n",
      "Train: 16 [8100/10009 ( 81%)]  Loss: 2.81 (3.02)  Time: 0.160s,  801.60/s  (0.166s,  771.11/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1344.721s\n",
      "Train: 16 [8150/10009 ( 81%)]  Loss: 3.11 (3.02)  Time: 0.162s,  790.10/s  (0.166s,  771.19/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1352.873s\n",
      "Train: 16 [8200/10009 ( 82%)]  Loss: 3.01 (3.02)  Time: 0.162s,  792.39/s  (0.166s,  771.14/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1361.255s\n",
      "Train: 16 [8250/10009 ( 82%)]  Loss: 3.18 (3.02)  Time: 0.160s,  800.67/s  (0.166s,  771.26/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1369.347s\n",
      "Train: 16 [8300/10009 ( 83%)]  Loss: 3.10 (3.02)  Time: 0.161s,  797.31/s  (0.166s,  771.18/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1377.793s\n",
      "Train: 16 [8350/10009 ( 83%)]  Loss: 3.22 (3.02)  Time: 0.161s,  795.78/s  (0.166s,  771.27/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1385.937s\n",
      "Train: 16 [8400/10009 ( 84%)]  Loss: 3.29 (3.02)  Time: 0.160s,  800.01/s  (0.166s,  771.34/s)  LR: 6.674e-06  Data: 0.004 (0.007)Time: 1394.092s\n",
      "Train: 16 [8450/10009 ( 84%)]  Loss: 3.00 (3.02)  Time: 0.159s,  804.65/s  (0.166s,  771.48/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1402.148s\n",
      "Train: 16 [8500/10009 ( 85%)]  Loss: 2.82 (3.02)  Time: 0.160s,  800.17/s  (0.166s,  771.60/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1410.224s\n",
      "Train: 16 [8550/10009 ( 85%)]  Loss: 3.08 (3.02)  Time: 0.170s,  753.51/s  (0.166s,  771.66/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1418.410s\n",
      "Train: 16 [8600/10009 ( 86%)]  Loss: 3.01 (3.02)  Time: 0.169s,  757.47/s  (0.166s,  771.55/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1426.899s\n",
      "Train: 16 [8650/10009 ( 86%)]  Loss: 2.81 (3.02)  Time: 0.161s,  792.95/s  (0.166s,  771.54/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1435.210s\n",
      "Train: 16 [8700/10009 ( 87%)]  Loss: 2.94 (3.02)  Time: 0.164s,  779.06/s  (0.166s,  771.51/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1443.559s\n",
      "Train: 16 [8750/10009 ( 87%)]  Loss: 2.93 (3.02)  Time: 0.177s,  725.09/s  (0.166s,  771.44/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1451.997s\n",
      "Train: 16 [8800/10009 ( 88%)]  Loss: 3.03 (3.02)  Time: 0.175s,  732.81/s  (0.166s,  771.49/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1460.203s\n",
      "Train: 16 [8850/10009 ( 88%)]  Loss: 3.34 (3.02)  Time: 0.163s,  787.60/s  (0.166s,  771.52/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1468.424s\n",
      "Train: 16 [8900/10009 ( 89%)]  Loss: 2.94 (3.02)  Time: 0.179s,  714.83/s  (0.166s,  771.53/s)  LR: 6.674e-06  Data: 0.010 (0.007)Time: 1476.714s\n",
      "Train: 16 [8950/10009 ( 89%)]  Loss: 3.11 (3.02)  Time: 0.161s,  794.28/s  (0.166s,  771.43/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1485.193s\n",
      "Train: 16 [9000/10009 ( 90%)]  Loss: 2.81 (3.02)  Time: 0.160s,  797.61/s  (0.166s,  771.50/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1493.365s\n",
      "Train: 16 [9050/10009 ( 90%)]  Loss: 3.02 (3.02)  Time: 0.161s,  795.17/s  (0.166s,  771.56/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1501.527s\n",
      "Train: 16 [9100/10009 ( 91%)]  Loss: 3.24 (3.02)  Time: 0.162s,  790.44/s  (0.166s,  771.55/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1509.847s\n",
      "Train: 16 [9150/10009 ( 91%)]  Loss: 2.94 (3.02)  Time: 0.161s,  796.03/s  (0.166s,  771.56/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1518.121s\n",
      "Train: 16 [9200/10009 ( 92%)]  Loss: 2.82 (3.02)  Time: 0.173s,  741.90/s  (0.166s,  771.52/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1526.495s\n",
      "Train: 16 [9250/10009 ( 92%)]  Loss: 2.80 (3.02)  Time: 0.159s,  804.56/s  (0.166s,  771.48/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1534.876s\n",
      "Train: 16 [9300/10009 ( 93%)]  Loss: 2.90 (3.02)  Time: 0.162s,  790.48/s  (0.166s,  771.53/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1543.070s\n",
      "Train: 16 [9350/10009 ( 93%)]  Loss: 3.25 (3.02)  Time: 0.160s,  801.19/s  (0.166s,  771.66/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1551.112s\n",
      "Train: 16 [9400/10009 ( 94%)]  Loss: 3.09 (3.02)  Time: 0.164s,  778.49/s  (0.166s,  771.70/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 1559.314s\n",
      "Train: 16 [9450/10009 ( 94%)]  Loss: 3.08 (3.02)  Time: 0.161s,  797.37/s  (0.166s,  771.73/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1567.545s\n",
      "Train: 16 [9500/10009 ( 95%)]  Loss: 2.94 (3.02)  Time: 0.163s,  787.19/s  (0.166s,  771.69/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1575.931s\n",
      "Train: 16 [9550/10009 ( 95%)]  Loss: 2.80 (3.02)  Time: 0.160s,  798.83/s  (0.166s,  771.71/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1584.167s\n",
      "Train: 16 [9600/10009 ( 96%)]  Loss: 3.16 (3.02)  Time: 0.165s,  777.77/s  (0.166s,  771.74/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1592.406s\n",
      "Train: 16 [9650/10009 ( 96%)]  Loss: 3.01 (3.02)  Time: 0.160s,  800.01/s  (0.166s,  771.68/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1600.831s\n",
      "Train: 16 [9700/10009 ( 97%)]  Loss: 2.88 (3.02)  Time: 0.164s,  780.74/s  (0.166s,  771.69/s)  LR: 6.674e-06  Data: 0.008 (0.007)Time: 1609.093s\n",
      "Train: 16 [9750/10009 ( 97%)]  Loss: 2.96 (3.02)  Time: 0.160s,  802.04/s  (0.166s,  771.77/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1617.223s\n",
      "Train: 16 [9800/10009 ( 98%)]  Loss: 2.99 (3.02)  Time: 0.166s,  770.48/s  (0.166s,  771.84/s)  LR: 6.674e-06  Data: 0.007 (0.007)Time: 1625.360s\n",
      "Train: 16 [9850/10009 ( 98%)]  Loss: 2.97 (3.02)  Time: 0.160s,  799.68/s  (0.166s,  771.82/s)  LR: 6.674e-06  Data: 0.005 (0.007)Time: 1633.705s\n",
      "Train: 16 [9900/10009 ( 99%)]  Loss: 2.94 (3.02)  Time: 0.168s,  763.97/s  (0.166s,  771.82/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1641.991s\n",
      "Train: 16 [9950/10009 ( 99%)]  Loss: 2.85 (3.02)  Time: 0.172s,  744.23/s  (0.166s,  771.75/s)  LR: 6.674e-06  Data: 0.006 (0.007)Time: 1650.441s\n",
      "Train: 16 [10000/10009 (100%)]  Loss: 3.30 (3.02)  Time: 0.212s,  603.52/s  (0.166s,  771.65/s)  LR: 6.674e-06  Data: 0.048 (0.007)Time: 1658.953s\n",
      "Test: [   0/390]  Time: 0.674 (0.674)  Loss:   1.124 ( 1.124)  Acc@1:  79.688 ( 79.688)  Acc@5:  89.844 ( 89.844)\n",
      "Test: [  50/390]  Time: 0.058 (0.148)  Loss:   1.129 ( 1.786)  Acc@1:  75.781 ( 61.703)  Acc@5:  93.750 ( 81.373)\n",
      "Test: [ 100/390]  Time: 0.056 (0.141)  Loss:   1.625 ( 1.785)  Acc@1:  62.500 ( 59.592)  Acc@5:  90.625 ( 82.766)\n",
      "Test: [ 150/390]  Time: 0.052 (0.144)  Loss:   1.623 ( 1.756)  Acc@1:  56.250 ( 60.203)  Acc@5:  86.719 ( 83.278)\n",
      "Test: [ 200/390]  Time: 0.254 (0.140)  Loss:   2.968 ( 1.937)  Acc@1:  31.250 ( 56.988)  Acc@5:  66.406 ( 80.426)\n",
      "Test: [ 250/390]  Time: 0.051 (0.138)  Loss:   2.068 ( 2.045)  Acc@1:  60.938 ( 55.210)  Acc@5:  76.562 ( 78.502)\n",
      "Test: [ 300/390]  Time: 0.533 (0.138)  Loss:   2.206 ( 2.138)  Acc@1:  60.938 ( 53.462)  Acc@5:  75.000 ( 76.939)\n",
      "Test: [ 350/390]  Time: 0.056 (0.136)  Loss:   2.500 ( 2.211)  Acc@1:  46.875 ( 52.215)  Acc@5:  72.656 ( 75.799)\n",
      "Test: [ 390/390]  Time: 0.034 (0.135)  Loss:   3.445 ( 2.184)  Acc@1:  21.250 ( 52.608)  Acc@5:  58.750 ( 76.268)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 52.608)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 51.51)\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 17 [   0/10009 (  0%)]  Loss: 3.27 (3.27)  Time: 0.725s,  176.48/s  (0.725s,  176.48/s)  LR: 4.344e-06  Data: 0.559 (0.559)Time: 0.726s\n",
      "Train: 17 [  50/10009 (  0%)]  Loss: 2.89 (3.00)  Time: 0.159s,  804.75/s  (0.178s,  718.17/s)  LR: 4.344e-06  Data: 0.006 (0.017)Time: 9.090s\n",
      "Train: 17 [ 100/10009 (  1%)]  Loss: 2.91 (2.99)  Time: 0.160s,  802.12/s  (0.171s,  747.82/s)  LR: 4.344e-06  Data: 0.006 (0.012)Time: 17.288s\n",
      "Train: 17 [ 150/10009 (  1%)]  Loss: 2.96 (2.99)  Time: 0.175s,  731.95/s  (0.171s,  750.10/s)  LR: 4.344e-06  Data: 0.006 (0.010)Time: 25.768s\n",
      "Train: 17 [ 200/10009 (  2%)]  Loss: 2.76 (2.99)  Time: 0.174s,  736.18/s  (0.170s,  753.25/s)  LR: 4.344e-06  Data: 0.006 (0.009)Time: 34.156s\n",
      "Train: 17 [ 250/10009 (  2%)]  Loss: 2.78 (2.99)  Time: 0.173s,  739.34/s  (0.170s,  754.50/s)  LR: 4.344e-06  Data: 0.006 (0.008)Time: 42.582s\n",
      "Train: 17 [ 300/10009 (  3%)]  Loss: 2.97 (2.99)  Time: 0.159s,  803.07/s  (0.168s,  759.69/s)  LR: 4.344e-06  Data: 0.006 (0.008)Time: 50.716s\n",
      "Train: 17 [ 350/10009 (  3%)]  Loss: 3.02 (2.98)  Time: 0.160s,  800.76/s  (0.167s,  764.32/s)  LR: 4.344e-06  Data: 0.005 (0.008)Time: 58.782s\n",
      "Train: 17 [ 400/10009 (  4%)]  Loss: 2.85 (2.97)  Time: 0.159s,  802.98/s  (0.167s,  768.32/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 66.806s\n",
      "Train: 17 [ 450/10009 (  4%)]  Loss: 2.85 (2.97)  Time: 0.160s,  799.10/s  (0.166s,  770.81/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 74.892s\n",
      "Train: 17 [ 500/10009 (  5%)]  Loss: 3.00 (2.97)  Time: 0.160s,  799.32/s  (0.166s,  773.28/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 82.930s\n",
      "Train: 17 [ 550/10009 (  5%)]  Loss: 2.72 (2.97)  Time: 0.161s,  794.44/s  (0.165s,  774.96/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 91.009s\n",
      "Train: 17 [ 600/10009 (  6%)]  Loss: 2.86 (2.97)  Time: 0.160s,  799.69/s  (0.165s,  776.38/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 99.085s\n",
      "Train: 17 [ 650/10009 (  6%)]  Loss: 2.71 (2.97)  Time: 0.160s,  801.01/s  (0.165s,  777.70/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 107.147s\n",
      "Train: 17 [ 700/10009 (  7%)]  Loss: 2.81 (2.96)  Time: 0.160s,  797.85/s  (0.164s,  778.67/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 115.233s\n",
      "Train: 17 [ 750/10009 (  7%)]  Loss: 2.97 (2.96)  Time: 0.169s,  759.03/s  (0.164s,  779.74/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 123.282s\n",
      "Train: 17 [ 800/10009 (  8%)]  Loss: 3.01 (2.97)  Time: 0.160s,  798.79/s  (0.164s,  780.62/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 131.342s\n",
      "Train: 17 [ 850/10009 (  8%)]  Loss: 2.92 (2.97)  Time: 0.160s,  800.58/s  (0.164s,  781.25/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 139.428s\n",
      "Train: 17 [ 900/10009 (  9%)]  Loss: 2.81 (2.97)  Time: 0.162s,  789.35/s  (0.164s,  781.99/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 147.481s\n",
      "Train: 17 [ 950/10009 (  9%)]  Loss: 3.08 (2.97)  Time: 0.160s,  798.63/s  (0.164s,  782.52/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 155.559s\n",
      "Train: 17 [1000/10009 ( 10%)]  Loss: 3.09 (2.97)  Time: 0.171s,  750.30/s  (0.163s,  783.01/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 163.636s\n",
      "Train: 17 [1050/10009 ( 10%)]  Loss: 2.76 (2.97)  Time: 0.160s,  800.13/s  (0.163s,  783.61/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 171.677s\n",
      "Train: 17 [1100/10009 ( 11%)]  Loss: 2.86 (2.97)  Time: 0.161s,  793.87/s  (0.163s,  783.93/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 179.770s\n",
      "Train: 17 [1150/10009 ( 11%)]  Loss: 2.85 (2.97)  Time: 0.175s,  733.10/s  (0.164s,  782.85/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 188.193s\n",
      "Train: 17 [1200/10009 ( 12%)]  Loss: 2.84 (2.97)  Time: 0.169s,  756.43/s  (0.164s,  780.86/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 196.871s\n",
      "Train: 17 [1250/10009 ( 12%)]  Loss: 2.80 (2.96)  Time: 0.164s,  782.43/s  (0.164s,  780.90/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 205.056s\n",
      "Train: 17 [1300/10009 ( 13%)]  Loss: 3.13 (2.96)  Time: 0.159s,  804.27/s  (0.164s,  781.40/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 213.116s\n",
      "Train: 17 [1350/10009 ( 13%)]  Loss: 3.13 (2.96)  Time: 0.160s,  798.89/s  (0.164s,  781.93/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 221.155s\n",
      "Train: 17 [1400/10009 ( 14%)]  Loss: 2.91 (2.96)  Time: 0.161s,  795.10/s  (0.164s,  782.31/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 229.230s\n",
      "Train: 17 [1450/10009 ( 14%)]  Loss: 3.04 (2.96)  Time: 0.162s,  790.61/s  (0.164s,  782.32/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 237.406s\n",
      "Train: 17 [1500/10009 ( 15%)]  Loss: 3.26 (2.96)  Time: 0.160s,  798.64/s  (0.164s,  782.61/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 245.496s\n",
      "Train: 17 [1550/10009 ( 15%)]  Loss: 2.81 (2.96)  Time: 0.160s,  798.94/s  (0.163s,  782.88/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 253.587s\n",
      "Train: 17 [1600/10009 ( 16%)]  Loss: 3.28 (2.96)  Time: 0.165s,  774.88/s  (0.164s,  782.86/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 261.767s\n",
      "Train: 17 [1650/10009 ( 16%)]  Loss: 2.82 (2.97)  Time: 0.160s,  797.55/s  (0.164s,  782.69/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 270.002s\n",
      "Train: 17 [1700/10009 ( 17%)]  Loss: 2.83 (2.97)  Time: 0.160s,  800.41/s  (0.164s,  782.26/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 278.332s\n",
      "Train: 17 [1750/10009 ( 17%)]  Loss: 3.14 (2.97)  Time: 0.171s,  747.17/s  (0.164s,  781.61/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 286.750s\n",
      "Train: 17 [1800/10009 ( 18%)]  Loss: 3.02 (2.97)  Time: 0.161s,  796.68/s  (0.164s,  781.71/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 294.901s\n",
      "Train: 17 [1850/10009 ( 18%)]  Loss: 2.79 (2.97)  Time: 0.163s,  786.87/s  (0.164s,  781.72/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 303.086s\n",
      "Train: 17 [1900/10009 ( 19%)]  Loss: 3.22 (2.97)  Time: 0.173s,  739.11/s  (0.164s,  781.84/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 311.223s\n",
      "Train: 17 [1950/10009 ( 19%)]  Loss: 3.02 (2.97)  Time: 0.161s,  797.06/s  (0.164s,  782.08/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 319.313s\n",
      "Train: 17 [2000/10009 ( 20%)]  Loss: 2.97 (2.97)  Time: 0.160s,  798.06/s  (0.164s,  782.20/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 327.447s\n",
      "Train: 17 [2050/10009 ( 20%)]  Loss: 2.79 (2.97)  Time: 0.167s,  765.76/s  (0.164s,  782.35/s)  LR: 4.344e-06  Data: 0.008 (0.006)Time: 335.564s\n",
      "Train: 17 [2100/10009 ( 21%)]  Loss: 3.02 (2.97)  Time: 0.160s,  799.60/s  (0.164s,  782.52/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 343.669s\n",
      "Train: 17 [2150/10009 ( 21%)]  Loss: 3.38 (2.97)  Time: 0.166s,  770.31/s  (0.164s,  782.62/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 351.803s\n",
      "Train: 17 [2200/10009 ( 22%)]  Loss: 2.91 (2.97)  Time: 0.161s,  794.25/s  (0.164s,  782.71/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 359.940s\n",
      "Train: 17 [2250/10009 ( 22%)]  Loss: 2.80 (2.97)  Time: 0.163s,  784.93/s  (0.163s,  782.92/s)  LR: 4.344e-06  Data: 0.008 (0.006)Time: 368.017s\n",
      "Train: 17 [2300/10009 ( 23%)]  Loss: 2.90 (2.97)  Time: 0.169s,  758.26/s  (0.163s,  783.09/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 376.111s\n",
      "Train: 17 [2350/10009 ( 23%)]  Loss: 3.06 (2.97)  Time: 0.167s,  766.31/s  (0.163s,  782.99/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 384.330s\n",
      "Train: 17 [2400/10009 ( 24%)]  Loss: 2.81 (2.97)  Time: 0.160s,  797.83/s  (0.163s,  783.07/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 392.466s\n",
      "Train: 17 [2450/10009 ( 24%)]  Loss: 2.95 (2.97)  Time: 0.164s,  781.67/s  (0.163s,  783.02/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 400.661s\n",
      "Train: 17 [2500/10009 ( 25%)]  Loss: 3.12 (2.97)  Time: 0.160s,  798.57/s  (0.163s,  783.21/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 408.737s\n",
      "Train: 17 [2550/10009 ( 25%)]  Loss: 3.10 (2.97)  Time: 0.177s,  723.72/s  (0.163s,  783.02/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 417.012s\n",
      "Train: 17 [2600/10009 ( 26%)]  Loss: 2.99 (2.97)  Time: 0.179s,  716.22/s  (0.164s,  781.89/s)  LR: 4.344e-06  Data: 0.010 (0.006)Time: 425.797s\n",
      "Train: 17 [2650/10009 ( 26%)]  Loss: 2.78 (2.97)  Time: 0.177s,  721.95/s  (0.164s,  780.75/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 434.618s\n",
      "Train: 17 [2700/10009 ( 27%)]  Loss: 2.65 (2.97)  Time: 0.169s,  758.93/s  (0.164s,  779.84/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 443.330s\n",
      "Train: 17 [2750/10009 ( 27%)]  Loss: 3.01 (2.96)  Time: 0.161s,  796.00/s  (0.164s,  779.59/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 451.682s\n",
      "Train: 17 [2800/10009 ( 28%)]  Loss: 3.21 (2.97)  Time: 0.160s,  797.82/s  (0.164s,  779.60/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 459.885s\n",
      "Train: 17 [2850/10009 ( 28%)]  Loss: 2.78 (2.97)  Time: 0.173s,  740.61/s  (0.164s,  779.59/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 468.102s\n",
      "Train: 17 [2900/10009 ( 29%)]  Loss: 2.79 (2.97)  Time: 0.166s,  772.65/s  (0.164s,  779.70/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 476.241s\n",
      "Train: 17 [2950/10009 ( 29%)]  Loss: 2.75 (2.97)  Time: 0.160s,  800.33/s  (0.164s,  779.49/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 484.581s\n",
      "Train: 17 [3000/10009 ( 30%)]  Loss: 3.28 (2.97)  Time: 0.174s,  735.85/s  (0.164s,  779.31/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 492.905s\n",
      "Train: 17 [3050/10009 ( 30%)]  Loss: 2.98 (2.97)  Time: 0.160s,  802.38/s  (0.164s,  779.33/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 501.107s\n",
      "Train: 17 [3100/10009 ( 31%)]  Loss: 2.79 (2.97)  Time: 0.162s,  791.05/s  (0.164s,  779.28/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 509.348s\n",
      "Train: 17 [3150/10009 ( 31%)]  Loss: 2.74 (2.97)  Time: 0.160s,  800.35/s  (0.164s,  779.35/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 517.518s\n",
      "Train: 17 [3200/10009 ( 32%)]  Loss: 2.98 (2.97)  Time: 0.160s,  798.04/s  (0.164s,  779.47/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 525.646s\n",
      "Train: 17 [3250/10009 ( 32%)]  Loss: 2.85 (2.97)  Time: 0.163s,  786.14/s  (0.164s,  779.69/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 533.707s\n",
      "Train: 17 [3300/10009 ( 33%)]  Loss: 2.81 (2.97)  Time: 0.162s,  791.39/s  (0.164s,  779.73/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 541.888s\n",
      "Train: 17 [3350/10009 ( 33%)]  Loss: 3.02 (2.97)  Time: 0.162s,  792.07/s  (0.164s,  779.74/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 550.089s\n",
      "Train: 17 [3400/10009 ( 34%)]  Loss: 2.95 (2.97)  Time: 0.161s,  797.18/s  (0.164s,  779.83/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 558.233s\n",
      "Train: 17 [3450/10009 ( 34%)]  Loss: 2.89 (2.97)  Time: 0.175s,  731.58/s  (0.164s,  779.82/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 566.448s\n",
      "Train: 17 [3500/10009 ( 35%)]  Loss: 3.06 (2.97)  Time: 0.177s,  724.25/s  (0.164s,  779.86/s)  LR: 4.344e-06  Data: 0.008 (0.006)Time: 574.628s\n",
      "Train: 17 [3550/10009 ( 35%)]  Loss: 3.10 (2.97)  Time: 0.168s,  761.81/s  (0.164s,  779.88/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 582.820s\n",
      "Train: 17 [3600/10009 ( 36%)]  Loss: 2.76 (2.97)  Time: 0.174s,  735.28/s  (0.164s,  779.80/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 591.086s\n",
      "Train: 17 [3650/10009 ( 36%)]  Loss: 3.08 (2.97)  Time: 0.160s,  797.72/s  (0.164s,  779.77/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 599.317s\n",
      "Train: 17 [3700/10009 ( 37%)]  Loss: 2.92 (2.97)  Time: 0.163s,  787.57/s  (0.164s,  779.79/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 607.506s\n",
      "Train: 17 [3750/10009 ( 37%)]  Loss: 2.89 (2.97)  Time: 0.169s,  756.77/s  (0.164s,  779.74/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 615.750s\n",
      "Train: 17 [3800/10009 ( 38%)]  Loss: 3.06 (2.97)  Time: 0.163s,  787.32/s  (0.164s,  779.71/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 623.981s\n",
      "Train: 17 [3850/10009 ( 38%)]  Loss: 2.79 (2.97)  Time: 0.161s,  794.65/s  (0.164s,  779.62/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 632.263s\n",
      "Train: 17 [3900/10009 ( 39%)]  Loss: 2.94 (2.97)  Time: 0.161s,  795.16/s  (0.164s,  779.74/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 640.380s\n",
      "Train: 17 [3950/10009 ( 39%)]  Loss: 3.07 (2.97)  Time: 0.161s,  797.41/s  (0.164s,  779.97/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 648.394s\n",
      "Train: 17 [4000/10009 ( 40%)]  Loss: 2.90 (2.97)  Time: 0.161s,  795.72/s  (0.164s,  780.19/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 656.411s\n",
      "Train: 17 [4050/10009 ( 40%)]  Loss: 3.03 (2.97)  Time: 0.160s,  799.29/s  (0.164s,  780.42/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 664.424s\n",
      "Train: 17 [4100/10009 ( 41%)]  Loss: 2.94 (2.97)  Time: 0.161s,  793.57/s  (0.164s,  780.61/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 672.453s\n",
      "Train: 17 [4150/10009 ( 41%)]  Loss: 2.77 (2.97)  Time: 0.160s,  798.93/s  (0.164s,  780.83/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 680.463s\n",
      "Train: 17 [4200/10009 ( 42%)]  Loss: 2.75 (2.97)  Time: 0.160s,  800.48/s  (0.164s,  781.03/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 688.483s\n",
      "Train: 17 [4250/10009 ( 42%)]  Loss: 2.83 (2.97)  Time: 0.160s,  798.47/s  (0.164s,  781.23/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 696.500s\n",
      "Train: 17 [4300/10009 ( 43%)]  Loss: 2.69 (2.97)  Time: 0.160s,  799.19/s  (0.164s,  781.41/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 704.534s\n",
      "Train: 17 [4350/10009 ( 43%)]  Loss: 2.69 (2.97)  Time: 0.160s,  798.05/s  (0.164s,  781.42/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 712.712s\n",
      "Train: 17 [4400/10009 ( 44%)]  Loss: 3.11 (2.97)  Time: 0.159s,  804.38/s  (0.164s,  781.58/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 720.751s\n",
      "Train: 17 [4450/10009 ( 44%)]  Loss: 2.82 (2.97)  Time: 0.161s,  796.07/s  (0.164s,  781.73/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 728.799s\n",
      "Train: 17 [4500/10009 ( 45%)]  Loss: 2.80 (2.97)  Time: 0.163s,  784.20/s  (0.164s,  781.85/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 736.873s\n",
      "Train: 17 [4550/10009 ( 45%)]  Loss: 3.24 (2.97)  Time: 0.165s,  777.60/s  (0.164s,  781.90/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 745.019s\n",
      "Train: 17 [4600/10009 ( 46%)]  Loss: 3.06 (2.97)  Time: 0.160s,  799.52/s  (0.164s,  781.77/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 753.329s\n",
      "Train: 17 [4650/10009 ( 46%)]  Loss: 2.93 (2.97)  Time: 0.162s,  789.58/s  (0.164s,  781.70/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 761.578s\n",
      "Train: 17 [4700/10009 ( 47%)]  Loss: 2.96 (2.97)  Time: 0.166s,  771.45/s  (0.164s,  781.45/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 770.011s\n",
      "Train: 17 [4750/10009 ( 47%)]  Loss: 2.92 (2.97)  Time: 0.170s,  752.61/s  (0.164s,  781.11/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 778.546s\n",
      "Train: 17 [4800/10009 ( 48%)]  Loss: 3.05 (2.97)  Time: 0.160s,  801.36/s  (0.164s,  781.12/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 786.723s\n",
      "Train: 17 [4850/10009 ( 48%)]  Loss: 3.05 (2.97)  Time: 0.171s,  747.44/s  (0.164s,  781.09/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 794.950s\n",
      "Train: 17 [4900/10009 ( 49%)]  Loss: 3.07 (2.97)  Time: 0.164s,  780.04/s  (0.164s,  780.77/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 803.476s\n",
      "Train: 17 [4950/10009 ( 49%)]  Loss: 3.06 (2.97)  Time: 0.161s,  795.97/s  (0.164s,  780.69/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 811.747s\n",
      "Train: 17 [5000/10009 ( 50%)]  Loss: 2.78 (2.97)  Time: 0.161s,  793.47/s  (0.164s,  780.62/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 820.026s\n",
      "Train: 17 [5050/10009 ( 50%)]  Loss: 3.01 (2.97)  Time: 0.171s,  749.31/s  (0.164s,  780.57/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 828.273s\n",
      "Train: 17 [5100/10009 ( 51%)]  Loss: 2.93 (2.97)  Time: 0.161s,  796.73/s  (0.164s,  780.53/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 836.515s\n",
      "Train: 17 [5150/10009 ( 51%)]  Loss: 3.07 (2.97)  Time: 0.160s,  799.81/s  (0.164s,  780.56/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 844.681s\n",
      "Train: 17 [5200/10009 ( 52%)]  Loss: 3.32 (2.97)  Time: 0.160s,  801.94/s  (0.164s,  780.62/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 852.814s\n",
      "Train: 17 [5250/10009 ( 52%)]  Loss: 2.76 (2.97)  Time: 0.163s,  787.59/s  (0.164s,  780.70/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 860.923s\n",
      "Train: 17 [5300/10009 ( 53%)]  Loss: 2.80 (2.97)  Time: 0.161s,  794.93/s  (0.164s,  780.80/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 869.015s\n",
      "Train: 17 [5350/10009 ( 53%)]  Loss: 3.35 (2.97)  Time: 0.168s,  763.04/s  (0.164s,  780.69/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 877.339s\n",
      "Train: 17 [5400/10009 ( 54%)]  Loss: 2.90 (2.97)  Time: 0.173s,  740.79/s  (0.164s,  780.64/s)  LR: 4.344e-06  Data: 0.005 (0.006)Time: 885.591s\n",
      "Train: 17 [5450/10009 ( 54%)]  Loss: 2.75 (2.97)  Time: 0.164s,  782.83/s  (0.164s,  780.77/s)  LR: 4.344e-06  Data: 0.008 (0.006)Time: 893.642s\n",
      "Train: 17 [5500/10009 ( 55%)]  Loss: 2.78 (2.97)  Time: 0.176s,  729.34/s  (0.164s,  780.72/s)  LR: 4.344e-06  Data: 0.007 (0.006)Time: 901.897s\n",
      "Train: 17 [5550/10009 ( 55%)]  Loss: 3.14 (2.97)  Time: 0.165s,  776.98/s  (0.164s,  780.63/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 910.192s\n",
      "Train: 17 [5600/10009 ( 56%)]  Loss: 2.85 (2.97)  Time: 0.160s,  797.86/s  (0.164s,  780.58/s)  LR: 4.344e-06  Data: 0.006 (0.006)Time: 918.447s\n",
      "Train: 17 [5650/10009 ( 56%)]  Loss: 2.93 (2.97)  Time: 0.187s,  683.93/s  (0.165s,  778.06/s)  LR: 4.344e-06  Data: 0.015 (0.007)Time: 929.651s\n",
      "Train: 17 [5700/10009 ( 57%)]  Loss: 2.75 (2.97)  Time: 0.182s,  703.36/s  (0.165s,  776.39/s)  LR: 4.344e-06  Data: 0.008 (0.007)Time: 939.893s\n",
      "Train: 17 [5750/10009 ( 57%)]  Loss: 2.67 (2.97)  Time: 0.162s,  791.77/s  (0.165s,  775.81/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 948.848s\n",
      "Train: 17 [5800/10009 ( 58%)]  Loss: 2.92 (2.97)  Time: 0.181s,  708.30/s  (0.165s,  775.36/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 957.654s\n",
      "Train: 17 [5850/10009 ( 58%)]  Loss: 2.98 (2.97)  Time: 0.171s,  747.85/s  (0.165s,  774.87/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 966.523s\n",
      "Train: 17 [5900/10009 ( 59%)]  Loss: 2.96 (2.97)  Time: 0.162s,  789.92/s  (0.165s,  774.75/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 974.924s\n",
      "Train: 17 [5950/10009 ( 59%)]  Loss: 3.18 (2.97)  Time: 0.161s,  797.29/s  (0.165s,  774.82/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 983.096s\n",
      "Train: 17 [6000/10009 ( 60%)]  Loss: 3.31 (2.97)  Time: 0.159s,  803.44/s  (0.165s,  774.89/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 991.267s\n",
      "Train: 17 [6050/10009 ( 60%)]  Loss: 2.82 (2.97)  Time: 0.160s,  797.90/s  (0.165s,  774.94/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 999.467s\n",
      "Train: 17 [6100/10009 ( 61%)]  Loss: 2.97 (2.97)  Time: 0.166s,  772.76/s  (0.165s,  775.07/s)  LR: 4.344e-06  Data: 0.009 (0.007)Time: 1007.555s\n",
      "Train: 17 [6150/10009 ( 61%)]  Loss: 2.53 (2.97)  Time: 0.160s,  799.97/s  (0.165s,  775.24/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1015.595s\n",
      "Train: 17 [6200/10009 ( 62%)]  Loss: 2.87 (2.97)  Time: 0.160s,  800.93/s  (0.165s,  775.33/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1023.727s\n",
      "Train: 17 [6250/10009 ( 62%)]  Loss: 2.96 (2.97)  Time: 0.160s,  797.93/s  (0.165s,  775.49/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1031.764s\n",
      "Train: 17 [6300/10009 ( 63%)]  Loss: 3.17 (2.97)  Time: 0.171s,  749.03/s  (0.165s,  775.57/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1039.908s\n",
      "Train: 17 [6350/10009 ( 63%)]  Loss: 2.99 (2.97)  Time: 0.162s,  788.06/s  (0.165s,  775.62/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1048.091s\n",
      "Train: 17 [6400/10009 ( 64%)]  Loss: 2.86 (2.97)  Time: 0.165s,  773.95/s  (0.165s,  775.73/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1056.199s\n",
      "Train: 17 [6450/10009 ( 64%)]  Loss: 3.18 (2.97)  Time: 0.162s,  789.98/s  (0.165s,  775.83/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1064.316s\n",
      "Train: 17 [6500/10009 ( 65%)]  Loss: 3.00 (2.97)  Time: 0.161s,  796.39/s  (0.165s,  775.96/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1072.384s\n",
      "Train: 17 [6550/10009 ( 65%)]  Loss: 2.81 (2.97)  Time: 0.162s,  789.41/s  (0.165s,  775.87/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1080.749s\n",
      "Train: 17 [6600/10009 ( 66%)]  Loss: 2.72 (2.97)  Time: 0.162s,  787.70/s  (0.165s,  775.78/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1089.124s\n",
      "Train: 17 [6650/10009 ( 66%)]  Loss: 3.23 (2.97)  Time: 0.175s,  731.99/s  (0.165s,  775.75/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1097.417s\n",
      "Train: 17 [6700/10009 ( 67%)]  Loss: 2.86 (2.97)  Time: 0.160s,  798.36/s  (0.165s,  775.80/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1105.607s\n",
      "Train: 17 [6750/10009 ( 67%)]  Loss: 2.74 (2.97)  Time: 0.175s,  730.54/s  (0.165s,  775.80/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1113.847s\n",
      "Train: 17 [6800/10009 ( 68%)]  Loss: 2.57 (2.97)  Time: 0.161s,  797.19/s  (0.165s,  775.78/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1122.130s\n",
      "Train: 17 [6850/10009 ( 68%)]  Loss: 3.07 (2.97)  Time: 0.173s,  741.74/s  (0.165s,  775.78/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1130.380s\n",
      "Train: 17 [6900/10009 ( 69%)]  Loss: 2.36 (2.97)  Time: 0.177s,  724.87/s  (0.165s,  775.80/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1138.594s\n",
      "Train: 17 [6950/10009 ( 69%)]  Loss: 3.05 (2.97)  Time: 0.160s,  799.39/s  (0.165s,  775.85/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1146.780s\n",
      "Train: 17 [7000/10009 ( 70%)]  Loss: 2.97 (2.97)  Time: 0.168s,  761.64/s  (0.165s,  775.76/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1155.152s\n",
      "Train: 17 [7050/10009 ( 70%)]  Loss: 2.90 (2.97)  Time: 0.174s,  736.54/s  (0.165s,  774.94/s)  LR: 4.344e-06  Data: 0.018 (0.007)Time: 1164.644s\n",
      "Train: 17 [7100/10009 ( 71%)]  Loss: 2.97 (2.97)  Time: 0.160s,  797.95/s  (0.165s,  774.30/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1173.861s\n",
      "Train: 17 [7150/10009 ( 71%)]  Loss: 2.78 (2.97)  Time: 0.177s,  723.22/s  (0.165s,  774.28/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1182.166s\n",
      "Train: 17 [7200/10009 ( 72%)]  Loss: 3.15 (2.97)  Time: 0.176s,  727.41/s  (0.165s,  774.31/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1190.377s\n",
      "Train: 17 [7250/10009 ( 72%)]  Loss: 2.75 (2.97)  Time: 0.161s,  796.18/s  (0.165s,  774.37/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1198.562s\n",
      "Train: 17 [7300/10009 ( 73%)]  Loss: 2.63 (2.97)  Time: 0.163s,  786.10/s  (0.165s,  774.44/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1206.705s\n",
      "Train: 17 [7350/10009 ( 73%)]  Loss: 3.23 (2.97)  Time: 0.162s,  792.01/s  (0.165s,  774.48/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1214.911s\n",
      "Train: 17 [7400/10009 ( 74%)]  Loss: 3.02 (2.97)  Time: 0.160s,  801.60/s  (0.165s,  774.58/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1223.017s\n",
      "Train: 17 [7450/10009 ( 74%)]  Loss: 2.88 (2.97)  Time: 0.169s,  759.13/s  (0.165s,  774.65/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1231.169s\n",
      "Train: 17 [7500/10009 ( 75%)]  Loss: 2.94 (2.97)  Time: 0.161s,  795.80/s  (0.165s,  774.75/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1239.264s\n",
      "Train: 17 [7550/10009 ( 75%)]  Loss: 3.12 (2.97)  Time: 0.160s,  800.99/s  (0.165s,  774.89/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1247.301s\n",
      "Train: 17 [7600/10009 ( 76%)]  Loss: 3.03 (2.97)  Time: 0.160s,  801.54/s  (0.165s,  774.96/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1255.451s\n",
      "Train: 17 [7650/10009 ( 76%)]  Loss: 3.01 (2.97)  Time: 0.162s,  790.19/s  (0.165s,  775.07/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1263.533s\n",
      "Train: 17 [7700/10009 ( 77%)]  Loss: 2.64 (2.97)  Time: 0.171s,  747.85/s  (0.165s,  775.07/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1271.782s\n",
      "Train: 17 [7750/10009 ( 77%)]  Loss: 3.08 (2.97)  Time: 0.159s,  804.30/s  (0.165s,  775.14/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1279.923s\n",
      "Train: 17 [7800/10009 ( 78%)]  Loss: 3.03 (2.97)  Time: 0.160s,  797.78/s  (0.165s,  775.17/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1288.131s\n",
      "Train: 17 [7850/10009 ( 78%)]  Loss: 2.92 (2.97)  Time: 0.161s,  795.03/s  (0.165s,  775.23/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1296.296s\n",
      "Train: 17 [7900/10009 ( 79%)]  Loss: 2.95 (2.97)  Time: 0.161s,  795.48/s  (0.165s,  775.25/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1304.509s\n",
      "Train: 17 [7950/10009 ( 79%)]  Loss: 2.78 (2.97)  Time: 0.161s,  796.76/s  (0.165s,  775.24/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1312.786s\n",
      "Train: 17 [8000/10009 ( 80%)]  Loss: 2.82 (2.97)  Time: 0.163s,  783.87/s  (0.165s,  775.23/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1321.060s\n",
      "Train: 17 [8050/10009 ( 80%)]  Loss: 2.96 (2.97)  Time: 0.170s,  751.72/s  (0.165s,  775.23/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1329.317s\n",
      "Train: 17 [8100/10009 ( 81%)]  Loss: 3.17 (2.97)  Time: 0.160s,  800.40/s  (0.165s,  775.31/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1337.434s\n",
      "Train: 17 [8150/10009 ( 81%)]  Loss: 2.70 (2.97)  Time: 0.160s,  797.59/s  (0.165s,  775.29/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1345.728s\n",
      "Train: 17 [8200/10009 ( 82%)]  Loss: 3.20 (2.97)  Time: 0.159s,  804.44/s  (0.165s,  775.35/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1353.871s\n",
      "Train: 17 [8250/10009 ( 82%)]  Loss: 2.72 (2.97)  Time: 0.162s,  789.50/s  (0.165s,  775.42/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1362.009s\n",
      "Train: 17 [8300/10009 ( 83%)]  Loss: 3.03 (2.97)  Time: 0.160s,  799.06/s  (0.165s,  775.37/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1370.348s\n",
      "Train: 17 [8350/10009 ( 83%)]  Loss: 2.77 (2.97)  Time: 0.161s,  794.80/s  (0.165s,  775.47/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1378.429s\n",
      "Train: 17 [8400/10009 ( 84%)]  Loss: 2.97 (2.97)  Time: 0.162s,  791.55/s  (0.165s,  775.44/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1386.730s\n",
      "Train: 17 [8450/10009 ( 84%)]  Loss: 2.96 (2.97)  Time: 0.162s,  790.21/s  (0.165s,  775.39/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1395.065s\n",
      "Train: 17 [8500/10009 ( 85%)]  Loss: 2.97 (2.97)  Time: 0.164s,  781.05/s  (0.165s,  775.44/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1403.244s\n",
      "Train: 17 [8550/10009 ( 85%)]  Loss: 2.85 (2.97)  Time: 0.161s,  795.33/s  (0.165s,  775.37/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1411.614s\n",
      "Train: 17 [8600/10009 ( 86%)]  Loss: 2.93 (2.97)  Time: 0.167s,  765.42/s  (0.165s,  775.34/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1419.926s\n",
      "Train: 17 [8650/10009 ( 86%)]  Loss: 2.98 (2.97)  Time: 0.171s,  750.60/s  (0.165s,  775.37/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1428.116s\n",
      "Train: 17 [8700/10009 ( 87%)]  Loss: 2.93 (2.97)  Time: 0.160s,  802.27/s  (0.165s,  775.48/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1436.168s\n",
      "Train: 17 [8750/10009 ( 87%)]  Loss: 2.73 (2.97)  Time: 0.169s,  756.54/s  (0.165s,  775.54/s)  LR: 4.344e-06  Data: 0.008 (0.007)Time: 1444.313s\n",
      "Train: 17 [8800/10009 ( 88%)]  Loss: 3.08 (2.97)  Time: 0.162s,  789.74/s  (0.165s,  775.54/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1452.568s\n",
      "Train: 17 [8850/10009 ( 88%)]  Loss: 3.08 (2.97)  Time: 0.161s,  796.45/s  (0.165s,  775.57/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1460.762s\n",
      "Train: 17 [8900/10009 ( 89%)]  Loss: 3.26 (2.97)  Time: 0.160s,  797.58/s  (0.165s,  775.62/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1468.912s\n",
      "Train: 17 [8950/10009 ( 89%)]  Loss: 2.91 (2.97)  Time: 0.161s,  793.73/s  (0.165s,  775.69/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1477.038s\n",
      "Train: 17 [9000/10009 ( 90%)]  Loss: 2.78 (2.97)  Time: 0.160s,  801.02/s  (0.165s,  775.73/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1485.219s\n",
      "Train: 17 [9050/10009 ( 90%)]  Loss: 3.00 (2.97)  Time: 0.159s,  802.72/s  (0.165s,  775.83/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1493.269s\n",
      "Train: 17 [9100/10009 ( 91%)]  Loss: 2.84 (2.97)  Time: 0.167s,  764.42/s  (0.165s,  775.89/s)  LR: 4.344e-06  Data: 0.008 (0.007)Time: 1501.404s\n",
      "Train: 17 [9150/10009 ( 91%)]  Loss: 3.26 (2.97)  Time: 0.160s,  801.85/s  (0.165s,  775.95/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1509.541s\n",
      "Train: 17 [9200/10009 ( 92%)]  Loss: 2.94 (2.97)  Time: 0.161s,  796.24/s  (0.165s,  776.06/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1517.565s\n",
      "Train: 17 [9250/10009 ( 92%)]  Loss: 2.73 (2.97)  Time: 0.160s,  798.76/s  (0.165s,  776.11/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1525.716s\n",
      "Train: 17 [9300/10009 ( 93%)]  Loss: 3.01 (2.97)  Time: 0.161s,  796.84/s  (0.165s,  776.14/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1533.907s\n",
      "Train: 17 [9350/10009 ( 93%)]  Loss: 3.12 (2.97)  Time: 0.172s,  744.76/s  (0.165s,  776.16/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1542.113s\n",
      "Train: 17 [9400/10009 ( 94%)]  Loss: 3.04 (2.97)  Time: 0.160s,  800.58/s  (0.165s,  776.17/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1550.331s\n",
      "Train: 17 [9450/10009 ( 94%)]  Loss: 3.04 (2.97)  Time: 0.159s,  803.15/s  (0.165s,  776.29/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1558.340s\n",
      "Train: 17 [9500/10009 ( 95%)]  Loss: 2.84 (2.97)  Time: 0.161s,  795.91/s  (0.165s,  776.39/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1566.392s\n",
      "Train: 17 [9550/10009 ( 95%)]  Loss: 2.78 (2.97)  Time: 0.163s,  787.29/s  (0.165s,  776.42/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1574.562s\n",
      "Train: 17 [9600/10009 ( 96%)]  Loss: 2.92 (2.97)  Time: 0.161s,  796.97/s  (0.165s,  776.43/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1582.791s\n",
      "Train: 17 [9650/10009 ( 96%)]  Loss: 2.82 (2.97)  Time: 0.174s,  735.31/s  (0.165s,  776.44/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1591.013s\n",
      "Train: 17 [9700/10009 ( 97%)]  Loss: 3.14 (2.97)  Time: 0.160s,  798.10/s  (0.165s,  776.41/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1599.319s\n",
      "Train: 17 [9750/10009 ( 97%)]  Loss: 3.13 (2.97)  Time: 0.160s,  801.64/s  (0.165s,  776.42/s)  LR: 4.344e-06  Data: 0.005 (0.007)Time: 1607.542s\n",
      "Train: 17 [9800/10009 ( 98%)]  Loss: 2.75 (2.97)  Time: 0.161s,  797.06/s  (0.165s,  776.51/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1615.595s\n",
      "Train: 17 [9850/10009 ( 98%)]  Loss: 3.06 (2.97)  Time: 0.160s,  798.60/s  (0.165s,  776.57/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1623.706s\n",
      "Train: 17 [9900/10009 ( 99%)]  Loss: 2.88 (2.97)  Time: 0.165s,  774.04/s  (0.165s,  776.60/s)  LR: 4.344e-06  Data: 0.007 (0.007)Time: 1631.879s\n",
      "Train: 17 [9950/10009 ( 99%)]  Loss: 2.79 (2.97)  Time: 0.175s,  731.69/s  (0.165s,  776.63/s)  LR: 4.344e-06  Data: 0.006 (0.007)Time: 1640.064s\n",
      "Train: 17 [10000/10009 (100%)]  Loss: 2.96 (2.97)  Time: 0.221s,  578.12/s  (0.165s,  776.60/s)  LR: 4.344e-06  Data: 0.054 (0.007)Time: 1648.368s\n",
      "Test: [   0/390]  Time: 0.710 (0.710)  Loss:   1.183 ( 1.183)  Acc@1:  76.562 ( 76.562)  Acc@5:  88.281 ( 88.281)\n",
      "Test: [  50/390]  Time: 0.052 (0.144)  Loss:   1.061 ( 1.749)  Acc@1:  76.562 ( 61.872)  Acc@5:  92.969 ( 82.123)\n",
      "Test: [ 100/390]  Time: 0.053 (0.136)  Loss:   1.818 ( 1.762)  Acc@1:  58.594 ( 59.739)  Acc@5:  89.844 ( 83.199)\n",
      "Test: [ 150/390]  Time: 0.053 (0.142)  Loss:   1.600 ( 1.727)  Acc@1:  60.156 ( 60.808)  Acc@5:  86.719 ( 83.770)\n",
      "Test: [ 200/390]  Time: 0.055 (0.139)  Loss:   2.801 ( 1.898)  Acc@1:  33.594 ( 57.696)  Acc@5:  67.969 ( 81.009)\n",
      "Test: [ 250/390]  Time: 0.051 (0.137)  Loss:   2.086 ( 2.011)  Acc@1:  61.719 ( 55.901)  Acc@5:  75.781 ( 79.109)\n",
      "Test: [ 300/390]  Time: 0.306 (0.135)  Loss:   2.250 ( 2.105)  Acc@1:  58.594 ( 54.075)  Acc@5:  71.875 ( 77.463)\n",
      "Test: [ 350/390]  Time: 0.052 (0.134)  Loss:   2.488 ( 2.176)  Acc@1:  47.656 ( 52.820)  Acc@5:  72.656 ( 76.307)\n",
      "Test: [ 390/390]  Time: 0.034 (0.133)  Loss:   3.286 ( 2.152)  Acc@1:  28.750 ( 53.226)  Acc@5:  58.750 ( 76.732)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 53.226)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 52.608)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 51.51)\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 18 [   0/10009 (  0%)]  Loss: 2.74 (2.74)  Time: 0.721s,  177.45/s  (0.721s,  177.45/s)  LR: 2.476e-06  Data: 0.568 (0.568)Time: 0.722s\n",
      "Train: 18 [  50/10009 (  0%)]  Loss: 2.72 (2.92)  Time: 0.165s,  774.59/s  (0.174s,  735.49/s)  LR: 2.476e-06  Data: 0.007 (0.017)Time: 8.876s\n",
      "Train: 18 [ 100/10009 (  1%)]  Loss: 3.05 (2.94)  Time: 0.162s,  790.72/s  (0.167s,  764.22/s)  LR: 2.476e-06  Data: 0.008 (0.012)Time: 16.917s\n",
      "Train: 18 [ 150/10009 (  1%)]  Loss: 3.24 (2.94)  Time: 0.171s,  746.77/s  (0.166s,  772.37/s)  LR: 2.476e-06  Data: 0.006 (0.010)Time: 25.025s\n",
      "Train: 18 [ 200/10009 (  2%)]  Loss: 3.10 (2.94)  Time: 0.163s,  785.78/s  (0.164s,  778.80/s)  LR: 2.476e-06  Data: 0.009 (0.009)Time: 33.036s\n",
      "Train: 18 [ 250/10009 (  2%)]  Loss: 2.93 (2.94)  Time: 0.174s,  736.25/s  (0.164s,  780.47/s)  LR: 2.476e-06  Data: 0.007 (0.009)Time: 41.165s\n",
      "Train: 18 [ 300/10009 (  3%)]  Loss: 2.75 (2.94)  Time: 0.160s,  802.02/s  (0.164s,  782.37/s)  LR: 2.476e-06  Data: 0.005 (0.008)Time: 49.246s\n",
      "Train: 18 [ 350/10009 (  3%)]  Loss: 3.02 (2.94)  Time: 0.160s,  800.43/s  (0.163s,  782.93/s)  LR: 2.476e-06  Data: 0.006 (0.008)Time: 57.385s\n",
      "Train: 18 [ 400/10009 (  4%)]  Loss: 3.07 (2.94)  Time: 0.173s,  741.79/s  (0.163s,  783.71/s)  LR: 2.476e-06  Data: 0.007 (0.008)Time: 65.494s\n",
      "Train: 18 [ 450/10009 (  4%)]  Loss: 2.66 (2.93)  Time: 0.170s,  751.96/s  (0.164s,  782.10/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 73.812s\n",
      "Train: 18 [ 500/10009 (  5%)]  Loss: 2.80 (2.93)  Time: 0.168s,  761.83/s  (0.164s,  780.27/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 82.187s\n",
      "Train: 18 [ 550/10009 (  5%)]  Loss: 3.21 (2.93)  Time: 0.160s,  799.01/s  (0.164s,  780.11/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 90.408s\n",
      "Train: 18 [ 600/10009 (  6%)]  Loss: 2.72 (2.93)  Time: 0.174s,  736.47/s  (0.164s,  780.96/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 98.505s\n",
      "Train: 18 [ 650/10009 (  6%)]  Loss: 2.72 (2.93)  Time: 0.163s,  785.65/s  (0.164s,  780.37/s)  LR: 2.476e-06  Data: 0.009 (0.007)Time: 106.780s\n",
      "Train: 18 [ 700/10009 (  7%)]  Loss: 3.09 (2.93)  Time: 0.163s,  783.85/s  (0.164s,  780.90/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 114.903s\n",
      "Train: 18 [ 750/10009 (  7%)]  Loss: 2.79 (2.93)  Time: 0.160s,  798.98/s  (0.164s,  781.36/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 123.027s\n",
      "Train: 18 [ 800/10009 (  8%)]  Loss: 2.77 (2.93)  Time: 0.161s,  796.87/s  (0.164s,  782.36/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 131.049s\n",
      "Train: 18 [ 850/10009 (  8%)]  Loss: 3.00 (2.93)  Time: 0.159s,  803.73/s  (0.164s,  781.58/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 139.370s\n",
      "Train: 18 [ 900/10009 (  9%)]  Loss: 2.88 (2.93)  Time: 0.165s,  776.70/s  (0.164s,  780.13/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 147.832s\n",
      "Train: 18 [ 950/10009 (  9%)]  Loss: 2.89 (2.93)  Time: 0.161s,  795.07/s  (0.164s,  779.58/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 156.147s\n",
      "Train: 18 [1000/10009 ( 10%)]  Loss: 2.94 (2.94)  Time: 0.172s,  745.15/s  (0.165s,  777.98/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 164.692s\n",
      "Train: 18 [1050/10009 ( 10%)]  Loss: 2.79 (2.93)  Time: 0.175s,  732.21/s  (0.165s,  776.46/s)  LR: 2.476e-06  Data: 0.008 (0.007)Time: 173.259s\n",
      "Train: 18 [1100/10009 ( 11%)]  Loss: 3.17 (2.94)  Time: 0.160s,  799.22/s  (0.165s,  777.44/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 181.273s\n",
      "Train: 18 [1150/10009 ( 11%)]  Loss: 2.90 (2.94)  Time: 0.181s,  706.22/s  (0.165s,  777.00/s)  LR: 2.476e-06  Data: 0.011 (0.007)Time: 189.610s\n",
      "Train: 18 [1200/10009 ( 12%)]  Loss: 2.61 (2.94)  Time: 0.159s,  802.56/s  (0.165s,  776.34/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 198.015s\n",
      "Train: 18 [1250/10009 ( 12%)]  Loss: 3.22 (2.94)  Time: 0.162s,  788.80/s  (0.165s,  776.93/s)  LR: 2.476e-06  Data: 0.008 (0.007)Time: 206.104s\n",
      "Train: 18 [1300/10009 ( 13%)]  Loss: 2.99 (2.94)  Time: 0.161s,  794.45/s  (0.165s,  777.65/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 214.142s\n",
      "Train: 18 [1350/10009 ( 13%)]  Loss: 2.77 (2.94)  Time: 0.160s,  799.63/s  (0.164s,  778.35/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 222.173s\n",
      "Train: 18 [1400/10009 ( 14%)]  Loss: 3.10 (2.94)  Time: 0.160s,  799.71/s  (0.164s,  779.01/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 230.201s\n",
      "Train: 18 [1450/10009 ( 14%)]  Loss: 2.86 (2.94)  Time: 0.160s,  801.69/s  (0.164s,  779.63/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 238.226s\n",
      "Train: 18 [1500/10009 ( 15%)]  Loss: 2.92 (2.94)  Time: 0.160s,  801.82/s  (0.164s,  780.22/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 246.249s\n",
      "Train: 18 [1550/10009 ( 15%)]  Loss: 2.77 (2.94)  Time: 0.160s,  801.71/s  (0.164s,  780.79/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 254.267s\n",
      "Train: 18 [1600/10009 ( 16%)]  Loss: 2.79 (2.94)  Time: 0.160s,  800.77/s  (0.164s,  781.27/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 262.302s\n",
      "Train: 18 [1650/10009 ( 16%)]  Loss: 2.70 (2.94)  Time: 0.161s,  793.09/s  (0.164s,  781.62/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 270.370s\n",
      "Train: 18 [1700/10009 ( 17%)]  Loss: 2.90 (2.94)  Time: 0.160s,  802.28/s  (0.164s,  782.09/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 278.393s\n",
      "Train: 18 [1750/10009 ( 17%)]  Loss: 2.96 (2.94)  Time: 0.160s,  798.31/s  (0.164s,  782.53/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 286.414s\n",
      "Train: 18 [1800/10009 ( 18%)]  Loss: 2.96 (2.94)  Time: 0.162s,  789.00/s  (0.163s,  782.93/s)  LR: 2.476e-06  Data: 0.008 (0.007)Time: 294.442s\n",
      "Train: 18 [1850/10009 ( 18%)]  Loss: 2.69 (2.94)  Time: 0.160s,  801.13/s  (0.163s,  783.15/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 302.532s\n",
      "Train: 18 [1900/10009 ( 19%)]  Loss: 2.91 (2.94)  Time: 0.163s,  787.04/s  (0.163s,  783.47/s)  LR: 2.476e-06  Data: 0.008 (0.007)Time: 310.575s\n",
      "Train: 18 [1950/10009 ( 19%)]  Loss: 2.90 (2.94)  Time: 0.162s,  792.25/s  (0.163s,  783.58/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 318.702s\n",
      "Train: 18 [2000/10009 ( 20%)]  Loss: 2.83 (2.94)  Time: 0.159s,  803.56/s  (0.163s,  783.90/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 326.734s\n",
      "Train: 18 [2050/10009 ( 20%)]  Loss: 2.96 (2.94)  Time: 0.161s,  794.93/s  (0.163s,  784.22/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 334.764s\n",
      "Train: 18 [2100/10009 ( 21%)]  Loss: 2.89 (2.94)  Time: 0.160s,  799.75/s  (0.163s,  784.50/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 342.803s\n",
      "Train: 18 [2150/10009 ( 21%)]  Loss: 2.81 (2.94)  Time: 0.160s,  802.42/s  (0.163s,  784.80/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 350.823s\n",
      "Train: 18 [2200/10009 ( 22%)]  Loss: 2.86 (2.94)  Time: 0.161s,  796.11/s  (0.163s,  785.08/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 358.851s\n",
      "Train: 18 [2250/10009 ( 22%)]  Loss: 2.86 (2.94)  Time: 0.161s,  796.60/s  (0.163s,  785.32/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 366.894s\n",
      "Train: 18 [2300/10009 ( 23%)]  Loss: 2.80 (2.94)  Time: 0.160s,  799.52/s  (0.163s,  785.54/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 374.937s\n",
      "Train: 18 [2350/10009 ( 23%)]  Loss: 2.85 (2.94)  Time: 0.161s,  795.83/s  (0.163s,  785.73/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 382.989s\n",
      "Train: 18 [2400/10009 ( 24%)]  Loss: 2.82 (2.94)  Time: 0.160s,  800.08/s  (0.163s,  785.97/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 391.017s\n",
      "Train: 18 [2450/10009 ( 24%)]  Loss: 3.05 (2.93)  Time: 0.160s,  799.48/s  (0.163s,  786.17/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 399.060s\n",
      "Train: 18 [2500/10009 ( 25%)]  Loss: 3.00 (2.93)  Time: 0.160s,  799.96/s  (0.163s,  786.39/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 407.085s\n",
      "Train: 18 [2550/10009 ( 25%)]  Loss: 2.88 (2.94)  Time: 0.159s,  804.32/s  (0.163s,  786.61/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 415.105s\n",
      "Train: 18 [2600/10009 ( 26%)]  Loss: 3.28 (2.93)  Time: 0.160s,  800.16/s  (0.163s,  786.82/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 423.129s\n",
      "Train: 18 [2650/10009 ( 26%)]  Loss: 2.98 (2.94)  Time: 0.159s,  802.95/s  (0.163s,  787.03/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 431.152s\n",
      "Train: 18 [2700/10009 ( 27%)]  Loss: 2.60 (2.93)  Time: 0.159s,  804.65/s  (0.163s,  787.22/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 439.176s\n",
      "Train: 18 [2750/10009 ( 27%)]  Loss: 2.99 (2.94)  Time: 0.160s,  798.72/s  (0.163s,  787.41/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 447.197s\n",
      "Train: 18 [2800/10009 ( 28%)]  Loss: 2.94 (2.93)  Time: 0.161s,  795.58/s  (0.163s,  787.59/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 455.222s\n",
      "Train: 18 [2850/10009 ( 28%)]  Loss: 2.81 (2.93)  Time: 0.160s,  798.97/s  (0.162s,  787.76/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 463.247s\n",
      "Train: 18 [2900/10009 ( 29%)]  Loss: 3.00 (2.93)  Time: 0.160s,  798.16/s  (0.162s,  787.93/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 471.269s\n",
      "Train: 18 [2950/10009 ( 29%)]  Loss: 2.77 (2.93)  Time: 0.161s,  797.42/s  (0.162s,  788.08/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 479.299s\n",
      "Train: 18 [3000/10009 ( 30%)]  Loss: 2.74 (2.93)  Time: 0.160s,  799.82/s  (0.162s,  788.23/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 487.326s\n",
      "Train: 18 [3050/10009 ( 30%)]  Loss: 2.89 (2.93)  Time: 0.160s,  800.13/s  (0.162s,  788.29/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 495.413s\n",
      "Train: 18 [3100/10009 ( 31%)]  Loss: 2.95 (2.94)  Time: 0.161s,  792.60/s  (0.162s,  788.41/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 503.450s\n",
      "Train: 18 [3150/10009 ( 31%)]  Loss: 2.97 (2.94)  Time: 0.161s,  795.75/s  (0.162s,  788.54/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 511.485s\n",
      "Train: 18 [3200/10009 ( 32%)]  Loss: 2.99 (2.94)  Time: 0.164s,  782.73/s  (0.162s,  788.57/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 519.582s\n",
      "Train: 18 [3250/10009 ( 32%)]  Loss: 2.90 (2.94)  Time: 0.160s,  798.18/s  (0.162s,  788.65/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 527.642s\n",
      "Train: 18 [3300/10009 ( 33%)]  Loss: 2.88 (2.93)  Time: 0.161s,  794.75/s  (0.162s,  788.78/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 535.669s\n",
      "Train: 18 [3350/10009 ( 33%)]  Loss: 2.80 (2.93)  Time: 0.161s,  794.56/s  (0.162s,  788.91/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 543.698s\n",
      "Train: 18 [3400/10009 ( 34%)]  Loss: 3.11 (2.93)  Time: 0.161s,  794.65/s  (0.162s,  789.02/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 551.729s\n",
      "Train: 18 [3450/10009 ( 34%)]  Loss: 2.81 (2.94)  Time: 0.164s,  781.71/s  (0.162s,  789.09/s)  LR: 2.476e-06  Data: 0.009 (0.006)Time: 559.795s\n",
      "Train: 18 [3500/10009 ( 35%)]  Loss: 3.13 (2.94)  Time: 0.161s,  795.58/s  (0.162s,  789.05/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 567.931s\n",
      "Train: 18 [3550/10009 ( 35%)]  Loss: 2.77 (2.93)  Time: 0.159s,  802.55/s  (0.162s,  789.17/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 575.955s\n",
      "Train: 18 [3600/10009 ( 36%)]  Loss: 3.06 (2.93)  Time: 0.162s,  792.45/s  (0.162s,  789.20/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 584.040s\n",
      "Train: 18 [3650/10009 ( 36%)]  Loss: 3.17 (2.93)  Time: 0.160s,  799.26/s  (0.162s,  789.29/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 592.087s\n",
      "Train: 18 [3700/10009 ( 37%)]  Loss: 2.94 (2.94)  Time: 0.159s,  804.95/s  (0.162s,  789.40/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 600.113s\n",
      "Train: 18 [3750/10009 ( 37%)]  Loss: 3.22 (2.93)  Time: 0.160s,  800.35/s  (0.162s,  789.50/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 608.140s\n",
      "Train: 18 [3800/10009 ( 38%)]  Loss: 2.94 (2.94)  Time: 0.160s,  799.94/s  (0.162s,  789.60/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 616.166s\n",
      "Train: 18 [3850/10009 ( 38%)]  Loss: 2.80 (2.94)  Time: 0.160s,  800.96/s  (0.162s,  789.71/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 624.189s\n",
      "Train: 18 [3900/10009 ( 39%)]  Loss: 3.16 (2.94)  Time: 0.161s,  796.94/s  (0.162s,  789.79/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 632.227s\n",
      "Train: 18 [3950/10009 ( 39%)]  Loss: 2.62 (2.94)  Time: 0.160s,  800.61/s  (0.162s,  789.87/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 640.264s\n",
      "Train: 18 [4000/10009 ( 40%)]  Loss: 2.99 (2.94)  Time: 0.160s,  802.39/s  (0.162s,  789.98/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 648.279s\n",
      "Train: 18 [4050/10009 ( 40%)]  Loss: 2.94 (2.94)  Time: 0.161s,  795.32/s  (0.162s,  790.06/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 656.316s\n",
      "Train: 18 [4100/10009 ( 41%)]  Loss: 3.00 (2.94)  Time: 0.160s,  798.97/s  (0.162s,  790.11/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 664.373s\n",
      "Train: 18 [4150/10009 ( 41%)]  Loss: 2.81 (2.93)  Time: 0.160s,  799.48/s  (0.162s,  790.18/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 672.416s\n",
      "Train: 18 [4200/10009 ( 42%)]  Loss: 2.65 (2.93)  Time: 0.160s,  798.65/s  (0.162s,  790.25/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 680.449s\n",
      "Train: 18 [4250/10009 ( 42%)]  Loss: 2.88 (2.93)  Time: 0.159s,  803.88/s  (0.162s,  790.32/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 688.487s\n",
      "Train: 18 [4300/10009 ( 43%)]  Loss: 2.84 (2.93)  Time: 0.160s,  799.77/s  (0.162s,  790.41/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 696.508s\n",
      "Train: 18 [4350/10009 ( 43%)]  Loss: 2.86 (2.93)  Time: 0.161s,  797.47/s  (0.162s,  790.48/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 704.541s\n",
      "Train: 18 [4400/10009 ( 44%)]  Loss: 2.91 (2.93)  Time: 0.161s,  797.36/s  (0.162s,  790.55/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 712.578s\n",
      "Train: 18 [4450/10009 ( 44%)]  Loss: 3.16 (2.93)  Time: 0.163s,  787.43/s  (0.162s,  790.60/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 720.623s\n",
      "Train: 18 [4500/10009 ( 45%)]  Loss: 2.82 (2.93)  Time: 0.160s,  801.02/s  (0.162s,  790.68/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 728.649s\n",
      "Train: 18 [4550/10009 ( 45%)]  Loss: 2.72 (2.93)  Time: 0.160s,  799.47/s  (0.162s,  790.75/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 736.674s\n",
      "Train: 18 [4600/10009 ( 46%)]  Loss: 2.88 (2.94)  Time: 0.159s,  803.19/s  (0.162s,  790.83/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 744.690s\n",
      "Train: 18 [4650/10009 ( 46%)]  Loss: 3.01 (2.93)  Time: 0.162s,  789.77/s  (0.162s,  790.91/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 752.707s\n",
      "Train: 18 [4700/10009 ( 47%)]  Loss: 2.90 (2.93)  Time: 0.160s,  799.77/s  (0.162s,  790.98/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 760.734s\n",
      "Train: 18 [4750/10009 ( 47%)]  Loss: 2.82 (2.93)  Time: 0.160s,  799.47/s  (0.162s,  791.06/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 768.752s\n",
      "Train: 18 [4800/10009 ( 48%)]  Loss: 3.15 (2.93)  Time: 0.161s,  795.79/s  (0.162s,  791.06/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 776.842s\n",
      "Train: 18 [4850/10009 ( 48%)]  Loss: 2.66 (2.93)  Time: 0.160s,  800.91/s  (0.162s,  791.10/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 784.893s\n",
      "Train: 18 [4900/10009 ( 49%)]  Loss: 2.68 (2.93)  Time: 0.160s,  800.06/s  (0.162s,  791.17/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 792.913s\n",
      "Train: 18 [4950/10009 ( 49%)]  Loss: 2.88 (2.93)  Time: 0.161s,  795.96/s  (0.162s,  791.17/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 800.996s\n",
      "Train: 18 [5000/10009 ( 50%)]  Loss: 2.92 (2.93)  Time: 0.160s,  801.91/s  (0.162s,  791.25/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 809.005s\n",
      "Train: 18 [5050/10009 ( 50%)]  Loss: 3.11 (2.93)  Time: 0.159s,  802.98/s  (0.162s,  791.31/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 817.029s\n",
      "Train: 18 [5100/10009 ( 51%)]  Loss: 2.92 (2.93)  Time: 0.160s,  801.94/s  (0.162s,  791.38/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 825.051s\n",
      "Train: 18 [5150/10009 ( 51%)]  Loss: 2.97 (2.93)  Time: 0.160s,  797.71/s  (0.162s,  791.45/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 833.064s\n",
      "Train: 18 [5200/10009 ( 52%)]  Loss: 3.05 (2.93)  Time: 0.160s,  799.87/s  (0.162s,  791.53/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 841.063s\n",
      "Train: 18 [5250/10009 ( 52%)]  Loss: 3.12 (2.93)  Time: 0.159s,  803.31/s  (0.162s,  791.60/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 849.070s\n",
      "Train: 18 [5300/10009 ( 53%)]  Loss: 2.90 (2.93)  Time: 0.161s,  794.39/s  (0.162s,  791.65/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 857.100s\n",
      "Train: 18 [5350/10009 ( 53%)]  Loss: 3.07 (2.93)  Time: 0.160s,  800.84/s  (0.162s,  791.71/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 865.127s\n",
      "Train: 18 [5400/10009 ( 54%)]  Loss: 3.13 (2.93)  Time: 0.161s,  796.97/s  (0.162s,  791.78/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 873.129s\n",
      "Train: 18 [5450/10009 ( 54%)]  Loss: 3.09 (2.93)  Time: 0.160s,  800.76/s  (0.162s,  791.84/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 881.147s\n",
      "Train: 18 [5500/10009 ( 55%)]  Loss: 2.98 (2.93)  Time: 0.160s,  801.03/s  (0.162s,  791.90/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 889.160s\n",
      "Train: 18 [5550/10009 ( 55%)]  Loss: 3.12 (2.93)  Time: 0.159s,  803.82/s  (0.162s,  791.97/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 897.161s\n",
      "Train: 18 [5600/10009 ( 56%)]  Loss: 2.79 (2.94)  Time: 0.160s,  802.29/s  (0.162s,  792.04/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 905.163s\n",
      "Train: 18 [5650/10009 ( 56%)]  Loss: 3.07 (2.94)  Time: 0.160s,  800.54/s  (0.162s,  792.10/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 913.179s\n",
      "Train: 18 [5700/10009 ( 57%)]  Loss: 3.12 (2.94)  Time: 0.160s,  801.66/s  (0.162s,  792.13/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 921.214s\n",
      "Train: 18 [5750/10009 ( 57%)]  Loss: 2.92 (2.94)  Time: 0.160s,  802.06/s  (0.162s,  792.20/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 929.220s\n",
      "Train: 18 [5800/10009 ( 58%)]  Loss: 2.97 (2.94)  Time: 0.161s,  796.93/s  (0.162s,  792.26/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 937.225s\n",
      "Train: 18 [5850/10009 ( 58%)]  Loss: 2.87 (2.94)  Time: 0.160s,  797.88/s  (0.162s,  792.32/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 945.235s\n",
      "Train: 18 [5900/10009 ( 59%)]  Loss: 2.89 (2.94)  Time: 0.161s,  797.01/s  (0.162s,  792.37/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 953.244s\n",
      "Train: 18 [5950/10009 ( 59%)]  Loss: 2.94 (2.94)  Time: 0.162s,  790.28/s  (0.162s,  792.42/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 961.260s\n",
      "Train: 18 [6000/10009 ( 60%)]  Loss: 3.07 (2.94)  Time: 0.160s,  797.72/s  (0.162s,  792.47/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 969.278s\n",
      "Train: 18 [6050/10009 ( 60%)]  Loss: 2.86 (2.94)  Time: 0.161s,  795.00/s  (0.162s,  792.51/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 977.310s\n",
      "Train: 18 [6100/10009 ( 61%)]  Loss: 2.75 (2.94)  Time: 0.160s,  799.95/s  (0.162s,  792.55/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 985.330s\n",
      "Train: 18 [6150/10009 ( 61%)]  Loss: 3.08 (2.94)  Time: 0.160s,  798.91/s  (0.161s,  792.61/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 993.333s\n",
      "Train: 18 [6200/10009 ( 62%)]  Loss: 2.98 (2.93)  Time: 0.160s,  802.10/s  (0.161s,  792.66/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1001.341s\n",
      "Train: 18 [6250/10009 ( 62%)]  Loss: 2.89 (2.93)  Time: 0.160s,  801.99/s  (0.161s,  792.71/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1009.351s\n",
      "Train: 18 [6300/10009 ( 63%)]  Loss: 2.87 (2.93)  Time: 0.159s,  803.10/s  (0.161s,  792.77/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1017.351s\n",
      "Train: 18 [6350/10009 ( 63%)]  Loss: 3.11 (2.93)  Time: 0.159s,  803.12/s  (0.161s,  792.82/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1025.355s\n",
      "Train: 18 [6400/10009 ( 64%)]  Loss: 3.05 (2.94)  Time: 0.160s,  798.48/s  (0.161s,  792.87/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1033.367s\n",
      "Train: 18 [6450/10009 ( 64%)]  Loss: 3.06 (2.94)  Time: 0.161s,  796.62/s  (0.161s,  792.92/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1041.380s\n",
      "Train: 18 [6500/10009 ( 65%)]  Loss: 2.85 (2.94)  Time: 0.160s,  801.07/s  (0.161s,  792.96/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1049.397s\n",
      "Train: 18 [6550/10009 ( 65%)]  Loss: 2.71 (2.94)  Time: 0.159s,  804.18/s  (0.161s,  793.00/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1057.404s\n",
      "Train: 18 [6600/10009 ( 66%)]  Loss: 2.85 (2.94)  Time: 0.160s,  801.46/s  (0.161s,  793.02/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1065.447s\n",
      "Train: 18 [6650/10009 ( 66%)]  Loss: 2.90 (2.93)  Time: 0.159s,  802.93/s  (0.161s,  793.07/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1073.462s\n",
      "Train: 18 [6700/10009 ( 67%)]  Loss: 3.03 (2.93)  Time: 0.160s,  798.97/s  (0.161s,  793.11/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1081.472s\n",
      "Train: 18 [6750/10009 ( 67%)]  Loss: 2.84 (2.94)  Time: 0.160s,  800.03/s  (0.161s,  793.15/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1089.491s\n",
      "Train: 18 [6800/10009 ( 68%)]  Loss: 3.00 (2.94)  Time: 0.161s,  794.04/s  (0.161s,  793.12/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1097.589s\n",
      "Train: 18 [6850/10009 ( 68%)]  Loss: 2.75 (2.93)  Time: 0.160s,  798.80/s  (0.161s,  792.99/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1105.842s\n",
      "Train: 18 [6900/10009 ( 69%)]  Loss: 3.13 (2.93)  Time: 0.162s,  792.54/s  (0.161s,  792.93/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1114.006s\n",
      "Train: 18 [6950/10009 ( 69%)]  Loss: 3.21 (2.94)  Time: 0.162s,  789.69/s  (0.161s,  792.92/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 1122.086s\n",
      "Train: 18 [7000/10009 ( 70%)]  Loss: 3.08 (2.93)  Time: 0.160s,  798.21/s  (0.161s,  792.89/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1130.195s\n",
      "Train: 18 [7050/10009 ( 70%)]  Loss: 2.84 (2.93)  Time: 0.160s,  798.19/s  (0.161s,  792.90/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1138.261s\n",
      "Train: 18 [7100/10009 ( 71%)]  Loss: 2.88 (2.93)  Time: 0.166s,  772.04/s  (0.161s,  792.88/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 1146.353s\n",
      "Train: 18 [7150/10009 ( 71%)]  Loss: 2.85 (2.93)  Time: 0.160s,  799.30/s  (0.161s,  792.73/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1154.644s\n",
      "Train: 18 [7200/10009 ( 72%)]  Loss: 3.23 (2.93)  Time: 0.168s,  761.99/s  (0.162s,  792.56/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 1162.967s\n",
      "Train: 18 [7250/10009 ( 72%)]  Loss: 3.09 (2.93)  Time: 0.160s,  800.67/s  (0.162s,  792.39/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1171.297s\n",
      "Train: 18 [7300/10009 ( 73%)]  Loss: 2.42 (2.93)  Time: 0.164s,  781.41/s  (0.162s,  792.09/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1179.822s\n",
      "Train: 18 [7350/10009 ( 73%)]  Loss: 2.97 (2.93)  Time: 0.163s,  785.52/s  (0.162s,  791.92/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1188.164s\n",
      "Train: 18 [7400/10009 ( 74%)]  Loss: 2.75 (2.93)  Time: 0.164s,  781.77/s  (0.162s,  791.78/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1196.457s\n",
      "Train: 18 [7450/10009 ( 74%)]  Loss: 3.04 (2.93)  Time: 0.164s,  780.50/s  (0.162s,  791.69/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1204.663s\n",
      "Train: 18 [7500/10009 ( 75%)]  Loss: 3.07 (2.93)  Time: 0.162s,  791.50/s  (0.162s,  791.60/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1212.889s\n",
      "Train: 18 [7550/10009 ( 75%)]  Loss: 2.93 (2.93)  Time: 0.161s,  793.14/s  (0.162s,  791.53/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1221.083s\n",
      "Train: 18 [7600/10009 ( 76%)]  Loss: 3.19 (2.93)  Time: 0.162s,  788.09/s  (0.162s,  791.46/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1229.279s\n",
      "Train: 18 [7650/10009 ( 76%)]  Loss: 2.94 (2.93)  Time: 0.163s,  784.74/s  (0.162s,  791.38/s)  LR: 2.476e-06  Data: 0.008 (0.006)Time: 1237.487s\n",
      "Train: 18 [7700/10009 ( 77%)]  Loss: 2.93 (2.93)  Time: 0.163s,  784.20/s  (0.162s,  791.29/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1245.724s\n",
      "Train: 18 [7750/10009 ( 77%)]  Loss: 2.82 (2.93)  Time: 0.164s,  782.29/s  (0.162s,  791.14/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1254.051s\n",
      "Train: 18 [7800/10009 ( 78%)]  Loss: 2.51 (2.93)  Time: 0.163s,  784.47/s  (0.162s,  790.98/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1262.395s\n",
      "Train: 18 [7850/10009 ( 78%)]  Loss: 2.92 (2.93)  Time: 0.160s,  800.91/s  (0.162s,  790.78/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1270.809s\n",
      "Train: 18 [7900/10009 ( 79%)]  Loss: 2.93 (2.93)  Time: 0.176s,  729.04/s  (0.162s,  790.52/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1279.320s\n",
      "Train: 18 [7950/10009 ( 79%)]  Loss: 2.79 (2.93)  Time: 0.160s,  799.77/s  (0.162s,  790.41/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1287.599s\n",
      "Train: 18 [8000/10009 ( 80%)]  Loss: 2.77 (2.93)  Time: 0.173s,  739.67/s  (0.162s,  790.27/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1295.919s\n",
      "Train: 18 [8050/10009 ( 80%)]  Loss: 3.06 (2.93)  Time: 0.160s,  801.08/s  (0.162s,  790.29/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1303.991s\n",
      "Train: 18 [8100/10009 ( 81%)]  Loss: 3.12 (2.93)  Time: 0.161s,  797.30/s  (0.162s,  790.26/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1312.125s\n",
      "Train: 18 [8150/10009 ( 81%)]  Loss: 2.92 (2.93)  Time: 0.160s,  801.82/s  (0.162s,  790.27/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1320.212s\n",
      "Train: 18 [8200/10009 ( 82%)]  Loss: 2.75 (2.93)  Time: 0.160s,  802.37/s  (0.162s,  790.27/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1328.307s\n",
      "Train: 18 [8250/10009 ( 82%)]  Loss: 2.95 (2.93)  Time: 0.160s,  798.69/s  (0.162s,  790.29/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1336.376s\n",
      "Train: 18 [8300/10009 ( 83%)]  Loss: 2.89 (2.93)  Time: 0.162s,  791.93/s  (0.162s,  790.30/s)  LR: 2.476e-06  Data: 0.007 (0.006)Time: 1344.457s\n",
      "Train: 18 [8350/10009 ( 83%)]  Loss: 2.77 (2.93)  Time: 0.160s,  799.56/s  (0.162s,  790.30/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1352.559s\n",
      "Train: 18 [8400/10009 ( 84%)]  Loss: 2.81 (2.93)  Time: 0.160s,  800.66/s  (0.162s,  790.29/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1360.678s\n",
      "Train: 18 [8450/10009 ( 84%)]  Loss: 2.98 (2.93)  Time: 0.160s,  801.16/s  (0.162s,  790.21/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1368.910s\n",
      "Train: 18 [8500/10009 ( 85%)]  Loss: 2.89 (2.93)  Time: 0.160s,  801.65/s  (0.162s,  790.16/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1377.100s\n",
      "Train: 18 [8550/10009 ( 85%)]  Loss: 2.91 (2.93)  Time: 0.161s,  796.82/s  (0.162s,  790.13/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1385.254s\n",
      "Train: 18 [8600/10009 ( 86%)]  Loss: 2.87 (2.93)  Time: 0.160s,  800.74/s  (0.162s,  790.12/s)  LR: 2.476e-06  Data: 0.006 (0.006)Time: 1393.358s\n",
      "Train: 18 [8650/10009 ( 86%)]  Loss: 3.07 (2.93)  Time: 0.160s,  800.71/s  (0.162s,  789.33/s)  LR: 2.476e-06  Data: 0.005 (0.006)Time: 1402.860s\n",
      "Train: 18 [8700/10009 ( 87%)]  Loss: 3.11 (2.93)  Time: 0.164s,  780.45/s  (0.162s,  788.19/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 1413.008s\n",
      "Train: 18 [8750/10009 ( 87%)]  Loss: 2.69 (2.93)  Time: 0.173s,  740.49/s  (0.162s,  788.09/s)  LR: 2.476e-06  Data: 0.010 (0.007)Time: 1421.309s\n",
      "Train: 18 [8800/10009 ( 88%)]  Loss: 2.91 (2.93)  Time: 0.161s,  795.10/s  (0.162s,  787.92/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 1429.738s\n",
      "Train: 18 [8850/10009 ( 88%)]  Loss: 2.81 (2.93)  Time: 0.160s,  802.27/s  (0.162s,  787.85/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1437.990s\n",
      "Train: 18 [8900/10009 ( 89%)]  Loss: 3.07 (2.93)  Time: 0.167s,  767.73/s  (0.162s,  787.83/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 1446.152s\n",
      "Train: 18 [8950/10009 ( 89%)]  Loss: 2.69 (2.93)  Time: 0.168s,  762.33/s  (0.162s,  787.80/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1454.344s\n",
      "Train: 18 [9000/10009 ( 90%)]  Loss: 2.76 (2.93)  Time: 0.161s,  797.06/s  (0.162s,  787.74/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1462.561s\n",
      "Train: 18 [9050/10009 ( 90%)]  Loss: 3.18 (2.93)  Time: 0.161s,  795.62/s  (0.163s,  787.69/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 1470.791s\n",
      "Train: 18 [9100/10009 ( 91%)]  Loss: 2.64 (2.93)  Time: 0.161s,  794.36/s  (0.162s,  787.72/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1478.865s\n",
      "Train: 18 [9150/10009 ( 91%)]  Loss: 3.03 (2.93)  Time: 0.160s,  801.49/s  (0.162s,  787.75/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1486.921s\n",
      "Train: 18 [9200/10009 ( 92%)]  Loss: 3.13 (2.93)  Time: 0.160s,  798.50/s  (0.162s,  787.74/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 1495.070s\n",
      "Train: 18 [9250/10009 ( 92%)]  Loss: 2.95 (2.93)  Time: 0.160s,  802.24/s  (0.163s,  787.69/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 1503.293s\n",
      "Train: 18 [9300/10009 ( 93%)]  Loss: 2.90 (2.93)  Time: 0.172s,  742.25/s  (0.163s,  787.62/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1511.555s\n",
      "Train: 18 [9350/10009 ( 93%)]  Loss: 2.81 (2.93)  Time: 0.160s,  798.98/s  (0.163s,  787.55/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1519.814s\n",
      "Train: 18 [9400/10009 ( 94%)]  Loss: 2.88 (2.93)  Time: 0.161s,  793.69/s  (0.163s,  787.52/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 1527.999s\n",
      "Train: 18 [9450/10009 ( 94%)]  Loss: 3.34 (2.93)  Time: 0.159s,  803.23/s  (0.163s,  787.50/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 1536.157s\n",
      "Train: 18 [9500/10009 ( 95%)]  Loss: 2.95 (2.93)  Time: 0.161s,  796.72/s  (0.163s,  787.53/s)  LR: 2.476e-06  Data: 0.007 (0.007)Time: 1544.224s\n",
      "Train: 18 [9550/10009 ( 95%)]  Loss: 2.98 (2.93)  Time: 0.159s,  802.91/s  (0.163s,  787.57/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 1552.280s\n",
      "Train: 18 [9600/10009 ( 96%)]  Loss: 3.10 (2.93)  Time: 0.160s,  799.56/s  (0.163s,  787.49/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1560.553s\n",
      "Train: 18 [9650/10009 ( 96%)]  Loss: 2.98 (2.93)  Time: 0.160s,  801.43/s  (0.163s,  787.48/s)  LR: 2.476e-06  Data: 0.005 (0.007)Time: 1568.714s\n",
      "Train: 18 [9700/10009 ( 97%)]  Loss: 3.04 (2.93)  Time: 0.161s,  794.48/s  (0.163s,  787.48/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1576.831s\n",
      "Train: 18 [9750/10009 ( 97%)]  Loss: 3.09 (2.93)  Time: 0.162s,  789.17/s  (0.163s,  787.47/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1584.978s\n",
      "Train: 18 [9800/10009 ( 98%)]  Loss: 3.12 (2.93)  Time: 0.166s,  770.67/s  (0.163s,  787.39/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1593.276s\n",
      "Train: 18 [9850/10009 ( 98%)]  Loss: 2.76 (2.93)  Time: 0.160s,  799.61/s  (0.163s,  787.34/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1601.491s\n",
      "Train: 18 [9900/10009 ( 99%)]  Loss: 2.89 (2.93)  Time: 0.161s,  796.87/s  (0.163s,  787.21/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1609.884s\n",
      "Train: 18 [9950/10009 ( 99%)]  Loss: 2.70 (2.93)  Time: 0.160s,  799.32/s  (0.163s,  787.12/s)  LR: 2.476e-06  Data: 0.006 (0.007)Time: 1618.207s\n",
      "Train: 18 [10000/10009 (100%)]  Loss: 2.82 (2.93)  Time: 0.204s,  626.72/s  (0.163s,  787.04/s)  LR: 2.476e-06  Data: 0.050 (0.007)Time: 1626.499s\n",
      "Test: [   0/390]  Time: 0.774 (0.774)  Loss:   1.065 ( 1.065)  Acc@1:  80.469 ( 80.469)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.056 (0.148)  Loss:   1.016 ( 1.733)  Acc@1:  78.906 ( 62.653)  Acc@5:  94.531 ( 82.184)\n",
      "Test: [ 100/390]  Time: 0.246 (0.141)  Loss:   1.685 ( 1.733)  Acc@1:  58.594 ( 60.651)  Acc@5:  92.188 ( 83.625)\n",
      "Test: [ 150/390]  Time: 0.052 (0.143)  Loss:   1.582 ( 1.704)  Acc@1:  56.250 ( 61.388)  Acc@5:  85.938 ( 84.090)\n",
      "Test: [ 200/390]  Time: 0.052 (0.142)  Loss:   2.824 ( 1.880)  Acc@1:  35.938 ( 58.046)  Acc@5:  68.750 ( 81.339)\n",
      "Test: [ 250/390]  Time: 0.052 (0.140)  Loss:   2.058 ( 1.989)  Acc@1:  60.938 ( 56.272)  Acc@5:  77.344 ( 79.504)\n",
      "Test: [ 300/390]  Time: 0.052 (0.138)  Loss:   2.384 ( 2.082)  Acc@1:  56.250 ( 54.527)  Acc@5:  72.656 ( 77.912)\n",
      "Test: [ 350/390]  Time: 0.051 (0.138)  Loss:   2.465 ( 2.153)  Acc@1:  50.000 ( 53.279)  Acc@5:  72.656 ( 76.816)\n",
      "Test: [ 390/390]  Time: 0.035 (0.137)  Loss:   3.278 ( 2.127)  Acc@1:  31.250 ( 53.728)  Acc@5:  58.750 ( 77.250)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-18.pth.tar', 53.728)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 53.226)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 52.608)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 51.51)\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 19 [   0/10009 (  0%)]  Loss: 3.20 (3.20)  Time: 0.797s,  160.53/s  (0.797s,  160.53/s)  LR: 1.111e-06  Data: 0.639 (0.639)Time: 0.798s\n",
      "Train: 19 [  50/10009 (  0%)]  Loss: 2.82 (2.92)  Time: 0.159s,  802.72/s  (0.178s,  720.32/s)  LR: 1.111e-06  Data: 0.006 (0.019)Time: 9.063s\n",
      "Train: 19 [ 100/10009 (  1%)]  Loss: 2.79 (2.91)  Time: 0.166s,  772.94/s  (0.170s,  753.70/s)  LR: 1.111e-06  Data: 0.006 (0.012)Time: 17.153s\n",
      "Train: 19 [ 150/10009 (  1%)]  Loss: 3.06 (2.91)  Time: 0.162s,  792.46/s  (0.167s,  766.02/s)  LR: 1.111e-06  Data: 0.007 (0.010)Time: 25.232s\n",
      "Train: 19 [ 200/10009 (  2%)]  Loss: 2.64 (2.91)  Time: 0.159s,  804.73/s  (0.166s,  773.36/s)  LR: 1.111e-06  Data: 0.006 (0.009)Time: 33.268s\n",
      "Train: 19 [ 250/10009 (  2%)]  Loss: 2.99 (2.91)  Time: 0.159s,  803.72/s  (0.165s,  775.97/s)  LR: 1.111e-06  Data: 0.006 (0.009)Time: 41.404s\n",
      "Train: 19 [ 300/10009 (  3%)]  Loss: 2.91 (2.91)  Time: 0.160s,  799.69/s  (0.164s,  778.50/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 49.490s\n",
      "Train: 19 [ 350/10009 (  3%)]  Loss: 2.83 (2.91)  Time: 0.164s,  781.26/s  (0.164s,  780.07/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 57.595s\n",
      "Train: 19 [ 400/10009 (  4%)]  Loss: 2.95 (2.91)  Time: 0.159s,  806.02/s  (0.164s,  780.97/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 65.724s\n",
      "Train: 19 [ 450/10009 (  4%)]  Loss: 2.91 (2.91)  Time: 0.160s,  800.12/s  (0.164s,  781.89/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 73.831s\n",
      "Train: 19 [ 500/10009 (  5%)]  Loss: 2.81 (2.90)  Time: 0.169s,  757.87/s  (0.164s,  781.12/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 82.097s\n",
      "Train: 19 [ 550/10009 (  5%)]  Loss: 2.75 (2.90)  Time: 0.160s,  802.37/s  (0.164s,  780.07/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 90.412s\n",
      "Train: 19 [ 600/10009 (  6%)]  Loss: 3.00 (2.91)  Time: 0.161s,  795.09/s  (0.164s,  778.85/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 98.772s\n",
      "Train: 19 [ 650/10009 (  6%)]  Loss: 3.04 (2.91)  Time: 0.160s,  799.76/s  (0.164s,  778.49/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 107.038s\n",
      "Train: 19 [ 700/10009 (  7%)]  Loss: 2.77 (2.91)  Time: 0.159s,  802.67/s  (0.164s,  778.79/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 115.214s\n",
      "Train: 19 [ 750/10009 (  7%)]  Loss: 2.86 (2.91)  Time: 0.160s,  801.44/s  (0.164s,  779.41/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 123.334s\n",
      "Train: 19 [ 800/10009 (  8%)]  Loss: 2.93 (2.91)  Time: 0.163s,  783.02/s  (0.164s,  779.02/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 131.612s\n",
      "Train: 19 [ 850/10009 (  8%)]  Loss: 2.82 (2.91)  Time: 0.161s,  794.84/s  (0.164s,  779.52/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 139.737s\n",
      "Train: 19 [ 900/10009 (  9%)]  Loss: 2.97 (2.91)  Time: 0.160s,  798.74/s  (0.164s,  779.54/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 147.943s\n",
      "Train: 19 [ 950/10009 (  9%)]  Loss: 2.91 (2.91)  Time: 0.159s,  802.52/s  (0.164s,  779.14/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 156.234s\n",
      "Train: 19 [1000/10009 ( 10%)]  Loss: 3.02 (2.91)  Time: 0.164s,  779.16/s  (0.164s,  778.68/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 164.544s\n",
      "Train: 19 [1050/10009 ( 10%)]  Loss: 2.93 (2.91)  Time: 0.160s,  797.51/s  (0.164s,  779.10/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 172.671s\n",
      "Train: 19 [1100/10009 ( 11%)]  Loss: 2.76 (2.91)  Time: 0.162s,  791.34/s  (0.164s,  779.45/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 180.804s\n",
      "Train: 19 [1150/10009 ( 11%)]  Loss: 2.86 (2.91)  Time: 0.160s,  800.63/s  (0.164s,  779.43/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 189.019s\n",
      "Train: 19 [1200/10009 ( 12%)]  Loss: 2.79 (2.91)  Time: 0.160s,  801.79/s  (0.164s,  780.09/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 197.064s\n",
      "Train: 19 [1250/10009 ( 12%)]  Loss: 2.86 (2.91)  Time: 0.160s,  800.11/s  (0.164s,  780.63/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 205.127s\n",
      "Train: 19 [1300/10009 ( 13%)]  Loss: 2.66 (2.91)  Time: 0.160s,  800.49/s  (0.164s,  781.01/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 213.221s\n",
      "Train: 19 [1350/10009 ( 13%)]  Loss: 2.66 (2.91)  Time: 0.160s,  801.51/s  (0.164s,  781.15/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 221.377s\n",
      "Train: 19 [1400/10009 ( 14%)]  Loss: 2.90 (2.91)  Time: 0.160s,  798.74/s  (0.164s,  781.53/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 229.457s\n",
      "Train: 19 [1450/10009 ( 14%)]  Loss: 2.74 (2.91)  Time: 0.171s,  747.12/s  (0.164s,  781.55/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 237.641s\n",
      "Train: 19 [1500/10009 ( 15%)]  Loss: 2.88 (2.91)  Time: 0.163s,  784.80/s  (0.164s,  781.69/s)  LR: 1.111e-06  Data: 0.008 (0.007)Time: 245.785s\n",
      "Train: 19 [1550/10009 ( 15%)]  Loss: 3.01 (2.91)  Time: 0.162s,  790.57/s  (0.164s,  781.78/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 253.942s\n",
      "Train: 19 [1600/10009 ( 16%)]  Loss: 2.99 (2.91)  Time: 0.160s,  799.72/s  (0.164s,  781.64/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 262.177s\n",
      "Train: 19 [1650/10009 ( 16%)]  Loss: 2.74 (2.91)  Time: 0.164s,  778.69/s  (0.164s,  781.65/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 270.362s\n",
      "Train: 19 [1700/10009 ( 17%)]  Loss: 3.18 (2.91)  Time: 0.161s,  795.20/s  (0.164s,  781.82/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 278.489s\n",
      "Train: 19 [1750/10009 ( 17%)]  Loss: 3.16 (2.91)  Time: 0.160s,  799.47/s  (0.164s,  781.94/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 286.630s\n",
      "Train: 19 [1800/10009 ( 18%)]  Loss: 2.62 (2.91)  Time: 0.160s,  801.41/s  (0.164s,  782.13/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 294.745s\n",
      "Train: 19 [1850/10009 ( 18%)]  Loss: 3.17 (2.91)  Time: 0.174s,  735.06/s  (0.164s,  782.21/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 302.896s\n",
      "Train: 19 [1900/10009 ( 19%)]  Loss: 2.81 (2.91)  Time: 0.160s,  799.85/s  (0.164s,  781.86/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 311.215s\n",
      "Train: 19 [1950/10009 ( 19%)]  Loss: 2.89 (2.91)  Time: 0.160s,  802.31/s  (0.164s,  781.95/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 319.366s\n",
      "Train: 19 [2000/10009 ( 20%)]  Loss: 2.89 (2.91)  Time: 0.160s,  799.17/s  (0.164s,  781.80/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 327.611s\n",
      "Train: 19 [2050/10009 ( 20%)]  Loss: 2.79 (2.91)  Time: 0.172s,  743.37/s  (0.164s,  781.81/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 335.794s\n",
      "Train: 19 [2100/10009 ( 21%)]  Loss: 2.96 (2.91)  Time: 0.160s,  797.56/s  (0.164s,  781.85/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 343.962s\n",
      "Train: 19 [2150/10009 ( 21%)]  Loss: 3.14 (2.91)  Time: 0.160s,  799.19/s  (0.164s,  781.73/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 352.201s\n",
      "Train: 19 [2200/10009 ( 22%)]  Loss: 2.88 (2.91)  Time: 0.163s,  786.05/s  (0.164s,  781.35/s)  LR: 1.111e-06  Data: 0.007 (0.006)Time: 360.563s\n",
      "Train: 19 [2250/10009 ( 22%)]  Loss: 2.70 (2.91)  Time: 0.163s,  784.86/s  (0.164s,  780.70/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 369.065s\n",
      "Train: 19 [2300/10009 ( 23%)]  Loss: 3.10 (2.91)  Time: 0.164s,  781.71/s  (0.164s,  780.68/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 377.271s\n",
      "Train: 19 [2350/10009 ( 23%)]  Loss: 2.64 (2.91)  Time: 0.172s,  745.56/s  (0.164s,  780.68/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 385.470s\n",
      "Train: 19 [2400/10009 ( 24%)]  Loss: 2.55 (2.91)  Time: 0.167s,  767.70/s  (0.164s,  780.59/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 393.710s\n",
      "Train: 19 [2450/10009 ( 24%)]  Loss: 2.86 (2.91)  Time: 0.163s,  783.26/s  (0.164s,  780.69/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 401.860s\n",
      "Train: 19 [2500/10009 ( 25%)]  Loss: 3.09 (2.91)  Time: 0.164s,  780.51/s  (0.164s,  780.76/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 410.022s\n",
      "Train: 19 [2550/10009 ( 25%)]  Loss: 3.26 (2.91)  Time: 0.161s,  795.38/s  (0.164s,  780.63/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 418.286s\n",
      "Train: 19 [2600/10009 ( 26%)]  Loss: 2.91 (2.91)  Time: 0.162s,  790.89/s  (0.164s,  780.67/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 426.464s\n",
      "Train: 19 [2650/10009 ( 26%)]  Loss: 2.63 (2.91)  Time: 0.161s,  794.74/s  (0.164s,  780.69/s)  LR: 1.111e-06  Data: 0.005 (0.006)Time: 434.653s\n",
      "Train: 19 [2700/10009 ( 27%)]  Loss: 2.97 (2.91)  Time: 0.163s,  783.82/s  (0.164s,  780.78/s)  LR: 1.111e-06  Data: 0.007 (0.006)Time: 442.797s\n",
      "Train: 19 [2750/10009 ( 27%)]  Loss: 2.64 (2.91)  Time: 0.164s,  778.39/s  (0.164s,  780.79/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 450.991s\n",
      "Train: 19 [2800/10009 ( 28%)]  Loss: 2.95 (2.91)  Time: 0.163s,  784.66/s  (0.164s,  780.47/s)  LR: 1.111e-06  Data: 0.007 (0.006)Time: 459.375s\n",
      "Train: 19 [2850/10009 ( 28%)]  Loss: 2.80 (2.91)  Time: 0.169s,  757.13/s  (0.164s,  780.34/s)  LR: 1.111e-06  Data: 0.005 (0.006)Time: 467.649s\n",
      "Train: 19 [2900/10009 ( 29%)]  Loss: 2.97 (2.91)  Time: 0.164s,  782.35/s  (0.164s,  780.32/s)  LR: 1.111e-06  Data: 0.006 (0.006)Time: 475.865s\n",
      "Train: 19 [2950/10009 ( 29%)]  Loss: 3.01 (2.91)  Time: 0.222s,  575.60/s  (0.165s,  776.94/s)  LR: 1.111e-06  Data: 0.056 (0.007)Time: 486.174s\n",
      "Train: 19 [3000/10009 ( 30%)]  Loss: 3.34 (2.91)  Time: 0.166s,  772.60/s  (0.165s,  775.81/s)  LR: 1.111e-06  Data: 0.008 (0.007)Time: 495.128s\n",
      "Train: 19 [3050/10009 ( 30%)]  Loss: 2.88 (2.91)  Time: 0.174s,  734.02/s  (0.165s,  775.41/s)  LR: 1.111e-06  Data: 0.010 (0.007)Time: 503.641s\n",
      "Train: 19 [3100/10009 ( 31%)]  Loss: 3.12 (2.91)  Time: 0.171s,  746.52/s  (0.165s,  774.91/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 512.224s\n",
      "Train: 19 [3150/10009 ( 31%)]  Loss: 3.11 (2.91)  Time: 0.172s,  745.20/s  (0.165s,  774.41/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 520.821s\n",
      "Train: 19 [3200/10009 ( 32%)]  Loss: 2.86 (2.91)  Time: 0.161s,  795.16/s  (0.165s,  774.60/s)  LR: 1.111e-06  Data: 0.005 (0.007)Time: 528.956s\n",
      "Train: 19 [3250/10009 ( 32%)]  Loss: 2.87 (2.91)  Time: 0.167s,  765.53/s  (0.165s,  774.70/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 537.148s\n",
      "Train: 19 [3300/10009 ( 33%)]  Loss: 3.04 (2.91)  Time: 0.163s,  783.28/s  (0.165s,  774.85/s)  LR: 1.111e-06  Data: 0.007 (0.007)Time: 545.299s\n",
      "Train: 19 [3350/10009 ( 33%)]  Loss: 2.89 (2.91)  Time: 0.170s,  755.03/s  (0.165s,  774.75/s)  LR: 1.111e-06  Data: 0.006 (0.007)Time: 553.632s\n",
      "Train: 19 [3400/10009 ( 34%)]  Loss: 2.81 (2.91)  Time: 0.380s,  336.41/s  (0.166s,  771.46/s)  LR: 1.111e-06  Data: 0.226 (0.008)Time: 564.288s\n",
      "Train: 19 [3450/10009 ( 34%)]  Loss: 2.79 (2.91)  Time: 0.167s,  768.66/s  (0.166s,  769.13/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 574.318s\n",
      "Train: 19 [3500/10009 ( 35%)]  Loss: 2.86 (2.91)  Time: 0.169s,  759.00/s  (0.166s,  769.11/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 582.655s\n",
      "Train: 19 [3550/10009 ( 35%)]  Loss: 2.70 (2.91)  Time: 0.161s,  796.98/s  (0.166s,  769.24/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 590.879s\n",
      "Train: 19 [3600/10009 ( 36%)]  Loss: 2.64 (2.91)  Time: 0.159s,  804.07/s  (0.166s,  769.17/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 599.249s\n",
      "Train: 19 [3650/10009 ( 36%)]  Loss: 2.76 (2.91)  Time: 0.173s,  741.95/s  (0.166s,  769.15/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 607.585s\n",
      "Train: 19 [3700/10009 ( 37%)]  Loss: 3.06 (2.91)  Time: 0.168s,  761.73/s  (0.166s,  769.21/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 615.863s\n",
      "Train: 19 [3750/10009 ( 37%)]  Loss: 2.75 (2.91)  Time: 0.160s,  798.44/s  (0.166s,  769.33/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 624.085s\n",
      "Train: 19 [3800/10009 ( 38%)]  Loss: 2.88 (2.91)  Time: 0.161s,  792.78/s  (0.166s,  769.49/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 632.273s\n",
      "Train: 19 [3850/10009 ( 38%)]  Loss: 2.86 (2.91)  Time: 0.160s,  801.18/s  (0.166s,  769.78/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 640.349s\n",
      "Train: 19 [3900/10009 ( 39%)]  Loss: 3.31 (2.91)  Time: 0.161s,  793.84/s  (0.166s,  770.06/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 648.428s\n",
      "Train: 19 [3950/10009 ( 39%)]  Loss: 2.76 (2.91)  Time: 0.162s,  790.27/s  (0.166s,  770.35/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 656.491s\n",
      "Train: 19 [4000/10009 ( 40%)]  Loss: 3.15 (2.91)  Time: 0.160s,  798.03/s  (0.166s,  770.64/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 664.547s\n",
      "Train: 19 [4050/10009 ( 40%)]  Loss: 2.87 (2.91)  Time: 0.162s,  790.81/s  (0.166s,  770.94/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 672.592s\n",
      "Train: 19 [4100/10009 ( 41%)]  Loss: 3.11 (2.91)  Time: 0.173s,  740.15/s  (0.166s,  771.22/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 680.646s\n",
      "Train: 19 [4150/10009 ( 41%)]  Loss: 2.86 (2.91)  Time: 0.163s,  787.02/s  (0.166s,  771.27/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 688.901s\n",
      "Train: 19 [4200/10009 ( 42%)]  Loss: 3.30 (2.91)  Time: 0.164s,  782.49/s  (0.166s,  771.45/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 697.030s\n",
      "Train: 19 [4250/10009 ( 42%)]  Loss: 2.98 (2.91)  Time: 0.160s,  798.20/s  (0.166s,  771.65/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 705.147s\n",
      "Train: 19 [4300/10009 ( 43%)]  Loss: 2.98 (2.91)  Time: 0.161s,  795.40/s  (0.166s,  771.77/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 713.327s\n",
      "Train: 19 [4350/10009 ( 43%)]  Loss: 2.96 (2.91)  Time: 0.176s,  727.31/s  (0.166s,  771.88/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 721.516s\n",
      "Train: 19 [4400/10009 ( 44%)]  Loss: 2.87 (2.91)  Time: 0.164s,  779.57/s  (0.166s,  772.11/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 729.594s\n",
      "Train: 19 [4450/10009 ( 44%)]  Loss: 2.77 (2.91)  Time: 0.161s,  797.30/s  (0.166s,  772.27/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 737.733s\n",
      "Train: 19 [4500/10009 ( 45%)]  Loss: 2.90 (2.91)  Time: 0.160s,  798.72/s  (0.166s,  772.44/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 745.853s\n",
      "Train: 19 [4550/10009 ( 45%)]  Loss: 2.84 (2.91)  Time: 0.163s,  783.58/s  (0.166s,  772.48/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 754.098s\n",
      "Train: 19 [4600/10009 ( 46%)]  Loss: 2.89 (2.91)  Time: 0.161s,  795.90/s  (0.166s,  772.69/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 762.181s\n",
      "Train: 19 [4650/10009 ( 46%)]  Loss: 2.67 (2.91)  Time: 0.164s,  781.31/s  (0.166s,  772.79/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 770.361s\n",
      "Train: 19 [4700/10009 ( 47%)]  Loss: 2.89 (2.91)  Time: 0.160s,  799.30/s  (0.166s,  772.88/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 778.553s\n",
      "Train: 19 [4750/10009 ( 47%)]  Loss: 2.64 (2.91)  Time: 0.172s,  745.83/s  (0.166s,  773.01/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 786.702s\n",
      "Train: 19 [4800/10009 ( 48%)]  Loss: 3.18 (2.91)  Time: 0.162s,  788.20/s  (0.166s,  773.18/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 794.808s\n",
      "Train: 19 [4850/10009 ( 48%)]  Loss: 2.78 (2.91)  Time: 0.159s,  802.53/s  (0.166s,  773.40/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 802.853s\n",
      "Train: 19 [4900/10009 ( 49%)]  Loss: 2.72 (2.91)  Time: 0.160s,  802.04/s  (0.165s,  773.62/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 810.900s\n",
      "Train: 19 [4950/10009 ( 49%)]  Loss: 2.98 (2.91)  Time: 0.162s,  790.18/s  (0.165s,  773.82/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 818.962s\n",
      "Train: 19 [5000/10009 ( 50%)]  Loss: 2.92 (2.91)  Time: 0.160s,  799.12/s  (0.165s,  774.03/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 827.006s\n",
      "Train: 19 [5050/10009 ( 50%)]  Loss: 2.96 (2.91)  Time: 0.162s,  788.40/s  (0.165s,  774.24/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 835.049s\n",
      "Train: 19 [5100/10009 ( 51%)]  Loss: 2.97 (2.91)  Time: 0.160s,  801.50/s  (0.165s,  774.44/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 843.100s\n",
      "Train: 19 [5150/10009 ( 51%)]  Loss: 2.75 (2.91)  Time: 0.160s,  799.12/s  (0.165s,  774.61/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 851.173s\n",
      "Train: 19 [5200/10009 ( 52%)]  Loss: 2.79 (2.91)  Time: 0.160s,  799.68/s  (0.165s,  774.77/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 859.253s\n",
      "Train: 19 [5250/10009 ( 52%)]  Loss: 3.08 (2.91)  Time: 0.161s,  795.75/s  (0.165s,  774.95/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 867.311s\n",
      "Train: 19 [5300/10009 ( 53%)]  Loss: 2.81 (2.91)  Time: 0.161s,  793.40/s  (0.165s,  775.01/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 875.504s\n",
      "Train: 19 [5350/10009 ( 53%)]  Loss: 3.42 (2.91)  Time: 0.163s,  784.22/s  (0.165s,  775.14/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 883.619s\n",
      "Train: 19 [5400/10009 ( 54%)]  Loss: 3.08 (2.91)  Time: 0.160s,  798.76/s  (0.165s,  775.31/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 891.672s\n",
      "Train: 19 [5450/10009 ( 54%)]  Loss: 3.20 (2.91)  Time: 0.163s,  785.67/s  (0.165s,  775.45/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 899.773s\n",
      "Train: 19 [5500/10009 ( 55%)]  Loss: 3.01 (2.91)  Time: 0.161s,  796.69/s  (0.165s,  775.62/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 907.826s\n",
      "Train: 19 [5550/10009 ( 55%)]  Loss: 2.94 (2.91)  Time: 0.163s,  785.31/s  (0.165s,  775.68/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 915.999s\n",
      "Train: 19 [5600/10009 ( 56%)]  Loss: 2.88 (2.91)  Time: 0.161s,  793.25/s  (0.165s,  775.70/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 924.235s\n",
      "Train: 19 [5650/10009 ( 56%)]  Loss: 3.11 (2.91)  Time: 0.160s,  799.36/s  (0.165s,  775.75/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 932.421s\n",
      "Train: 19 [5700/10009 ( 57%)]  Loss: 2.92 (2.91)  Time: 0.173s,  739.37/s  (0.165s,  775.83/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 940.580s\n",
      "Train: 19 [5750/10009 ( 57%)]  Loss: 3.27 (2.91)  Time: 0.163s,  787.62/s  (0.165s,  775.87/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 948.777s\n",
      "Train: 19 [5800/10009 ( 58%)]  Loss: 2.87 (2.91)  Time: 0.162s,  792.34/s  (0.165s,  775.96/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 956.917s\n",
      "Train: 19 [5850/10009 ( 58%)]  Loss: 2.91 (2.91)  Time: 0.161s,  797.32/s  (0.165s,  775.98/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 965.138s\n",
      "Train: 19 [5900/10009 ( 59%)]  Loss: 3.12 (2.91)  Time: 0.160s,  800.69/s  (0.165s,  774.68/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 975.023s\n",
      "Train: 19 [5950/10009 ( 59%)]  Loss: 2.82 (2.91)  Time: 0.164s,  779.24/s  (0.165s,  773.57/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 984.691s\n",
      "Train: 19 [6000/10009 ( 60%)]  Loss: 2.80 (2.91)  Time: 0.160s,  799.82/s  (0.165s,  773.66/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 992.845s\n",
      "Train: 19 [6050/10009 ( 60%)]  Loss: 2.71 (2.91)  Time: 0.160s,  800.36/s  (0.165s,  773.84/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1000.885s\n",
      "Train: 19 [6100/10009 ( 61%)]  Loss: 2.99 (2.91)  Time: 0.161s,  797.21/s  (0.165s,  774.02/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1008.926s\n",
      "Train: 19 [6150/10009 ( 61%)]  Loss: 2.70 (2.91)  Time: 0.160s,  801.66/s  (0.165s,  774.15/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1017.021s\n",
      "Train: 19 [6200/10009 ( 62%)]  Loss: 3.07 (2.91)  Time: 0.179s,  715.15/s  (0.165s,  774.28/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 1025.113s\n",
      "Train: 19 [6250/10009 ( 62%)]  Loss: 2.63 (2.91)  Time: 0.162s,  789.72/s  (0.165s,  774.40/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1033.216s\n",
      "Train: 19 [6300/10009 ( 63%)]  Loss: 2.69 (2.91)  Time: 0.160s,  799.09/s  (0.165s,  774.57/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1041.255s\n",
      "Train: 19 [6350/10009 ( 63%)]  Loss: 3.12 (2.91)  Time: 0.164s,  779.03/s  (0.165s,  774.74/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1049.295s\n",
      "Train: 19 [6400/10009 ( 64%)]  Loss: 3.23 (2.91)  Time: 0.162s,  791.01/s  (0.165s,  774.89/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1057.341s\n",
      "Train: 19 [6450/10009 ( 64%)]  Loss: 3.03 (2.91)  Time: 0.160s,  798.52/s  (0.165s,  775.01/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1065.441s\n",
      "Train: 19 [6500/10009 ( 65%)]  Loss: 2.81 (2.91)  Time: 0.162s,  792.34/s  (0.165s,  775.15/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1073.499s\n",
      "Train: 19 [6550/10009 ( 65%)]  Loss: 3.10 (2.91)  Time: 0.161s,  795.98/s  (0.165s,  775.23/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1081.652s\n",
      "Train: 19 [6600/10009 ( 66%)]  Loss: 2.97 (2.91)  Time: 0.160s,  800.04/s  (0.165s,  775.33/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1089.764s\n",
      "Train: 19 [6650/10009 ( 66%)]  Loss: 2.93 (2.91)  Time: 0.162s,  790.90/s  (0.165s,  775.43/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1097.877s\n",
      "Train: 19 [6700/10009 ( 67%)]  Loss: 2.86 (2.91)  Time: 0.161s,  793.50/s  (0.165s,  775.53/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1105.986s\n",
      "Train: 19 [6750/10009 ( 67%)]  Loss: 2.91 (2.91)  Time: 0.160s,  799.50/s  (0.165s,  775.66/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1114.051s\n",
      "Train: 19 [6800/10009 ( 68%)]  Loss: 2.73 (2.91)  Time: 0.160s,  800.46/s  (0.165s,  775.78/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1122.134s\n",
      "Train: 19 [6850/10009 ( 68%)]  Loss: 3.16 (2.91)  Time: 0.163s,  782.96/s  (0.165s,  775.92/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1130.172s\n",
      "Train: 19 [6900/10009 ( 69%)]  Loss: 2.97 (2.91)  Time: 0.162s,  791.44/s  (0.165s,  776.06/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1138.215s\n",
      "Train: 19 [6950/10009 ( 69%)]  Loss: 3.11 (2.91)  Time: 0.164s,  779.65/s  (0.165s,  776.17/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1146.309s\n",
      "Train: 19 [7000/10009 ( 70%)]  Loss: 2.99 (2.91)  Time: 0.160s,  800.93/s  (0.165s,  776.30/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1154.360s\n",
      "Train: 19 [7050/10009 ( 70%)]  Loss: 2.93 (2.91)  Time: 0.160s,  800.49/s  (0.165s,  776.44/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1162.385s\n",
      "Train: 19 [7100/10009 ( 71%)]  Loss: 3.00 (2.91)  Time: 0.162s,  791.80/s  (0.165s,  776.56/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1170.446s\n",
      "Train: 19 [7150/10009 ( 71%)]  Loss: 2.84 (2.91)  Time: 0.163s,  784.70/s  (0.165s,  776.70/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1178.486s\n",
      "Train: 19 [7200/10009 ( 72%)]  Loss: 2.81 (2.91)  Time: 0.168s,  761.90/s  (0.165s,  776.72/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1186.697s\n",
      "Train: 19 [7250/10009 ( 72%)]  Loss: 2.83 (2.91)  Time: 0.162s,  791.06/s  (0.165s,  776.73/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1194.906s\n",
      "Train: 19 [7300/10009 ( 73%)]  Loss: 2.73 (2.91)  Time: 0.160s,  799.36/s  (0.165s,  776.82/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1203.012s\n",
      "Train: 19 [7350/10009 ( 73%)]  Loss: 3.05 (2.91)  Time: 0.163s,  786.98/s  (0.165s,  776.74/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1211.376s\n",
      "Train: 19 [7400/10009 ( 74%)]  Loss: 2.92 (2.91)  Time: 0.165s,  774.14/s  (0.165s,  775.90/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 1220.939s\n",
      "Train: 19 [7450/10009 ( 74%)]  Loss: 3.04 (2.91)  Time: 0.159s,  803.26/s  (0.165s,  775.91/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1229.169s\n",
      "Train: 19 [7500/10009 ( 75%)]  Loss: 2.57 (2.91)  Time: 0.168s,  762.80/s  (0.165s,  775.88/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1237.467s\n",
      "Train: 19 [7550/10009 ( 75%)]  Loss: 2.66 (2.91)  Time: 0.166s,  773.32/s  (0.165s,  775.85/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1245.756s\n",
      "Train: 19 [7600/10009 ( 76%)]  Loss: 2.81 (2.91)  Time: 0.160s,  799.68/s  (0.165s,  775.92/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1253.897s\n",
      "Train: 19 [7650/10009 ( 76%)]  Loss: 2.98 (2.91)  Time: 0.171s,  747.91/s  (0.165s,  775.96/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1262.085s\n",
      "Train: 19 [7700/10009 ( 77%)]  Loss: 2.86 (2.91)  Time: 0.181s,  707.52/s  (0.165s,  775.94/s)  LR: 1.111e-06  Data: 0.010 (0.008)Time: 1270.359s\n",
      "Train: 19 [7750/10009 ( 77%)]  Loss: 3.04 (2.91)  Time: 0.161s,  796.48/s  (0.165s,  775.97/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1278.565s\n",
      "Train: 19 [7800/10009 ( 78%)]  Loss: 3.04 (2.91)  Time: 0.164s,  779.21/s  (0.165s,  776.02/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 1286.729s\n",
      "Train: 19 [7850/10009 ( 78%)]  Loss: 2.89 (2.91)  Time: 0.161s,  794.17/s  (0.165s,  776.10/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1294.844s\n",
      "Train: 19 [7900/10009 ( 79%)]  Loss: 3.17 (2.91)  Time: 0.165s,  776.29/s  (0.165s,  776.13/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 1303.028s\n",
      "Train: 19 [7950/10009 ( 79%)]  Loss: 2.70 (2.91)  Time: 0.165s,  777.21/s  (0.165s,  776.10/s)  LR: 1.111e-06  Data: 0.010 (0.008)Time: 1311.336s\n",
      "Train: 19 [8000/10009 ( 80%)]  Loss: 2.72 (2.91)  Time: 0.161s,  794.36/s  (0.165s,  776.21/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1319.390s\n",
      "Train: 19 [8050/10009 ( 80%)]  Loss: 2.89 (2.91)  Time: 0.161s,  796.13/s  (0.165s,  776.27/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1327.535s\n",
      "Train: 19 [8100/10009 ( 81%)]  Loss: 3.10 (2.91)  Time: 0.161s,  793.67/s  (0.165s,  776.38/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1335.593s\n",
      "Train: 19 [8150/10009 ( 81%)]  Loss: 2.97 (2.91)  Time: 0.160s,  801.02/s  (0.165s,  776.47/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1343.671s\n",
      "Train: 19 [8200/10009 ( 82%)]  Loss: 3.04 (2.91)  Time: 0.162s,  790.54/s  (0.165s,  776.59/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1351.708s\n",
      "Train: 19 [8250/10009 ( 82%)]  Loss: 2.82 (2.91)  Time: 0.161s,  792.93/s  (0.165s,  776.63/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1359.884s\n",
      "Train: 19 [8300/10009 ( 83%)]  Loss: 2.51 (2.91)  Time: 0.170s,  751.71/s  (0.165s,  776.66/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1368.078s\n",
      "Train: 19 [8350/10009 ( 83%)]  Loss: 2.89 (2.91)  Time: 0.163s,  786.44/s  (0.165s,  776.69/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1376.258s\n",
      "Train: 19 [8400/10009 ( 84%)]  Loss: 2.75 (2.91)  Time: 0.161s,  796.32/s  (0.165s,  776.76/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1384.378s\n",
      "Train: 19 [8450/10009 ( 84%)]  Loss: 2.94 (2.91)  Time: 0.162s,  790.33/s  (0.165s,  776.81/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1392.527s\n",
      "Train: 19 [8500/10009 ( 85%)]  Loss: 2.75 (2.91)  Time: 0.163s,  784.15/s  (0.165s,  776.89/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1400.625s\n",
      "Train: 19 [8550/10009 ( 85%)]  Loss: 3.00 (2.91)  Time: 0.160s,  801.35/s  (0.165s,  776.90/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1408.831s\n",
      "Train: 19 [8600/10009 ( 86%)]  Loss: 3.22 (2.91)  Time: 0.159s,  804.81/s  (0.165s,  776.98/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1416.927s\n",
      "Train: 19 [8650/10009 ( 86%)]  Loss: 2.78 (2.91)  Time: 0.167s,  767.04/s  (0.165s,  776.94/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 1425.233s\n",
      "Train: 19 [8700/10009 ( 87%)]  Loss: 2.86 (2.91)  Time: 0.160s,  799.22/s  (0.165s,  776.91/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1433.535s\n",
      "Train: 19 [8750/10009 ( 87%)]  Loss: 2.88 (2.91)  Time: 0.181s,  708.92/s  (0.165s,  776.95/s)  LR: 1.111e-06  Data: 0.011 (0.008)Time: 1441.700s\n",
      "Train: 19 [8800/10009 ( 88%)]  Loss: 2.77 (2.91)  Time: 0.161s,  797.30/s  (0.165s,  776.89/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1450.052s\n",
      "Train: 19 [8850/10009 ( 88%)]  Loss: 2.99 (2.91)  Time: 0.161s,  793.78/s  (0.165s,  776.95/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1458.165s\n",
      "Train: 19 [8900/10009 ( 89%)]  Loss: 2.98 (2.91)  Time: 0.166s,  772.42/s  (0.165s,  776.85/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1466.604s\n",
      "Train: 19 [8950/10009 ( 89%)]  Loss: 2.74 (2.91)  Time: 0.161s,  797.05/s  (0.165s,  776.77/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1474.987s\n",
      "Train: 19 [9000/10009 ( 90%)]  Loss: 2.88 (2.91)  Time: 0.164s,  782.71/s  (0.165s,  776.71/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1483.340s\n",
      "Train: 19 [9050/10009 ( 90%)]  Loss: 3.18 (2.91)  Time: 0.172s,  743.37/s  (0.165s,  776.69/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1491.618s\n",
      "Train: 19 [9100/10009 ( 91%)]  Loss: 2.73 (2.91)  Time: 0.160s,  798.05/s  (0.165s,  776.77/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1499.698s\n",
      "Train: 19 [9150/10009 ( 91%)]  Loss: 3.13 (2.91)  Time: 0.163s,  787.22/s  (0.165s,  776.82/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1507.853s\n",
      "Train: 19 [9200/10009 ( 92%)]  Loss: 2.49 (2.91)  Time: 0.162s,  790.22/s  (0.165s,  776.87/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1515.994s\n",
      "Train: 19 [9250/10009 ( 92%)]  Loss: 2.92 (2.91)  Time: 0.163s,  787.00/s  (0.165s,  776.93/s)  LR: 1.111e-06  Data: 0.008 (0.008)Time: 1524.113s\n",
      "Train: 19 [9300/10009 ( 93%)]  Loss: 2.72 (2.91)  Time: 0.174s,  736.36/s  (0.165s,  776.98/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1532.248s\n",
      "Train: 19 [9350/10009 ( 93%)]  Loss: 2.91 (2.91)  Time: 0.162s,  788.24/s  (0.165s,  776.97/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1540.498s\n",
      "Train: 19 [9400/10009 ( 94%)]  Loss: 2.78 (2.91)  Time: 0.160s,  801.66/s  (0.165s,  776.96/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1548.751s\n",
      "Train: 19 [9450/10009 ( 94%)]  Loss: 2.97 (2.91)  Time: 0.160s,  797.69/s  (0.165s,  776.97/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1556.979s\n",
      "Train: 19 [9500/10009 ( 95%)]  Loss: 2.80 (2.91)  Time: 0.161s,  795.76/s  (0.165s,  777.02/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1565.110s\n",
      "Train: 19 [9550/10009 ( 95%)]  Loss: 2.66 (2.91)  Time: 0.160s,  798.99/s  (0.165s,  777.04/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1573.307s\n",
      "Train: 19 [9600/10009 ( 96%)]  Loss: 3.03 (2.91)  Time: 0.161s,  793.38/s  (0.165s,  777.09/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1581.448s\n",
      "Train: 19 [9650/10009 ( 96%)]  Loss: 2.79 (2.91)  Time: 0.159s,  803.08/s  (0.165s,  777.15/s)  LR: 1.111e-06  Data: 0.005 (0.008)Time: 1589.560s\n",
      "Train: 19 [9700/10009 ( 97%)]  Loss: 2.90 (2.91)  Time: 0.160s,  799.29/s  (0.165s,  777.21/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1597.660s\n",
      "Train: 19 [9750/10009 ( 97%)]  Loss: 2.91 (2.91)  Time: 0.161s,  792.68/s  (0.165s,  777.28/s)  LR: 1.111e-06  Data: 0.007 (0.008)Time: 1605.750s\n",
      "Train: 19 [9800/10009 ( 98%)]  Loss: 2.78 (2.91)  Time: 0.160s,  800.45/s  (0.165s,  777.34/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1613.865s\n",
      "Train: 19 [9850/10009 ( 98%)]  Loss: 3.06 (2.91)  Time: 0.160s,  798.13/s  (0.165s,  777.35/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1622.081s\n",
      "Train: 19 [9900/10009 ( 99%)]  Loss: 3.02 (2.91)  Time: 0.160s,  798.71/s  (0.165s,  777.43/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1630.137s\n",
      "Train: 19 [9950/10009 ( 99%)]  Loss: 2.69 (2.91)  Time: 0.162s,  790.96/s  (0.165s,  777.52/s)  LR: 1.111e-06  Data: 0.006 (0.008)Time: 1638.189s\n",
      "Train: 19 [10000/10009 (100%)]  Loss: 2.86 (2.91)  Time: 0.205s,  625.81/s  (0.165s,  777.54/s)  LR: 1.111e-06  Data: 0.049 (0.008)Time: 1646.382s\n",
      "Test: [   0/390]  Time: 0.806 (0.806)  Loss:   1.032 ( 1.032)  Acc@1:  81.250 ( 81.250)  Acc@5:  92.188 ( 92.188)\n",
      "Test: [  50/390]  Time: 0.052 (0.158)  Loss:   1.062 ( 1.714)  Acc@1:  74.219 ( 62.990)  Acc@5:  94.531 ( 82.567)\n",
      "Test: [ 100/390]  Time: 0.451 (0.152)  Loss:   1.679 ( 1.715)  Acc@1:  60.156 ( 61.030)  Acc@5:  90.625 ( 83.834)\n",
      "Test: [ 150/390]  Time: 0.051 (0.147)  Loss:   1.520 ( 1.688)  Acc@1:  60.156 ( 61.683)  Acc@5:  89.062 ( 84.365)\n",
      "Test: [ 200/390]  Time: 0.055 (0.144)  Loss:   2.798 ( 1.864)  Acc@1:  34.375 ( 58.419)  Acc@5:  68.750 ( 81.643)\n",
      "Test: [ 250/390]  Time: 0.050 (0.154)  Loss:   2.026 ( 1.971)  Acc@1:  60.938 ( 56.655)  Acc@5:  75.781 ( 79.744)\n",
      "Test: [ 300/390]  Time: 0.298 (0.159)  Loss:   2.307 ( 2.064)  Acc@1:  57.031 ( 54.877)  Acc@5:  72.656 ( 78.161)\n",
      "Test: [ 350/390]  Time: 0.335 (0.154)  Loss:   2.355 ( 2.134)  Acc@1:  53.125 ( 53.655)  Acc@5:  74.219 ( 77.079)\n",
      "Test: [ 390/390]  Time: 0.034 (0.152)  Loss:   3.401 ( 2.109)  Acc@1:  28.750 ( 54.076)  Acc@5:  58.750 ( 77.540)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-19.pth.tar', 54.076)\n",
      " ('./output/budgeted/checkpoint-18.pth.tar', 53.728)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 53.226)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 52.608)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 51.51)\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 43.77)\n",
      "\n",
      "Train: 20 [   0/10009 (  0%)]  Loss: 2.80 (2.80)  Time: 0.777s,  164.84/s  (0.777s,  164.84/s)  LR: 2.792e-07  Data: 0.624 (0.624)Time: 0.777s\n",
      "Train: 20 [  50/10009 (  0%)]  Loss: 2.79 (2.88)  Time: 0.171s,  747.64/s  (0.176s,  725.51/s)  LR: 2.792e-07  Data: 0.005 (0.019)Time: 8.998s\n",
      "Train: 20 [ 100/10009 (  1%)]  Loss: 2.53 (2.88)  Time: 0.159s,  805.04/s  (0.169s,  756.90/s)  LR: 2.792e-07  Data: 0.006 (0.013)Time: 17.081s\n",
      "Train: 20 [ 150/10009 (  1%)]  Loss: 2.81 (2.89)  Time: 0.161s,  794.58/s  (0.166s,  771.06/s)  LR: 2.792e-07  Data: 0.006 (0.010)Time: 25.067s\n",
      "Train: 20 [ 200/10009 (  2%)]  Loss: 2.96 (2.90)  Time: 0.162s,  790.62/s  (0.165s,  777.22/s)  LR: 2.792e-07  Data: 0.007 (0.009)Time: 33.103s\n",
      "Train: 20 [ 250/10009 (  2%)]  Loss: 2.87 (2.89)  Time: 0.161s,  792.88/s  (0.164s,  780.77/s)  LR: 2.792e-07  Data: 0.008 (0.009)Time: 41.150s\n",
      "Train: 20 [ 300/10009 (  3%)]  Loss: 2.97 (2.89)  Time: 0.159s,  806.15/s  (0.164s,  781.96/s)  LR: 2.792e-07  Data: 0.006 (0.008)Time: 49.272s\n",
      "Train: 20 [ 350/10009 (  3%)]  Loss: 2.90 (2.89)  Time: 0.161s,  795.65/s  (0.163s,  784.20/s)  LR: 2.792e-07  Data: 0.007 (0.008)Time: 57.292s\n",
      "Train: 20 [ 400/10009 (  4%)]  Loss: 2.95 (2.89)  Time: 0.162s,  789.56/s  (0.163s,  785.91/s)  LR: 2.792e-07  Data: 0.006 (0.008)Time: 65.311s\n",
      "Train: 20 [ 450/10009 (  4%)]  Loss: 2.74 (2.89)  Time: 0.159s,  805.76/s  (0.163s,  787.15/s)  LR: 2.792e-07  Data: 0.005 (0.008)Time: 73.338s\n",
      "Train: 20 [ 500/10009 (  5%)]  Loss: 2.97 (2.89)  Time: 0.160s,  799.32/s  (0.162s,  788.01/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 81.380s\n",
      "Train: 20 [ 550/10009 (  5%)]  Loss: 2.87 (2.89)  Time: 0.163s,  786.20/s  (0.162s,  788.74/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 89.419s\n",
      "Train: 20 [ 600/10009 (  6%)]  Loss: 2.98 (2.90)  Time: 0.160s,  799.79/s  (0.162s,  789.11/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 97.487s\n",
      "Train: 20 [ 650/10009 (  6%)]  Loss: 3.01 (2.90)  Time: 0.161s,  794.94/s  (0.162s,  789.35/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 105.566s\n",
      "Train: 20 [ 700/10009 (  7%)]  Loss: 2.76 (2.90)  Time: 0.161s,  794.66/s  (0.162s,  788.98/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 113.728s\n",
      "Train: 20 [ 750/10009 (  7%)]  Loss: 2.98 (2.90)  Time: 0.161s,  796.69/s  (0.162s,  788.12/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 121.972s\n",
      "Train: 20 [ 800/10009 (  8%)]  Loss: 2.74 (2.90)  Time: 0.168s,  763.76/s  (0.163s,  787.21/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 130.242s\n",
      "Train: 20 [ 850/10009 (  8%)]  Loss: 2.84 (2.90)  Time: 0.167s,  766.55/s  (0.163s,  786.93/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 138.422s\n",
      "Train: 20 [ 900/10009 (  9%)]  Loss: 3.05 (2.90)  Time: 0.161s,  796.87/s  (0.163s,  787.06/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 146.531s\n",
      "Train: 20 [ 950/10009 (  9%)]  Loss: 2.74 (2.90)  Time: 0.161s,  797.01/s  (0.163s,  787.43/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 154.588s\n",
      "Train: 20 [1000/10009 ( 10%)]  Loss: 2.97 (2.90)  Time: 0.160s,  797.78/s  (0.162s,  787.81/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 162.638s\n",
      "Train: 20 [1050/10009 ( 10%)]  Loss: 2.77 (2.90)  Time: 0.163s,  785.45/s  (0.163s,  787.58/s)  LR: 2.792e-07  Data: 0.009 (0.007)Time: 170.811s\n",
      "Train: 20 [1100/10009 ( 11%)]  Loss: 2.77 (2.90)  Time: 0.162s,  789.98/s  (0.163s,  787.23/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 179.017s\n",
      "Train: 20 [1150/10009 ( 11%)]  Loss: 2.64 (2.90)  Time: 0.160s,  800.56/s  (0.163s,  787.30/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 187.131s\n",
      "Train: 20 [1200/10009 ( 12%)]  Loss: 2.57 (2.90)  Time: 0.170s,  752.55/s  (0.163s,  786.68/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 195.413s\n",
      "Train: 20 [1250/10009 ( 12%)]  Loss: 2.81 (2.90)  Time: 0.174s,  734.13/s  (0.163s,  786.33/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 203.639s\n",
      "Train: 20 [1300/10009 ( 13%)]  Loss: 2.73 (2.90)  Time: 0.175s,  732.02/s  (0.163s,  785.20/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 212.084s\n",
      "Train: 20 [1350/10009 ( 13%)]  Loss: 2.72 (2.90)  Time: 0.161s,  795.12/s  (0.163s,  784.02/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 220.566s\n",
      "Train: 20 [1400/10009 ( 14%)]  Loss: 2.90 (2.90)  Time: 0.162s,  791.52/s  (0.163s,  783.72/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 228.816s\n",
      "Train: 20 [1450/10009 ( 14%)]  Loss: 2.91 (2.90)  Time: 0.160s,  798.80/s  (0.163s,  783.89/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 236.932s\n",
      "Train: 20 [1500/10009 ( 15%)]  Loss: 2.78 (2.90)  Time: 0.163s,  786.77/s  (0.163s,  784.20/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 244.999s\n",
      "Train: 20 [1550/10009 ( 15%)]  Loss: 2.70 (2.90)  Time: 0.160s,  800.62/s  (0.163s,  784.44/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 253.083s\n",
      "Train: 20 [1600/10009 ( 16%)]  Loss: 2.71 (2.90)  Time: 0.162s,  792.55/s  (0.163s,  784.68/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 261.160s\n",
      "Train: 20 [1650/10009 ( 16%)]  Loss: 2.91 (2.90)  Time: 0.161s,  795.59/s  (0.163s,  784.78/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 269.284s\n",
      "Train: 20 [1700/10009 ( 17%)]  Loss: 3.02 (2.90)  Time: 0.160s,  801.25/s  (0.163s,  785.09/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 277.328s\n",
      "Train: 20 [1750/10009 ( 17%)]  Loss: 2.74 (2.90)  Time: 0.163s,  787.44/s  (0.163s,  785.37/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 285.380s\n",
      "Train: 20 [1800/10009 ( 18%)]  Loss: 3.11 (2.90)  Time: 0.160s,  801.50/s  (0.163s,  785.66/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 293.419s\n",
      "Train: 20 [1850/10009 ( 18%)]  Loss: 2.84 (2.90)  Time: 0.162s,  791.70/s  (0.163s,  785.82/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 301.503s\n",
      "Train: 20 [1900/10009 ( 19%)]  Loss: 2.95 (2.90)  Time: 0.160s,  798.08/s  (0.163s,  785.91/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 309.614s\n",
      "Train: 20 [1950/10009 ( 19%)]  Loss: 2.90 (2.90)  Time: 0.161s,  793.09/s  (0.163s,  786.12/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 317.669s\n",
      "Train: 20 [2000/10009 ( 20%)]  Loss: 2.64 (2.90)  Time: 0.160s,  798.63/s  (0.163s,  786.39/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 325.700s\n",
      "Train: 20 [2050/10009 ( 20%)]  Loss: 2.76 (2.90)  Time: 0.161s,  796.17/s  (0.163s,  786.59/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 333.756s\n",
      "Train: 20 [2100/10009 ( 21%)]  Loss: 2.62 (2.90)  Time: 0.161s,  795.89/s  (0.163s,  786.83/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 341.788s\n",
      "Train: 20 [2150/10009 ( 21%)]  Loss: 3.19 (2.90)  Time: 0.160s,  798.70/s  (0.163s,  787.06/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 349.820s\n",
      "Train: 20 [2200/10009 ( 22%)]  Loss: 3.05 (2.90)  Time: 0.161s,  797.12/s  (0.163s,  787.21/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 357.883s\n",
      "Train: 20 [2250/10009 ( 22%)]  Loss: 2.96 (2.90)  Time: 0.163s,  786.13/s  (0.163s,  787.41/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 365.917s\n",
      "Train: 20 [2300/10009 ( 23%)]  Loss: 2.71 (2.90)  Time: 0.170s,  752.10/s  (0.163s,  787.14/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 374.173s\n",
      "Train: 20 [2350/10009 ( 23%)]  Loss: 3.05 (2.90)  Time: 0.161s,  794.55/s  (0.163s,  786.95/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 382.398s\n",
      "Train: 20 [2400/10009 ( 24%)]  Loss: 2.97 (2.90)  Time: 0.161s,  793.80/s  (0.163s,  786.90/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 390.557s\n",
      "Train: 20 [2450/10009 ( 24%)]  Loss: 3.04 (2.90)  Time: 0.172s,  742.82/s  (0.163s,  786.72/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 398.777s\n",
      "Train: 20 [2500/10009 ( 25%)]  Loss: 2.94 (2.90)  Time: 0.160s,  800.50/s  (0.163s,  786.50/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 407.030s\n",
      "Train: 20 [2550/10009 ( 25%)]  Loss: 2.88 (2.90)  Time: 0.163s,  783.28/s  (0.163s,  786.46/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 415.186s\n",
      "Train: 20 [2600/10009 ( 26%)]  Loss: 2.94 (2.90)  Time: 0.162s,  788.18/s  (0.163s,  786.54/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 423.282s\n",
      "Train: 20 [2650/10009 ( 26%)]  Loss: 2.77 (2.90)  Time: 0.160s,  800.61/s  (0.163s,  786.50/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 431.442s\n",
      "Train: 20 [2700/10009 ( 27%)]  Loss: 2.72 (2.90)  Time: 0.162s,  789.06/s  (0.163s,  786.64/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 439.498s\n",
      "Train: 20 [2750/10009 ( 27%)]  Loss: 2.80 (2.90)  Time: 0.159s,  803.93/s  (0.163s,  786.77/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 447.560s\n",
      "Train: 20 [2800/10009 ( 28%)]  Loss: 2.90 (2.90)  Time: 0.159s,  804.11/s  (0.163s,  786.93/s)  LR: 2.792e-07  Data: 0.005 (0.006)Time: 455.605s\n",
      "Train: 20 [2850/10009 ( 28%)]  Loss: 2.85 (2.90)  Time: 0.173s,  738.91/s  (0.163s,  786.90/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 463.754s\n",
      "Train: 20 [2900/10009 ( 29%)]  Loss: 2.89 (2.90)  Time: 0.160s,  798.90/s  (0.163s,  786.83/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 471.927s\n",
      "Train: 20 [2950/10009 ( 29%)]  Loss: 2.91 (2.90)  Time: 0.159s,  804.85/s  (0.163s,  786.98/s)  LR: 2.792e-07  Data: 0.005 (0.006)Time: 479.971s\n",
      "Train: 20 [3000/10009 ( 30%)]  Loss: 2.96 (2.90)  Time: 0.160s,  800.61/s  (0.163s,  787.13/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 488.011s\n",
      "Train: 20 [3050/10009 ( 30%)]  Loss: 2.85 (2.90)  Time: 0.159s,  804.57/s  (0.163s,  787.27/s)  LR: 2.792e-07  Data: 0.005 (0.006)Time: 496.053s\n",
      "Train: 20 [3100/10009 ( 31%)]  Loss: 3.11 (2.90)  Time: 0.160s,  800.64/s  (0.163s,  787.40/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 504.100s\n",
      "Train: 20 [3150/10009 ( 31%)]  Loss: 2.78 (2.90)  Time: 0.160s,  798.37/s  (0.163s,  787.42/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 512.213s\n",
      "Train: 20 [3200/10009 ( 32%)]  Loss: 3.00 (2.90)  Time: 0.162s,  788.29/s  (0.163s,  787.48/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 520.305s\n",
      "Train: 20 [3250/10009 ( 32%)]  Loss: 2.97 (2.90)  Time: 0.164s,  781.19/s  (0.163s,  787.58/s)  LR: 2.792e-07  Data: 0.007 (0.006)Time: 528.363s\n",
      "Train: 20 [3300/10009 ( 33%)]  Loss: 2.89 (2.90)  Time: 0.161s,  797.04/s  (0.163s,  787.62/s)  LR: 2.792e-07  Data: 0.005 (0.006)Time: 536.461s\n",
      "Train: 20 [3350/10009 ( 33%)]  Loss: 2.91 (2.90)  Time: 0.161s,  796.63/s  (0.163s,  787.59/s)  LR: 2.792e-07  Data: 0.007 (0.006)Time: 544.606s\n",
      "Train: 20 [3400/10009 ( 34%)]  Loss: 2.86 (2.90)  Time: 0.161s,  792.84/s  (0.163s,  787.67/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 552.675s\n",
      "Train: 20 [3450/10009 ( 34%)]  Loss: 2.89 (2.90)  Time: 0.160s,  800.32/s  (0.162s,  787.74/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 560.755s\n",
      "Train: 20 [3500/10009 ( 35%)]  Loss: 2.70 (2.90)  Time: 0.162s,  789.14/s  (0.162s,  787.75/s)  LR: 2.792e-07  Data: 0.008 (0.006)Time: 568.869s\n",
      "Train: 20 [3550/10009 ( 35%)]  Loss: 2.71 (2.90)  Time: 0.161s,  796.14/s  (0.163s,  787.50/s)  LR: 2.792e-07  Data: 0.007 (0.006)Time: 577.178s\n",
      "Train: 20 [3600/10009 ( 36%)]  Loss: 2.82 (2.90)  Time: 0.160s,  800.97/s  (0.163s,  787.41/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 585.374s\n",
      "Train: 20 [3650/10009 ( 36%)]  Loss: 2.77 (2.90)  Time: 0.162s,  791.81/s  (0.163s,  787.31/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 593.571s\n",
      "Train: 20 [3700/10009 ( 37%)]  Loss: 2.92 (2.90)  Time: 0.173s,  741.60/s  (0.163s,  787.28/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 601.728s\n",
      "Train: 20 [3750/10009 ( 37%)]  Loss: 2.93 (2.90)  Time: 0.160s,  799.64/s  (0.163s,  787.33/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 609.813s\n",
      "Train: 20 [3800/10009 ( 38%)]  Loss: 2.86 (2.90)  Time: 0.173s,  739.25/s  (0.163s,  787.38/s)  LR: 2.792e-07  Data: 0.006 (0.006)Time: 617.910s\n",
      "Train: 20 [3850/10009 ( 38%)]  Loss: 3.03 (2.90)  Time: 0.162s,  791.28/s  (0.163s,  786.75/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 626.538s\n",
      "Train: 20 [3900/10009 ( 39%)]  Loss: 2.79 (2.90)  Time: 0.160s,  802.02/s  (0.163s,  786.76/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 634.659s\n",
      "Train: 20 [3950/10009 ( 39%)]  Loss: 2.80 (2.90)  Time: 0.162s,  791.70/s  (0.163s,  786.76/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 642.796s\n",
      "Train: 20 [4000/10009 ( 40%)]  Loss: 2.76 (2.90)  Time: 0.164s,  779.73/s  (0.163s,  786.73/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 650.958s\n",
      "Train: 20 [4050/10009 ( 40%)]  Loss: 2.76 (2.90)  Time: 0.160s,  798.44/s  (0.163s,  786.64/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 659.163s\n",
      "Train: 20 [4100/10009 ( 41%)]  Loss: 2.78 (2.90)  Time: 0.160s,  799.74/s  (0.163s,  786.61/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 667.332s\n",
      "Train: 20 [4150/10009 ( 41%)]  Loss: 3.02 (2.90)  Time: 0.161s,  794.16/s  (0.163s,  786.44/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 675.607s\n",
      "Train: 20 [4200/10009 ( 42%)]  Loss: 2.94 (2.90)  Time: 0.163s,  784.42/s  (0.163s,  786.52/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 683.678s\n",
      "Train: 20 [4250/10009 ( 42%)]  Loss: 2.61 (2.90)  Time: 0.161s,  795.67/s  (0.163s,  786.60/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 691.749s\n",
      "Train: 20 [4300/10009 ( 43%)]  Loss: 2.82 (2.90)  Time: 0.162s,  791.46/s  (0.163s,  786.70/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 699.796s\n",
      "Train: 20 [4350/10009 ( 43%)]  Loss: 2.88 (2.90)  Time: 0.524s,  244.43/s  (0.163s,  784.28/s)  LR: 2.792e-07  Data: 0.371 (0.007)Time: 710.114s\n",
      "Train: 20 [4400/10009 ( 44%)]  Loss: 2.74 (2.90)  Time: 0.162s,  788.12/s  (0.164s,  782.79/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 719.644s\n",
      "Train: 20 [4450/10009 ( 44%)]  Loss: 2.91 (2.90)  Time: 0.160s,  798.28/s  (0.163s,  782.93/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 727.686s\n",
      "Train: 20 [4500/10009 ( 45%)]  Loss: 2.94 (2.90)  Time: 0.159s,  803.17/s  (0.163s,  783.08/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 735.715s\n",
      "Train: 20 [4550/10009 ( 45%)]  Loss: 3.14 (2.90)  Time: 0.163s,  784.21/s  (0.163s,  783.20/s)  LR: 2.792e-07  Data: 0.009 (0.007)Time: 743.778s\n",
      "Train: 20 [4600/10009 ( 46%)]  Loss: 3.06 (2.90)  Time: 0.159s,  805.24/s  (0.163s,  783.24/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 751.907s\n",
      "Train: 20 [4650/10009 ( 46%)]  Loss: 2.78 (2.90)  Time: 0.160s,  798.27/s  (0.163s,  783.28/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 760.041s\n",
      "Train: 20 [4700/10009 ( 47%)]  Loss: 3.04 (2.90)  Time: 0.160s,  801.65/s  (0.163s,  783.22/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 768.275s\n",
      "Train: 20 [4750/10009 ( 47%)]  Loss: 2.99 (2.90)  Time: 0.176s,  728.00/s  (0.163s,  782.98/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 776.681s\n",
      "Train: 20 [4800/10009 ( 48%)]  Loss: 2.78 (2.90)  Time: 0.172s,  742.69/s  (0.164s,  782.58/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 785.255s\n",
      "Train: 20 [4850/10009 ( 48%)]  Loss: 2.75 (2.90)  Time: 0.163s,  786.22/s  (0.164s,  782.50/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 793.516s\n",
      "Train: 20 [4900/10009 ( 49%)]  Loss: 2.81 (2.90)  Time: 0.161s,  793.78/s  (0.164s,  782.47/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 801.727s\n",
      "Train: 20 [4950/10009 ( 49%)]  Loss: 2.86 (2.90)  Time: 0.167s,  767.19/s  (0.164s,  782.50/s)  LR: 2.792e-07  Data: 0.010 (0.007)Time: 809.870s\n",
      "Train: 20 [5000/10009 ( 50%)]  Loss: 3.05 (2.90)  Time: 0.161s,  793.92/s  (0.164s,  782.62/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 817.925s\n",
      "Train: 20 [5050/10009 ( 50%)]  Loss: 2.63 (2.90)  Time: 0.162s,  788.80/s  (0.164s,  782.69/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 826.029s\n",
      "Train: 20 [5100/10009 ( 51%)]  Loss: 2.99 (2.90)  Time: 0.163s,  784.50/s  (0.164s,  782.79/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 834.099s\n",
      "Train: 20 [5150/10009 ( 51%)]  Loss: 3.31 (2.90)  Time: 0.163s,  785.39/s  (0.164s,  782.86/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 842.202s\n",
      "Train: 20 [5200/10009 ( 52%)]  Loss: 2.78 (2.90)  Time: 0.160s,  800.04/s  (0.163s,  782.88/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 850.360s\n",
      "Train: 20 [5250/10009 ( 52%)]  Loss: 2.90 (2.90)  Time: 0.161s,  795.04/s  (0.163s,  782.96/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 858.448s\n",
      "Train: 20 [5300/10009 ( 53%)]  Loss: 3.09 (2.90)  Time: 0.159s,  805.01/s  (0.163s,  783.02/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 866.549s\n",
      "Train: 20 [5350/10009 ( 53%)]  Loss: 2.87 (2.90)  Time: 0.159s,  802.84/s  (0.163s,  783.14/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 874.591s\n",
      "Train: 20 [5400/10009 ( 54%)]  Loss: 2.83 (2.90)  Time: 0.162s,  788.27/s  (0.163s,  783.23/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 882.655s\n",
      "Train: 20 [5450/10009 ( 54%)]  Loss: 3.27 (2.90)  Time: 0.160s,  801.38/s  (0.163s,  783.32/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 890.735s\n",
      "Train: 20 [5500/10009 ( 55%)]  Loss: 2.99 (2.90)  Time: 0.160s,  798.15/s  (0.163s,  783.40/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 898.811s\n",
      "Train: 20 [5550/10009 ( 55%)]  Loss: 2.95 (2.90)  Time: 0.170s,  752.44/s  (0.163s,  783.41/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 906.968s\n",
      "Train: 20 [5600/10009 ( 56%)]  Loss: 3.03 (2.90)  Time: 0.160s,  799.51/s  (0.163s,  783.36/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 915.191s\n",
      "Train: 20 [5650/10009 ( 56%)]  Loss: 2.95 (2.90)  Time: 0.161s,  796.66/s  (0.163s,  783.46/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 923.246s\n",
      "Train: 20 [5700/10009 ( 57%)]  Loss: 2.73 (2.90)  Time: 0.161s,  796.82/s  (0.163s,  783.56/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 931.293s\n",
      "Train: 20 [5750/10009 ( 57%)]  Loss: 3.37 (2.90)  Time: 0.162s,  792.13/s  (0.163s,  783.66/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 939.346s\n",
      "Train: 20 [5800/10009 ( 58%)]  Loss: 2.82 (2.90)  Time: 0.163s,  784.66/s  (0.163s,  783.67/s)  LR: 2.792e-07  Data: 0.009 (0.007)Time: 947.502s\n",
      "Train: 20 [5850/10009 ( 58%)]  Loss: 2.95 (2.90)  Time: 0.171s,  748.07/s  (0.163s,  783.57/s)  LR: 2.792e-07  Data: 0.014 (0.007)Time: 955.789s\n",
      "Train: 20 [5900/10009 ( 59%)]  Loss: 2.83 (2.90)  Time: 0.160s,  801.19/s  (0.163s,  783.57/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 963.960s\n",
      "Train: 20 [5950/10009 ( 59%)]  Loss: 3.03 (2.90)  Time: 0.160s,  800.10/s  (0.163s,  783.67/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 971.996s\n",
      "Train: 20 [6000/10009 ( 60%)]  Loss: 2.91 (2.89)  Time: 0.162s,  791.35/s  (0.163s,  783.78/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 980.026s\n",
      "Train: 20 [6050/10009 ( 60%)]  Loss: 2.98 (2.89)  Time: 0.160s,  800.70/s  (0.163s,  783.88/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 988.063s\n",
      "Train: 20 [6100/10009 ( 61%)]  Loss: 3.07 (2.89)  Time: 0.165s,  777.53/s  (0.163s,  783.95/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 996.140s\n",
      "Train: 20 [6150/10009 ( 61%)]  Loss: 2.84 (2.90)  Time: 0.161s,  796.47/s  (0.163s,  783.99/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1004.253s\n",
      "Train: 20 [6200/10009 ( 62%)]  Loss: 3.18 (2.90)  Time: 0.160s,  798.45/s  (0.163s,  783.69/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1012.801s\n",
      "Train: 20 [6250/10009 ( 62%)]  Loss: 2.97 (2.90)  Time: 0.162s,  791.52/s  (0.163s,  783.74/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1020.904s\n",
      "Train: 20 [6300/10009 ( 63%)]  Loss: 2.79 (2.90)  Time: 0.215s,  594.12/s  (0.163s,  783.75/s)  LR: 2.792e-07  Data: 0.063 (0.007)Time: 1029.056s\n",
      "Train: 20 [6350/10009 ( 63%)]  Loss: 2.83 (2.90)  Time: 0.165s,  776.64/s  (0.163s,  783.83/s)  LR: 2.792e-07  Data: 0.010 (0.007)Time: 1037.120s\n",
      "Train: 20 [6400/10009 ( 64%)]  Loss: 2.91 (2.90)  Time: 0.161s,  796.42/s  (0.163s,  783.86/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1045.241s\n",
      "Train: 20 [6450/10009 ( 64%)]  Loss: 2.73 (2.90)  Time: 0.163s,  784.04/s  (0.163s,  783.93/s)  LR: 2.792e-07  Data: 0.009 (0.007)Time: 1053.322s\n",
      "Train: 20 [6500/10009 ( 65%)]  Loss: 3.12 (2.90)  Time: 0.168s,  762.80/s  (0.163s,  783.98/s)  LR: 2.792e-07  Data: 0.012 (0.007)Time: 1061.414s\n",
      "Train: 20 [6550/10009 ( 65%)]  Loss: 2.94 (2.90)  Time: 0.161s,  796.44/s  (0.163s,  783.93/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1069.640s\n",
      "Train: 20 [6600/10009 ( 66%)]  Loss: 2.85 (2.90)  Time: 0.160s,  798.39/s  (0.163s,  783.61/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1078.246s\n",
      "Train: 20 [6650/10009 ( 66%)]  Loss: 2.60 (2.90)  Time: 0.161s,  795.04/s  (0.163s,  783.63/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1086.391s\n",
      "Train: 20 [6700/10009 ( 67%)]  Loss: 2.65 (2.90)  Time: 0.160s,  797.87/s  (0.163s,  783.70/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1094.452s\n",
      "Train: 20 [6750/10009 ( 67%)]  Loss: 3.05 (2.89)  Time: 0.161s,  795.42/s  (0.163s,  783.76/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1102.540s\n",
      "Train: 20 [6800/10009 ( 68%)]  Loss: 3.19 (2.90)  Time: 0.161s,  794.52/s  (0.163s,  783.53/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1111.031s\n",
      "Train: 20 [6850/10009 ( 68%)]  Loss: 2.73 (2.90)  Time: 0.168s,  759.97/s  (0.163s,  783.60/s)  LR: 2.792e-07  Data: 0.015 (0.007)Time: 1119.098s\n",
      "Train: 20 [6900/10009 ( 69%)]  Loss: 3.11 (2.90)  Time: 0.160s,  801.91/s  (0.163s,  783.68/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1127.146s\n",
      "Train: 20 [6950/10009 ( 69%)]  Loss: 3.00 (2.89)  Time: 0.177s,  721.98/s  (0.163s,  783.53/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1135.542s\n",
      "Train: 20 [7000/10009 ( 70%)]  Loss: 2.91 (2.89)  Time: 0.162s,  788.70/s  (0.163s,  783.49/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1143.762s\n",
      "Train: 20 [7050/10009 ( 70%)]  Loss: 2.87 (2.89)  Time: 0.162s,  792.33/s  (0.163s,  783.56/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1151.824s\n",
      "Train: 20 [7100/10009 ( 71%)]  Loss: 2.65 (2.89)  Time: 0.161s,  796.89/s  (0.163s,  783.66/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1159.853s\n",
      "Train: 20 [7150/10009 ( 71%)]  Loss: 2.79 (2.89)  Time: 0.163s,  784.92/s  (0.163s,  783.75/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1167.885s\n",
      "Train: 20 [7200/10009 ( 72%)]  Loss: 2.92 (2.89)  Time: 0.160s,  799.38/s  (0.163s,  783.84/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1175.915s\n",
      "Train: 20 [7250/10009 ( 72%)]  Loss: 2.87 (2.89)  Time: 0.160s,  799.74/s  (0.163s,  783.74/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1184.220s\n",
      "Train: 20 [7300/10009 ( 73%)]  Loss: 2.75 (2.89)  Time: 0.160s,  798.55/s  (0.163s,  783.80/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1192.299s\n",
      "Train: 20 [7350/10009 ( 73%)]  Loss: 3.07 (2.89)  Time: 0.160s,  799.24/s  (0.163s,  783.90/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1200.307s\n",
      "Train: 20 [7400/10009 ( 74%)]  Loss: 2.81 (2.89)  Time: 0.162s,  789.24/s  (0.163s,  784.00/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1208.324s\n",
      "Train: 20 [7450/10009 ( 74%)]  Loss: 2.75 (2.89)  Time: 0.162s,  788.41/s  (0.163s,  784.08/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1216.369s\n",
      "Train: 20 [7500/10009 ( 75%)]  Loss: 3.13 (2.89)  Time: 0.160s,  801.65/s  (0.163s,  784.17/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1224.389s\n",
      "Train: 20 [7550/10009 ( 75%)]  Loss: 3.06 (2.89)  Time: 0.161s,  795.30/s  (0.163s,  784.26/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1232.407s\n",
      "Train: 20 [7600/10009 ( 76%)]  Loss: 3.14 (2.89)  Time: 0.160s,  798.34/s  (0.163s,  784.33/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1240.454s\n",
      "Train: 20 [7650/10009 ( 76%)]  Loss: 2.64 (2.89)  Time: 0.162s,  791.88/s  (0.163s,  784.39/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1248.525s\n",
      "Train: 20 [7700/10009 ( 77%)]  Loss: 2.94 (2.89)  Time: 0.160s,  800.92/s  (0.163s,  784.42/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1256.625s\n",
      "Train: 20 [7750/10009 ( 77%)]  Loss: 2.97 (2.89)  Time: 0.159s,  805.11/s  (0.163s,  784.31/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1264.972s\n",
      "Train: 20 [7800/10009 ( 78%)]  Loss: 2.93 (2.89)  Time: 0.160s,  800.35/s  (0.163s,  784.33/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1273.101s\n",
      "Train: 20 [7850/10009 ( 78%)]  Loss: 3.03 (2.89)  Time: 0.160s,  798.93/s  (0.163s,  784.35/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1281.225s\n",
      "Train: 20 [7900/10009 ( 79%)]  Loss: 2.77 (2.89)  Time: 0.160s,  799.94/s  (0.163s,  784.18/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1289.664s\n",
      "Train: 20 [7950/10009 ( 79%)]  Loss: 2.71 (2.89)  Time: 0.160s,  800.47/s  (0.163s,  784.26/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1297.688s\n",
      "Train: 20 [8000/10009 ( 80%)]  Loss: 2.86 (2.89)  Time: 0.160s,  800.19/s  (0.163s,  784.35/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1305.705s\n",
      "Train: 20 [8050/10009 ( 80%)]  Loss: 3.14 (2.89)  Time: 0.160s,  799.60/s  (0.163s,  784.42/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1313.738s\n",
      "Train: 20 [8100/10009 ( 81%)]  Loss: 2.76 (2.89)  Time: 0.163s,  787.50/s  (0.163s,  784.49/s)  LR: 2.792e-07  Data: 0.009 (0.007)Time: 1321.781s\n",
      "Train: 20 [8150/10009 ( 81%)]  Loss: 3.00 (2.89)  Time: 0.168s,  761.99/s  (0.163s,  784.54/s)  LR: 2.792e-07  Data: 0.013 (0.007)Time: 1329.864s\n",
      "Train: 20 [8200/10009 ( 82%)]  Loss: 2.72 (2.89)  Time: 0.160s,  799.96/s  (0.163s,  784.60/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1337.917s\n",
      "Train: 20 [8250/10009 ( 82%)]  Loss: 2.72 (2.89)  Time: 0.161s,  792.85/s  (0.163s,  784.64/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1346.002s\n",
      "Train: 20 [8300/10009 ( 83%)]  Loss: 3.15 (2.89)  Time: 0.160s,  801.39/s  (0.163s,  784.71/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1354.029s\n",
      "Train: 20 [8350/10009 ( 83%)]  Loss: 2.93 (2.89)  Time: 0.160s,  800.62/s  (0.163s,  784.78/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1362.073s\n",
      "Train: 20 [8400/10009 ( 84%)]  Loss: 3.21 (2.89)  Time: 0.160s,  799.75/s  (0.163s,  784.83/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1370.140s\n",
      "Train: 20 [8450/10009 ( 84%)]  Loss: 2.94 (2.89)  Time: 0.160s,  797.74/s  (0.163s,  784.88/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1378.197s\n",
      "Train: 20 [8500/10009 ( 85%)]  Loss: 2.85 (2.89)  Time: 0.159s,  802.52/s  (0.163s,  784.95/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1386.243s\n",
      "Train: 20 [8550/10009 ( 85%)]  Loss: 2.95 (2.89)  Time: 0.163s,  785.24/s  (0.163s,  784.95/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1394.380s\n",
      "Train: 20 [8600/10009 ( 86%)]  Loss: 2.89 (2.89)  Time: 0.160s,  801.39/s  (0.163s,  785.02/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1402.420s\n",
      "Train: 20 [8650/10009 ( 86%)]  Loss: 2.97 (2.89)  Time: 0.159s,  805.00/s  (0.163s,  785.09/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1410.438s\n",
      "Train: 20 [8700/10009 ( 87%)]  Loss: 2.87 (2.89)  Time: 0.160s,  799.66/s  (0.163s,  785.16/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1418.460s\n",
      "Train: 20 [8750/10009 ( 87%)]  Loss: 2.84 (2.89)  Time: 0.161s,  794.36/s  (0.163s,  785.23/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1426.484s\n",
      "Train: 20 [8800/10009 ( 88%)]  Loss: 3.09 (2.89)  Time: 0.160s,  798.53/s  (0.163s,  785.30/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1434.523s\n",
      "Train: 20 [8850/10009 ( 88%)]  Loss: 3.00 (2.89)  Time: 0.162s,  790.70/s  (0.163s,  785.36/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1442.548s\n",
      "Train: 20 [8900/10009 ( 89%)]  Loss: 2.94 (2.89)  Time: 0.163s,  784.23/s  (0.163s,  785.43/s)  LR: 2.792e-07  Data: 0.009 (0.007)Time: 1450.575s\n",
      "Train: 20 [8950/10009 ( 89%)]  Loss: 2.70 (2.89)  Time: 0.161s,  796.33/s  (0.163s,  785.46/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1458.673s\n",
      "Train: 20 [9000/10009 ( 90%)]  Loss: 2.65 (2.89)  Time: 0.159s,  803.46/s  (0.163s,  785.49/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1466.762s\n",
      "Train: 20 [9050/10009 ( 90%)]  Loss: 2.83 (2.89)  Time: 0.160s,  798.84/s  (0.163s,  785.52/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1474.848s\n",
      "Train: 20 [9100/10009 ( 91%)]  Loss: 2.81 (2.89)  Time: 0.160s,  801.75/s  (0.163s,  785.55/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1482.933s\n",
      "Train: 20 [9150/10009 ( 91%)]  Loss: 3.06 (2.89)  Time: 0.162s,  789.90/s  (0.163s,  785.61/s)  LR: 2.792e-07  Data: 0.008 (0.007)Time: 1490.965s\n",
      "Train: 20 [9200/10009 ( 92%)]  Loss: 3.03 (2.89)  Time: 0.160s,  802.00/s  (0.163s,  785.66/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1499.024s\n",
      "Train: 20 [9250/10009 ( 92%)]  Loss: 2.78 (2.89)  Time: 0.161s,  796.80/s  (0.163s,  785.72/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1507.063s\n",
      "Train: 20 [9300/10009 ( 93%)]  Loss: 2.99 (2.89)  Time: 0.160s,  799.79/s  (0.163s,  785.78/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1515.088s\n",
      "Train: 20 [9350/10009 ( 93%)]  Loss: 2.76 (2.89)  Time: 0.160s,  799.78/s  (0.163s,  785.84/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1523.116s\n",
      "Train: 20 [9400/10009 ( 94%)]  Loss: 2.79 (2.89)  Time: 0.160s,  801.46/s  (0.163s,  785.89/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1531.153s\n",
      "Train: 20 [9450/10009 ( 94%)]  Loss: 2.90 (2.89)  Time: 0.159s,  803.15/s  (0.163s,  785.90/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1539.294s\n",
      "Train: 20 [9500/10009 ( 95%)]  Loss: 2.89 (2.89)  Time: 0.160s,  798.41/s  (0.163s,  785.94/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1547.345s\n",
      "Train: 20 [9550/10009 ( 95%)]  Loss: 2.73 (2.89)  Time: 0.162s,  789.91/s  (0.163s,  785.99/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1555.399s\n",
      "Train: 20 [9600/10009 ( 96%)]  Loss: 3.00 (2.89)  Time: 0.160s,  799.97/s  (0.163s,  786.04/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1563.441s\n",
      "Train: 20 [9650/10009 ( 96%)]  Loss: 2.94 (2.89)  Time: 0.160s,  799.71/s  (0.163s,  786.08/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1571.506s\n",
      "Train: 20 [9700/10009 ( 97%)]  Loss: 2.85 (2.89)  Time: 0.161s,  794.29/s  (0.163s,  786.09/s)  LR: 2.792e-07  Data: 0.007 (0.007)Time: 1579.612s\n",
      "Train: 20 [9750/10009 ( 97%)]  Loss: 2.82 (2.89)  Time: 0.160s,  797.92/s  (0.163s,  786.16/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1587.626s\n",
      "Train: 20 [9800/10009 ( 98%)]  Loss: 2.92 (2.89)  Time: 0.160s,  798.54/s  (0.163s,  786.20/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1595.672s\n",
      "Train: 20 [9850/10009 ( 98%)]  Loss: 2.82 (2.89)  Time: 0.161s,  795.77/s  (0.163s,  786.24/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1603.741s\n",
      "Train: 20 [9900/10009 ( 99%)]  Loss: 2.67 (2.89)  Time: 0.160s,  797.86/s  (0.163s,  786.29/s)  LR: 2.792e-07  Data: 0.006 (0.007)Time: 1611.776s\n",
      "Train: 20 [9950/10009 ( 99%)]  Loss: 2.67 (2.89)  Time: 0.160s,  802.09/s  (0.163s,  786.35/s)  LR: 2.792e-07  Data: 0.005 (0.007)Time: 1619.796s\n",
      "Train: 20 [10000/10009 (100%)]  Loss: 3.02 (2.89)  Time: 0.202s,  634.45/s  (0.163s,  786.36/s)  LR: 2.792e-07  Data: 0.048 (0.007)Time: 1627.908s\n",
      "Test: [   0/390]  Time: 0.740 (0.740)  Loss:   1.016 ( 1.016)  Acc@1:  81.250 ( 81.250)  Acc@5:  92.188 ( 92.188)\n",
      "Test: [  50/390]  Time: 0.052 (0.154)  Loss:   1.030 ( 1.710)  Acc@1:  75.781 ( 63.235)  Acc@5:  95.312 ( 82.567)\n",
      "Test: [ 100/390]  Time: 0.310 (0.145)  Loss:   1.656 ( 1.706)  Acc@1:  63.281 ( 61.378)  Acc@5:  91.406 ( 84.042)\n",
      "Test: [ 150/390]  Time: 0.052 (0.145)  Loss:   1.531 ( 1.679)  Acc@1:  60.156 ( 62.065)  Acc@5:  87.500 ( 84.520)\n",
      "Test: [ 200/390]  Time: 0.053 (0.143)  Loss:   2.812 ( 1.858)  Acc@1:  33.594 ( 58.629)  Acc@5:  68.750 ( 81.751)\n",
      "Test: [ 250/390]  Time: 0.052 (0.142)  Loss:   2.068 ( 1.966)  Acc@1:  61.719 ( 56.873)  Acc@5:  77.344 ( 79.859)\n",
      "Test: [ 300/390]  Time: 0.406 (0.141)  Loss:   2.274 ( 2.057)  Acc@1:  57.031 ( 55.069)  Acc@5:  71.875 ( 78.317)\n",
      "Test: [ 350/390]  Time: 0.058 (0.141)  Loss:   2.358 ( 2.128)  Acc@1:  53.125 ( 53.828)  Acc@5:  74.219 ( 77.195)\n",
      "Test: [ 390/390]  Time: 0.034 (0.140)  Loss:   3.344 ( 2.103)  Acc@1:  30.000 ( 54.244)  Acc@5:  57.500 ( 77.634)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-20.pth.tar', 54.244)\n",
      " ('./output/budgeted/checkpoint-19.pth.tar', 54.076)\n",
      " ('./output/budgeted/checkpoint-18.pth.tar', 53.728)\n",
      " ('./output/budgeted/checkpoint-17.pth.tar', 53.226)\n",
      " ('./output/budgeted/checkpoint-16.pth.tar', 52.608)\n",
      " ('./output/budgeted/checkpoint-15.pth.tar', 51.51)\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 50.648)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 49.058)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.984)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 45.678)\n",
      "\n",
      "*** Best metric: 54.244 (epoch 20)\n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        for epoch in range(10, 21):\n",
    "            if hasattr(dataset_train, 'set_epoch'):\n",
    "                dataset_train.set_epoch(epoch)\n",
    "            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n",
    "                loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                train_loss_fn,\n",
    "                args,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                saver=saver,\n",
    "                output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast,\n",
    "                loss_scaler=loss_scaler,\n",
    "                model_ema=model_ema,\n",
    "                mixup_fn=mixup_fn,\n",
    "                # fish: add preconditioner\n",
    "                preconditioner=preconditioner,\n",
    "            )\n",
    "\n",
    "            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                if utils.is_primary(args):\n",
    "                    _logger.info(\"Distributing BatchNorm running means and vars\")\n",
    "                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "            eval_metrics = validate(\n",
    "                model,\n",
    "                loader_eval,\n",
    "                validate_loss_fn,\n",
    "                args,\n",
    "                amp_autocast=amp_autocast,\n",
    "            )\n",
    "\n",
    "            if model_ema is not None and not args.model_ema_force_cpu:\n",
    "                if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                    utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "                ema_eval_metrics = validate(\n",
    "                    model_ema.module,\n",
    "                    loader_eval,\n",
    "                    validate_loss_fn,\n",
    "                    args,\n",
    "                    amp_autocast=amp_autocast,\n",
    "                    log_suffix=' (EMA)',\n",
    "                )\n",
    "                eval_metrics = ema_eval_metrics\n",
    "\n",
    "            if output_dir is not None:\n",
    "                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "                utils.update_summary(\n",
    "                    epoch,\n",
    "                    train_metrics,\n",
    "                    eval_metrics,\n",
    "                    filename=os.path.join(output_dir, 'summary.csv'),\n",
    "                    lr=sum(lrs) / len(lrs),\n",
    "                    write_header=best_metric is None,\n",
    "                    log_wandb=args.log_wandb and has_wandb,\n",
    "                )\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lion (\n",
       "Parameter Group 0\n",
       "    betas: (0.9, 0.99)\n",
       "    foreach: True\n",
       "    initial_lr: 5e-05\n",
       "    lr: 0\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       "\n",
       "Parameter Group 1\n",
       "    betas: (0.9, 0.99)\n",
       "    foreach: True\n",
       "    initial_lr: 5e-05\n",
       "    lr: 0\n",
       "    maximize: False\n",
       "    weight_decay: 0.001\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
