{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本笔记希望复现 huang gao 组的 budgeted training of vision transformers。我们基于已有的 train.py 进行修改。\n",
    "\n",
    "首先是必要的头文件和一些 args 的处理。可以折叠该部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" ImageNet Training Script\n",
    "\n",
    "This is intended to be a lean and easily modifiable ImageNet training script that reproduces ImageNet\n",
    "training results with some of the latest networks and training techniques. It favours canonical PyTorch\n",
    "and standard Python style over trying to be able to 'do it all.' That said, it offers quite a few speed\n",
    "and training result improvements over the usual PyTorch example scripts. Repurpose as you see fit.\n",
    "\n",
    "This script was started from an early version of the PyTorch ImageNet example\n",
    "(https://github.com/pytorch/examples/tree/master/imagenet)\n",
    "\n",
    "NVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n",
    "(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman (https://github.com/rwightman)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ADDITIONAL NOTES BY fish:\n",
    "    - This script is a modified version of the original train.py script from Ross Wightman's timm library.\n",
    "    - for the current version, the \n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.utils\n",
    "import yaml\n",
    "from torch.nn.parallel import DistributedDataParallel as NativeDDP\n",
    "\n",
    "from timm import utils\n",
    "from timm.data import create_dataset, create_loader, resolve_data_config, Mixup, FastCollateMixup, AugMixDataset\n",
    "from timm.layers import convert_splitbn_model, convert_sync_batchnorm, set_fast_norm\n",
    "from timm.loss import JsdCrossEntropy, SoftTargetCrossEntropy, BinaryCrossEntropy, LabelSmoothingCrossEntropy\n",
    "from timm.models import create_model, safe_model_name, resume_checkpoint, load_checkpoint, model_parameters\n",
    "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
    "from timm.scheduler import create_scheduler_v2, scheduler_kwargs\n",
    "from timm.utils import ApexScaler, NativeScaler\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    from apex.parallel import DistributedDataParallel as ApexDDP\n",
    "    from apex.parallel import convert_syncbn_model\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    has_apex = False\n",
    "\n",
    "has_native_amp = False\n",
    "try:\n",
    "    if getattr(torch.cuda.amp, 'autocast') is not None:\n",
    "        has_native_amp = True\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    has_wandb = True\n",
    "except ImportError:\n",
    "    has_wandb = False\n",
    "\n",
    "try:\n",
    "    from functorch.compile import memory_efficient_fusion\n",
    "    has_functorch = True\n",
    "except ImportError as e:\n",
    "    has_functorch = False\n",
    "\n",
    "# fish: secondorder\n",
    "from eva import KFAC as Eva\n",
    "from eva import KFACParamScheduler\n",
    "\n",
    "has_compile = hasattr(torch, 'compile')\n",
    "\n",
    "\n",
    "_logger = logging.getLogger('train')\n",
    "\n",
    "# The first arg parser parses out only the --config argument, this argument is used to\n",
    "# load a yaml file containing key-values that override the defaults for the main parser below\n",
    "config_parser = parser = argparse.ArgumentParser(description='Training Config', add_help=False)\n",
    "parser.add_argument('-c', '--config', default='', type=str, metavar='FILE',\n",
    "                    help='YAML config file specifying default arguments')\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "\n",
    "# Dataset parameters\n",
    "group = parser.add_argument_group('Dataset parameters')\n",
    "# Keep this argument outside the dataset group because it is positional.\n",
    "parser.add_argument('data', nargs='?', metavar='DIR', const=None,\n",
    "                    help='path to dataset (positional is *deprecated*, use --data-dir)')\n",
    "parser.add_argument('--data-dir', metavar='DIR',\n",
    "                    help='path to dataset (root dir)')\n",
    "parser.add_argument('--dataset', metavar='NAME', default='',\n",
    "                    help='dataset type + name (\"<type>/<name>\") (default: ImageFolder or ImageTar if empty)')\n",
    "group.add_argument('--train-split', metavar='NAME', default='train',\n",
    "                   help='dataset train split (default: train)')\n",
    "group.add_argument('--val-split', metavar='NAME', default='validation',\n",
    "                   help='dataset validation split (default: validation)')\n",
    "group.add_argument('--dataset-download', action='store_true', default=False,\n",
    "                   help='Allow download of dataset for torch/ and tfds/ datasets that support it.')\n",
    "group.add_argument('--class-map', default='', type=str, metavar='FILENAME',\n",
    "                   help='path to class to idx mapping file (default: \"\")')\n",
    "\n",
    "# Model parameters\n",
    "group = parser.add_argument_group('Model parameters')\n",
    "group.add_argument('--model', default='resnet50', type=str, metavar='MODEL',\n",
    "                   help='Name of model to train (default: \"resnet50\")')\n",
    "group.add_argument('--pretrained', action='store_true', default=False,\n",
    "                   help='Start with pretrained version of specified network (if avail)')\n",
    "group.add_argument('--initial-checkpoint', default='', type=str, metavar='PATH',\n",
    "                   help='Initialize model from this checkpoint (default: none)')\n",
    "group.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                   help='Resume full model and optimizer state from checkpoint (default: none)')\n",
    "group.add_argument('--no-resume-opt', action='store_true', default=False,\n",
    "                   help='prevent resume of optimizer state when resuming model')\n",
    "group.add_argument('--num-classes', type=int, default=None, metavar='N',\n",
    "                   help='number of label classes (Model default if None)')\n",
    "group.add_argument('--gp', default=None, type=str, metavar='POOL',\n",
    "                   help='Global pool type, one of (fast, avg, max, avgmax, avgmaxc). Model default if None.')\n",
    "group.add_argument('--img-size', type=int, default=None, metavar='N',\n",
    "                   help='Image size (default: None => model default)')\n",
    "group.add_argument('--in-chans', type=int, default=None, metavar='N',\n",
    "                   help='Image input channels (default: None => 3)')\n",
    "group.add_argument('--input-size', default=None, nargs=3, type=int,\n",
    "                   metavar='N N N',\n",
    "                   help='Input all image dimensions (d h w, e.g. --input-size 3 224 224), uses model default if empty')\n",
    "group.add_argument('--crop-pct', default=None, type=float,\n",
    "                   metavar='N', help='Input image center crop percent (for validation only)')\n",
    "group.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n",
    "                   help='Override mean pixel value of dataset')\n",
    "group.add_argument('--std', type=float, nargs='+', default=None, metavar='STD',\n",
    "                   help='Override std deviation of dataset')\n",
    "group.add_argument('--interpolation', default='', type=str, metavar='NAME',\n",
    "                   help='Image resize interpolation type (overrides model)')\n",
    "group.add_argument('-b', '--batch-size', type=int, default=128, metavar='N',\n",
    "                   help='Input batch size for training (default: 128)')\n",
    "group.add_argument('-vb', '--validation-batch-size', type=int, default=None, metavar='N',\n",
    "                   help='Validation batch size override (default: None)')\n",
    "group.add_argument('--channels-last', action='store_true', default=False,\n",
    "                   help='Use channels_last memory layout')\n",
    "group.add_argument('--fuser', default='', type=str,\n",
    "                   help=\"Select jit fuser. One of ('', 'te', 'old', 'nvfuser')\")\n",
    "group.add_argument('--grad-accum-steps', type=int, default=1, metavar='N',\n",
    "                   help='The number of steps to accumulate gradients (default: 1)')\n",
    "group.add_argument('--grad-checkpointing', action='store_true', default=False,\n",
    "                   help='Enable gradient checkpointing through model blocks/stages')\n",
    "group.add_argument('--fast-norm', default=False, action='store_true',\n",
    "                   help='enable experimental fast-norm')\n",
    "group.add_argument('--model-kwargs', nargs='*', default={}, action=utils.ParseKwargs)\n",
    "group.add_argument('--head-init-scale', default=None, type=float,\n",
    "                   help='Head initialization scale')\n",
    "group.add_argument('--head-init-bias', default=None, type=float,\n",
    "                   help='Head initialization bias value')\n",
    "\n",
    "# scripting / codegen\n",
    "scripting_group = group.add_mutually_exclusive_group()\n",
    "scripting_group.add_argument('--torchscript', dest='torchscript', action='store_true',\n",
    "                             help='torch.jit.script the full model')\n",
    "scripting_group.add_argument('--torchcompile', nargs='?', type=str, default=None, const='inductor',\n",
    "                             help=\"Enable compilation w/ specified backend (default: inductor).\")\n",
    "\n",
    "# Optimizer parameters\n",
    "group = parser.add_argument_group('Optimizer parameters')\n",
    "group.add_argument('--opt', default='sgd', type=str, metavar='OPTIMIZER',\n",
    "                   help='Optimizer (default: \"sgd\")')\n",
    "group.add_argument('--opt-eps', default=None, type=float, metavar='EPSILON',\n",
    "                   help='Optimizer Epsilon (default: None, use opt default)')\n",
    "group.add_argument('--opt-betas', default=None, type=float, nargs='+', metavar='BETA',\n",
    "                   help='Optimizer Betas (default: None, use opt default)')\n",
    "group.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                   help='Optimizer momentum (default: 0.9)')\n",
    "group.add_argument('--weight-decay', type=float, default=2e-5,\n",
    "                   help='weight decay (default: 2e-5)')\n",
    "group.add_argument('--clip-grad', type=float, default=None, metavar='NORM',\n",
    "                   help='Clip gradient norm (default: None, no clipping)')\n",
    "group.add_argument('--clip-mode', type=str, default='norm',\n",
    "                   help='Gradient clipping mode. One of (\"norm\", \"value\", \"agc\")')\n",
    "group.add_argument('--layer-decay', type=float, default=None,\n",
    "                   help='layer-wise learning rate decay (default: None)')\n",
    "group.add_argument('--opt-kwargs', nargs='*', default={}, action=utils.ParseKwargs)\n",
    "\n",
    "# fish: EVA preconditioner parameters\n",
    "group = parser.add_argument_group('Preconditioner parameters')\n",
    "\n",
    "# eva second conditioner arguments\n",
    "parser.add_argument('--use-eva', action='store_true',default=False,help='use Eva preconditioner')\n",
    "\n",
    "# KFAC Parameters\n",
    "# parser.add_argument('--kfac-name', type=str, default='inverse',\n",
    "#         help='choises: %s' % kfac.kfac_mappers.keys() + ', default: '+'inverse')\n",
    "parser.add_argument('--exclude-parts', type=str, default='',\n",
    "        help='choises: CommunicateInverse,ComputeInverse,CommunicateFactor,ComputeFactor')\n",
    "parser.add_argument('--kfac-update-freq', type=int, default=1,\n",
    "                    help='iters between kfac inv ops (0 = no kfac) (default: 1)')\n",
    "parser.add_argument('--kfac-cov-update-freq', type=int, default=1,\n",
    "                    help='iters between kfac cov ops (default: 1)')\n",
    "parser.add_argument('--kfac-update-freq-alpha', type=float, default=10,\n",
    "                    help='KFAC update freq multiplier (default: 10)')\n",
    "parser.add_argument('--kfac-update-freq-decay', nargs='+', type=int, default=None,\n",
    "                    help='KFAC update freq schedule (default None)')\n",
    "parser.add_argument('--stat-decay', type=float, default=0.95,\n",
    "                    help='Alpha value for covariance accumulation (default: 0.95)')\n",
    "parser.add_argument('--damping', type=float, default=0.001,\n",
    "                    help='KFAC damping factor (default 0.001)')\n",
    "parser.add_argument('--damping-alpha', type=float, default=0.5,\n",
    "                    help='KFAC damping decay factor (default: 0.5)')\n",
    "parser.add_argument('--damping-decay', nargs='+', type=int, default=None,\n",
    "                    help='KFAC damping decay schedule (default None)')\n",
    "parser.add_argument('--kl-clip', type=float, default=0.001,\n",
    "                    help='KL clip (default: 0.001)')\n",
    "parser.add_argument('--diag-blocks', type=int, default=1,\n",
    "                    help='Number of blocks to approx layer factor with (default: 1)')\n",
    "parser.add_argument('--diag-warmup', type=int, default=0,\n",
    "                    help='Epoch to start diag block approximation at (default: 0)')\n",
    "parser.add_argument('--distribute-layer-factors', action='store_true', default=None,\n",
    "                    help='Compute A and G for a single layer on different workers. '\n",
    "                            'None to determine automatically based on worker and '\n",
    "                            'layer count.')\n",
    "\n",
    "\n",
    "# Learning rate schedule parameters\n",
    "group = parser.add_argument_group('Learning rate schedule parameters')\n",
    "group.add_argument('--sched', type=str, default='cosine', metavar='SCHEDULER',\n",
    "                   help='LR scheduler (default: \"step\"')\n",
    "group.add_argument('--sched-on-updates', action='store_true', default=False,\n",
    "                   help='Apply LR scheduler step on update instead of epoch end.')\n",
    "group.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                   help='learning rate, overrides lr-base if set (default: None)')\n",
    "group.add_argument('--lr-base', type=float, default=0.1, metavar='LR',\n",
    "                   help='base learning rate: lr = lr_base * global_batch_size / base_size')\n",
    "group.add_argument('--lr-base-size', type=int, default=256, metavar='DIV',\n",
    "                   help='base learning rate batch size (divisor, default: 256).')\n",
    "group.add_argument('--lr-base-scale', type=str, default='', metavar='SCALE',\n",
    "                   help='base learning rate vs batch_size scaling (\"linear\", \"sqrt\", based on opt if empty)')\n",
    "group.add_argument('--lr-noise', type=float, nargs='+', default=None, metavar='pct, pct',\n",
    "                   help='learning rate noise on/off epoch percentages')\n",
    "group.add_argument('--lr-noise-pct', type=float, default=0.67, metavar='PERCENT',\n",
    "                   help='learning rate noise limit percent (default: 0.67)')\n",
    "group.add_argument('--lr-noise-std', type=float, default=1.0, metavar='STDDEV',\n",
    "                   help='learning rate noise std-dev (default: 1.0)')\n",
    "group.add_argument('--lr-cycle-mul', type=float, default=1.0, metavar='MULT',\n",
    "                   help='learning rate cycle len multiplier (default: 1.0)')\n",
    "group.add_argument('--lr-cycle-decay', type=float, default=0.5, metavar='MULT',\n",
    "                   help='amount to decay each learning rate cycle (default: 0.5)')\n",
    "group.add_argument('--lr-cycle-limit', type=int, default=1, metavar='N',\n",
    "                   help='learning rate cycle limit, cycles enabled if > 1')\n",
    "group.add_argument('--lr-k-decay', type=float, default=1.0,\n",
    "                   help='learning rate k-decay for cosine/poly (default: 1.0)')\n",
    "group.add_argument('--warmup-lr', type=float, default=1e-5, metavar='LR',\n",
    "                   help='warmup learning rate (default: 1e-5)')\n",
    "group.add_argument('--min-lr', type=float, default=0, metavar='LR',\n",
    "                   help='lower lr bound for cyclic schedulers that hit 0 (default: 0)')\n",
    "group.add_argument('--epochs', type=int, default=300, metavar='N',\n",
    "                   help='number of epochs to train (default: 300)')\n",
    "group.add_argument('--epoch-repeats', type=float, default=0., metavar='N',\n",
    "                   help='epoch repeat multiplier (number of times to repeat dataset epoch per train epoch).')\n",
    "group.add_argument('--start-epoch', default=None, type=int, metavar='N',\n",
    "                   help='manual epoch number (useful on restarts)')\n",
    "group.add_argument('--decay-milestones', default=[90, 180, 270], type=int, nargs='+', metavar=\"MILESTONES\",\n",
    "                   help='list of decay epoch indices for multistep lr. must be increasing')\n",
    "group.add_argument('--decay-epochs', type=float, default=90, metavar='N',\n",
    "                   help='epoch interval to decay LR')\n",
    "group.add_argument('--warmup-epochs', type=int, default=5, metavar='N',\n",
    "                   help='epochs to warmup LR, if scheduler supports')\n",
    "group.add_argument('--warmup-prefix', action='store_true', default=False,\n",
    "                   help='Exclude warmup period from decay schedule.'),\n",
    "group.add_argument('--cooldown-epochs', type=int, default=0, metavar='N',\n",
    "                   help='epochs to cooldown LR at min_lr, after cyclic schedule ends')\n",
    "group.add_argument('--patience-epochs', type=int, default=10, metavar='N',\n",
    "                   help='patience epochs for Plateau LR scheduler (default: 10)')\n",
    "group.add_argument('--decay-rate', '--dr', type=float, default=0.1, metavar='RATE',\n",
    "                   help='LR decay rate (default: 0.1)')\n",
    "\n",
    "# Augmentation & regularization parameters\n",
    "group = parser.add_argument_group('Augmentation and regularization parameters')\n",
    "group.add_argument('--no-aug', action='store_true', default=False,\n",
    "                   help='Disable all training augmentation, override other train aug args')\n",
    "group.add_argument('--scale', type=float, nargs='+', default=[0.08, 1.0], metavar='PCT',\n",
    "                   help='Random resize scale (default: 0.08 1.0)')\n",
    "group.add_argument('--ratio', type=float, nargs='+', default=[3. / 4., 4. / 3.], metavar='RATIO',\n",
    "                   help='Random resize aspect ratio (default: 0.75 1.33)')\n",
    "group.add_argument('--hflip', type=float, default=0.5,\n",
    "                   help='Horizontal flip training aug probability')\n",
    "group.add_argument('--vflip', type=float, default=0.,\n",
    "                   help='Vertical flip training aug probability')\n",
    "group.add_argument('--color-jitter', type=float, default=0.4, metavar='PCT',\n",
    "                   help='Color jitter factor (default: 0.4)')\n",
    "group.add_argument('--aa', type=str, default=None, metavar='NAME',\n",
    "                   help='Use AutoAugment policy. \"v0\" or \"original\". (default: None)'),\n",
    "group.add_argument('--aug-repeats', type=float, default=0,\n",
    "                   help='Number of augmentation repetitions (distributed training only) (default: 0)')\n",
    "group.add_argument('--aug-splits', type=int, default=0,\n",
    "                   help='Number of augmentation splits (default: 0, valid: 0 or >=2)')\n",
    "group.add_argument('--jsd-loss', action='store_true', default=False,\n",
    "                   help='Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.')\n",
    "group.add_argument('--bce-loss', action='store_true', default=False,\n",
    "                   help='Enable BCE loss w/ Mixup/CutMix use.')\n",
    "group.add_argument('--bce-target-thresh', type=float, default=None,\n",
    "                   help='Threshold for binarizing softened BCE targets (default: None, disabled)')\n",
    "group.add_argument('--reprob', type=float, default=0., metavar='PCT',\n",
    "                   help='Random erase prob (default: 0.)')\n",
    "group.add_argument('--remode', type=str, default='pixel',\n",
    "                   help='Random erase mode (default: \"pixel\")')\n",
    "group.add_argument('--recount', type=int, default=1,\n",
    "                   help='Random erase count (default: 1)')\n",
    "group.add_argument('--resplit', action='store_true', default=False,\n",
    "                   help='Do not random erase first (clean) augmentation split')\n",
    "group.add_argument('--mixup', type=float, default=0.0,\n",
    "                   help='mixup alpha, mixup enabled if > 0. (default: 0.)')\n",
    "group.add_argument('--cutmix', type=float, default=0.0,\n",
    "                   help='cutmix alpha, cutmix enabled if > 0. (default: 0.)')\n",
    "group.add_argument('--cutmix-minmax', type=float, nargs='+', default=None,\n",
    "                   help='cutmix min/max ratio, overrides alpha and enables cutmix if set (default: None)')\n",
    "group.add_argument('--mixup-prob', type=float, default=1.0,\n",
    "                   help='Probability of performing mixup or cutmix when either/both is enabled')\n",
    "group.add_argument('--mixup-switch-prob', type=float, default=0.5,\n",
    "                   help='Probability of switching to cutmix when both mixup and cutmix enabled')\n",
    "group.add_argument('--mixup-mode', type=str, default='batch',\n",
    "                   help='How to apply mixup/cutmix params. Per \"batch\", \"pair\", or \"elem\"')\n",
    "group.add_argument('--mixup-off-epoch', default=0, type=int, metavar='N',\n",
    "                   help='Turn off mixup after this epoch, disabled if 0 (default: 0)')\n",
    "group.add_argument('--smoothing', type=float, default=0.1,\n",
    "                   help='Label smoothing (default: 0.1)')\n",
    "group.add_argument('--train-interpolation', type=str, default='random',\n",
    "                   help='Training interpolation (random, bilinear, bicubic default: \"random\")')\n",
    "group.add_argument('--drop', type=float, default=0.0, metavar='PCT',\n",
    "                   help='Dropout rate (default: 0.)')\n",
    "group.add_argument('--drop-connect', type=float, default=None, metavar='PCT',\n",
    "                   help='Drop connect rate, DEPRECATED, use drop-path (default: None)')\n",
    "group.add_argument('--drop-path', type=float, default=None, metavar='PCT',\n",
    "                   help='Drop path rate (default: None)')\n",
    "group.add_argument('--drop-block', type=float, default=None, metavar='PCT',\n",
    "                   help='Drop block rate (default: None)')\n",
    "\n",
    "# Batch norm parameters (only works with gen_efficientnet based models currently)\n",
    "group = parser.add_argument_group('Batch norm parameters', 'Only works with gen_efficientnet based models currently.')\n",
    "group.add_argument('--bn-momentum', type=float, default=None,\n",
    "                   help='BatchNorm momentum override (if not None)')\n",
    "group.add_argument('--bn-eps', type=float, default=None,\n",
    "                   help='BatchNorm epsilon override (if not None)')\n",
    "group.add_argument('--sync-bn', action='store_true',\n",
    "                   help='Enable NVIDIA Apex or Torch synchronized BatchNorm.')\n",
    "group.add_argument('--dist-bn', type=str, default='reduce',\n",
    "                   help='Distribute BatchNorm stats between nodes after each epoch (\"broadcast\", \"reduce\", or \"\")')\n",
    "group.add_argument('--split-bn', action='store_true',\n",
    "                   help='Enable separate BN layers per augmentation split.')\n",
    "\n",
    "# Model Exponential Moving Average\n",
    "group = parser.add_argument_group('Model exponential moving average parameters')\n",
    "group.add_argument('--model-ema', action='store_true', default=False,\n",
    "                   help='Enable tracking moving average of model weights')\n",
    "group.add_argument('--model-ema-force-cpu', action='store_true', default=False,\n",
    "                   help='Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.')\n",
    "group.add_argument('--model-ema-decay', type=float, default=0.9998,\n",
    "                   help='decay factor for model weights moving average (default: 0.9998)')\n",
    "\n",
    "# Misc\n",
    "group = parser.add_argument_group('Miscellaneous parameters')\n",
    "group.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "                   help='random seed (default: 42)')\n",
    "group.add_argument('--worker-seeding', type=str, default='all',\n",
    "                   help='worker seed mode (default: all)')\n",
    "group.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                   help='how many batches to wait before logging training status')\n",
    "group.add_argument('--recovery-interval', type=int, default=0, metavar='N',\n",
    "                   help='how many batches to wait before writing recovery checkpoint')\n",
    "group.add_argument('--checkpoint-hist', type=int, default=10, metavar='N',\n",
    "                   help='number of checkpoints to keep (default: 10)')\n",
    "group.add_argument('-j', '--workers', type=int, default=4, metavar='N',\n",
    "                   help='how many training processes to use (default: 4)')\n",
    "group.add_argument('--save-images', action='store_true', default=False,\n",
    "                   help='save images of input bathes every log interval for debugging')\n",
    "group.add_argument('--amp', action='store_true', default=False,\n",
    "                   help='use NVIDIA Apex AMP or Native AMP for mixed precision training')\n",
    "group.add_argument('--amp-dtype', default='float16', type=str,\n",
    "                   help='lower precision AMP dtype (default: float16)')\n",
    "group.add_argument('--amp-impl', default='native', type=str,\n",
    "                   help='AMP impl to use, \"native\" or \"apex\" (default: native)')\n",
    "group.add_argument('--no-ddp-bb', action='store_true', default=False,\n",
    "                   help='Force broadcast buffers for native DDP to off.')\n",
    "group.add_argument('--synchronize-step', action='store_true', default=False,\n",
    "                   help='torch.cuda.synchronize() end of each step')\n",
    "group.add_argument('--pin-mem', action='store_true', default=False,\n",
    "                   help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "group.add_argument('--no-prefetcher', action='store_true', default=False,\n",
    "                   help='disable fast prefetcher')\n",
    "group.add_argument('--output', default='', type=str, metavar='PATH',\n",
    "                   help='path to output folder (default: none, current dir)')\n",
    "group.add_argument('--experiment', default='', type=str, metavar='NAME',\n",
    "                   help='name of train experiment, name of sub-folder for output')\n",
    "group.add_argument('--eval-metric', default='top1', type=str, metavar='EVAL_METRIC',\n",
    "                   help='Best metric (default: \"top1\"')\n",
    "group.add_argument('--tta', type=int, default=0, metavar='N',\n",
    "                   help='Test/inference time augmentation (oversampling) factor. 0=None (default: 0)')\n",
    "group.add_argument(\"--local_rank\", default=0, type=int)\n",
    "group.add_argument('--use-multi-epochs-loader', action='store_true', default=False,\n",
    "                   help='use the multi-epochs-loader to save time at the beginning of every epoch')\n",
    "group.add_argument('--log-wandb', action='store_true', default=False,\n",
    "                   help='log training and validation metrics to wandb')\n",
    "\n",
    "\n",
    "def _parse_args():\n",
    "    # Do we have a config file to parse?\n",
    "    args_config, remaining = config_parser.parse_known_args()\n",
    "    if args_config.config:\n",
    "        with open(args_config.config, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "            parser.set_defaults(**cfg)\n",
    "\n",
    "    # The main arg parser parses the rest of the args, the usual\n",
    "    # defaults will have been overridden if config file specified.\n",
    "    args = parser.parse_args(remaining)\n",
    "\n",
    "    # Cache the args as a text string to save them in the output dir later\n",
    "    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "    return args, args_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重写之后，我们尝试新建一个 budgeted training 的 model，设置为初始 stage，在 config 里我们已经将其设置为 baby model。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data=None, data_dir='/home/fish/Documents/imagenet/', dataset='', train_split='train', val_split='validation', dataset_download=False, class_map='', model='bud_small_patch16_224_baby', pretrained=False, initial_checkpoint='', resume='', no_resume_opt=False, num_classes=None, gp=None, img_size=None, in_chans=None, input_size=None, crop_pct=None, mean=None, std=None, interpolation='', batch_size=128, validation_batch_size=None, channels_last=False, fuser='', grad_accum_steps=1, grad_checkpointing=False, fast_norm=False, model_kwargs={}, head_init_scale=None, head_init_bias=None, torchscript=False, torchcompile=None, opt='lion', opt_eps=None, opt_betas=None, momentum=0.9, weight_decay=0.001, clip_grad=None, clip_mode='norm', layer_decay=None, opt_kwargs={}, use_eva=False, exclude_parts='', kfac_update_freq=1, kfac_cov_update_freq=1, kfac_update_freq_alpha=10, kfac_update_freq_decay=None, stat_decay=0.95, damping=0.001, damping_alpha=0.5, damping_decay=None, kl_clip=0.001, diag_blocks=1, diag_warmup=0, distribute_layer_factors=None, sched='cosine', sched_on_updates=False, lr=None, lr_base=0.0001, lr_base_size=256, lr_base_scale='', lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, lr_cycle_mul=1.0, lr_cycle_decay=0.5, lr_cycle_limit=1, lr_k_decay=1.0, warmup_lr=1e-05, min_lr=0, epochs=15, epoch_repeats=0.0, start_epoch=None, decay_milestones=[90, 180, 270], decay_epochs=90, warmup_epochs=0, warmup_prefix=False, cooldown_epochs=0, patience_epochs=10, decay_rate=0.1, no_aug=False, scale=[0.08, 1.0], ratio=[0.75, 1.3333333333333333], hflip=0.5, vflip=0.0, color_jitter=0.4, aa=None, aug_repeats=0, aug_splits=0, jsd_loss=False, bce_loss=False, bce_target_thresh=None, reprob=0.0, remode='pixel', recount=1, resplit=False, mixup=0.0, cutmix=0.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', mixup_off_epoch=0, smoothing=0.1, train_interpolation='random', drop=0.0, drop_connect=None, drop_path=None, drop_block=None, bn_momentum=None, bn_eps=None, sync_bn=False, dist_bn='reduce', split_bn=False, model_ema=False, model_ema_force_cpu=False, model_ema_decay=0.9998, seed=42, worker_seeding='all', log_interval=50, recovery_interval=0, checkpoint_hist=10, workers=4, save_images=False, amp=True, amp_dtype='float16', amp_impl='native', no_ddp_bb=False, synchronize_step=False, pin_mem=False, no_prefetcher=False, output='./output', experiment='budgeted-flop', eval_metric='top1', tta=0, local_rank=0, use_multi_epochs_loader=False, log_wandb=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_line_args = [\n",
    "    \"--model\", \"bud_small_patch16_224_baby\",\n",
    "    \"--batch-size\", \"128\",\n",
    "    \"--epochs\", \"15\",\n",
    "    \"--warmup-epochs\", \"0\",\n",
    "    \"--data-dir\", \"/home/fish/Documents/imagenet/\",\n",
    "    \"--output\", \"./output\",\n",
    "    \"--experiment\", \"budgeted-flop\",\n",
    "    # \"--num-stages\", \"3\",\n",
    "    \"--amp\",\n",
    "    \"--opt\", \"lion\",\n",
    "    \"--lr-base\", \"1e-4\",\n",
    "    \"--weight-decay\", \"1e-3\",\n",
    "    # \"--log-wandb\",\n",
    "]\n",
    "\n",
    "args = parser.parse_args(cmd_line_args)\n",
    "args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置 logging，注意只运行一次该格，不然会有重复输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.setup_default_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下面的代码进行训练的初始设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training with a single process on 1 device (cuda:0).\n",
      "Model bud_small_patch16_224_baby created, param count:6101224\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     model \u001b[39m=\u001b[39m convert_splitbn_model(model, \u001b[39mmax\u001b[39m(num_aug_splits, \u001b[39m2\u001b[39m))\n\u001b[1;32m     93\u001b[0m \u001b[39m# move model to GPU, enable channels last layout if set\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m model\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mchannels_last:\n\u001b[1;32m     96\u001b[0m     model\u001b[39m.\u001b[39mto(memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mchannels_last)\n",
      "File \u001b[0;32m~/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/timm/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "    # utils.setup_default_logging()\n",
    "    # args, args_text = _parse_args() # args 我们已经另外做了处理\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    args.prefetcher = not args.no_prefetcher\n",
    "    args.grad_accum_steps = max(1, args.grad_accum_steps)\n",
    "    device = utils.init_distributed_device(args)\n",
    "    if args.distributed:\n",
    "        _logger.info(\n",
    "            'Training in distributed mode with multiple processes, 1 device per process.'\n",
    "            f'Process {args.rank}, total {args.world_size}, device {args.device}.')\n",
    "    else:\n",
    "        _logger.info(f'Training with a single process on 1 device ({args.device}).')\n",
    "    assert args.rank >= 0\n",
    "\n",
    "    # resolve AMP arguments based on PyTorch / Apex availability\n",
    "    use_amp = None\n",
    "    amp_dtype = torch.float16\n",
    "    if args.amp:\n",
    "        if args.amp_impl == 'apex':\n",
    "            assert has_apex, 'AMP impl specified as APEX but APEX is not installed.'\n",
    "            use_amp = 'apex'\n",
    "            assert args.amp_dtype == 'float16'\n",
    "        else:\n",
    "            assert has_native_amp, 'Please update PyTorch to a version with native AMP (or use APEX).'\n",
    "            use_amp = 'native'\n",
    "            assert args.amp_dtype in ('float16', 'bfloat16')\n",
    "        if args.amp_dtype == 'bfloat16':\n",
    "            amp_dtype = torch.bfloat16\n",
    "\n",
    "    utils.random_seed(args.seed, args.rank)\n",
    "\n",
    "    if args.fuser:\n",
    "        utils.set_jit_fuser(args.fuser)\n",
    "    if args.fast_norm:\n",
    "        set_fast_norm()\n",
    "\n",
    "    in_chans = 3\n",
    "    if args.in_chans is not None:\n",
    "        in_chans = args.in_chans\n",
    "    elif args.input_size is not None:\n",
    "        in_chans = args.input_size[0]\n",
    "\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # create the train and eval datasets\n",
    "    if args.data and not args.data_dir:\n",
    "        args.data_dir = args.data\n",
    "    dataset_train = create_dataset(\n",
    "        args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split=args.train_split,\n",
    "        is_training=True,\n",
    "        class_map=args.class_map,\n",
    "        download=args.dataset_download,\n",
    "        batch_size=args.batch_size,\n",
    "        seed=args.seed,\n",
    "        repeats=args.epoch_repeats,\n",
    "    )\n",
    "\n",
    "    dataset_eval = create_dataset(\n",
    "        args.dataset,\n",
    "        root=args.data_dir,\n",
    "        split=args.val_split,\n",
    "        is_training=False,\n",
    "        class_map=args.class_map,\n",
    "        download=args.dataset_download,\n",
    "        batch_size=args.batch_size,\n",
    "    )\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "    loader_train = create_loader(\n",
    "        dataset_train,\n",
    "        input_size=data_config['input_size'],\n",
    "        batch_size=args.batch_size,\n",
    "        is_training=True,\n",
    "        use_prefetcher=args.prefetcher,\n",
    "        no_aug=args.no_aug,\n",
    "        re_prob=args.reprob,\n",
    "        re_mode=args.remode,\n",
    "        re_count=args.recount,\n",
    "        re_split=args.resplit,\n",
    "        scale=args.scale,\n",
    "        ratio=args.ratio,\n",
    "        hflip=args.hflip,\n",
    "        vflip=args.vflip,\n",
    "        color_jitter=args.color_jitter,\n",
    "        auto_augment=args.aa,\n",
    "        num_aug_repeats=args.aug_repeats,\n",
    "        num_aug_splits=num_aug_splits,\n",
    "        interpolation=train_interpolation,\n",
    "        mean=data_config['mean'],\n",
    "        std=data_config['std'],\n",
    "        num_workers=args.workers,\n",
    "        distributed=args.distributed,\n",
    "        collate_fn=collate_fn,\n",
    "        pin_memory=args.pin_mem,\n",
    "        device=device,\n",
    "        use_multi_epochs_loader=args.use_multi_epochs_loader,\n",
    "        worker_seeding=args.worker_seeding,\n",
    "    )\n",
    "\n",
    "    eval_workers = args.workers\n",
    "    if args.distributed and ('tfds' in args.dataset or 'wds' in args.dataset):\n",
    "        # FIXME reduces validation padding issues when using TFDS, WDS w/ workers and distributed training\n",
    "        eval_workers = min(2, args.workers)\n",
    "    loader_eval = create_loader(\n",
    "        dataset_eval,\n",
    "        input_size=data_config['input_size'],\n",
    "        batch_size=args.validation_batch_size or args.batch_size,\n",
    "        is_training=False,\n",
    "        use_prefetcher=args.prefetcher,\n",
    "        interpolation=data_config['interpolation'],\n",
    "        mean=data_config['mean'],\n",
    "        std=data_config['std'],\n",
    "        num_workers=eval_workers,\n",
    "        distributed=args.distributed,\n",
    "        crop_pct=data_config['crop_pct'],\n",
    "        pin_memory=args.pin_mem,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm', name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 0\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthop\u001b[39;00m \u001b[39mimport\u001b[39;00m profile\n\u001b[1;32m      2\u001b[0m \u001b[39m# model = resnet50()\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m macs, params \u001b[39m=\u001b[39m profile(model, inputs\u001b[39m=\u001b[39m(\u001b[39minput\u001b[39m, ))\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(macs, params)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "# model = resnet50()\n",
    "input = torch.randn(1, 3, 224, 224).to('cuda')\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model bud_small_patch16_224_child created, param count:11417704\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n",
      "Using native Torch AMP. Training in mixed precision.\n",
      "Scheduled epochs: 15. LR stepped per epoch.\n"
     ]
    }
   ],
   "source": [
    "    old_model = model\n",
    "    old_optimizer = optimizer\n",
    "    old_loss_scaler = loss_scaler\n",
    "    old_lr_scheduler = lr_scheduler\n",
    "    if args.use_eva:\n",
    "        old_preconditioner = preconditioner\n",
    "\n",
    "    args.model = args.model.replace('baby', 'child')\n",
    "    # args.model = args.model.replace('child', 'man')\n",
    "    # args.model = args.model.replace('man', 'baby')\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm',name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 5\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "2157267456.0 11341672.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "# model = resnet50()\n",
    "input = torch.randn(1, 3, 224, 224).to('cuda')\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model bud_small_patch16_224_man created, param count:22050664\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.9\n",
      "\tcrop_mode: center\n",
      "Using native Torch AMP. Training in mixed precision.\n",
      "Scheduled epochs: 15. LR stepped per epoch.\n"
     ]
    }
   ],
   "source": [
    "    old_model = model\n",
    "    old_optimizer = optimizer\n",
    "    old_loss_scaler = loss_scaler\n",
    "    old_lr_scheduler = lr_scheduler\n",
    "    if args.use_eva:\n",
    "        old_preconditioner = preconditioner\n",
    "\n",
    "    # args.model = args.model.replace('baby', 'child')\n",
    "    args.model = args.model.replace('child', 'man')\n",
    "    # args.model = args.model.replace('man', 'baby')\n",
    "    model = create_model(\n",
    "        args.model,\n",
    "        pretrained=args.pretrained,\n",
    "        in_chans=in_chans,\n",
    "        num_classes=args.num_classes,\n",
    "        drop_rate=args.drop,\n",
    "        drop_path_rate=args.drop_path,\n",
    "        drop_block_rate=args.drop_block,\n",
    "        global_pool=args.gp,\n",
    "        bn_momentum=args.bn_momentum,\n",
    "        bn_eps=args.bn_eps,\n",
    "        scriptable=args.torchscript,\n",
    "        checkpoint_path=args.initial_checkpoint,\n",
    "        **args.model_kwargs,\n",
    "    )\n",
    "    if args.head_init_scale is not None:\n",
    "        with torch.no_grad():\n",
    "            model.get_classifier().weight.mul_(args.head_init_scale)\n",
    "            model.get_classifier().bias.mul_(args.head_init_scale)\n",
    "    if args.head_init_bias is not None:\n",
    "        nn.init.constant_(model.get_classifier().bias, args.head_init_bias)\n",
    "\n",
    "    if args.num_classes is None:\n",
    "        assert hasattr(model, 'num_classes'), 'Model must have `num_classes` attr if not set on cmd line/config.'\n",
    "        args.num_classes = model.num_classes  # FIXME handle model default vs config num_classes more elegantly\n",
    "\n",
    "    if args.grad_checkpointing:\n",
    "        model.set_grad_checkpointing(enable=True)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Model {safe_model_name(args.model)} created, param count:{sum([m.numel() for m in model.parameters()])}')\n",
    "\n",
    "    data_config = resolve_data_config(vars(args), model=model, verbose=utils.is_primary(args))\n",
    "\n",
    "    # setup augmentation batch splits for contrastive loss or split bn\n",
    "    num_aug_splits = 0\n",
    "    if args.aug_splits > 0:\n",
    "        assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "        num_aug_splits = args.aug_splits\n",
    "\n",
    "    # enable split bn (separate bn stats per batch-portion)\n",
    "    if args.split_bn:\n",
    "        assert num_aug_splits > 1 or args.resplit\n",
    "        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "    # move model to GPU, enable channels last layout if set\n",
    "    model.to(device=device)\n",
    "    if args.channels_last:\n",
    "        model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    # setup synchronized BatchNorm for distributed training\n",
    "    if args.distributed and args.sync_bn:\n",
    "        args.dist_bn = ''  # disable dist_bn when sync BN active\n",
    "        assert not args.split_bn\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex SyncBN used with Apex AMP\n",
    "            # WARNING this won't currently work with models using BatchNormAct2d\n",
    "            model = convert_syncbn_model(model)\n",
    "        else:\n",
    "            model = convert_sync_batchnorm(model)\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using '\n",
    "                'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.')\n",
    "\n",
    "    if args.torchscript:\n",
    "        assert not args.torchcompile\n",
    "        assert not use_amp == 'apex', 'Cannot use APEX AMP with torchscripted model'\n",
    "        assert not args.sync_bn, 'Cannot use SyncBatchNorm with torchscripted model'\n",
    "        model = torch.jit.script(model)\n",
    "\n",
    "    if not args.lr:\n",
    "        global_batch_size = args.batch_size * args.world_size * args.grad_accum_steps\n",
    "        batch_ratio = global_batch_size / args.lr_base_size\n",
    "        if not args.lr_base_scale:\n",
    "            on = args.opt.lower()\n",
    "            args.lr_base_scale = 'sqrt' if any([o in on for o in ('ada', 'lamb')]) else 'linear'\n",
    "        if args.lr_base_scale == 'sqrt':\n",
    "            batch_ratio = batch_ratio ** 0.5\n",
    "        args.lr = args.lr_base * batch_ratio\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info(\n",
    "                f'Learning rate ({args.lr}) calculated from base learning rate ({args.lr_base}) '\n",
    "                f'and effective global batch size ({global_batch_size}) with {args.lr_base_scale} scaling.')\n",
    "\n",
    "    optimizer = create_optimizer_v2(\n",
    "        model,\n",
    "        **optimizer_kwargs(cfg=args),\n",
    "        **args.opt_kwargs,\n",
    "    )\n",
    "\n",
    "    # fish: add eva preconditioner, not sure if to use model without ddp\n",
    "    if args.use_eva:\n",
    "        \n",
    "        # preconditioner = Eva(model_without_ddp)\n",
    "        preconditioner = Eva(\n",
    "                model, lr=args.lr, factor_decay=args.stat_decay,\n",
    "                damping=args.damping, kl_clip=args.kl_clip,\n",
    "                fac_update_freq=args.kfac_cov_update_freq,\n",
    "                kfac_update_freq=args.kfac_update_freq,\n",
    "                #diag_blocks=args.diag_blocks,\n",
    "                #diag_warmup=args.diag_warmup,\n",
    "                #distribute_layer_factors=args.distribute_layer_factors, \n",
    "                exclude_parts=args.exclude_parts)\n",
    "\n",
    "        kfac_param_scheduler = KFACParamScheduler(\n",
    "               preconditioner,\n",
    "               damping_alpha=args.damping_alpha,\n",
    "               damping_schedule=args.damping_decay,\n",
    "               update_freq_alpha=args.kfac_update_freq_alpha,\n",
    "               update_freq_schedule=args.kfac_update_freq_decay,\n",
    "               start_epoch=args.start_epoch)\n",
    "\n",
    "        print(f\"preconditioner eva is adapted\")\n",
    "\n",
    "    else:\n",
    "        preconditioner = None\n",
    "\n",
    "    # setup automatic mixed-precision (AMP) loss scaling and op casting\n",
    "    amp_autocast = suppress  # do nothing\n",
    "    loss_scaler = None\n",
    "    if use_amp == 'apex':\n",
    "        assert device.type == 'cuda'\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "        loss_scaler = ApexScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "    elif use_amp == 'native':\n",
    "        try:\n",
    "            amp_autocast = partial(torch.autocast, device_type=device.type, dtype=amp_dtype)\n",
    "        except (AttributeError, TypeError):\n",
    "            # fallback to CUDA only AMP for PyTorch < 1.10\n",
    "            assert device.type == 'cuda'\n",
    "            amp_autocast = torch.cuda.amp.autocast\n",
    "        if device.type == 'cuda' and amp_dtype == torch.float16:\n",
    "            # loss scaler only used for float16 (half) dtype, bfloat16 does not need it\n",
    "            loss_scaler = NativeScaler()\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "    else:\n",
    "        if utils.is_primary(args):\n",
    "            _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    resume_epoch = None\n",
    "    if args.resume:\n",
    "        resume_epoch = resume_checkpoint(\n",
    "            model,\n",
    "            args.resume,\n",
    "            optimizer=None if args.no_resume_opt else optimizer,\n",
    "            loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "            log_info=utils.is_primary(args),\n",
    "        )\n",
    "\n",
    "    # setup exponential moving average of model weights, SWA could be used here too\n",
    "    model_ema = None\n",
    "    if args.model_ema:\n",
    "        # Important to create EMA model after cuda(), DP wrapper, and AMP but before DDP wrapper\n",
    "        model_ema = utils.ModelEmaV2(\n",
    "            model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "        if args.resume:\n",
    "            load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "    # setup distributed training\n",
    "    if args.distributed:\n",
    "        if has_apex and use_amp == 'apex':\n",
    "            # Apex DDP preferred unless native amp is activated\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using NVIDIA APEX DistributedDataParallel.\")\n",
    "            model = ApexDDP(model, delay_allreduce=True)\n",
    "        else:\n",
    "            if utils.is_primary(args):\n",
    "                _logger.info(\"Using native Torch DistributedDataParallel.\")\n",
    "            model = NativeDDP(model, device_ids=[device], broadcast_buffers=not args.no_ddp_bb)\n",
    "        # NOTE: EMA model does not need to be wrapped by DDP\n",
    "\n",
    "    if args.torchcompile:\n",
    "        # torch compile should be done after DDP\n",
    "        assert has_compile, 'A version of torch w/ torch.compile() is required for --compile, possibly a nightly.'\n",
    "        model = torch.compile(model, backend=args.torchcompile)\n",
    "\n",
    "    # setup mixup / cutmix\n",
    "    collate_fn = None\n",
    "    mixup_fn = None\n",
    "    mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "    if mixup_active:\n",
    "        mixup_args = dict(\n",
    "            mixup_alpha=args.mixup,\n",
    "            cutmix_alpha=args.cutmix,\n",
    "            cutmix_minmax=args.cutmix_minmax,\n",
    "            prob=args.mixup_prob,\n",
    "            switch_prob=args.mixup_switch_prob,\n",
    "            mode=args.mixup_mode,\n",
    "            label_smoothing=args.smoothing,\n",
    "            num_classes=args.num_classes\n",
    "        )\n",
    "        if args.prefetcher:\n",
    "            assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "            collate_fn = FastCollateMixup(**mixup_args)\n",
    "        else:\n",
    "            mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "    # wrap dataset in AugMix helper\n",
    "    if num_aug_splits > 1:\n",
    "        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "    # create data loaders w/ augmentation pipeiine\n",
    "    train_interpolation = args.train_interpolation\n",
    "    if args.no_aug or not train_interpolation:\n",
    "        train_interpolation = data_config['interpolation']\n",
    "\n",
    "    # setup loss function\n",
    "    if args.jsd_loss:\n",
    "        assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing)\n",
    "    elif mixup_active:\n",
    "        # smoothing is handled with mixup target transform which outputs sparse, soft targets\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = SoftTargetCrossEntropy()\n",
    "    elif args.smoothing:\n",
    "        if args.bce_loss:\n",
    "            train_loss_fn = BinaryCrossEntropy(smoothing=args.smoothing, target_threshold=args.bce_target_thresh)\n",
    "        else:\n",
    "            train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing)\n",
    "    else:\n",
    "        train_loss_fn = nn.CrossEntropyLoss()\n",
    "    train_loss_fn = train_loss_fn.to(device=device)\n",
    "    validate_loss_fn = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "    # setup checkpoint saver and eval metric tracking\n",
    "    eval_metric = args.eval_metric\n",
    "    best_metric = None\n",
    "    best_epoch = None\n",
    "    saver = None\n",
    "    output_dir = None\n",
    "    if utils.is_primary(args):\n",
    "        if args.experiment:\n",
    "            exp_name = args.experiment\n",
    "        else:\n",
    "            exp_name = '-'.join([\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                safe_model_name(args.model),\n",
    "                str(data_config['input_size'][-1])\n",
    "            ])\n",
    "        output_dir = utils.get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "        decreasing = True if eval_metric == 'loss' else False\n",
    "        saver = utils.CheckpointSaver(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            args=args,\n",
    "            model_ema=model_ema,\n",
    "            amp_scaler=loss_scaler,\n",
    "            checkpoint_dir=output_dir,\n",
    "            recovery_dir=output_dir,\n",
    "            decreasing=decreasing,\n",
    "            max_history=args.checkpoint_hist\n",
    "        )\n",
    "        with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "            f.write(args_text)\n",
    "\n",
    "    if utils.is_primary(args) and args.log_wandb:\n",
    "        if has_wandb:\n",
    "            wandb.init(project='timm',name=args.experiment, config=args)\n",
    "        else:\n",
    "            _logger.warning(\n",
    "                \"You've requested to log metrics to wandb but package not found. \"\n",
    "                \"Metrics not being logged to wandb, try `pip install wandb`\")\n",
    "\n",
    "    # setup learning rate schedule and starting epoch\n",
    "    updates_per_epoch = (len(loader_train) + args.grad_accum_steps - 1) // args.grad_accum_steps\n",
    "    lr_scheduler, num_epochs = create_scheduler_v2(\n",
    "        optimizer,\n",
    "        **scheduler_kwargs(args),\n",
    "        updates_per_epoch=updates_per_epoch,\n",
    "    )\n",
    "    start_epoch = 10\n",
    "    if args.start_epoch is not None:\n",
    "        # a specified start_epoch will always override the resume epoch\n",
    "        start_epoch = args.start_epoch\n",
    "    elif resume_epoch is not None:\n",
    "        start_epoch = resume_epoch\n",
    "    if lr_scheduler is not None and start_epoch > 0:\n",
    "        if args.sched_on_updates:\n",
    "            lr_scheduler.step_update(start_epoch * updates_per_epoch)\n",
    "        else:\n",
    "            lr_scheduler.step(start_epoch)\n",
    "\n",
    "    if utils.is_primary(args):\n",
    "        _logger.info(\n",
    "            f'Scheduled epochs: {num_epochs}. LR stepped per {\"epoch\" if lr_scheduler.t_in_epochs else \"update\"}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "4248783360.0 21974632.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "# model = resnet50()\n",
    "input = torch.randn(1, 3, 224, 224).to('cuda')\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(macs, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0047,  0.0291,  0.0084,  ...,  0.0061,  0.0207,  0.0245],\n",
      "        [-0.0064, -0.0107, -0.0039,  ..., -0.0381, -0.0248, -0.0018],\n",
      "        [ 0.0030,  0.0182, -0.0012,  ...,  0.0175, -0.0210, -0.0017],\n",
      "        ...,\n",
      "        [-0.0133,  0.0130,  0.0257,  ..., -0.0031,  0.0145,  0.0067],\n",
      "        [ 0.0034,  0.0407,  0.0003,  ..., -0.0187,  0.0027,  0.0183],\n",
      "        [ 0.0138, -0.0381, -0.0353,  ...,  0.0069, -0.0030,  0.0216]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0384,  0.0526,  0.0231,  ...,  0.0080, -0.0127, -0.0318],\n",
      "        [ 0.0123,  0.0325, -0.0157,  ..., -0.0021, -0.1333, -0.1377],\n",
      "        [-0.0235, -0.0248, -0.1257,  ...,  0.0762, -0.0377, -0.0869],\n",
      "        ...,\n",
      "        [-0.0528,  0.0271,  0.0363,  ...,  0.0818,  0.0323, -0.0372],\n",
      "        [ 0.0458,  0.0301, -0.0118,  ...,  0.0388, -0.0197,  0.0305],\n",
      "        [ 0.1512,  0.0830, -0.0156,  ...,  0.0436,  0.0078,  0.0451]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0384,  0.0526,  0.0231,  ...,  0.0080, -0.0127, -0.0318],\n",
      "        [ 0.0123,  0.0325, -0.0157,  ..., -0.0021, -0.1333, -0.1377],\n",
      "        [-0.0235, -0.0248, -0.1257,  ...,  0.0762, -0.0377, -0.0869],\n",
      "        ...,\n",
      "        [-0.0528,  0.0271,  0.0363,  ...,  0.0818,  0.0323, -0.0372],\n",
      "        [ 0.0458,  0.0301, -0.0118,  ...,  0.0388, -0.0197,  0.0305],\n",
      "        [ 0.1512,  0.0830, -0.0156,  ...,  0.0436,  0.0078,  0.0451]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "print(old_model.state_dict()['blocks.0.attn.qkv.weight'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    tmp = old_model.state_dict()\n",
    "    for param_tensor in old_model.state_dict(): # 字典的遍历默认是遍历 key，所以param_tensor实际上是键值\n",
    "        # print(param_tensor,'\\t',model.state_dict()[param_tensor].size())\n",
    "        # 如果不是 attn 或者 mlp 层\n",
    "        if 'attn' not in param_tensor and 'mlp' not in param_tensor:\n",
    "            continue\n",
    "        old_size = old_model.state_dict()[param_tensor].size()\n",
    "        new_size = model.state_dict()[param_tensor].size()\n",
    "        flag = False\n",
    "        if len(old_size) != len(new_size):\n",
    "            continue\n",
    "        elif len(old_size) == 1:\n",
    "            # 如果整除\n",
    "            if new_size[0] % old_size[0] == 0:\n",
    "                times = new_size[0] // old_size[0]\n",
    "                tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "            # if old_size[0]*2 == new_size[0]:\n",
    "            #     tmp[param_tensor] = torch.cat((tmp[param_tensor], tmp[param_tensor]), 0)\n",
    "                flag = True\n",
    "            # 如果是 2:3\n",
    "            elif old_size[0]*3 == new_size[0]*2:\n",
    "                # 拼接tensor和它自己的一半\n",
    "                if old_size[0] % 2 == 0:\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                    # tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][]), 0)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    # 未实现\n",
    "                    print('error')\n",
    "        elif len(old_size) == 2:\n",
    "            if old_size[0] == new_size[0]:\n",
    "                # if old_size[1]*2 == new_size[1]:\n",
    "                if new_size[1] % old_size[1] == 0:\n",
    "                    times = new_size[1] // old_size[1]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 1)\n",
    "                    flag = True\n",
    "                elif old_size[1]*3 == new_size[1]*2:\n",
    "                    if old_size[1] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:,old_size[1]//2]), 1)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "            elif old_size[1] == new_size[1]:\n",
    "                # if old_size[0]*2 == new_size[0]:\n",
    "                if new_size[0] % old_size[0] == 0:\n",
    "                    times = new_size[0] // old_size[0]\n",
    "                    tmp[param_tensor] = torch.cat((tmp[param_tensor],)*times, 0)\n",
    "                    flag = True\n",
    "                elif old_size[0]*3 == new_size[0]*2:    \n",
    "                    if old_size[0] % 2 == 0:\n",
    "                        tmp[param_tensor] = torch.cat((tmp[param_tensor],tmp[param_tensor][:old_size[0]//2]), 0)\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        # 未实现\n",
    "                        print('error')\n",
    "        if not flag:\n",
    "            if param_tensor in model.state_dict():\n",
    "                tmp[param_tensor] = model.state_dict()[param_tensor]\n",
    "            else:\n",
    "                continue\n",
    "    model.load_state_dict(tmp, strict=False)\n",
    "    print(model.state_dict()['blocks.0.attn.qkv.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 384]) torch.Size([1, 1, 384])\n",
      "torch.Size([1, 197, 384]) torch.Size([1, 197, 384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([576]) torch.Size([1152])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([768]) torch.Size([1536])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([384]) torch.Size([384])\n",
      "torch.Size([1000]) torch.Size([1000])\n",
      "torch.Size([384, 3, 16, 16]) torch.Size([384, 3, 16, 16])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([576, 384]) torch.Size([1152, 384])\n",
      "torch.Size([384, 192]) torch.Size([384, 384])\n",
      "torch.Size([768, 384]) torch.Size([1536, 384])\n",
      "torch.Size([384, 768]) torch.Size([384, 1536])\n",
      "torch.Size([1000, 384]) torch.Size([1000, 384])\n"
     ]
    }
   ],
   "source": [
    "len(old_optimizer.param_groups[0]['params'])\n",
    "# 接下来，遍历原始模型的参数，把他们加入新的优化器中\n",
    "for param_group in range(len(old_optimizer.param_groups)):\n",
    "    for i, (old_p, new_p) in enumerate(zip(old_optimizer.param_groups[param_group]['params'], optimizer.param_groups[param_group]['params'])):\n",
    "        # 查看 old_p 和 new_p 的 shape\n",
    "        print(old_p.shape, new_p.shape)\n",
    "        with torch.no_grad():\n",
    "            if old_p.shape == new_p.shape:\n",
    "                new_p = old_p\n",
    "            else:\n",
    "                # 否则，用 0 填补 new_p 的后半部分\n",
    "                # 如果 new_p 是 1 维\n",
    "                if len(new_p.shape) == 1:\n",
    "                    new_p[:old_p.shape[0]] = old_p\n",
    "                # 如果 new_p 是 2 维\n",
    "                elif len(new_p.shape) == 2:\n",
    "                    new_p[:old_p.shape[0], :old_p.shape[1]] = old_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 10 [   0/10009 (  0%)]  Loss: 7.12 (7.12)  Time: 1.011s,  126.66/s  (1.011s,  126.66/s)  LR: 1.250e-05  Data: 0.641 (0.641)Time: 1.011s\n",
      "Train: 10 [  50/10009 (  0%)]  Loss: 5.17 (5.95)  Time: 0.166s,  773.36/s  (0.183s,  699.49/s)  LR: 1.250e-05  Data: 0.006 (0.019)Time: 9.333s\n",
      "Train: 10 [ 100/10009 (  1%)]  Loss: 4.69 (5.42)  Time: 0.172s,  743.37/s  (0.176s,  727.37/s)  LR: 1.250e-05  Data: 0.008 (0.013)Time: 17.774s\n",
      "Train: 10 [ 150/10009 (  1%)]  Loss: 4.39 (5.10)  Time: 0.172s,  742.70/s  (0.174s,  734.21/s)  LR: 1.250e-05  Data: 0.008 (0.011)Time: 26.325s\n",
      "Train: 10 [ 200/10009 (  2%)]  Loss: 4.23 (4.89)  Time: 0.170s,  750.99/s  (0.174s,  737.72/s)  LR: 1.250e-05  Data: 0.005 (0.009)Time: 34.876s\n",
      "Train: 10 [ 250/10009 (  2%)]  Loss: 3.89 (4.73)  Time: 0.170s,  753.43/s  (0.173s,  740.10/s)  LR: 1.250e-05  Data: 0.006 (0.009)Time: 43.411s\n",
      "Train: 10 [ 300/10009 (  3%)]  Loss: 4.14 (4.61)  Time: 0.173s,  741.73/s  (0.173s,  741.90/s)  LR: 1.250e-05  Data: 0.007 (0.008)Time: 51.932s\n",
      "Train: 10 [ 350/10009 (  3%)]  Loss: 3.91 (4.50)  Time: 0.171s,  749.14/s  (0.172s,  743.37/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 60.439s\n",
      "Train: 10 [ 400/10009 (  4%)]  Loss: 3.85 (4.42)  Time: 0.171s,  750.21/s  (0.172s,  744.42/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 68.950s\n",
      "Train: 10 [ 450/10009 (  4%)]  Loss: 3.73 (4.35)  Time: 0.170s,  754.08/s  (0.172s,  744.97/s)  LR: 1.250e-05  Data: 0.006 (0.008)Time: 77.491s\n",
      "Train: 10 [ 500/10009 (  5%)]  Loss: 3.91 (4.29)  Time: 0.171s,  750.30/s  (0.172s,  745.69/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 85.999s\n",
      "Train: 10 [ 550/10009 (  5%)]  Loss: 3.31 (4.24)  Time: 0.170s,  752.69/s  (0.172s,  745.02/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 94.666s\n",
      "Train: 10 [ 600/10009 (  6%)]  Loss: 3.57 (4.20)  Time: 0.175s,  730.97/s  (0.172s,  744.80/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 103.288s\n",
      "Train: 10 [ 650/10009 (  6%)]  Loss: 3.83 (4.16)  Time: 0.175s,  730.44/s  (0.172s,  744.25/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 111.962s\n",
      "Train: 10 [ 700/10009 (  7%)]  Loss: 3.63 (4.12)  Time: 0.172s,  743.10/s  (0.172s,  743.65/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 120.659s\n",
      "Train: 10 [ 750/10009 (  7%)]  Loss: 3.59 (4.09)  Time: 0.175s,  732.46/s  (0.172s,  743.22/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 129.340s\n",
      "Train: 10 [ 800/10009 (  8%)]  Loss: 3.45 (4.07)  Time: 0.173s,  741.97/s  (0.172s,  742.77/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 138.034s\n",
      "Train: 10 [ 850/10009 (  8%)]  Loss: 3.47 (4.04)  Time: 0.174s,  737.28/s  (0.172s,  742.38/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 146.728s\n",
      "Train: 10 [ 900/10009 (  9%)]  Loss: 3.64 (4.02)  Time: 0.173s,  741.65/s  (0.172s,  742.03/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 155.422s\n",
      "Train: 10 [ 950/10009 (  9%)]  Loss: 3.70 (4.00)  Time: 0.174s,  735.57/s  (0.173s,  741.70/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 164.121s\n",
      "Train: 10 [1000/10009 ( 10%)]  Loss: 3.51 (3.98)  Time: 0.173s,  741.95/s  (0.173s,  741.38/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 172.824s\n",
      "Train: 10 [1050/10009 ( 10%)]  Loss: 3.61 (3.96)  Time: 0.171s,  749.46/s  (0.173s,  741.52/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 181.422s\n",
      "Train: 10 [1100/10009 ( 11%)]  Loss: 3.63 (3.94)  Time: 0.174s,  736.81/s  (0.173s,  741.69/s)  LR: 1.250e-05  Data: 0.010 (0.007)Time: 190.009s\n",
      "Train: 10 [1150/10009 ( 11%)]  Loss: 3.36 (3.93)  Time: 0.171s,  748.02/s  (0.173s,  741.84/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 198.597s\n",
      "Train: 10 [1200/10009 ( 12%)]  Loss: 3.68 (3.91)  Time: 0.175s,  732.87/s  (0.173s,  741.86/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 207.221s\n",
      "Train: 10 [1250/10009 ( 12%)]  Loss: 3.75 (3.90)  Time: 0.173s,  740.02/s  (0.173s,  741.75/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 215.879s\n",
      "Train: 10 [1300/10009 ( 13%)]  Loss: 3.33 (3.89)  Time: 0.173s,  739.48/s  (0.173s,  741.68/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 224.527s\n",
      "Train: 10 [1350/10009 ( 13%)]  Loss: 3.53 (3.88)  Time: 0.173s,  738.37/s  (0.173s,  741.65/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 233.166s\n",
      "Train: 10 [1400/10009 ( 14%)]  Loss: 3.53 (3.87)  Time: 0.174s,  734.61/s  (0.173s,  741.65/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 241.797s\n",
      "Train: 10 [1450/10009 ( 14%)]  Loss: 3.82 (3.86)  Time: 0.172s,  742.37/s  (0.173s,  741.67/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 250.418s\n",
      "Train: 10 [1500/10009 ( 15%)]  Loss: 3.72 (3.85)  Time: 0.171s,  746.94/s  (0.173s,  741.74/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 259.022s\n",
      "Train: 10 [1550/10009 ( 15%)]  Loss: 3.47 (3.84)  Time: 0.171s,  747.26/s  (0.173s,  741.83/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 267.620s\n",
      "Train: 10 [1600/10009 ( 16%)]  Loss: 3.57 (3.83)  Time: 0.175s,  730.36/s  (0.173s,  741.53/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 276.357s\n",
      "Train: 10 [1650/10009 ( 16%)]  Loss: 3.67 (3.82)  Time: 0.176s,  726.16/s  (0.173s,  741.24/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 285.102s\n",
      "Train: 10 [1700/10009 ( 17%)]  Loss: 3.82 (3.81)  Time: 0.175s,  733.46/s  (0.173s,  740.94/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 293.855s\n",
      "Train: 10 [1750/10009 ( 17%)]  Loss: 3.72 (3.81)  Time: 0.176s,  727.48/s  (0.173s,  740.66/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 302.604s\n",
      "Train: 10 [1800/10009 ( 18%)]  Loss: 3.70 (3.80)  Time: 0.174s,  734.05/s  (0.173s,  740.34/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 311.380s\n",
      "Train: 10 [1850/10009 ( 18%)]  Loss: 3.73 (3.79)  Time: 0.172s,  742.75/s  (0.173s,  740.15/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 320.110s\n",
      "Train: 10 [1900/10009 ( 19%)]  Loss: 3.14 (3.79)  Time: 0.172s,  744.00/s  (0.173s,  740.05/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 328.800s\n",
      "Train: 10 [1950/10009 ( 19%)]  Loss: 3.66 (3.78)  Time: 0.174s,  735.90/s  (0.173s,  739.89/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 337.518s\n",
      "Train: 10 [2000/10009 ( 20%)]  Loss: 3.36 (3.77)  Time: 0.174s,  734.58/s  (0.173s,  739.72/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 346.250s\n",
      "Train: 10 [2050/10009 ( 20%)]  Loss: 3.54 (3.77)  Time: 0.175s,  732.21/s  (0.173s,  739.52/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 354.999s\n",
      "Train: 10 [2100/10009 ( 21%)]  Loss: 3.68 (3.76)  Time: 0.175s,  730.02/s  (0.173s,  739.32/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 363.751s\n",
      "Train: 10 [2150/10009 ( 21%)]  Loss: 3.67 (3.76)  Time: 0.175s,  730.48/s  (0.173s,  739.13/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 372.503s\n",
      "Train: 10 [2200/10009 ( 22%)]  Loss: 3.38 (3.75)  Time: 0.172s,  742.36/s  (0.173s,  739.01/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 381.221s\n",
      "Train: 10 [2250/10009 ( 22%)]  Loss: 3.54 (3.75)  Time: 0.174s,  736.85/s  (0.173s,  738.96/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 389.909s\n",
      "Train: 10 [2300/10009 ( 23%)]  Loss: 3.93 (3.74)  Time: 0.173s,  741.40/s  (0.173s,  738.96/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 398.569s\n",
      "Train: 10 [2350/10009 ( 23%)]  Loss: 3.68 (3.74)  Time: 0.173s,  741.09/s  (0.173s,  739.08/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 407.167s\n",
      "Train: 10 [2400/10009 ( 24%)]  Loss: 3.37 (3.73)  Time: 0.172s,  745.30/s  (0.173s,  739.18/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 415.771s\n",
      "Train: 10 [2450/10009 ( 24%)]  Loss: 3.29 (3.73)  Time: 0.171s,  746.90/s  (0.173s,  739.27/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 424.377s\n",
      "Train: 10 [2500/10009 ( 25%)]  Loss: 3.64 (3.73)  Time: 0.172s,  744.51/s  (0.173s,  739.33/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 432.997s\n",
      "Train: 10 [2550/10009 ( 25%)]  Loss: 3.36 (3.72)  Time: 0.177s,  724.04/s  (0.173s,  739.24/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 441.706s\n",
      "Train: 10 [2600/10009 ( 26%)]  Loss: 3.48 (3.72)  Time: 0.172s,  742.92/s  (0.173s,  739.10/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 450.448s\n",
      "Train: 10 [2650/10009 ( 26%)]  Loss: 3.78 (3.71)  Time: 0.172s,  744.94/s  (0.173s,  739.12/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 459.095s\n",
      "Train: 10 [2700/10009 ( 27%)]  Loss: 3.83 (3.71)  Time: 0.175s,  731.28/s  (0.173s,  739.06/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 467.791s\n",
      "Train: 10 [2750/10009 ( 27%)]  Loss: 3.21 (3.71)  Time: 0.174s,  735.24/s  (0.173s,  738.91/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 476.547s\n",
      "Train: 10 [2800/10009 ( 28%)]  Loss: 3.62 (3.70)  Time: 0.175s,  730.88/s  (0.173s,  738.79/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 485.288s\n",
      "Train: 10 [2850/10009 ( 28%)]  Loss: 3.80 (3.70)  Time: 0.175s,  733.26/s  (0.173s,  738.66/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 494.037s\n",
      "Train: 10 [2900/10009 ( 29%)]  Loss: 3.44 (3.70)  Time: 0.175s,  732.72/s  (0.173s,  738.53/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 502.793s\n",
      "Train: 10 [2950/10009 ( 29%)]  Loss: 3.49 (3.69)  Time: 0.175s,  731.99/s  (0.173s,  738.40/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 511.551s\n",
      "Train: 10 [3000/10009 ( 30%)]  Loss: 3.38 (3.69)  Time: 0.176s,  728.79/s  (0.173s,  738.27/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 520.310s\n",
      "Train: 10 [3050/10009 ( 30%)]  Loss: 3.42 (3.69)  Time: 0.175s,  731.78/s  (0.173s,  738.14/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 529.068s\n",
      "Train: 10 [3100/10009 ( 31%)]  Loss: 3.02 (3.68)  Time: 0.175s,  730.07/s  (0.173s,  738.01/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 537.831s\n",
      "Train: 10 [3150/10009 ( 31%)]  Loss: 3.66 (3.68)  Time: 0.175s,  732.35/s  (0.173s,  737.90/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 546.587s\n",
      "Train: 10 [3200/10009 ( 32%)]  Loss: 3.60 (3.68)  Time: 0.175s,  730.89/s  (0.173s,  737.79/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 555.346s\n",
      "Train: 10 [3250/10009 ( 32%)]  Loss: 3.45 (3.68)  Time: 0.172s,  742.66/s  (0.174s,  737.68/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 564.106s\n",
      "Train: 10 [3300/10009 ( 33%)]  Loss: 3.57 (3.67)  Time: 0.176s,  728.33/s  (0.174s,  737.60/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 572.844s\n",
      "Train: 10 [3350/10009 ( 33%)]  Loss: 3.43 (3.67)  Time: 0.175s,  731.34/s  (0.174s,  737.51/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 581.585s\n",
      "Train: 10 [3400/10009 ( 34%)]  Loss: 3.60 (3.67)  Time: 0.173s,  738.30/s  (0.174s,  737.45/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 590.316s\n",
      "Train: 10 [3450/10009 ( 34%)]  Loss: 3.66 (3.67)  Time: 0.174s,  735.44/s  (0.174s,  737.39/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 599.046s\n",
      "Train: 10 [3500/10009 ( 35%)]  Loss: 3.51 (3.66)  Time: 0.173s,  740.14/s  (0.174s,  737.34/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 607.760s\n",
      "Train: 10 [3550/10009 ( 35%)]  Loss: 3.73 (3.66)  Time: 0.173s,  741.51/s  (0.174s,  737.31/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 616.469s\n",
      "Train: 10 [3600/10009 ( 36%)]  Loss: 3.20 (3.66)  Time: 0.173s,  737.95/s  (0.174s,  737.31/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 625.149s\n",
      "Train: 10 [3650/10009 ( 36%)]  Loss: 3.44 (3.66)  Time: 0.173s,  738.09/s  (0.174s,  737.27/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 633.863s\n",
      "Train: 10 [3700/10009 ( 37%)]  Loss: 3.42 (3.65)  Time: 0.175s,  729.97/s  (0.174s,  737.19/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 642.611s\n",
      "Train: 10 [3750/10009 ( 37%)]  Loss: 3.26 (3.65)  Time: 0.176s,  728.25/s  (0.174s,  737.11/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 651.365s\n",
      "Train: 10 [3800/10009 ( 38%)]  Loss: 3.42 (3.65)  Time: 0.175s,  731.05/s  (0.174s,  737.03/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 660.122s\n",
      "Train: 10 [3850/10009 ( 38%)]  Loss: 3.46 (3.65)  Time: 0.174s,  736.81/s  (0.174s,  736.97/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 668.858s\n",
      "Train: 10 [3900/10009 ( 39%)]  Loss: 3.28 (3.65)  Time: 0.172s,  742.73/s  (0.174s,  736.92/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 677.583s\n",
      "Train: 10 [3950/10009 ( 39%)]  Loss: 3.66 (3.64)  Time: 0.173s,  741.44/s  (0.174s,  736.96/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 686.234s\n",
      "Train: 10 [4000/10009 ( 40%)]  Loss: 3.64 (3.64)  Time: 0.174s,  735.93/s  (0.174s,  736.93/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 694.944s\n",
      "Train: 10 [4050/10009 ( 40%)]  Loss: 3.32 (3.64)  Time: 0.173s,  741.82/s  (0.174s,  736.98/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 703.586s\n",
      "Train: 10 [4100/10009 ( 41%)]  Loss: 3.63 (3.64)  Time: 0.173s,  739.03/s  (0.174s,  737.01/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 712.237s\n",
      "Train: 10 [4150/10009 ( 41%)]  Loss: 3.36 (3.64)  Time: 0.172s,  742.17/s  (0.174s,  737.07/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 720.865s\n",
      "Train: 10 [4200/10009 ( 42%)]  Loss: 3.35 (3.63)  Time: 0.172s,  744.86/s  (0.174s,  737.13/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 729.489s\n",
      "Train: 10 [4250/10009 ( 42%)]  Loss: 3.59 (3.63)  Time: 0.172s,  744.56/s  (0.174s,  737.20/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 738.096s\n",
      "Train: 10 [4300/10009 ( 43%)]  Loss: 3.57 (3.63)  Time: 0.172s,  745.49/s  (0.174s,  737.28/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 746.704s\n",
      "Train: 10 [4350/10009 ( 43%)]  Loss: 3.37 (3.63)  Time: 0.175s,  730.19/s  (0.174s,  737.25/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 755.410s\n",
      "Train: 10 [4400/10009 ( 44%)]  Loss: 3.64 (3.63)  Time: 0.174s,  733.89/s  (0.174s,  737.17/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 764.176s\n",
      "Train: 10 [4450/10009 ( 44%)]  Loss: 3.62 (3.63)  Time: 0.176s,  729.25/s  (0.174s,  737.11/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 772.923s\n",
      "Train: 10 [4500/10009 ( 45%)]  Loss: 3.81 (3.62)  Time: 0.175s,  729.67/s  (0.174s,  737.06/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 781.651s\n",
      "Train: 10 [4550/10009 ( 45%)]  Loss: 3.36 (3.62)  Time: 0.173s,  739.39/s  (0.174s,  737.02/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 790.387s\n",
      "Train: 10 [4600/10009 ( 46%)]  Loss: 3.51 (3.62)  Time: 0.174s,  733.56/s  (0.174s,  736.95/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 799.139s\n",
      "Train: 10 [4650/10009 ( 46%)]  Loss: 3.55 (3.62)  Time: 0.173s,  739.13/s  (0.174s,  736.99/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 807.778s\n",
      "Train: 10 [4700/10009 ( 47%)]  Loss: 3.50 (3.62)  Time: 0.173s,  739.20/s  (0.174s,  736.95/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 816.509s\n",
      "Train: 10 [4750/10009 ( 47%)]  Loss: 3.51 (3.62)  Time: 0.177s,  724.45/s  (0.174s,  736.90/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 825.252s\n",
      "Train: 10 [4800/10009 ( 48%)]  Loss: 3.53 (3.61)  Time: 0.175s,  731.70/s  (0.174s,  736.84/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 834.000s\n",
      "Train: 10 [4850/10009 ( 48%)]  Loss: 3.66 (3.61)  Time: 0.174s,  734.70/s  (0.174s,  736.42/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 843.165s\n",
      "Train: 10 [4900/10009 ( 49%)]  Loss: 3.26 (3.61)  Time: 0.175s,  732.97/s  (0.174s,  735.21/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 853.264s\n",
      "Train: 10 [4950/10009 ( 49%)]  Loss: 3.56 (3.61)  Time: 0.173s,  738.14/s  (0.174s,  735.18/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 862.004s\n",
      "Train: 10 [5000/10009 ( 50%)]  Loss: 3.87 (3.61)  Time: 0.174s,  737.27/s  (0.174s,  735.15/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 870.741s\n",
      "Train: 10 [5050/10009 ( 50%)]  Loss: 3.65 (3.61)  Time: 0.175s,  732.20/s  (0.174s,  735.13/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 879.475s\n",
      "Train: 10 [5100/10009 ( 51%)]  Loss: 3.16 (3.61)  Time: 0.174s,  734.01/s  (0.174s,  735.12/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 888.192s\n",
      "Train: 10 [5150/10009 ( 51%)]  Loss: 3.63 (3.61)  Time: 0.175s,  731.94/s  (0.174s,  735.09/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 896.931s\n",
      "Train: 10 [5200/10009 ( 52%)]  Loss: 3.23 (3.61)  Time: 0.174s,  735.26/s  (0.174s,  735.08/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 905.658s\n",
      "Train: 10 [5250/10009 ( 52%)]  Loss: 3.34 (3.60)  Time: 0.174s,  735.26/s  (0.174s,  735.06/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 914.382s\n",
      "Train: 10 [5300/10009 ( 53%)]  Loss: 3.63 (3.60)  Time: 0.171s,  750.14/s  (0.174s,  735.08/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 923.062s\n",
      "Train: 10 [5350/10009 ( 53%)]  Loss: 3.38 (3.60)  Time: 0.174s,  735.78/s  (0.174s,  735.08/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 931.776s\n",
      "Train: 10 [5400/10009 ( 54%)]  Loss: 3.36 (3.60)  Time: 0.176s,  728.60/s  (0.174s,  735.11/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 940.443s\n",
      "Train: 10 [5450/10009 ( 54%)]  Loss: 3.25 (3.60)  Time: 0.173s,  738.95/s  (0.174s,  735.07/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 949.191s\n",
      "Train: 10 [5500/10009 ( 55%)]  Loss: 3.35 (3.60)  Time: 0.175s,  732.35/s  (0.174s,  735.03/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 957.953s\n",
      "Train: 10 [5550/10009 ( 55%)]  Loss: 3.14 (3.60)  Time: 0.176s,  727.90/s  (0.174s,  735.00/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 966.709s\n",
      "Train: 10 [5600/10009 ( 56%)]  Loss: 3.75 (3.60)  Time: 0.176s,  728.07/s  (0.174s,  734.95/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 975.483s\n",
      "Train: 10 [5650/10009 ( 56%)]  Loss: 3.38 (3.59)  Time: 0.176s,  728.71/s  (0.174s,  734.95/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 984.190s\n",
      "Train: 10 [5700/10009 ( 57%)]  Loss: 3.45 (3.59)  Time: 0.177s,  724.01/s  (0.174s,  734.91/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 992.947s\n",
      "Train: 10 [5750/10009 ( 57%)]  Loss: 3.36 (3.59)  Time: 0.171s,  746.53/s  (0.174s,  734.97/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1001.579s\n",
      "Train: 10 [5800/10009 ( 58%)]  Loss: 3.62 (3.59)  Time: 0.171s,  748.91/s  (0.174s,  735.04/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 1010.190s\n",
      "Train: 10 [5850/10009 ( 58%)]  Loss: 3.15 (3.59)  Time: 0.173s,  738.70/s  (0.174s,  735.11/s)  LR: 1.250e-05  Data: 0.008 (0.007)Time: 1018.798s\n",
      "Train: 10 [5900/10009 ( 59%)]  Loss: 3.38 (3.59)  Time: 0.172s,  745.90/s  (0.174s,  735.17/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1027.417s\n",
      "Train: 10 [5950/10009 ( 59%)]  Loss: 3.41 (3.59)  Time: 0.174s,  733.62/s  (0.174s,  735.22/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1036.051s\n",
      "Train: 10 [6000/10009 ( 60%)]  Loss: 3.34 (3.59)  Time: 0.174s,  737.02/s  (0.174s,  735.20/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1044.792s\n",
      "Train: 10 [6050/10009 ( 60%)]  Loss: 3.66 (3.59)  Time: 0.174s,  737.39/s  (0.174s,  735.18/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 1053.512s\n",
      "Train: 10 [6100/10009 ( 61%)]  Loss: 3.46 (3.58)  Time: 0.177s,  724.69/s  (0.174s,  735.16/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 1062.253s\n",
      "Train: 10 [6150/10009 ( 61%)]  Loss: 3.30 (3.58)  Time: 0.161s,  794.80/s  (0.174s,  735.39/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1070.626s\n",
      "Train: 10 [6200/10009 ( 62%)]  Loss: 3.45 (3.58)  Time: 0.160s,  799.58/s  (0.174s,  735.82/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1078.693s\n",
      "Train: 10 [6250/10009 ( 62%)]  Loss: 3.60 (3.58)  Time: 0.160s,  798.96/s  (0.174s,  736.26/s)  LR: 1.250e-05  Data: 0.005 (0.007)Time: 1086.739s\n",
      "Train: 10 [6300/10009 ( 63%)]  Loss: 3.40 (3.58)  Time: 0.161s,  795.17/s  (0.174s,  736.70/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1094.788s\n",
      "Train: 10 [6350/10009 ( 63%)]  Loss: 3.78 (3.58)  Time: 0.161s,  796.64/s  (0.174s,  737.11/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1102.849s\n",
      "Train: 10 [6400/10009 ( 64%)]  Loss: 3.77 (3.58)  Time: 0.161s,  794.91/s  (0.174s,  737.54/s)  LR: 1.250e-05  Data: 0.007 (0.007)Time: 1110.898s\n",
      "Train: 10 [6450/10009 ( 64%)]  Loss: 3.72 (3.58)  Time: 0.160s,  797.79/s  (0.173s,  737.96/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1118.932s\n",
      "Train: 10 [6500/10009 ( 65%)]  Loss: 3.65 (3.58)  Time: 0.160s,  799.41/s  (0.173s,  738.37/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1126.979s\n",
      "Train: 10 [6550/10009 ( 65%)]  Loss: 3.61 (3.58)  Time: 0.161s,  797.04/s  (0.173s,  738.76/s)  LR: 1.250e-05  Data: 0.006 (0.007)Time: 1135.040s\n",
      "Train: 10 [6600/10009 ( 66%)]  Loss: 3.47 (3.58)  Time: 0.161s,  795.63/s  (0.173s,  739.16/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1143.084s\n",
      "Train: 10 [6650/10009 ( 66%)]  Loss: 3.33 (3.57)  Time: 0.161s,  793.49/s  (0.173s,  739.55/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1151.140s\n",
      "Train: 10 [6700/10009 ( 67%)]  Loss: 3.48 (3.57)  Time: 0.162s,  787.95/s  (0.173s,  739.94/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1159.187s\n",
      "Train: 10 [6750/10009 ( 67%)]  Loss: 3.37 (3.57)  Time: 0.162s,  788.79/s  (0.173s,  740.31/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1167.242s\n",
      "Train: 10 [6800/10009 ( 68%)]  Loss: 3.37 (3.57)  Time: 0.161s,  796.58/s  (0.173s,  740.67/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1175.318s\n",
      "Train: 10 [6850/10009 ( 68%)]  Loss: 3.38 (3.57)  Time: 0.161s,  796.40/s  (0.173s,  741.04/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1183.365s\n",
      "Train: 10 [6900/10009 ( 69%)]  Loss: 3.48 (3.57)  Time: 0.162s,  791.96/s  (0.173s,  741.41/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1191.414s\n",
      "Train: 10 [6950/10009 ( 69%)]  Loss: 3.47 (3.57)  Time: 0.160s,  798.67/s  (0.173s,  741.77/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1199.456s\n",
      "Train: 10 [7000/10009 ( 70%)]  Loss: 3.37 (3.57)  Time: 0.161s,  794.40/s  (0.172s,  742.13/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1207.506s\n",
      "Train: 10 [7050/10009 ( 70%)]  Loss: 3.24 (3.57)  Time: 0.160s,  798.84/s  (0.172s,  742.48/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1215.558s\n",
      "Train: 10 [7100/10009 ( 71%)]  Loss: 3.22 (3.57)  Time: 0.162s,  790.59/s  (0.172s,  742.82/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1223.611s\n",
      "Train: 10 [7150/10009 ( 71%)]  Loss: 3.51 (3.57)  Time: 0.162s,  790.07/s  (0.172s,  743.16/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1231.668s\n",
      "Train: 10 [7200/10009 ( 72%)]  Loss: 3.44 (3.57)  Time: 0.161s,  795.34/s  (0.172s,  743.50/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1239.713s\n",
      "Train: 10 [7250/10009 ( 72%)]  Loss: 3.32 (3.57)  Time: 0.160s,  799.00/s  (0.172s,  743.83/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1247.758s\n",
      "Train: 10 [7300/10009 ( 73%)]  Loss: 3.37 (3.56)  Time: 0.160s,  798.05/s  (0.172s,  744.16/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1255.814s\n",
      "Train: 10 [7350/10009 ( 73%)]  Loss: 3.27 (3.56)  Time: 0.160s,  798.50/s  (0.172s,  744.47/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1263.880s\n",
      "Train: 10 [7400/10009 ( 74%)]  Loss: 3.78 (3.56)  Time: 0.161s,  796.97/s  (0.172s,  744.79/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1271.930s\n",
      "Train: 10 [7450/10009 ( 74%)]  Loss: 3.56 (3.56)  Time: 0.161s,  796.36/s  (0.172s,  745.11/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1279.987s\n",
      "Train: 10 [7500/10009 ( 75%)]  Loss: 3.34 (3.56)  Time: 0.161s,  793.52/s  (0.172s,  745.42/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1288.036s\n",
      "Train: 10 [7550/10009 ( 75%)]  Loss: 3.19 (3.56)  Time: 0.161s,  794.56/s  (0.172s,  745.73/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1296.083s\n",
      "Train: 10 [7600/10009 ( 76%)]  Loss: 3.25 (3.56)  Time: 0.163s,  785.94/s  (0.172s,  746.02/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1304.149s\n",
      "Train: 10 [7650/10009 ( 76%)]  Loss: 3.46 (3.56)  Time: 0.163s,  786.86/s  (0.172s,  746.33/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1312.186s\n",
      "Train: 10 [7700/10009 ( 77%)]  Loss: 3.61 (3.56)  Time: 0.162s,  787.98/s  (0.171s,  746.63/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1320.234s\n",
      "Train: 10 [7750/10009 ( 77%)]  Loss: 3.51 (3.56)  Time: 0.160s,  797.78/s  (0.171s,  746.92/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1328.295s\n",
      "Train: 10 [7800/10009 ( 78%)]  Loss: 3.48 (3.56)  Time: 0.161s,  796.39/s  (0.171s,  747.20/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1336.353s\n",
      "Train: 10 [7850/10009 ( 78%)]  Loss: 3.45 (3.56)  Time: 0.160s,  800.17/s  (0.171s,  747.49/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1344.406s\n",
      "Train: 10 [7900/10009 ( 79%)]  Loss: 3.37 (3.56)  Time: 0.161s,  796.41/s  (0.171s,  747.77/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1352.448s\n",
      "Train: 10 [7950/10009 ( 79%)]  Loss: 3.26 (3.55)  Time: 0.161s,  797.14/s  (0.171s,  748.06/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1360.495s\n",
      "Train: 10 [8000/10009 ( 80%)]  Loss: 3.22 (3.55)  Time: 0.160s,  797.60/s  (0.171s,  748.33/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1368.548s\n",
      "Train: 10 [8050/10009 ( 80%)]  Loss: 3.55 (3.55)  Time: 0.161s,  793.35/s  (0.171s,  748.60/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1376.600s\n",
      "Train: 10 [8100/10009 ( 81%)]  Loss: 3.27 (3.55)  Time: 0.161s,  796.73/s  (0.171s,  748.82/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1384.744s\n",
      "Train: 10 [8150/10009 ( 81%)]  Loss: 3.30 (3.55)  Time: 0.161s,  796.09/s  (0.171s,  749.08/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1392.806s\n",
      "Train: 10 [8200/10009 ( 82%)]  Loss: 3.69 (3.55)  Time: 0.160s,  799.47/s  (0.171s,  749.35/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1400.851s\n",
      "Train: 10 [8250/10009 ( 82%)]  Loss: 3.67 (3.55)  Time: 0.162s,  788.25/s  (0.171s,  749.61/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 1408.908s\n",
      "Train: 10 [8300/10009 ( 83%)]  Loss: 3.43 (3.55)  Time: 0.161s,  796.01/s  (0.171s,  749.86/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1416.958s\n",
      "Train: 10 [8350/10009 ( 83%)]  Loss: 3.32 (3.55)  Time: 0.160s,  798.46/s  (0.171s,  750.13/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1424.994s\n",
      "Train: 10 [8400/10009 ( 84%)]  Loss: 3.62 (3.55)  Time: 0.162s,  792.40/s  (0.171s,  750.38/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1433.044s\n",
      "Train: 10 [8450/10009 ( 84%)]  Loss: 3.35 (3.55)  Time: 0.160s,  798.09/s  (0.171s,  750.63/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1441.089s\n",
      "Train: 10 [8500/10009 ( 85%)]  Loss: 3.27 (3.55)  Time: 0.161s,  795.45/s  (0.170s,  750.88/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1449.133s\n",
      "Train: 10 [8550/10009 ( 85%)]  Loss: 3.34 (3.55)  Time: 0.162s,  791.34/s  (0.170s,  751.12/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1457.197s\n",
      "Train: 10 [8600/10009 ( 86%)]  Loss: 3.37 (3.55)  Time: 0.161s,  793.54/s  (0.170s,  751.35/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1465.266s\n",
      "Train: 10 [8650/10009 ( 86%)]  Loss: 3.34 (3.54)  Time: 0.162s,  791.39/s  (0.170s,  751.58/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1473.337s\n",
      "Train: 10 [8700/10009 ( 87%)]  Loss: 3.41 (3.54)  Time: 0.160s,  800.19/s  (0.170s,  751.81/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1481.393s\n",
      "Train: 10 [8750/10009 ( 87%)]  Loss: 3.44 (3.54)  Time: 0.161s,  793.41/s  (0.170s,  752.05/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1489.436s\n",
      "Train: 10 [8800/10009 ( 88%)]  Loss: 3.45 (3.54)  Time: 0.160s,  801.12/s  (0.170s,  752.27/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1497.497s\n",
      "Train: 10 [8850/10009 ( 88%)]  Loss: 3.21 (3.54)  Time: 0.160s,  799.01/s  (0.170s,  752.50/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1505.546s\n",
      "Train: 10 [8900/10009 ( 89%)]  Loss: 3.59 (3.54)  Time: 0.161s,  796.29/s  (0.170s,  752.73/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1513.595s\n",
      "Train: 10 [8950/10009 ( 89%)]  Loss: 3.55 (3.54)  Time: 0.161s,  795.53/s  (0.170s,  752.96/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1521.637s\n",
      "Train: 10 [9000/10009 ( 90%)]  Loss: 3.61 (3.54)  Time: 0.162s,  790.66/s  (0.170s,  753.17/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1529.693s\n",
      "Train: 10 [9050/10009 ( 90%)]  Loss: 3.35 (3.54)  Time: 0.162s,  792.02/s  (0.170s,  753.39/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 1537.744s\n",
      "Train: 10 [9100/10009 ( 91%)]  Loss: 3.44 (3.54)  Time: 0.161s,  794.94/s  (0.170s,  753.61/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1545.788s\n",
      "Train: 10 [9150/10009 ( 91%)]  Loss: 3.56 (3.54)  Time: 0.160s,  800.87/s  (0.170s,  753.83/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1553.838s\n",
      "Train: 10 [9200/10009 ( 92%)]  Loss: 3.17 (3.54)  Time: 0.161s,  796.69/s  (0.170s,  754.05/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1561.871s\n",
      "Train: 10 [9250/10009 ( 92%)]  Loss: 3.46 (3.54)  Time: 0.161s,  797.30/s  (0.170s,  754.25/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1569.934s\n",
      "Train: 10 [9300/10009 ( 93%)]  Loss: 3.18 (3.54)  Time: 0.161s,  794.66/s  (0.170s,  754.46/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1577.989s\n",
      "Train: 10 [9350/10009 ( 93%)]  Loss: 3.35 (3.54)  Time: 0.160s,  800.02/s  (0.170s,  754.67/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1586.028s\n",
      "Train: 10 [9400/10009 ( 94%)]  Loss: 2.96 (3.54)  Time: 0.161s,  793.60/s  (0.170s,  754.87/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1594.089s\n",
      "Train: 10 [9450/10009 ( 94%)]  Loss: 3.07 (3.53)  Time: 0.162s,  788.89/s  (0.170s,  755.06/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1602.157s\n",
      "Train: 10 [9500/10009 ( 95%)]  Loss: 3.27 (3.53)  Time: 0.162s,  792.10/s  (0.169s,  755.26/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1610.206s\n",
      "Train: 10 [9550/10009 ( 95%)]  Loss: 3.49 (3.53)  Time: 0.163s,  784.67/s  (0.169s,  755.46/s)  LR: 1.250e-05  Data: 0.008 (0.006)Time: 1618.255s\n",
      "Train: 10 [9600/10009 ( 96%)]  Loss: 3.38 (3.53)  Time: 0.160s,  798.41/s  (0.169s,  755.65/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1626.310s\n",
      "Train: 10 [9650/10009 ( 96%)]  Loss: 3.11 (3.53)  Time: 0.160s,  797.82/s  (0.169s,  755.84/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1634.366s\n",
      "Train: 10 [9700/10009 ( 97%)]  Loss: 3.30 (3.53)  Time: 0.161s,  796.62/s  (0.169s,  756.03/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1642.434s\n",
      "Train: 10 [9750/10009 ( 97%)]  Loss: 3.08 (3.53)  Time: 0.160s,  798.35/s  (0.169s,  756.22/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1650.475s\n",
      "Train: 10 [9800/10009 ( 98%)]  Loss: 3.45 (3.53)  Time: 0.161s,  795.52/s  (0.169s,  756.41/s)  LR: 1.250e-05  Data: 0.007 (0.006)Time: 1658.518s\n",
      "Train: 10 [9850/10009 ( 98%)]  Loss: 3.49 (3.53)  Time: 0.161s,  793.36/s  (0.169s,  756.60/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1666.563s\n",
      "Train: 10 [9900/10009 ( 99%)]  Loss: 3.25 (3.53)  Time: 0.160s,  798.70/s  (0.169s,  756.79/s)  LR: 1.250e-05  Data: 0.005 (0.006)Time: 1674.600s\n",
      "Train: 10 [9950/10009 ( 99%)]  Loss: 3.15 (3.53)  Time: 0.161s,  797.19/s  (0.169s,  756.97/s)  LR: 1.250e-05  Data: 0.006 (0.006)Time: 1682.659s\n",
      "Train: 10 [10000/10009 (100%)]  Loss: 3.34 (3.53)  Time: 0.203s,  631.67/s  (0.169s,  757.13/s)  LR: 1.250e-05  Data: 0.049 (0.006)Time: 1690.765s\n",
      "Test: [   0/390]  Time: 0.674 (0.674)  Loss:   1.293 ( 1.293)  Acc@1:  77.344 ( 77.344)  Acc@5:  89.844 ( 89.844)\n",
      "Test: [  50/390]  Time: 0.053 (0.140)  Loss:   1.117 ( 2.156)  Acc@1:  72.656 ( 54.810)  Acc@5:  89.844 ( 76.210)\n",
      "Test: [ 100/390]  Time: 0.120 (0.135)  Loss:   2.358 ( 2.177)  Acc@1:  43.750 ( 51.323)  Acc@5:  77.344 ( 76.709)\n",
      "Test: [ 150/390]  Time: 0.051 (0.136)  Loss:   1.972 ( 2.167)  Acc@1:  48.438 ( 51.630)  Acc@5:  82.031 ( 76.857)\n",
      "Test: [ 200/390]  Time: 0.051 (0.134)  Loss:   3.377 ( 2.353)  Acc@1:  23.438 ( 48.453)  Acc@5:  57.031 ( 73.791)\n",
      "Test: [ 250/390]  Time: 0.051 (0.134)  Loss:   2.423 ( 2.468)  Acc@1:  55.469 ( 46.791)  Acc@5:  69.531 ( 71.704)\n",
      "Test: [ 300/390]  Time: 0.391 (0.134)  Loss:   2.856 ( 2.561)  Acc@1:  47.656 ( 45.315)  Acc@5:  63.281 ( 69.944)\n",
      "Test: [ 350/390]  Time: 0.051 (0.133)  Loss:   2.860 ( 2.633)  Acc@1:  42.188 ( 44.017)  Acc@5:  64.844 ( 68.739)\n",
      "Test: [ 390/390]  Time: 0.034 (0.134)  Loss:   3.787 ( 2.593)  Acc@1:  22.500 ( 44.822)  Acc@5:  55.000 ( 69.428)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 11 [   0/10009 (  0%)]  Loss: 3.15 (3.15)  Time: 0.790s,  162.02/s  (0.790s,  162.02/s)  LR: 8.272e-06  Data: 0.636 (0.636)Time: 0.790s\n",
      "Train: 11 [  50/10009 (  0%)]  Loss: 3.28 (3.38)  Time: 0.160s,  802.06/s  (0.172s,  745.25/s)  LR: 8.272e-06  Data: 0.006 (0.019)Time: 8.760s\n",
      "Train: 11 [ 100/10009 (  1%)]  Loss: 2.92 (3.36)  Time: 0.159s,  804.34/s  (0.166s,  772.79/s)  LR: 8.272e-06  Data: 0.006 (0.013)Time: 16.729s\n",
      "Train: 11 [ 150/10009 (  1%)]  Loss: 3.10 (3.35)  Time: 0.159s,  805.66/s  (0.164s,  781.88/s)  LR: 8.272e-06  Data: 0.006 (0.011)Time: 24.720s\n",
      "Train: 11 [ 200/10009 (  2%)]  Loss: 3.59 (3.37)  Time: 0.161s,  794.80/s  (0.163s,  785.96/s)  LR: 8.272e-06  Data: 0.006 (0.010)Time: 32.735s\n",
      "Train: 11 [ 250/10009 (  2%)]  Loss: 3.22 (3.36)  Time: 0.161s,  794.85/s  (0.162s,  788.32/s)  LR: 8.272e-06  Data: 0.006 (0.009)Time: 40.755s\n",
      "Train: 11 [ 300/10009 (  3%)]  Loss: 3.56 (3.37)  Time: 0.163s,  786.91/s  (0.162s,  789.43/s)  LR: 8.272e-06  Data: 0.007 (0.008)Time: 48.805s\n",
      "Train: 11 [ 350/10009 (  3%)]  Loss: 3.10 (3.36)  Time: 0.161s,  795.72/s  (0.162s,  790.56/s)  LR: 8.272e-06  Data: 0.005 (0.008)Time: 56.831s\n",
      "Train: 11 [ 400/10009 (  4%)]  Loss: 3.55 (3.37)  Time: 0.161s,  793.15/s  (0.162s,  791.46/s)  LR: 8.272e-06  Data: 0.007 (0.008)Time: 64.852s\n",
      "Train: 11 [ 450/10009 (  4%)]  Loss: 3.06 (3.37)  Time: 0.161s,  794.32/s  (0.162s,  791.87/s)  LR: 8.272e-06  Data: 0.007 (0.008)Time: 72.901s\n",
      "Train: 11 [ 500/10009 (  5%)]  Loss: 3.48 (3.37)  Time: 0.160s,  800.03/s  (0.162s,  792.21/s)  LR: 8.272e-06  Data: 0.005 (0.008)Time: 80.949s\n",
      "Train: 11 [ 550/10009 (  5%)]  Loss: 3.26 (3.37)  Time: 0.160s,  797.53/s  (0.162s,  792.39/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 89.007s\n",
      "Train: 11 [ 600/10009 (  6%)]  Loss: 3.51 (3.36)  Time: 0.160s,  798.86/s  (0.161s,  792.71/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 97.044s\n",
      "Train: 11 [ 650/10009 (  6%)]  Loss: 3.20 (3.36)  Time: 0.160s,  798.47/s  (0.161s,  792.98/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 105.082s\n",
      "Train: 11 [ 700/10009 (  7%)]  Loss: 3.09 (3.36)  Time: 0.161s,  793.18/s  (0.161s,  793.17/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 113.127s\n",
      "Train: 11 [ 750/10009 (  7%)]  Loss: 3.65 (3.37)  Time: 0.160s,  801.44/s  (0.161s,  793.38/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 121.162s\n",
      "Train: 11 [ 800/10009 (  8%)]  Loss: 3.63 (3.36)  Time: 0.160s,  799.70/s  (0.161s,  793.60/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 129.193s\n",
      "Train: 11 [ 850/10009 (  8%)]  Loss: 3.64 (3.36)  Time: 0.160s,  799.41/s  (0.161s,  793.73/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 137.235s\n",
      "Train: 11 [ 900/10009 (  9%)]  Loss: 3.44 (3.36)  Time: 0.162s,  788.51/s  (0.161s,  793.85/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 145.277s\n",
      "Train: 11 [ 950/10009 (  9%)]  Loss: 3.33 (3.37)  Time: 0.160s,  799.73/s  (0.161s,  793.88/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 153.332s\n",
      "Train: 11 [1000/10009 ( 10%)]  Loss: 3.40 (3.37)  Time: 0.162s,  792.32/s  (0.161s,  793.97/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 161.376s\n",
      "Train: 11 [1050/10009 ( 10%)]  Loss: 3.40 (3.37)  Time: 0.160s,  799.72/s  (0.161s,  794.07/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 169.417s\n",
      "Train: 11 [1100/10009 ( 11%)]  Loss: 3.60 (3.37)  Time: 0.161s,  794.75/s  (0.161s,  794.16/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 177.454s\n",
      "Train: 11 [1150/10009 ( 11%)]  Loss: 3.40 (3.37)  Time: 0.162s,  788.66/s  (0.161s,  794.00/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 185.553s\n",
      "Train: 11 [1200/10009 ( 12%)]  Loss: 3.32 (3.37)  Time: 0.162s,  788.72/s  (0.161s,  794.08/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 193.593s\n",
      "Train: 11 [1250/10009 ( 12%)]  Loss: 3.21 (3.37)  Time: 0.160s,  798.95/s  (0.161s,  794.18/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 201.628s\n",
      "Train: 11 [1300/10009 ( 13%)]  Loss: 3.26 (3.37)  Time: 0.160s,  800.72/s  (0.161s,  794.23/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 209.672s\n",
      "Train: 11 [1350/10009 ( 13%)]  Loss: 3.46 (3.36)  Time: 0.161s,  795.95/s  (0.161s,  794.22/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 217.733s\n",
      "Train: 11 [1400/10009 ( 14%)]  Loss: 3.68 (3.37)  Time: 0.161s,  793.61/s  (0.161s,  794.28/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 225.775s\n",
      "Train: 11 [1450/10009 ( 14%)]  Loss: 3.43 (3.36)  Time: 0.162s,  791.44/s  (0.161s,  794.36/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 233.809s\n",
      "Train: 11 [1500/10009 ( 15%)]  Loss: 3.54 (3.36)  Time: 0.161s,  793.72/s  (0.161s,  794.36/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 241.866s\n",
      "Train: 11 [1550/10009 ( 15%)]  Loss: 3.53 (3.36)  Time: 0.161s,  796.78/s  (0.161s,  794.41/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 249.906s\n",
      "Train: 11 [1600/10009 ( 16%)]  Loss: 3.41 (3.36)  Time: 0.160s,  798.25/s  (0.161s,  794.44/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 257.951s\n",
      "Train: 11 [1650/10009 ( 16%)]  Loss: 3.39 (3.36)  Time: 0.163s,  786.74/s  (0.161s,  794.47/s)  LR: 8.272e-06  Data: 0.009 (0.007)Time: 265.998s\n",
      "Train: 11 [1700/10009 ( 17%)]  Loss: 3.42 (3.36)  Time: 0.160s,  799.31/s  (0.161s,  794.50/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 274.043s\n",
      "Train: 11 [1750/10009 ( 17%)]  Loss: 3.53 (3.36)  Time: 0.160s,  800.23/s  (0.161s,  794.53/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 282.087s\n",
      "Train: 11 [1800/10009 ( 18%)]  Loss: 3.27 (3.36)  Time: 0.160s,  801.39/s  (0.161s,  794.60/s)  LR: 8.272e-06  Data: 0.005 (0.007)Time: 290.120s\n",
      "Train: 11 [1850/10009 ( 18%)]  Loss: 3.20 (3.36)  Time: 0.161s,  794.32/s  (0.161s,  794.66/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 298.151s\n",
      "Train: 11 [1900/10009 ( 19%)]  Loss: 2.89 (3.36)  Time: 0.161s,  797.17/s  (0.161s,  794.67/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 306.198s\n",
      "Train: 11 [1950/10009 ( 19%)]  Loss: 3.14 (3.36)  Time: 0.162s,  788.23/s  (0.161s,  794.70/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 314.241s\n",
      "Train: 11 [2000/10009 ( 20%)]  Loss: 3.25 (3.36)  Time: 0.161s,  793.54/s  (0.161s,  794.69/s)  LR: 8.272e-06  Data: 0.007 (0.007)Time: 322.300s\n",
      "Train: 11 [2050/10009 ( 20%)]  Loss: 3.42 (3.36)  Time: 0.161s,  795.88/s  (0.161s,  794.67/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 330.359s\n",
      "Train: 11 [2100/10009 ( 21%)]  Loss: 3.31 (3.36)  Time: 0.160s,  798.30/s  (0.161s,  794.70/s)  LR: 8.272e-06  Data: 0.006 (0.007)Time: 338.401s\n",
      "Train: 11 [2150/10009 ( 21%)]  Loss: 3.29 (3.36)  Time: 0.161s,  795.51/s  (0.161s,  794.71/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 346.451s\n",
      "Train: 11 [2200/10009 ( 22%)]  Loss: 3.43 (3.36)  Time: 0.160s,  798.43/s  (0.161s,  794.74/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 354.490s\n",
      "Train: 11 [2250/10009 ( 22%)]  Loss: 3.40 (3.36)  Time: 0.160s,  800.26/s  (0.161s,  794.75/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 362.539s\n",
      "Train: 11 [2300/10009 ( 23%)]  Loss: 3.15 (3.36)  Time: 0.162s,  787.88/s  (0.161s,  794.77/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 370.583s\n",
      "Train: 11 [2350/10009 ( 23%)]  Loss: 3.64 (3.35)  Time: 0.160s,  798.46/s  (0.161s,  794.79/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 378.626s\n",
      "Train: 11 [2400/10009 ( 24%)]  Loss: 3.19 (3.35)  Time: 0.160s,  798.30/s  (0.161s,  794.78/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 386.682s\n",
      "Train: 11 [2450/10009 ( 24%)]  Loss: 3.41 (3.35)  Time: 0.162s,  788.95/s  (0.161s,  794.78/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 394.737s\n",
      "Train: 11 [2500/10009 ( 25%)]  Loss: 3.44 (3.35)  Time: 0.161s,  796.12/s  (0.161s,  794.78/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 402.786s\n",
      "Train: 11 [2550/10009 ( 25%)]  Loss: 3.38 (3.35)  Time: 0.162s,  788.75/s  (0.161s,  794.80/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 410.832s\n",
      "Train: 11 [2600/10009 ( 26%)]  Loss: 3.50 (3.35)  Time: 0.161s,  794.58/s  (0.161s,  794.82/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 418.869s\n",
      "Train: 11 [2650/10009 ( 26%)]  Loss: 3.33 (3.35)  Time: 0.160s,  798.41/s  (0.161s,  794.84/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 426.910s\n",
      "Train: 11 [2700/10009 ( 27%)]  Loss: 3.31 (3.35)  Time: 0.161s,  794.68/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 434.954s\n",
      "Train: 11 [2750/10009 ( 27%)]  Loss: 3.48 (3.35)  Time: 0.160s,  798.41/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 442.992s\n",
      "Train: 11 [2800/10009 ( 28%)]  Loss: 3.48 (3.35)  Time: 0.161s,  796.85/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 451.053s\n",
      "Train: 11 [2850/10009 ( 28%)]  Loss: 3.20 (3.35)  Time: 0.161s,  793.65/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 459.106s\n",
      "Train: 11 [2900/10009 ( 29%)]  Loss: 3.55 (3.35)  Time: 0.160s,  800.57/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 467.157s\n",
      "Train: 11 [2950/10009 ( 29%)]  Loss: 3.24 (3.35)  Time: 0.160s,  798.82/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 475.203s\n",
      "Train: 11 [3000/10009 ( 30%)]  Loss: 3.42 (3.35)  Time: 0.162s,  791.13/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 483.248s\n",
      "Train: 11 [3050/10009 ( 30%)]  Loss: 3.23 (3.35)  Time: 0.160s,  800.20/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 491.291s\n",
      "Train: 11 [3100/10009 ( 31%)]  Loss: 2.98 (3.35)  Time: 0.161s,  796.64/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 499.347s\n",
      "Train: 11 [3150/10009 ( 31%)]  Loss: 3.33 (3.35)  Time: 0.163s,  783.53/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 507.394s\n",
      "Train: 11 [3200/10009 ( 32%)]  Loss: 3.32 (3.35)  Time: 0.162s,  787.94/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 515.452s\n",
      "Train: 11 [3250/10009 ( 32%)]  Loss: 3.68 (3.35)  Time: 0.166s,  770.23/s  (0.161s,  794.85/s)  LR: 8.272e-06  Data: 0.011 (0.006)Time: 523.532s\n",
      "Train: 11 [3300/10009 ( 33%)]  Loss: 3.16 (3.35)  Time: 0.160s,  798.65/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 531.574s\n",
      "Train: 11 [3350/10009 ( 33%)]  Loss: 3.67 (3.35)  Time: 0.161s,  796.91/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 539.616s\n",
      "Train: 11 [3400/10009 ( 34%)]  Loss: 3.69 (3.35)  Time: 0.162s,  791.48/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 547.659s\n",
      "Train: 11 [3450/10009 ( 34%)]  Loss: 3.36 (3.35)  Time: 0.161s,  792.77/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 555.712s\n",
      "Train: 11 [3500/10009 ( 35%)]  Loss: 3.14 (3.35)  Time: 0.162s,  791.97/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 563.771s\n",
      "Train: 11 [3550/10009 ( 35%)]  Loss: 3.17 (3.35)  Time: 0.163s,  787.67/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 571.825s\n",
      "Train: 11 [3600/10009 ( 36%)]  Loss: 3.45 (3.35)  Time: 0.161s,  794.56/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 579.872s\n",
      "Train: 11 [3650/10009 ( 36%)]  Loss: 3.26 (3.35)  Time: 0.161s,  796.40/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 587.919s\n",
      "Train: 11 [3700/10009 ( 37%)]  Loss: 3.36 (3.35)  Time: 0.161s,  794.40/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 595.965s\n",
      "Train: 11 [3750/10009 ( 37%)]  Loss: 3.21 (3.35)  Time: 0.161s,  795.46/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 604.008s\n",
      "Train: 11 [3800/10009 ( 38%)]  Loss: 3.26 (3.35)  Time: 0.162s,  788.00/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 612.060s\n",
      "Train: 11 [3850/10009 ( 38%)]  Loss: 3.43 (3.35)  Time: 0.161s,  795.58/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 620.108s\n",
      "Train: 11 [3900/10009 ( 39%)]  Loss: 3.17 (3.35)  Time: 0.161s,  792.76/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 628.160s\n",
      "Train: 11 [3950/10009 ( 39%)]  Loss: 3.51 (3.35)  Time: 0.160s,  798.30/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 636.211s\n",
      "Train: 11 [4000/10009 ( 40%)]  Loss: 3.26 (3.35)  Time: 0.162s,  792.48/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 644.285s\n",
      "Train: 11 [4050/10009 ( 40%)]  Loss: 3.25 (3.35)  Time: 0.161s,  795.05/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 652.327s\n",
      "Train: 11 [4100/10009 ( 41%)]  Loss: 3.23 (3.35)  Time: 0.161s,  795.55/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 660.375s\n",
      "Train: 11 [4150/10009 ( 41%)]  Loss: 3.16 (3.35)  Time: 0.160s,  798.75/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 668.414s\n",
      "Train: 11 [4200/10009 ( 42%)]  Loss: 3.10 (3.35)  Time: 0.161s,  797.49/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 676.470s\n",
      "Train: 11 [4250/10009 ( 42%)]  Loss: 3.25 (3.35)  Time: 0.160s,  798.75/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 684.525s\n",
      "Train: 11 [4300/10009 ( 43%)]  Loss: 3.58 (3.35)  Time: 0.160s,  798.95/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 692.575s\n",
      "Train: 11 [4350/10009 ( 43%)]  Loss: 3.34 (3.35)  Time: 0.161s,  795.36/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 700.643s\n",
      "Train: 11 [4400/10009 ( 44%)]  Loss: 3.35 (3.35)  Time: 0.161s,  797.10/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 708.696s\n",
      "Train: 11 [4450/10009 ( 44%)]  Loss: 3.36 (3.35)  Time: 0.161s,  793.09/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 716.744s\n",
      "Train: 11 [4500/10009 ( 45%)]  Loss: 3.29 (3.35)  Time: 0.161s,  796.72/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 724.783s\n",
      "Train: 11 [4550/10009 ( 45%)]  Loss: 3.42 (3.34)  Time: 0.164s,  780.91/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 732.841s\n",
      "Train: 11 [4600/10009 ( 46%)]  Loss: 3.56 (3.34)  Time: 0.162s,  787.74/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 740.901s\n",
      "Train: 11 [4650/10009 ( 46%)]  Loss: 2.90 (3.34)  Time: 0.160s,  797.61/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 748.961s\n",
      "Train: 11 [4700/10009 ( 47%)]  Loss: 3.45 (3.34)  Time: 0.161s,  797.37/s  (0.161s,  794.86/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 757.024s\n",
      "Train: 11 [4750/10009 ( 47%)]  Loss: 3.47 (3.34)  Time: 0.162s,  792.06/s  (0.161s,  794.85/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 765.083s\n",
      "Train: 11 [4800/10009 ( 48%)]  Loss: 3.41 (3.34)  Time: 0.161s,  796.13/s  (0.161s,  794.85/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 773.134s\n",
      "Train: 11 [4850/10009 ( 48%)]  Loss: 3.54 (3.35)  Time: 0.161s,  794.12/s  (0.161s,  794.87/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 781.163s\n",
      "Train: 11 [4900/10009 ( 49%)]  Loss: 3.10 (3.35)  Time: 0.161s,  797.44/s  (0.161s,  794.88/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 789.205s\n",
      "Train: 11 [4950/10009 ( 49%)]  Loss: 3.40 (3.35)  Time: 0.161s,  795.07/s  (0.161s,  794.89/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 797.251s\n",
      "Train: 11 [5000/10009 ( 50%)]  Loss: 3.11 (3.35)  Time: 0.160s,  799.83/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 805.292s\n",
      "Train: 11 [5050/10009 ( 50%)]  Loss: 3.28 (3.35)  Time: 0.160s,  798.89/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 813.324s\n",
      "Train: 11 [5100/10009 ( 51%)]  Loss: 3.00 (3.34)  Time: 0.161s,  797.33/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 821.370s\n",
      "Train: 11 [5150/10009 ( 51%)]  Loss: 3.50 (3.34)  Time: 0.161s,  795.38/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 829.429s\n",
      "Train: 11 [5200/10009 ( 52%)]  Loss: 3.34 (3.34)  Time: 0.163s,  783.74/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 837.482s\n",
      "Train: 11 [5250/10009 ( 52%)]  Loss: 3.54 (3.34)  Time: 0.161s,  793.78/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 845.537s\n",
      "Train: 11 [5300/10009 ( 53%)]  Loss: 3.20 (3.34)  Time: 0.160s,  797.70/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 853.589s\n",
      "Train: 11 [5350/10009 ( 53%)]  Loss: 3.38 (3.34)  Time: 0.160s,  797.65/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 861.630s\n",
      "Train: 11 [5400/10009 ( 54%)]  Loss: 3.60 (3.34)  Time: 0.161s,  795.93/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 869.679s\n",
      "Train: 11 [5450/10009 ( 54%)]  Loss: 3.24 (3.34)  Time: 0.161s,  796.76/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 877.728s\n",
      "Train: 11 [5500/10009 ( 55%)]  Loss: 3.49 (3.34)  Time: 0.161s,  797.15/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 885.784s\n",
      "Train: 11 [5550/10009 ( 55%)]  Loss: 3.44 (3.34)  Time: 0.161s,  794.01/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 893.831s\n",
      "Train: 11 [5600/10009 ( 56%)]  Loss: 3.27 (3.34)  Time: 0.161s,  794.38/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 901.882s\n",
      "Train: 11 [5650/10009 ( 56%)]  Loss: 3.62 (3.34)  Time: 0.162s,  791.15/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 909.935s\n",
      "Train: 11 [5700/10009 ( 57%)]  Loss: 3.24 (3.34)  Time: 0.161s,  795.47/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 917.974s\n",
      "Train: 11 [5750/10009 ( 57%)]  Loss: 3.58 (3.34)  Time: 0.160s,  799.39/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 926.028s\n",
      "Train: 11 [5800/10009 ( 58%)]  Loss: 3.48 (3.34)  Time: 0.161s,  794.23/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 934.080s\n",
      "Train: 11 [5850/10009 ( 58%)]  Loss: 3.35 (3.34)  Time: 0.161s,  794.32/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 942.131s\n",
      "Train: 11 [5900/10009 ( 59%)]  Loss: 3.83 (3.34)  Time: 0.162s,  792.26/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 950.182s\n",
      "Train: 11 [5950/10009 ( 59%)]  Loss: 3.39 (3.34)  Time: 0.160s,  799.83/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 958.220s\n",
      "Train: 11 [6000/10009 ( 60%)]  Loss: 3.16 (3.34)  Time: 0.160s,  799.68/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 966.261s\n",
      "Train: 11 [6050/10009 ( 60%)]  Loss: 3.21 (3.34)  Time: 0.161s,  794.10/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 974.307s\n",
      "Train: 11 [6100/10009 ( 61%)]  Loss: 3.43 (3.34)  Time: 0.161s,  794.65/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 982.371s\n",
      "Train: 11 [6150/10009 ( 61%)]  Loss: 3.25 (3.34)  Time: 0.160s,  798.57/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 990.420s\n",
      "Train: 11 [6200/10009 ( 62%)]  Loss: 3.26 (3.34)  Time: 0.160s,  798.42/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 998.459s\n",
      "Train: 11 [6250/10009 ( 62%)]  Loss: 3.47 (3.34)  Time: 0.162s,  790.13/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1006.503s\n",
      "Train: 11 [6300/10009 ( 63%)]  Loss: 3.33 (3.34)  Time: 0.161s,  793.57/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1014.549s\n",
      "Train: 11 [6350/10009 ( 63%)]  Loss: 3.53 (3.34)  Time: 0.161s,  796.55/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1022.615s\n",
      "Train: 11 [6400/10009 ( 64%)]  Loss: 3.45 (3.34)  Time: 0.162s,  791.36/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 1030.663s\n",
      "Train: 11 [6450/10009 ( 64%)]  Loss: 3.23 (3.34)  Time: 0.162s,  789.45/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1038.729s\n",
      "Train: 11 [6500/10009 ( 65%)]  Loss: 3.53 (3.34)  Time: 0.161s,  795.70/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1046.776s\n",
      "Train: 11 [6550/10009 ( 65%)]  Loss: 3.39 (3.34)  Time: 0.161s,  795.69/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1054.816s\n",
      "Train: 11 [6600/10009 ( 66%)]  Loss: 3.30 (3.34)  Time: 0.161s,  792.85/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1062.877s\n",
      "Train: 11 [6650/10009 ( 66%)]  Loss: 3.38 (3.34)  Time: 0.161s,  793.09/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1070.923s\n",
      "Train: 11 [6700/10009 ( 67%)]  Loss: 3.61 (3.34)  Time: 0.160s,  797.77/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1078.963s\n",
      "Train: 11 [6750/10009 ( 67%)]  Loss: 3.45 (3.34)  Time: 0.163s,  785.59/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 1087.013s\n",
      "Train: 11 [6800/10009 ( 68%)]  Loss: 2.95 (3.34)  Time: 0.161s,  795.43/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1095.093s\n",
      "Train: 11 [6850/10009 ( 68%)]  Loss: 3.31 (3.34)  Time: 0.161s,  794.40/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1103.149s\n",
      "Train: 11 [6900/10009 ( 69%)]  Loss: 3.52 (3.34)  Time: 0.161s,  795.87/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1111.203s\n",
      "Train: 11 [6950/10009 ( 69%)]  Loss: 3.29 (3.34)  Time: 0.161s,  796.07/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1119.255s\n",
      "Train: 11 [7000/10009 ( 70%)]  Loss: 3.41 (3.34)  Time: 0.161s,  794.70/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1127.308s\n",
      "Train: 11 [7050/10009 ( 70%)]  Loss: 3.48 (3.34)  Time: 0.160s,  797.79/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1135.387s\n",
      "Train: 11 [7100/10009 ( 71%)]  Loss: 3.18 (3.34)  Time: 0.161s,  794.92/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1143.447s\n",
      "Train: 11 [7150/10009 ( 71%)]  Loss: 3.41 (3.34)  Time: 0.161s,  794.07/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1151.491s\n",
      "Train: 11 [7200/10009 ( 72%)]  Loss: 3.11 (3.34)  Time: 0.161s,  795.09/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1159.548s\n",
      "Train: 11 [7250/10009 ( 72%)]  Loss: 3.19 (3.34)  Time: 0.160s,  799.53/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1167.596s\n",
      "Train: 11 [7300/10009 ( 73%)]  Loss: 3.05 (3.34)  Time: 0.160s,  800.82/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1175.641s\n",
      "Train: 11 [7350/10009 ( 73%)]  Loss: 3.36 (3.34)  Time: 0.161s,  796.96/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1183.685s\n",
      "Train: 11 [7400/10009 ( 74%)]  Loss: 3.15 (3.34)  Time: 0.161s,  797.46/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1191.734s\n",
      "Train: 11 [7450/10009 ( 74%)]  Loss: 3.46 (3.34)  Time: 0.161s,  794.11/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1199.810s\n",
      "Train: 11 [7500/10009 ( 75%)]  Loss: 3.49 (3.34)  Time: 0.161s,  794.89/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1207.860s\n",
      "Train: 11 [7550/10009 ( 75%)]  Loss: 2.95 (3.34)  Time: 0.160s,  798.34/s  (0.161s,  794.90/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1215.904s\n",
      "Train: 11 [7600/10009 ( 76%)]  Loss: 3.36 (3.34)  Time: 0.162s,  791.61/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1223.943s\n",
      "Train: 11 [7650/10009 ( 76%)]  Loss: 3.58 (3.34)  Time: 0.160s,  798.89/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1231.997s\n",
      "Train: 11 [7700/10009 ( 77%)]  Loss: 3.30 (3.34)  Time: 0.161s,  794.69/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1240.045s\n",
      "Train: 11 [7750/10009 ( 77%)]  Loss: 3.40 (3.34)  Time: 0.160s,  800.62/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1248.103s\n",
      "Train: 11 [7800/10009 ( 78%)]  Loss: 3.23 (3.34)  Time: 0.161s,  797.47/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1256.150s\n",
      "Train: 11 [7850/10009 ( 78%)]  Loss: 3.66 (3.34)  Time: 0.160s,  798.12/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1264.198s\n",
      "Train: 11 [7900/10009 ( 79%)]  Loss: 3.23 (3.34)  Time: 0.160s,  798.61/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1272.246s\n",
      "Train: 11 [7950/10009 ( 79%)]  Loss: 3.53 (3.34)  Time: 0.161s,  795.46/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1280.290s\n",
      "Train: 11 [8000/10009 ( 80%)]  Loss: 3.22 (3.34)  Time: 0.160s,  798.88/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1288.341s\n",
      "Train: 11 [8050/10009 ( 80%)]  Loss: 3.44 (3.34)  Time: 0.160s,  798.63/s  (0.161s,  794.91/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1296.398s\n",
      "Train: 11 [8100/10009 ( 81%)]  Loss: 3.36 (3.34)  Time: 0.160s,  799.95/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1304.441s\n",
      "Train: 11 [8150/10009 ( 81%)]  Loss: 3.19 (3.34)  Time: 0.162s,  790.89/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1312.489s\n",
      "Train: 11 [8200/10009 ( 82%)]  Loss: 3.69 (3.34)  Time: 0.161s,  797.22/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1320.532s\n",
      "Train: 11 [8250/10009 ( 82%)]  Loss: 3.24 (3.34)  Time: 0.161s,  794.67/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1328.587s\n",
      "Train: 11 [8300/10009 ( 83%)]  Loss: 3.50 (3.34)  Time: 0.161s,  796.34/s  (0.161s,  794.92/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1336.642s\n",
      "Train: 11 [8350/10009 ( 83%)]  Loss: 3.45 (3.34)  Time: 0.160s,  798.42/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1344.681s\n",
      "Train: 11 [8400/10009 ( 84%)]  Loss: 3.22 (3.34)  Time: 0.161s,  793.02/s  (0.161s,  794.93/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1352.735s\n",
      "Train: 11 [8450/10009 ( 84%)]  Loss: 3.23 (3.34)  Time: 0.161s,  793.93/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1360.771s\n",
      "Train: 11 [8500/10009 ( 85%)]  Loss: 3.48 (3.34)  Time: 0.161s,  796.86/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1368.821s\n",
      "Train: 11 [8550/10009 ( 85%)]  Loss: 3.17 (3.34)  Time: 0.161s,  794.27/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1376.855s\n",
      "Train: 11 [8600/10009 ( 86%)]  Loss: 3.21 (3.34)  Time: 0.161s,  793.92/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1384.907s\n",
      "Train: 11 [8650/10009 ( 86%)]  Loss: 3.23 (3.34)  Time: 0.161s,  794.42/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1392.962s\n",
      "Train: 11 [8700/10009 ( 87%)]  Loss: 3.41 (3.34)  Time: 0.161s,  796.86/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1401.012s\n",
      "Train: 11 [8750/10009 ( 87%)]  Loss: 3.46 (3.34)  Time: 0.161s,  796.60/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1409.063s\n",
      "Train: 11 [8800/10009 ( 88%)]  Loss: 3.49 (3.34)  Time: 0.160s,  799.48/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1417.112s\n",
      "Train: 11 [8850/10009 ( 88%)]  Loss: 3.64 (3.34)  Time: 0.161s,  793.46/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1425.163s\n",
      "Train: 11 [8900/10009 ( 89%)]  Loss: 3.38 (3.34)  Time: 0.161s,  796.46/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1433.216s\n",
      "Train: 11 [8950/10009 ( 89%)]  Loss: 3.07 (3.34)  Time: 0.160s,  799.74/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1441.270s\n",
      "Train: 11 [9000/10009 ( 90%)]  Loss: 3.22 (3.34)  Time: 0.162s,  792.29/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1449.310s\n",
      "Train: 11 [9050/10009 ( 90%)]  Loss: 3.32 (3.34)  Time: 0.162s,  788.07/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1457.360s\n",
      "Train: 11 [9100/10009 ( 91%)]  Loss: 3.37 (3.34)  Time: 0.161s,  793.76/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1465.421s\n",
      "Train: 11 [9150/10009 ( 91%)]  Loss: 3.52 (3.34)  Time: 0.161s,  794.30/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1473.464s\n",
      "Train: 11 [9200/10009 ( 92%)]  Loss: 3.29 (3.34)  Time: 0.161s,  793.24/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1481.500s\n",
      "Train: 11 [9250/10009 ( 92%)]  Loss: 3.60 (3.34)  Time: 0.161s,  795.59/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1489.538s\n",
      "Train: 11 [9300/10009 ( 93%)]  Loss: 3.30 (3.34)  Time: 0.161s,  796.13/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1497.583s\n",
      "Train: 11 [9350/10009 ( 93%)]  Loss: 3.61 (3.34)  Time: 0.161s,  797.26/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1505.632s\n",
      "Train: 11 [9400/10009 ( 94%)]  Loss: 3.39 (3.34)  Time: 0.161s,  796.90/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1513.676s\n",
      "Train: 11 [9450/10009 ( 94%)]  Loss: 3.40 (3.34)  Time: 0.160s,  798.57/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1521.733s\n",
      "Train: 11 [9500/10009 ( 95%)]  Loss: 3.36 (3.34)  Time: 0.162s,  792.53/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.005 (0.006)Time: 1529.788s\n",
      "Train: 11 [9550/10009 ( 95%)]  Loss: 3.23 (3.34)  Time: 0.161s,  793.50/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1537.844s\n",
      "Train: 11 [9600/10009 ( 96%)]  Loss: 3.19 (3.34)  Time: 0.161s,  795.64/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1545.887s\n",
      "Train: 11 [9650/10009 ( 96%)]  Loss: 3.33 (3.34)  Time: 0.161s,  795.05/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1553.932s\n",
      "Train: 11 [9700/10009 ( 97%)]  Loss: 3.48 (3.34)  Time: 0.162s,  790.11/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.008 (0.006)Time: 1561.983s\n",
      "Train: 11 [9750/10009 ( 97%)]  Loss: 3.04 (3.34)  Time: 0.161s,  792.58/s  (0.161s,  794.97/s)  LR: 8.272e-06  Data: 0.007 (0.006)Time: 1570.034s\n",
      "Train: 11 [9800/10009 ( 98%)]  Loss: 3.34 (3.34)  Time: 0.161s,  794.65/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1578.092s\n",
      "Train: 11 [9850/10009 ( 98%)]  Loss: 3.15 (3.34)  Time: 0.161s,  796.87/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1586.148s\n",
      "Train: 11 [9900/10009 ( 99%)]  Loss: 3.43 (3.34)  Time: 0.161s,  797.40/s  (0.161s,  794.96/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1594.207s\n",
      "Train: 11 [9950/10009 ( 99%)]  Loss: 3.59 (3.34)  Time: 0.161s,  795.32/s  (0.161s,  794.95/s)  LR: 8.272e-06  Data: 0.006 (0.006)Time: 1602.261s\n",
      "Train: 11 [10000/10009 (100%)]  Loss: 3.42 (3.34)  Time: 0.202s,  632.24/s  (0.161s,  794.94/s)  LR: 8.272e-06  Data: 0.048 (0.006)Time: 1610.344s\n",
      "Test: [   0/390]  Time: 0.678 (0.678)  Loss:   1.237 ( 1.237)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.145)  Loss:   1.066 ( 2.035)  Acc@1:  76.562 ( 56.878)  Acc@5:  90.625 ( 78.064)\n",
      "Test: [ 100/390]  Time: 0.052 (0.133)  Loss:   2.191 ( 2.074)  Acc@1:  50.781 ( 53.187)  Acc@5:  80.469 ( 78.434)\n",
      "Test: [ 150/390]  Time: 0.052 (0.138)  Loss:   1.975 ( 2.047)  Acc@1:  46.875 ( 53.839)  Acc@5:  79.688 ( 78.844)\n",
      "Test: [ 200/390]  Time: 0.052 (0.135)  Loss:   2.979 ( 2.231)  Acc@1:  32.031 ( 50.723)  Acc@5:  65.625 ( 75.836)\n",
      "Test: [ 250/390]  Time: 0.052 (0.134)  Loss:   2.435 ( 2.351)  Acc@1:  57.812 ( 49.044)  Acc@5:  72.656 ( 73.795)\n",
      "Test: [ 300/390]  Time: 0.262 (0.132)  Loss:   2.650 ( 2.449)  Acc@1:  50.000 ( 47.376)  Acc@5:  66.406 ( 72.000)\n",
      "Test: [ 350/390]  Time: 0.190 (0.131)  Loss:   2.732 ( 2.524)  Acc@1:  46.094 ( 46.049)  Acc@5:  70.312 ( 70.791)\n",
      "Test: [ 390/390]  Time: 0.035 (0.131)  Loss:   3.945 ( 2.490)  Acc@1:  16.250 ( 46.698)  Acc@5:  55.000 ( 71.312)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 12 [   0/10009 (  0%)]  Loss: 3.23 (3.23)  Time: 0.666s,  192.14/s  (0.666s,  192.14/s)  LR: 4.775e-06  Data: 0.515 (0.515)Time: 0.667s\n",
      "Train: 12 [  50/10009 (  0%)]  Loss: 3.32 (3.29)  Time: 0.159s,  804.93/s  (0.170s,  754.56/s)  LR: 4.775e-06  Data: 0.006 (0.017)Time: 8.652s\n",
      "Train: 12 [ 100/10009 (  1%)]  Loss: 3.13 (3.29)  Time: 0.159s,  803.15/s  (0.165s,  777.40/s)  LR: 4.775e-06  Data: 0.006 (0.012)Time: 16.630s\n",
      "Train: 12 [ 150/10009 (  1%)]  Loss: 3.17 (3.28)  Time: 0.160s,  801.51/s  (0.163s,  785.04/s)  LR: 4.775e-06  Data: 0.006 (0.010)Time: 24.621s\n",
      "Train: 12 [ 200/10009 (  2%)]  Loss: 3.11 (3.28)  Time: 0.160s,  799.54/s  (0.162s,  788.32/s)  LR: 4.775e-06  Data: 0.006 (0.009)Time: 32.637s\n",
      "Train: 12 [ 250/10009 (  2%)]  Loss: 3.22 (3.28)  Time: 0.160s,  801.55/s  (0.162s,  790.27/s)  LR: 4.775e-06  Data: 0.006 (0.008)Time: 40.655s\n",
      "Train: 12 [ 300/10009 (  3%)]  Loss: 3.25 (3.27)  Time: 0.160s,  800.98/s  (0.162s,  791.57/s)  LR: 4.775e-06  Data: 0.006 (0.008)Time: 48.673s\n",
      "Train: 12 [ 350/10009 (  3%)]  Loss: 3.16 (3.27)  Time: 0.160s,  800.27/s  (0.162s,  792.53/s)  LR: 4.775e-06  Data: 0.006 (0.008)Time: 56.689s\n",
      "Train: 12 [ 400/10009 (  4%)]  Loss: 3.29 (3.26)  Time: 0.161s,  792.75/s  (0.161s,  793.13/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 64.716s\n",
      "Train: 12 [ 450/10009 (  4%)]  Loss: 3.25 (3.26)  Time: 0.160s,  800.63/s  (0.161s,  793.57/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 72.745s\n",
      "Train: 12 [ 500/10009 (  5%)]  Loss: 3.24 (3.26)  Time: 0.160s,  798.50/s  (0.161s,  793.72/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 80.794s\n",
      "Train: 12 [ 550/10009 (  5%)]  Loss: 3.35 (3.26)  Time: 0.160s,  798.19/s  (0.161s,  794.03/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 88.823s\n",
      "Train: 12 [ 600/10009 (  6%)]  Loss: 3.19 (3.26)  Time: 0.160s,  800.70/s  (0.161s,  794.25/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 96.856s\n",
      "Train: 12 [ 650/10009 (  6%)]  Loss: 3.25 (3.26)  Time: 0.160s,  802.19/s  (0.161s,  794.45/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 104.888s\n",
      "Train: 12 [ 700/10009 (  7%)]  Loss: 3.37 (3.26)  Time: 0.162s,  790.25/s  (0.161s,  794.62/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 112.920s\n",
      "Train: 12 [ 750/10009 (  7%)]  Loss: 3.25 (3.25)  Time: 0.161s,  796.19/s  (0.161s,  794.75/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 120.953s\n",
      "Train: 12 [ 800/10009 (  8%)]  Loss: 3.06 (3.25)  Time: 0.161s,  796.00/s  (0.161s,  794.89/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 128.984s\n",
      "Train: 12 [ 850/10009 (  8%)]  Loss: 2.90 (3.25)  Time: 0.161s,  793.99/s  (0.161s,  794.98/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 137.020s\n",
      "Train: 12 [ 900/10009 (  9%)]  Loss: 3.40 (3.26)  Time: 0.161s,  797.30/s  (0.161s,  795.03/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 145.061s\n",
      "Train: 12 [ 950/10009 (  9%)]  Loss: 3.25 (3.26)  Time: 0.161s,  793.95/s  (0.161s,  795.05/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 153.107s\n",
      "Train: 12 [1000/10009 ( 10%)]  Loss: 3.23 (3.26)  Time: 0.160s,  797.69/s  (0.161s,  795.10/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 161.147s\n",
      "Train: 12 [1050/10009 ( 10%)]  Loss: 3.39 (3.26)  Time: 0.161s,  797.31/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 169.179s\n",
      "Train: 12 [1100/10009 ( 11%)]  Loss: 3.24 (3.26)  Time: 0.160s,  798.94/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.005 (0.007)Time: 177.212s\n",
      "Train: 12 [1150/10009 ( 11%)]  Loss: 3.18 (3.26)  Time: 0.160s,  798.40/s  (0.161s,  795.28/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 185.254s\n",
      "Train: 12 [1200/10009 ( 12%)]  Loss: 3.02 (3.26)  Time: 0.160s,  798.59/s  (0.161s,  795.32/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 193.290s\n",
      "Train: 12 [1250/10009 ( 12%)]  Loss: 3.18 (3.26)  Time: 0.161s,  794.92/s  (0.161s,  795.35/s)  LR: 4.775e-06  Data: 0.006 (0.007)Time: 201.330s\n",
      "Train: 12 [1300/10009 ( 13%)]  Loss: 3.29 (3.26)  Time: 0.161s,  793.58/s  (0.161s,  795.37/s)  LR: 4.775e-06  Data: 0.007 (0.007)Time: 209.371s\n",
      "Train: 12 [1350/10009 ( 13%)]  Loss: 3.52 (3.26)  Time: 0.161s,  794.31/s  (0.161s,  795.38/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 217.415s\n",
      "Train: 12 [1400/10009 ( 14%)]  Loss: 3.23 (3.26)  Time: 0.160s,  798.90/s  (0.161s,  795.40/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 225.457s\n",
      "Train: 12 [1450/10009 ( 14%)]  Loss: 3.47 (3.26)  Time: 0.162s,  789.49/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 233.504s\n",
      "Train: 12 [1500/10009 ( 15%)]  Loss: 3.49 (3.26)  Time: 0.160s,  799.45/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 241.551s\n",
      "Train: 12 [1550/10009 ( 15%)]  Loss: 3.32 (3.26)  Time: 0.161s,  794.15/s  (0.161s,  795.40/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 249.593s\n",
      "Train: 12 [1600/10009 ( 16%)]  Loss: 3.34 (3.26)  Time: 0.160s,  800.96/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 257.624s\n",
      "Train: 12 [1650/10009 ( 16%)]  Loss: 3.19 (3.26)  Time: 0.161s,  796.17/s  (0.161s,  795.49/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 265.659s\n",
      "Train: 12 [1700/10009 ( 17%)]  Loss: 3.04 (3.26)  Time: 0.160s,  798.28/s  (0.161s,  795.49/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 273.704s\n",
      "Train: 12 [1750/10009 ( 17%)]  Loss: 3.33 (3.26)  Time: 0.161s,  796.91/s  (0.161s,  795.52/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 281.737s\n",
      "Train: 12 [1800/10009 ( 18%)]  Loss: 3.27 (3.26)  Time: 0.160s,  798.16/s  (0.161s,  795.50/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 289.788s\n",
      "Train: 12 [1850/10009 ( 18%)]  Loss: 3.01 (3.26)  Time: 0.160s,  799.02/s  (0.161s,  795.51/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 297.830s\n",
      "Train: 12 [1900/10009 ( 19%)]  Loss: 3.31 (3.26)  Time: 0.161s,  793.08/s  (0.161s,  795.53/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 305.869s\n",
      "Train: 12 [1950/10009 ( 19%)]  Loss: 3.55 (3.26)  Time: 0.161s,  793.14/s  (0.161s,  795.52/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 313.917s\n",
      "Train: 12 [2000/10009 ( 20%)]  Loss: 3.23 (3.26)  Time: 0.161s,  792.83/s  (0.161s,  795.53/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 321.957s\n",
      "Train: 12 [2050/10009 ( 20%)]  Loss: 2.87 (3.26)  Time: 0.162s,  789.77/s  (0.161s,  795.54/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 329.997s\n",
      "Train: 12 [2100/10009 ( 21%)]  Loss: 3.20 (3.26)  Time: 0.162s,  792.27/s  (0.161s,  795.54/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 338.044s\n",
      "Train: 12 [2150/10009 ( 21%)]  Loss: 3.24 (3.26)  Time: 0.162s,  790.08/s  (0.161s,  795.55/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 346.085s\n",
      "Train: 12 [2200/10009 ( 22%)]  Loss: 3.41 (3.26)  Time: 0.160s,  798.61/s  (0.161s,  795.55/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 354.130s\n",
      "Train: 12 [2250/10009 ( 22%)]  Loss: 3.00 (3.26)  Time: 0.160s,  797.90/s  (0.161s,  795.55/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 362.176s\n",
      "Train: 12 [2300/10009 ( 23%)]  Loss: 3.50 (3.26)  Time: 0.160s,  797.97/s  (0.161s,  795.50/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 370.244s\n",
      "Train: 12 [2350/10009 ( 23%)]  Loss: 3.32 (3.26)  Time: 0.161s,  793.84/s  (0.161s,  795.49/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 378.294s\n",
      "Train: 12 [2400/10009 ( 24%)]  Loss: 3.27 (3.26)  Time: 0.162s,  792.21/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 386.357s\n",
      "Train: 12 [2450/10009 ( 24%)]  Loss: 3.45 (3.26)  Time: 0.160s,  798.58/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 394.402s\n",
      "Train: 12 [2500/10009 ( 25%)]  Loss: 3.31 (3.26)  Time: 0.161s,  796.01/s  (0.161s,  795.46/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 402.445s\n",
      "Train: 12 [2550/10009 ( 25%)]  Loss: 3.29 (3.26)  Time: 0.161s,  793.37/s  (0.161s,  795.46/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 410.491s\n",
      "Train: 12 [2600/10009 ( 26%)]  Loss: 3.14 (3.26)  Time: 0.161s,  794.63/s  (0.161s,  795.46/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 418.537s\n",
      "Train: 12 [2650/10009 ( 26%)]  Loss: 3.09 (3.26)  Time: 0.161s,  797.01/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 426.584s\n",
      "Train: 12 [2700/10009 ( 27%)]  Loss: 3.25 (3.26)  Time: 0.163s,  787.32/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 434.631s\n",
      "Train: 12 [2750/10009 ( 27%)]  Loss: 3.24 (3.26)  Time: 0.161s,  797.23/s  (0.161s,  795.43/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 442.688s\n",
      "Train: 12 [2800/10009 ( 28%)]  Loss: 3.29 (3.26)  Time: 0.160s,  798.42/s  (0.161s,  795.45/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 450.722s\n",
      "Train: 12 [2850/10009 ( 28%)]  Loss: 3.13 (3.26)  Time: 0.161s,  796.29/s  (0.161s,  795.43/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 458.777s\n",
      "Train: 12 [2900/10009 ( 29%)]  Loss: 3.06 (3.26)  Time: 0.160s,  797.64/s  (0.161s,  795.44/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 466.817s\n",
      "Train: 12 [2950/10009 ( 29%)]  Loss: 3.52 (3.26)  Time: 0.160s,  799.54/s  (0.161s,  795.43/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 474.871s\n",
      "Train: 12 [3000/10009 ( 30%)]  Loss: 3.26 (3.26)  Time: 0.161s,  792.70/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 482.942s\n",
      "Train: 12 [3050/10009 ( 30%)]  Loss: 3.24 (3.26)  Time: 0.162s,  790.21/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 490.991s\n",
      "Train: 12 [3100/10009 ( 31%)]  Loss: 3.15 (3.26)  Time: 0.161s,  796.59/s  (0.161s,  795.38/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 499.042s\n",
      "Train: 12 [3150/10009 ( 31%)]  Loss: 3.18 (3.26)  Time: 0.161s,  793.57/s  (0.161s,  795.39/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 507.083s\n",
      "Train: 12 [3200/10009 ( 32%)]  Loss: 3.39 (3.26)  Time: 0.161s,  794.36/s  (0.161s,  795.38/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 515.135s\n",
      "Train: 12 [3250/10009 ( 32%)]  Loss: 3.30 (3.26)  Time: 0.160s,  797.71/s  (0.161s,  795.36/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 523.191s\n",
      "Train: 12 [3300/10009 ( 33%)]  Loss: 3.40 (3.26)  Time: 0.161s,  794.88/s  (0.161s,  795.35/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 531.249s\n",
      "Train: 12 [3350/10009 ( 33%)]  Loss: 3.03 (3.26)  Time: 0.161s,  797.15/s  (0.161s,  795.35/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 539.295s\n",
      "Train: 12 [3400/10009 ( 34%)]  Loss: 3.31 (3.26)  Time: 0.161s,  796.64/s  (0.161s,  795.34/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 547.345s\n",
      "Train: 12 [3450/10009 ( 34%)]  Loss: 3.42 (3.26)  Time: 0.161s,  797.19/s  (0.161s,  795.33/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 555.401s\n",
      "Train: 12 [3500/10009 ( 35%)]  Loss: 3.02 (3.26)  Time: 0.163s,  786.44/s  (0.161s,  795.31/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 563.466s\n",
      "Train: 12 [3550/10009 ( 35%)]  Loss: 3.38 (3.26)  Time: 0.160s,  799.87/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 571.537s\n",
      "Train: 12 [3600/10009 ( 36%)]  Loss: 3.24 (3.26)  Time: 0.160s,  797.84/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 579.588s\n",
      "Train: 12 [3650/10009 ( 36%)]  Loss: 3.05 (3.26)  Time: 0.162s,  789.32/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 587.645s\n",
      "Train: 12 [3700/10009 ( 37%)]  Loss: 3.28 (3.26)  Time: 0.161s,  796.49/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 595.695s\n",
      "Train: 12 [3750/10009 ( 37%)]  Loss: 3.25 (3.26)  Time: 0.161s,  793.03/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 603.736s\n",
      "Train: 12 [3800/10009 ( 38%)]  Loss: 3.33 (3.26)  Time: 0.161s,  797.29/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 611.776s\n",
      "Train: 12 [3850/10009 ( 38%)]  Loss: 2.93 (3.26)  Time: 0.161s,  796.47/s  (0.161s,  795.27/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 619.826s\n",
      "Train: 12 [3900/10009 ( 39%)]  Loss: 3.35 (3.26)  Time: 0.160s,  799.54/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 627.879s\n",
      "Train: 12 [3950/10009 ( 39%)]  Loss: 3.54 (3.26)  Time: 0.161s,  793.16/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 635.930s\n",
      "Train: 12 [4000/10009 ( 40%)]  Loss: 3.28 (3.26)  Time: 0.161s,  793.01/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 643.975s\n",
      "Train: 12 [4050/10009 ( 40%)]  Loss: 2.96 (3.26)  Time: 0.161s,  794.81/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 652.024s\n",
      "Train: 12 [4100/10009 ( 41%)]  Loss: 3.29 (3.26)  Time: 0.161s,  793.39/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 660.068s\n",
      "Train: 12 [4150/10009 ( 41%)]  Loss: 3.27 (3.25)  Time: 0.162s,  789.70/s  (0.161s,  795.24/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 668.131s\n",
      "Train: 12 [4200/10009 ( 42%)]  Loss: 3.26 (3.25)  Time: 0.160s,  798.84/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 676.172s\n",
      "Train: 12 [4250/10009 ( 42%)]  Loss: 3.15 (3.26)  Time: 0.161s,  797.30/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 684.220s\n",
      "Train: 12 [4300/10009 ( 43%)]  Loss: 3.49 (3.25)  Time: 0.161s,  793.58/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 692.264s\n",
      "Train: 12 [4350/10009 ( 43%)]  Loss: 3.34 (3.26)  Time: 0.163s,  784.89/s  (0.161s,  795.25/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 700.314s\n",
      "Train: 12 [4400/10009 ( 44%)]  Loss: 3.36 (3.26)  Time: 0.161s,  794.83/s  (0.161s,  795.26/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 708.358s\n",
      "Train: 12 [4450/10009 ( 44%)]  Loss: 3.33 (3.25)  Time: 0.162s,  792.29/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 716.429s\n",
      "Train: 12 [4500/10009 ( 45%)]  Loss: 3.20 (3.25)  Time: 0.161s,  796.21/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 724.478s\n",
      "Train: 12 [4550/10009 ( 45%)]  Loss: 2.97 (3.26)  Time: 0.161s,  794.83/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 732.531s\n",
      "Train: 12 [4600/10009 ( 46%)]  Loss: 3.19 (3.25)  Time: 0.156s,  820.35/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 740.588s\n",
      "Train: 12 [4650/10009 ( 46%)]  Loss: 3.14 (3.26)  Time: 0.161s,  795.84/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 748.637s\n",
      "Train: 12 [4700/10009 ( 47%)]  Loss: 3.66 (3.26)  Time: 0.160s,  799.52/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 756.680s\n",
      "Train: 12 [4750/10009 ( 47%)]  Loss: 3.30 (3.25)  Time: 0.160s,  798.33/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 764.725s\n",
      "Train: 12 [4800/10009 ( 48%)]  Loss: 3.20 (3.26)  Time: 0.161s,  793.53/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 772.775s\n",
      "Train: 12 [4850/10009 ( 48%)]  Loss: 3.26 (3.25)  Time: 0.160s,  797.53/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 780.824s\n",
      "Train: 12 [4900/10009 ( 49%)]  Loss: 3.19 (3.26)  Time: 0.160s,  799.34/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 788.870s\n",
      "Train: 12 [4950/10009 ( 49%)]  Loss: 3.38 (3.26)  Time: 0.160s,  798.82/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 796.912s\n",
      "Train: 12 [5000/10009 ( 50%)]  Loss: 3.34 (3.25)  Time: 0.161s,  793.75/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 804.956s\n",
      "Train: 12 [5050/10009 ( 50%)]  Loss: 3.07 (3.25)  Time: 0.162s,  789.39/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 813.001s\n",
      "Train: 12 [5100/10009 ( 51%)]  Loss: 3.15 (3.25)  Time: 0.163s,  786.21/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 821.055s\n",
      "Train: 12 [5150/10009 ( 51%)]  Loss: 3.59 (3.25)  Time: 0.160s,  799.80/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 829.101s\n",
      "Train: 12 [5200/10009 ( 52%)]  Loss: 3.41 (3.25)  Time: 0.160s,  800.30/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 837.152s\n",
      "Train: 12 [5250/10009 ( 52%)]  Loss: 3.31 (3.25)  Time: 0.161s,  795.03/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 845.206s\n",
      "Train: 12 [5300/10009 ( 53%)]  Loss: 3.48 (3.25)  Time: 0.160s,  797.78/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 853.249s\n",
      "Train: 12 [5350/10009 ( 53%)]  Loss: 3.14 (3.25)  Time: 0.161s,  796.37/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 861.291s\n",
      "Train: 12 [5400/10009 ( 54%)]  Loss: 3.46 (3.25)  Time: 0.160s,  798.22/s  (0.161s,  795.23/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 869.340s\n",
      "Train: 12 [5450/10009 ( 54%)]  Loss: 3.46 (3.26)  Time: 0.161s,  796.72/s  (0.161s,  795.22/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 877.398s\n",
      "Train: 12 [5500/10009 ( 55%)]  Loss: 3.20 (3.26)  Time: 0.161s,  795.82/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 885.462s\n",
      "Train: 12 [5550/10009 ( 55%)]  Loss: 3.13 (3.26)  Time: 0.161s,  793.71/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 893.514s\n",
      "Train: 12 [5600/10009 ( 56%)]  Loss: 3.10 (3.26)  Time: 0.161s,  795.43/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 901.560s\n",
      "Train: 12 [5650/10009 ( 56%)]  Loss: 3.13 (3.26)  Time: 0.161s,  795.41/s  (0.161s,  795.21/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 909.606s\n",
      "Train: 12 [5700/10009 ( 57%)]  Loss: 3.25 (3.26)  Time: 0.161s,  794.54/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 917.663s\n",
      "Train: 12 [5750/10009 ( 57%)]  Loss: 3.56 (3.26)  Time: 0.161s,  797.13/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 925.718s\n",
      "Train: 12 [5800/10009 ( 58%)]  Loss: 3.56 (3.26)  Time: 0.162s,  792.34/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 933.767s\n",
      "Train: 12 [5850/10009 ( 58%)]  Loss: 3.06 (3.26)  Time: 0.160s,  798.90/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 941.821s\n",
      "Train: 12 [5900/10009 ( 59%)]  Loss: 3.54 (3.26)  Time: 0.160s,  798.43/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 949.866s\n",
      "Train: 12 [5950/10009 ( 59%)]  Loss: 3.21 (3.26)  Time: 0.162s,  790.16/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 957.919s\n",
      "Train: 12 [6000/10009 ( 60%)]  Loss: 3.15 (3.26)  Time: 0.161s,  793.95/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 965.956s\n",
      "Train: 12 [6050/10009 ( 60%)]  Loss: 3.03 (3.26)  Time: 0.161s,  793.52/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 974.000s\n",
      "Train: 12 [6100/10009 ( 61%)]  Loss: 3.12 (3.26)  Time: 0.162s,  792.11/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 982.050s\n",
      "Train: 12 [6150/10009 ( 61%)]  Loss: 3.12 (3.26)  Time: 0.160s,  797.57/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 990.102s\n",
      "Train: 12 [6200/10009 ( 62%)]  Loss: 3.54 (3.26)  Time: 0.162s,  788.51/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 998.151s\n",
      "Train: 12 [6250/10009 ( 62%)]  Loss: 3.12 (3.26)  Time: 0.162s,  791.92/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1006.203s\n",
      "Train: 12 [6300/10009 ( 63%)]  Loss: 2.97 (3.26)  Time: 0.160s,  799.74/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1014.246s\n",
      "Train: 12 [6350/10009 ( 63%)]  Loss: 3.26 (3.26)  Time: 0.161s,  795.97/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1022.301s\n",
      "Train: 12 [6400/10009 ( 64%)]  Loss: 3.17 (3.26)  Time: 0.161s,  793.37/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1030.351s\n",
      "Train: 12 [6450/10009 ( 64%)]  Loss: 3.31 (3.26)  Time: 0.161s,  794.74/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1038.416s\n",
      "Train: 12 [6500/10009 ( 65%)]  Loss: 3.08 (3.26)  Time: 0.160s,  799.42/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1046.466s\n",
      "Train: 12 [6550/10009 ( 65%)]  Loss: 3.44 (3.26)  Time: 0.162s,  790.45/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1054.509s\n",
      "Train: 12 [6600/10009 ( 66%)]  Loss: 3.65 (3.26)  Time: 0.161s,  797.34/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1062.553s\n",
      "Train: 12 [6650/10009 ( 66%)]  Loss: 3.32 (3.26)  Time: 0.162s,  790.76/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1070.587s\n",
      "Train: 12 [6700/10009 ( 67%)]  Loss: 2.90 (3.26)  Time: 0.161s,  797.21/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1078.634s\n",
      "Train: 12 [6750/10009 ( 67%)]  Loss: 3.38 (3.26)  Time: 0.160s,  800.19/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1086.680s\n",
      "Train: 12 [6800/10009 ( 68%)]  Loss: 3.22 (3.26)  Time: 0.162s,  790.10/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1094.725s\n",
      "Train: 12 [6850/10009 ( 68%)]  Loss: 3.47 (3.26)  Time: 0.161s,  797.15/s  (0.161s,  795.20/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1102.781s\n",
      "Train: 12 [6900/10009 ( 69%)]  Loss: 3.29 (3.26)  Time: 0.160s,  797.75/s  (0.161s,  795.19/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1110.841s\n",
      "Train: 12 [6950/10009 ( 69%)]  Loss: 3.25 (3.26)  Time: 0.160s,  799.00/s  (0.161s,  795.18/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1118.900s\n",
      "Train: 12 [7000/10009 ( 70%)]  Loss: 3.46 (3.26)  Time: 0.164s,  779.96/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1126.966s\n",
      "Train: 12 [7050/10009 ( 70%)]  Loss: 3.37 (3.26)  Time: 0.161s,  793.35/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1135.029s\n",
      "Train: 12 [7100/10009 ( 71%)]  Loss: 3.17 (3.26)  Time: 0.160s,  798.83/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1143.080s\n",
      "Train: 12 [7150/10009 ( 71%)]  Loss: 3.19 (3.26)  Time: 0.160s,  799.74/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1151.127s\n",
      "Train: 12 [7200/10009 ( 72%)]  Loss: 2.89 (3.26)  Time: 0.160s,  799.81/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1159.182s\n",
      "Train: 12 [7250/10009 ( 72%)]  Loss: 3.01 (3.26)  Time: 0.161s,  795.06/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1167.227s\n",
      "Train: 12 [7300/10009 ( 73%)]  Loss: 3.07 (3.26)  Time: 0.160s,  798.99/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1175.288s\n",
      "Train: 12 [7350/10009 ( 73%)]  Loss: 3.29 (3.26)  Time: 0.162s,  791.70/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1183.331s\n",
      "Train: 12 [7400/10009 ( 74%)]  Loss: 3.17 (3.26)  Time: 0.161s,  793.97/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1191.382s\n",
      "Train: 12 [7450/10009 ( 74%)]  Loss: 3.30 (3.26)  Time: 0.161s,  794.74/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1199.436s\n",
      "Train: 12 [7500/10009 ( 75%)]  Loss: 3.14 (3.26)  Time: 0.161s,  797.51/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1207.486s\n",
      "Train: 12 [7550/10009 ( 75%)]  Loss: 3.19 (3.26)  Time: 0.161s,  794.69/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1215.534s\n",
      "Train: 12 [7600/10009 ( 76%)]  Loss: 3.35 (3.26)  Time: 0.160s,  799.93/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1223.582s\n",
      "Train: 12 [7650/10009 ( 76%)]  Loss: 3.47 (3.26)  Time: 0.161s,  797.24/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1231.622s\n",
      "Train: 12 [7700/10009 ( 77%)]  Loss: 3.22 (3.26)  Time: 0.161s,  797.31/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1239.667s\n",
      "Train: 12 [7750/10009 ( 77%)]  Loss: 3.26 (3.26)  Time: 0.160s,  797.86/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1247.707s\n",
      "Train: 12 [7800/10009 ( 78%)]  Loss: 3.43 (3.26)  Time: 0.161s,  793.66/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1255.752s\n",
      "Train: 12 [7850/10009 ( 78%)]  Loss: 3.20 (3.26)  Time: 0.161s,  797.17/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1263.805s\n",
      "Train: 12 [7900/10009 ( 79%)]  Loss: 3.43 (3.26)  Time: 0.160s,  797.89/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1271.847s\n",
      "Train: 12 [7950/10009 ( 79%)]  Loss: 3.19 (3.26)  Time: 0.161s,  794.03/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1279.885s\n",
      "Train: 12 [8000/10009 ( 80%)]  Loss: 3.06 (3.26)  Time: 0.162s,  792.22/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1287.939s\n",
      "Train: 12 [8050/10009 ( 80%)]  Loss: 2.90 (3.26)  Time: 0.161s,  793.64/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1295.990s\n",
      "Train: 12 [8100/10009 ( 81%)]  Loss: 3.20 (3.26)  Time: 0.162s,  791.34/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1304.035s\n",
      "Train: 12 [8150/10009 ( 81%)]  Loss: 3.65 (3.26)  Time: 0.160s,  797.68/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1312.080s\n",
      "Train: 12 [8200/10009 ( 82%)]  Loss: 3.28 (3.26)  Time: 0.162s,  789.69/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 1320.134s\n",
      "Train: 12 [8250/10009 ( 82%)]  Loss: 2.92 (3.26)  Time: 0.161s,  797.39/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1328.188s\n",
      "Train: 12 [8300/10009 ( 83%)]  Loss: 3.12 (3.26)  Time: 0.161s,  795.92/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1336.259s\n",
      "Train: 12 [8350/10009 ( 83%)]  Loss: 3.19 (3.26)  Time: 0.161s,  794.43/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1344.345s\n",
      "Train: 12 [8400/10009 ( 84%)]  Loss: 3.26 (3.26)  Time: 0.160s,  797.68/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1352.392s\n",
      "Train: 12 [8450/10009 ( 84%)]  Loss: 3.24 (3.26)  Time: 0.162s,  788.38/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1360.439s\n",
      "Train: 12 [8500/10009 ( 85%)]  Loss: 2.97 (3.26)  Time: 0.163s,  784.06/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1368.486s\n",
      "Train: 12 [8550/10009 ( 85%)]  Loss: 3.27 (3.26)  Time: 0.160s,  800.71/s  (0.161s,  795.12/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1376.550s\n",
      "Train: 12 [8600/10009 ( 86%)]  Loss: 3.32 (3.26)  Time: 0.160s,  798.61/s  (0.161s,  795.12/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1384.594s\n",
      "Train: 12 [8650/10009 ( 86%)]  Loss: 3.04 (3.26)  Time: 0.160s,  798.13/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1392.632s\n",
      "Train: 12 [8700/10009 ( 87%)]  Loss: 3.11 (3.26)  Time: 0.161s,  794.38/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1400.681s\n",
      "Train: 12 [8750/10009 ( 87%)]  Loss: 3.29 (3.26)  Time: 0.160s,  797.61/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1408.726s\n",
      "Train: 12 [8800/10009 ( 88%)]  Loss: 3.18 (3.26)  Time: 0.161s,  797.44/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1416.763s\n",
      "Train: 12 [8850/10009 ( 88%)]  Loss: 3.36 (3.26)  Time: 0.161s,  796.35/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1424.827s\n",
      "Train: 12 [8900/10009 ( 89%)]  Loss: 3.21 (3.26)  Time: 0.161s,  794.25/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1432.882s\n",
      "Train: 12 [8950/10009 ( 89%)]  Loss: 3.38 (3.26)  Time: 0.161s,  796.64/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1440.929s\n",
      "Train: 12 [9000/10009 ( 90%)]  Loss: 3.08 (3.26)  Time: 0.162s,  788.86/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1448.969s\n",
      "Train: 12 [9050/10009 ( 90%)]  Loss: 3.09 (3.26)  Time: 0.161s,  795.60/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1457.019s\n",
      "Train: 12 [9100/10009 ( 91%)]  Loss: 3.39 (3.26)  Time: 0.162s,  788.10/s  (0.161s,  795.13/s)  LR: 4.775e-06  Data: 0.008 (0.006)Time: 1465.066s\n",
      "Train: 12 [9150/10009 ( 91%)]  Loss: 3.19 (3.26)  Time: 0.160s,  799.87/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1473.113s\n",
      "Train: 12 [9200/10009 ( 92%)]  Loss: 3.59 (3.26)  Time: 0.160s,  797.70/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1481.158s\n",
      "Train: 12 [9250/10009 ( 92%)]  Loss: 3.33 (3.26)  Time: 0.160s,  798.76/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1489.204s\n",
      "Train: 12 [9300/10009 ( 93%)]  Loss: 3.14 (3.26)  Time: 0.161s,  795.91/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1497.245s\n",
      "Train: 12 [9350/10009 ( 93%)]  Loss: 3.06 (3.26)  Time: 0.160s,  799.11/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1505.293s\n",
      "Train: 12 [9400/10009 ( 94%)]  Loss: 3.28 (3.26)  Time: 0.160s,  798.17/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1513.341s\n",
      "Train: 12 [9450/10009 ( 94%)]  Loss: 3.39 (3.26)  Time: 0.160s,  799.71/s  (0.161s,  795.14/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1521.390s\n",
      "Train: 12 [9500/10009 ( 95%)]  Loss: 3.30 (3.26)  Time: 0.161s,  793.31/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1529.434s\n",
      "Train: 12 [9550/10009 ( 95%)]  Loss: 3.37 (3.26)  Time: 0.161s,  796.07/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1537.486s\n",
      "Train: 12 [9600/10009 ( 96%)]  Loss: 3.08 (3.26)  Time: 0.161s,  792.99/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1545.528s\n",
      "Train: 12 [9650/10009 ( 96%)]  Loss: 3.48 (3.26)  Time: 0.160s,  800.13/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1553.563s\n",
      "Train: 12 [9700/10009 ( 97%)]  Loss: 3.30 (3.26)  Time: 0.160s,  801.20/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1561.605s\n",
      "Train: 12 [9750/10009 ( 97%)]  Loss: 3.43 (3.26)  Time: 0.160s,  800.41/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1569.642s\n",
      "Train: 12 [9800/10009 ( 98%)]  Loss: 3.50 (3.26)  Time: 0.161s,  794.29/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.007 (0.006)Time: 1577.690s\n",
      "Train: 12 [9850/10009 ( 98%)]  Loss: 3.22 (3.26)  Time: 0.160s,  800.02/s  (0.161s,  795.16/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1585.743s\n",
      "Train: 12 [9900/10009 ( 99%)]  Loss: 3.31 (3.26)  Time: 0.160s,  797.89/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.006 (0.006)Time: 1593.785s\n",
      "Train: 12 [9950/10009 ( 99%)]  Loss: 3.52 (3.26)  Time: 0.160s,  802.18/s  (0.161s,  795.17/s)  LR: 4.775e-06  Data: 0.005 (0.006)Time: 1601.830s\n",
      "Train: 12 [10000/10009 (100%)]  Loss: 3.07 (3.26)  Time: 0.204s,  626.37/s  (0.161s,  795.15/s)  LR: 4.775e-06  Data: 0.050 (0.006)Time: 1609.913s\n",
      "Test: [   0/390]  Time: 0.679 (0.679)  Loss:   1.249 ( 1.249)  Acc@1:  79.688 ( 79.688)  Acc@5:  91.406 ( 91.406)\n",
      "Test: [  50/390]  Time: 0.052 (0.138)  Loss:   1.147 ( 2.019)  Acc@1:  72.656 ( 57.430)  Acc@5:  92.188 ( 78.202)\n",
      "Test: [ 100/390]  Time: 0.263 (0.132)  Loss:   1.933 ( 2.045)  Acc@1:  56.250 ( 54.022)  Acc@5:  82.031 ( 78.837)\n",
      "Test: [ 150/390]  Time: 0.052 (0.135)  Loss:   1.790 ( 2.008)  Acc@1:  57.031 ( 54.838)  Acc@5:  83.594 ( 79.367)\n",
      "Test: [ 200/390]  Time: 0.051 (0.132)  Loss:   3.167 ( 2.190)  Acc@1:  29.688 ( 51.710)  Acc@5:  60.938 ( 76.294)\n",
      "Test: [ 250/390]  Time: 0.052 (0.131)  Loss:   2.287 ( 2.303)  Acc@1:  56.250 ( 50.093)  Acc@5:  71.094 ( 74.406)\n",
      "Test: [ 300/390]  Time: 0.363 (0.131)  Loss:   2.656 ( 2.401)  Acc@1:  51.562 ( 48.438)  Acc@5:  67.188 ( 72.654)\n",
      "Test: [ 350/390]  Time: 0.052 (0.129)  Loss:   2.729 ( 2.473)  Acc@1:  42.969 ( 47.086)  Acc@5:  67.188 ( 71.443)\n",
      "Test: [ 390/390]  Time: 0.034 (0.130)  Loss:   3.650 ( 2.440)  Acc@1:  23.750 ( 47.692)  Acc@5:  57.500 ( 72.006)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 13 [   0/10009 (  0%)]  Loss: 3.24 (3.24)  Time: 0.666s,  192.14/s  (0.666s,  192.14/s)  LR: 2.161e-06  Data: 0.515 (0.515)Time: 0.667s\n",
      "Train: 13 [  50/10009 (  0%)]  Loss: 3.30 (3.22)  Time: 0.159s,  803.23/s  (0.169s,  758.09/s)  LR: 2.161e-06  Data: 0.006 (0.016)Time: 8.612s\n",
      "Train: 13 [ 100/10009 (  1%)]  Loss: 2.74 (3.20)  Time: 0.161s,  795.50/s  (0.164s,  779.95/s)  LR: 2.161e-06  Data: 0.008 (0.011)Time: 16.576s\n",
      "Train: 13 [ 150/10009 (  1%)]  Loss: 3.17 (3.20)  Time: 0.159s,  803.50/s  (0.163s,  786.67/s)  LR: 2.161e-06  Data: 0.006 (0.010)Time: 24.570s\n",
      "Train: 13 [ 200/10009 (  2%)]  Loss: 3.21 (3.20)  Time: 0.161s,  796.79/s  (0.162s,  789.59/s)  LR: 2.161e-06  Data: 0.007 (0.009)Time: 32.585s\n",
      "Train: 13 [ 250/10009 (  2%)]  Loss: 3.08 (3.21)  Time: 0.161s,  793.79/s  (0.162s,  791.22/s)  LR: 2.161e-06  Data: 0.007 (0.008)Time: 40.606s\n",
      "Train: 13 [ 300/10009 (  3%)]  Loss: 3.10 (3.21)  Time: 0.161s,  795.77/s  (0.162s,  792.31/s)  LR: 2.161e-06  Data: 0.006 (0.008)Time: 48.628s\n",
      "Train: 13 [ 350/10009 (  3%)]  Loss: 3.12 (3.20)  Time: 0.160s,  798.53/s  (0.161s,  792.96/s)  LR: 2.161e-06  Data: 0.006 (0.008)Time: 56.659s\n",
      "Train: 13 [ 400/10009 (  4%)]  Loss: 3.09 (3.20)  Time: 0.160s,  798.48/s  (0.161s,  793.35/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 64.698s\n",
      "Train: 13 [ 450/10009 (  4%)]  Loss: 3.60 (3.20)  Time: 0.160s,  801.08/s  (0.161s,  793.65/s)  LR: 2.161e-06  Data: 0.005 (0.007)Time: 72.738s\n",
      "Train: 13 [ 500/10009 (  5%)]  Loss: 3.30 (3.20)  Time: 0.160s,  799.46/s  (0.161s,  793.96/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 80.770s\n",
      "Train: 13 [ 550/10009 (  5%)]  Loss: 3.42 (3.20)  Time: 0.161s,  794.48/s  (0.161s,  794.23/s)  LR: 2.161e-06  Data: 0.007 (0.007)Time: 88.801s\n",
      "Train: 13 [ 600/10009 (  6%)]  Loss: 3.18 (3.20)  Time: 0.161s,  794.51/s  (0.161s,  794.46/s)  LR: 2.161e-06  Data: 0.007 (0.007)Time: 96.831s\n",
      "Train: 13 [ 650/10009 (  6%)]  Loss: 3.04 (3.20)  Time: 0.160s,  797.92/s  (0.161s,  794.67/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 104.859s\n",
      "Train: 13 [ 700/10009 (  7%)]  Loss: 3.73 (3.20)  Time: 0.161s,  796.99/s  (0.161s,  794.83/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 112.890s\n",
      "Train: 13 [ 750/10009 (  7%)]  Loss: 3.37 (3.20)  Time: 0.162s,  792.12/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 120.925s\n",
      "Train: 13 [ 800/10009 (  8%)]  Loss: 3.12 (3.21)  Time: 0.161s,  795.29/s  (0.161s,  794.97/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 128.972s\n",
      "Train: 13 [ 850/10009 (  8%)]  Loss: 3.04 (3.21)  Time: 0.161s,  794.25/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.005 (0.007)Time: 137.012s\n",
      "Train: 13 [ 900/10009 (  9%)]  Loss: 2.95 (3.21)  Time: 0.161s,  793.42/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 145.061s\n",
      "Train: 13 [ 950/10009 (  9%)]  Loss: 3.26 (3.21)  Time: 0.160s,  798.38/s  (0.161s,  795.10/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 153.099s\n",
      "Train: 13 [1000/10009 ( 10%)]  Loss: 3.14 (3.21)  Time: 0.160s,  798.19/s  (0.161s,  795.18/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 161.132s\n",
      "Train: 13 [1050/10009 ( 10%)]  Loss: 3.23 (3.21)  Time: 0.160s,  797.98/s  (0.161s,  795.27/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 169.161s\n",
      "Train: 13 [1100/10009 ( 11%)]  Loss: 3.35 (3.21)  Time: 0.160s,  798.17/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 177.193s\n",
      "Train: 13 [1150/10009 ( 11%)]  Loss: 2.98 (3.21)  Time: 0.162s,  789.10/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.007 (0.007)Time: 185.238s\n",
      "Train: 13 [1200/10009 ( 12%)]  Loss: 3.32 (3.21)  Time: 0.160s,  798.87/s  (0.161s,  795.36/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 193.282s\n",
      "Train: 13 [1250/10009 ( 12%)]  Loss: 3.40 (3.21)  Time: 0.160s,  798.40/s  (0.161s,  795.39/s)  LR: 2.161e-06  Data: 0.006 (0.007)Time: 201.321s\n",
      "Train: 13 [1300/10009 ( 13%)]  Loss: 3.10 (3.21)  Time: 0.160s,  798.23/s  (0.161s,  795.46/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 209.349s\n",
      "Train: 13 [1350/10009 ( 13%)]  Loss: 3.15 (3.21)  Time: 0.160s,  798.97/s  (0.161s,  795.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 217.392s\n",
      "Train: 13 [1400/10009 ( 14%)]  Loss: 3.32 (3.21)  Time: 0.162s,  788.17/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.009 (0.006)Time: 225.434s\n",
      "Train: 13 [1450/10009 ( 14%)]  Loss: 3.33 (3.21)  Time: 0.161s,  794.28/s  (0.161s,  795.46/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 233.484s\n",
      "Train: 13 [1500/10009 ( 15%)]  Loss: 3.11 (3.21)  Time: 0.161s,  795.46/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 241.526s\n",
      "Train: 13 [1550/10009 ( 15%)]  Loss: 3.12 (3.21)  Time: 0.160s,  799.16/s  (0.161s,  795.51/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 249.560s\n",
      "Train: 13 [1600/10009 ( 16%)]  Loss: 3.23 (3.21)  Time: 0.161s,  792.91/s  (0.161s,  795.52/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 257.603s\n",
      "Train: 13 [1650/10009 ( 16%)]  Loss: 3.26 (3.21)  Time: 0.160s,  800.01/s  (0.161s,  795.45/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 265.671s\n",
      "Train: 13 [1700/10009 ( 17%)]  Loss: 3.15 (3.21)  Time: 0.161s,  792.95/s  (0.161s,  795.41/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 273.730s\n",
      "Train: 13 [1750/10009 ( 17%)]  Loss: 3.45 (3.20)  Time: 0.160s,  800.82/s  (0.161s,  795.43/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 281.769s\n",
      "Train: 13 [1800/10009 ( 18%)]  Loss: 3.27 (3.20)  Time: 0.160s,  797.98/s  (0.161s,  795.45/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 289.807s\n",
      "Train: 13 [1850/10009 ( 18%)]  Loss: 3.06 (3.21)  Time: 0.160s,  797.52/s  (0.161s,  795.46/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 297.851s\n",
      "Train: 13 [1900/10009 ( 19%)]  Loss: 3.34 (3.21)  Time: 0.162s,  789.37/s  (0.161s,  795.44/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 305.905s\n",
      "Train: 13 [1950/10009 ( 19%)]  Loss: 3.42 (3.20)  Time: 0.162s,  792.25/s  (0.161s,  795.45/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 313.946s\n",
      "Train: 13 [2000/10009 ( 20%)]  Loss: 3.06 (3.20)  Time: 0.162s,  792.00/s  (0.161s,  795.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 321.982s\n",
      "Train: 13 [2050/10009 ( 20%)]  Loss: 3.13 (3.20)  Time: 0.161s,  794.87/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 330.025s\n",
      "Train: 13 [2100/10009 ( 21%)]  Loss: 3.60 (3.20)  Time: 0.161s,  794.56/s  (0.161s,  795.48/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 338.070s\n",
      "Train: 13 [2150/10009 ( 21%)]  Loss: 3.21 (3.20)  Time: 0.160s,  802.11/s  (0.161s,  795.50/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 346.108s\n",
      "Train: 13 [2200/10009 ( 22%)]  Loss: 3.47 (3.21)  Time: 0.160s,  797.85/s  (0.161s,  795.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 354.167s\n",
      "Train: 13 [2250/10009 ( 22%)]  Loss: 2.99 (3.20)  Time: 0.161s,  795.57/s  (0.161s,  795.44/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 362.224s\n",
      "Train: 13 [2300/10009 ( 23%)]  Loss: 2.93 (3.20)  Time: 0.161s,  797.38/s  (0.161s,  795.44/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 370.272s\n",
      "Train: 13 [2350/10009 ( 23%)]  Loss: 3.30 (3.20)  Time: 0.160s,  798.18/s  (0.161s,  795.43/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 378.321s\n",
      "Train: 13 [2400/10009 ( 24%)]  Loss: 3.26 (3.20)  Time: 0.160s,  800.15/s  (0.161s,  795.40/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 386.379s\n",
      "Train: 13 [2450/10009 ( 24%)]  Loss: 3.41 (3.20)  Time: 0.161s,  794.81/s  (0.161s,  795.41/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 394.423s\n",
      "Train: 13 [2500/10009 ( 25%)]  Loss: 3.15 (3.20)  Time: 0.161s,  794.81/s  (0.161s,  795.41/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 402.470s\n",
      "Train: 13 [2550/10009 ( 25%)]  Loss: 3.04 (3.20)  Time: 0.161s,  797.41/s  (0.161s,  795.40/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 410.522s\n",
      "Train: 13 [2600/10009 ( 26%)]  Loss: 3.12 (3.20)  Time: 0.162s,  792.57/s  (0.161s,  795.39/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 418.571s\n",
      "Train: 13 [2650/10009 ( 26%)]  Loss: 3.21 (3.20)  Time: 0.161s,  797.05/s  (0.161s,  795.39/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 426.619s\n",
      "Train: 13 [2700/10009 ( 27%)]  Loss: 3.06 (3.20)  Time: 0.160s,  798.02/s  (0.161s,  795.35/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 434.686s\n",
      "Train: 13 [2750/10009 ( 27%)]  Loss: 3.18 (3.20)  Time: 0.161s,  794.76/s  (0.161s,  795.35/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 442.733s\n",
      "Train: 13 [2800/10009 ( 28%)]  Loss: 3.32 (3.20)  Time: 0.161s,  795.72/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 450.790s\n",
      "Train: 13 [2850/10009 ( 28%)]  Loss: 2.98 (3.20)  Time: 0.160s,  798.56/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 458.841s\n",
      "Train: 13 [2900/10009 ( 29%)]  Loss: 3.20 (3.20)  Time: 0.160s,  799.89/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 466.905s\n",
      "Train: 13 [2950/10009 ( 29%)]  Loss: 3.29 (3.20)  Time: 0.161s,  793.91/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 474.945s\n",
      "Train: 13 [3000/10009 ( 30%)]  Loss: 3.12 (3.20)  Time: 0.161s,  793.49/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 482.998s\n",
      "Train: 13 [3050/10009 ( 30%)]  Loss: 2.94 (3.20)  Time: 0.162s,  791.35/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 491.044s\n",
      "Train: 13 [3100/10009 ( 31%)]  Loss: 3.10 (3.20)  Time: 0.161s,  794.57/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 499.095s\n",
      "Train: 13 [3150/10009 ( 31%)]  Loss: 3.29 (3.20)  Time: 0.160s,  797.96/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 507.143s\n",
      "Train: 13 [3200/10009 ( 32%)]  Loss: 3.57 (3.20)  Time: 0.161s,  796.18/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 515.188s\n",
      "Train: 13 [3250/10009 ( 32%)]  Loss: 3.03 (3.20)  Time: 0.160s,  799.73/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 523.218s\n",
      "Train: 13 [3300/10009 ( 33%)]  Loss: 3.43 (3.20)  Time: 0.160s,  798.91/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 531.266s\n",
      "Train: 13 [3350/10009 ( 33%)]  Loss: 3.14 (3.20)  Time: 0.160s,  798.69/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 539.314s\n",
      "Train: 13 [3400/10009 ( 34%)]  Loss: 3.39 (3.20)  Time: 0.162s,  789.18/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 547.358s\n",
      "Train: 13 [3450/10009 ( 34%)]  Loss: 3.40 (3.20)  Time: 0.161s,  795.81/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 555.403s\n",
      "Train: 13 [3500/10009 ( 35%)]  Loss: 3.09 (3.20)  Time: 0.162s,  792.04/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 563.448s\n",
      "Train: 13 [3550/10009 ( 35%)]  Loss: 3.19 (3.20)  Time: 0.161s,  793.01/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 571.490s\n",
      "Train: 13 [3600/10009 ( 36%)]  Loss: 3.28 (3.20)  Time: 0.161s,  795.29/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 579.534s\n",
      "Train: 13 [3650/10009 ( 36%)]  Loss: 3.37 (3.20)  Time: 0.161s,  797.24/s  (0.161s,  795.36/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 587.568s\n",
      "Train: 13 [3700/10009 ( 37%)]  Loss: 3.29 (3.20)  Time: 0.160s,  798.70/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 595.603s\n",
      "Train: 13 [3750/10009 ( 37%)]  Loss: 3.16 (3.20)  Time: 0.161s,  796.91/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 603.650s\n",
      "Train: 13 [3800/10009 ( 38%)]  Loss: 3.01 (3.20)  Time: 0.160s,  800.06/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 611.703s\n",
      "Train: 13 [3850/10009 ( 38%)]  Loss: 3.20 (3.20)  Time: 0.160s,  798.47/s  (0.161s,  795.37/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 619.747s\n",
      "Train: 13 [3900/10009 ( 39%)]  Loss: 3.11 (3.20)  Time: 0.160s,  798.87/s  (0.161s,  795.34/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 627.812s\n",
      "Train: 13 [3950/10009 ( 39%)]  Loss: 3.38 (3.20)  Time: 0.160s,  797.94/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 635.869s\n",
      "Train: 13 [4000/10009 ( 40%)]  Loss: 2.83 (3.20)  Time: 0.161s,  795.08/s  (0.161s,  795.33/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 643.921s\n",
      "Train: 13 [4050/10009 ( 40%)]  Loss: 3.24 (3.20)  Time: 0.162s,  791.25/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 651.982s\n",
      "Train: 13 [4100/10009 ( 41%)]  Loss: 2.80 (3.20)  Time: 0.163s,  787.69/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 660.029s\n",
      "Train: 13 [4150/10009 ( 41%)]  Loss: 3.21 (3.20)  Time: 0.161s,  793.57/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 668.080s\n",
      "Train: 13 [4200/10009 ( 42%)]  Loss: 3.28 (3.20)  Time: 0.161s,  795.53/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 676.132s\n",
      "Train: 13 [4250/10009 ( 42%)]  Loss: 3.20 (3.20)  Time: 0.161s,  796.02/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 684.175s\n",
      "Train: 13 [4300/10009 ( 43%)]  Loss: 3.19 (3.20)  Time: 0.161s,  796.47/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 692.219s\n",
      "Train: 13 [4350/10009 ( 43%)]  Loss: 3.31 (3.20)  Time: 0.160s,  798.88/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 700.261s\n",
      "Train: 13 [4400/10009 ( 44%)]  Loss: 3.36 (3.20)  Time: 0.160s,  798.66/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 708.313s\n",
      "Train: 13 [4450/10009 ( 44%)]  Loss: 3.16 (3.20)  Time: 0.160s,  799.22/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 716.357s\n",
      "Train: 13 [4500/10009 ( 45%)]  Loss: 3.17 (3.20)  Time: 0.160s,  799.13/s  (0.161s,  795.32/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 724.397s\n",
      "Train: 13 [4550/10009 ( 45%)]  Loss: 3.04 (3.20)  Time: 0.160s,  799.63/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 732.455s\n",
      "Train: 13 [4600/10009 ( 46%)]  Loss: 2.85 (3.20)  Time: 0.160s,  798.22/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 740.507s\n",
      "Train: 13 [4650/10009 ( 46%)]  Loss: 3.13 (3.20)  Time: 0.161s,  793.94/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 748.568s\n",
      "Train: 13 [4700/10009 ( 47%)]  Loss: 2.87 (3.20)  Time: 0.161s,  796.08/s  (0.161s,  795.29/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 756.616s\n",
      "Train: 13 [4750/10009 ( 47%)]  Loss: 3.25 (3.20)  Time: 0.161s,  796.91/s  (0.161s,  795.28/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 764.666s\n",
      "Train: 13 [4800/10009 ( 48%)]  Loss: 3.08 (3.20)  Time: 0.160s,  798.14/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 772.694s\n",
      "Train: 13 [4850/10009 ( 48%)]  Loss: 2.97 (3.20)  Time: 0.161s,  796.84/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 780.741s\n",
      "Train: 13 [4900/10009 ( 49%)]  Loss: 2.98 (3.20)  Time: 0.160s,  797.84/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 788.790s\n",
      "Train: 13 [4950/10009 ( 49%)]  Loss: 3.37 (3.20)  Time: 0.161s,  797.20/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 796.833s\n",
      "Train: 13 [5000/10009 ( 50%)]  Loss: 3.12 (3.20)  Time: 0.161s,  796.45/s  (0.161s,  795.31/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 804.878s\n",
      "Train: 13 [5050/10009 ( 50%)]  Loss: 3.31 (3.20)  Time: 0.161s,  795.50/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 812.936s\n",
      "Train: 13 [5100/10009 ( 51%)]  Loss: 3.24 (3.20)  Time: 0.162s,  790.48/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 820.981s\n",
      "Train: 13 [5150/10009 ( 51%)]  Loss: 3.36 (3.20)  Time: 0.161s,  795.55/s  (0.161s,  795.30/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 829.033s\n",
      "Train: 13 [5200/10009 ( 52%)]  Loss: 3.25 (3.20)  Time: 0.161s,  796.95/s  (0.161s,  795.24/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 837.136s\n",
      "Train: 13 [5250/10009 ( 52%)]  Loss: 3.73 (3.20)  Time: 0.160s,  798.93/s  (0.161s,  795.24/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 845.184s\n",
      "Train: 13 [5300/10009 ( 53%)]  Loss: 3.02 (3.20)  Time: 0.161s,  796.78/s  (0.161s,  795.25/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 853.229s\n",
      "Train: 13 [5350/10009 ( 53%)]  Loss: 2.93 (3.20)  Time: 0.160s,  799.59/s  (0.161s,  795.25/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 861.274s\n",
      "Train: 13 [5400/10009 ( 54%)]  Loss: 3.15 (3.20)  Time: 0.160s,  798.90/s  (0.161s,  795.25/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 869.315s\n",
      "Train: 13 [5450/10009 ( 54%)]  Loss: 3.26 (3.20)  Time: 0.163s,  787.55/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 877.385s\n",
      "Train: 13 [5500/10009 ( 55%)]  Loss: 3.19 (3.20)  Time: 0.160s,  800.23/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 885.432s\n",
      "Train: 13 [5550/10009 ( 55%)]  Loss: 3.41 (3.20)  Time: 0.161s,  793.70/s  (0.161s,  795.24/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 893.478s\n",
      "Train: 13 [5600/10009 ( 56%)]  Loss: 3.42 (3.20)  Time: 0.161s,  794.29/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 901.533s\n",
      "Train: 13 [5650/10009 ( 56%)]  Loss: 2.74 (3.20)  Time: 0.160s,  801.16/s  (0.161s,  795.23/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 909.580s\n",
      "Train: 13 [5700/10009 ( 57%)]  Loss: 3.21 (3.20)  Time: 0.161s,  796.34/s  (0.161s,  795.22/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 917.640s\n",
      "Train: 13 [5750/10009 ( 57%)]  Loss: 3.05 (3.20)  Time: 0.161s,  793.79/s  (0.161s,  795.18/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 925.738s\n",
      "Train: 13 [5800/10009 ( 58%)]  Loss: 3.05 (3.20)  Time: 0.161s,  796.42/s  (0.161s,  795.17/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 933.796s\n",
      "Train: 13 [5850/10009 ( 58%)]  Loss: 3.11 (3.20)  Time: 0.160s,  800.12/s  (0.161s,  795.17/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 941.841s\n",
      "Train: 13 [5900/10009 ( 59%)]  Loss: 3.19 (3.20)  Time: 0.160s,  798.77/s  (0.161s,  795.17/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 949.893s\n",
      "Train: 13 [5950/10009 ( 59%)]  Loss: 3.20 (3.20)  Time: 0.162s,  791.78/s  (0.161s,  795.15/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 957.968s\n",
      "Train: 13 [6000/10009 ( 60%)]  Loss: 3.05 (3.20)  Time: 0.161s,  795.99/s  (0.161s,  795.10/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 966.079s\n",
      "Train: 13 [6050/10009 ( 60%)]  Loss: 3.59 (3.20)  Time: 0.161s,  796.37/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 974.136s\n",
      "Train: 13 [6100/10009 ( 61%)]  Loss: 3.33 (3.20)  Time: 0.161s,  796.39/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 982.184s\n",
      "Train: 13 [6150/10009 ( 61%)]  Loss: 3.31 (3.20)  Time: 0.160s,  799.08/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 990.231s\n",
      "Train: 13 [6200/10009 ( 62%)]  Loss: 3.21 (3.20)  Time: 0.160s,  798.09/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 998.288s\n",
      "Train: 13 [6250/10009 ( 62%)]  Loss: 3.09 (3.20)  Time: 0.161s,  795.69/s  (0.161s,  795.09/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1006.335s\n",
      "Train: 13 [6300/10009 ( 63%)]  Loss: 3.10 (3.20)  Time: 0.161s,  792.83/s  (0.161s,  795.08/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1014.392s\n",
      "Train: 13 [6350/10009 ( 63%)]  Loss: 3.02 (3.20)  Time: 0.161s,  796.87/s  (0.161s,  795.08/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1022.449s\n",
      "Train: 13 [6400/10009 ( 64%)]  Loss: 3.31 (3.20)  Time: 0.161s,  794.01/s  (0.161s,  795.07/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1030.512s\n",
      "Train: 13 [6450/10009 ( 64%)]  Loss: 3.12 (3.20)  Time: 0.161s,  796.62/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1038.605s\n",
      "Train: 13 [6500/10009 ( 65%)]  Loss: 2.99 (3.20)  Time: 0.161s,  796.54/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1046.655s\n",
      "Train: 13 [6550/10009 ( 65%)]  Loss: 3.45 (3.20)  Time: 0.162s,  790.81/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1054.707s\n",
      "Train: 13 [6600/10009 ( 66%)]  Loss: 2.87 (3.20)  Time: 0.162s,  789.40/s  (0.161s,  795.03/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1062.763s\n",
      "Train: 13 [6650/10009 ( 66%)]  Loss: 3.35 (3.20)  Time: 0.160s,  798.04/s  (0.161s,  795.02/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1070.819s\n",
      "Train: 13 [6700/10009 ( 67%)]  Loss: 3.23 (3.20)  Time: 0.161s,  796.27/s  (0.161s,  795.01/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1078.887s\n",
      "Train: 13 [6750/10009 ( 67%)]  Loss: 3.06 (3.20)  Time: 0.162s,  788.46/s  (0.161s,  794.99/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1086.964s\n",
      "Train: 13 [6800/10009 ( 68%)]  Loss: 3.21 (3.20)  Time: 0.161s,  795.67/s  (0.161s,  794.97/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1095.037s\n",
      "Train: 13 [6850/10009 ( 68%)]  Loss: 3.05 (3.20)  Time: 0.161s,  797.40/s  (0.161s,  794.96/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1103.101s\n",
      "Train: 13 [6900/10009 ( 69%)]  Loss: 3.22 (3.20)  Time: 0.160s,  797.79/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1111.186s\n",
      "Train: 13 [6950/10009 ( 69%)]  Loss: 3.04 (3.20)  Time: 0.161s,  797.36/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1119.235s\n",
      "Train: 13 [7000/10009 ( 70%)]  Loss: 3.19 (3.20)  Time: 0.163s,  782.90/s  (0.161s,  794.94/s)  LR: 2.161e-06  Data: 0.009 (0.006)Time: 1127.289s\n",
      "Train: 13 [7050/10009 ( 70%)]  Loss: 3.33 (3.20)  Time: 0.162s,  792.43/s  (0.161s,  794.91/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1135.387s\n",
      "Train: 13 [7100/10009 ( 71%)]  Loss: 3.10 (3.20)  Time: 0.160s,  798.51/s  (0.161s,  794.90/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1143.447s\n",
      "Train: 13 [7150/10009 ( 71%)]  Loss: 3.27 (3.20)  Time: 0.161s,  794.75/s  (0.161s,  794.89/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1151.509s\n",
      "Train: 13 [7200/10009 ( 72%)]  Loss: 3.48 (3.20)  Time: 0.160s,  798.78/s  (0.161s,  794.88/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1159.576s\n",
      "Train: 13 [7250/10009 ( 72%)]  Loss: 3.34 (3.20)  Time: 0.160s,  797.95/s  (0.161s,  794.88/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1167.625s\n",
      "Train: 13 [7300/10009 ( 73%)]  Loss: 2.98 (3.20)  Time: 0.162s,  789.76/s  (0.161s,  794.87/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1175.703s\n",
      "Train: 13 [7350/10009 ( 73%)]  Loss: 3.18 (3.20)  Time: 0.160s,  798.92/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1183.757s\n",
      "Train: 13 [7400/10009 ( 74%)]  Loss: 3.30 (3.20)  Time: 0.161s,  794.69/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1191.813s\n",
      "Train: 13 [7450/10009 ( 74%)]  Loss: 3.01 (3.20)  Time: 0.163s,  784.85/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 1199.868s\n",
      "Train: 13 [7500/10009 ( 75%)]  Loss: 2.87 (3.20)  Time: 0.162s,  791.44/s  (0.161s,  794.86/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1207.925s\n",
      "Train: 13 [7550/10009 ( 75%)]  Loss: 3.15 (3.20)  Time: 0.162s,  788.39/s  (0.161s,  794.85/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1215.981s\n",
      "Train: 13 [7600/10009 ( 76%)]  Loss: 3.31 (3.20)  Time: 0.161s,  794.60/s  (0.161s,  794.85/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1224.040s\n",
      "Train: 13 [7650/10009 ( 76%)]  Loss: 3.15 (3.20)  Time: 0.161s,  794.60/s  (0.161s,  794.83/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1232.121s\n",
      "Train: 13 [7700/10009 ( 77%)]  Loss: 3.06 (3.20)  Time: 0.162s,  790.73/s  (0.161s,  794.80/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1240.215s\n",
      "Train: 13 [7750/10009 ( 77%)]  Loss: 2.99 (3.20)  Time: 0.160s,  799.08/s  (0.161s,  794.77/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1248.315s\n",
      "Train: 13 [7800/10009 ( 78%)]  Loss: 3.37 (3.20)  Time: 0.162s,  788.51/s  (0.161s,  794.75/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 1256.400s\n",
      "Train: 13 [7850/10009 ( 78%)]  Loss: 3.45 (3.20)  Time: 0.161s,  796.41/s  (0.161s,  794.74/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1264.467s\n",
      "Train: 13 [7900/10009 ( 79%)]  Loss: 2.94 (3.20)  Time: 0.160s,  797.65/s  (0.161s,  794.74/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1272.525s\n",
      "Train: 13 [7950/10009 ( 79%)]  Loss: 3.43 (3.20)  Time: 0.161s,  797.43/s  (0.161s,  794.73/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1280.594s\n",
      "Train: 13 [8000/10009 ( 80%)]  Loss: 3.19 (3.20)  Time: 0.161s,  793.89/s  (0.161s,  794.72/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1288.661s\n",
      "Train: 13 [8050/10009 ( 80%)]  Loss: 3.31 (3.20)  Time: 0.164s,  781.88/s  (0.161s,  794.71/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1296.735s\n",
      "Train: 13 [8100/10009 ( 81%)]  Loss: 3.13 (3.20)  Time: 0.162s,  791.80/s  (0.161s,  794.69/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1304.811s\n",
      "Train: 13 [8150/10009 ( 81%)]  Loss: 2.98 (3.20)  Time: 0.161s,  795.79/s  (0.161s,  794.69/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1312.877s\n",
      "Train: 13 [8200/10009 ( 82%)]  Loss: 3.13 (3.20)  Time: 0.162s,  791.95/s  (0.161s,  794.68/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1320.945s\n",
      "Train: 13 [8250/10009 ( 82%)]  Loss: 3.20 (3.20)  Time: 0.161s,  795.56/s  (0.161s,  794.66/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1329.030s\n",
      "Train: 13 [8300/10009 ( 83%)]  Loss: 3.24 (3.20)  Time: 0.161s,  796.42/s  (0.161s,  794.66/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1337.083s\n",
      "Train: 13 [8350/10009 ( 83%)]  Loss: 3.33 (3.20)  Time: 0.161s,  796.22/s  (0.161s,  794.64/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1345.162s\n",
      "Train: 13 [8400/10009 ( 84%)]  Loss: 3.03 (3.20)  Time: 0.160s,  799.08/s  (0.161s,  794.65/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1353.213s\n",
      "Train: 13 [8450/10009 ( 84%)]  Loss: 3.19 (3.20)  Time: 0.161s,  795.67/s  (0.161s,  794.64/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1361.279s\n",
      "Train: 13 [8500/10009 ( 85%)]  Loss: 3.18 (3.20)  Time: 0.162s,  792.51/s  (0.161s,  794.61/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1369.387s\n",
      "Train: 13 [8550/10009 ( 85%)]  Loss: 3.60 (3.20)  Time: 0.160s,  798.70/s  (0.161s,  794.61/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1377.439s\n",
      "Train: 13 [8600/10009 ( 86%)]  Loss: 3.15 (3.20)  Time: 0.161s,  795.57/s  (0.161s,  794.61/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1385.493s\n",
      "Train: 13 [8650/10009 ( 86%)]  Loss: 3.05 (3.20)  Time: 0.161s,  793.65/s  (0.161s,  794.60/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1393.555s\n",
      "Train: 13 [8700/10009 ( 87%)]  Loss: 3.24 (3.20)  Time: 0.161s,  796.35/s  (0.161s,  794.60/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1401.625s\n",
      "Train: 13 [8750/10009 ( 87%)]  Loss: 3.09 (3.20)  Time: 0.161s,  797.06/s  (0.161s,  794.59/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1409.697s\n",
      "Train: 13 [8800/10009 ( 88%)]  Loss: 3.18 (3.20)  Time: 0.161s,  793.87/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1417.761s\n",
      "Train: 13 [8850/10009 ( 88%)]  Loss: 3.36 (3.20)  Time: 0.160s,  799.46/s  (0.161s,  794.59/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1425.806s\n",
      "Train: 13 [8900/10009 ( 89%)]  Loss: 3.09 (3.20)  Time: 0.161s,  793.62/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1433.873s\n",
      "Train: 13 [8950/10009 ( 89%)]  Loss: 3.47 (3.20)  Time: 0.162s,  787.93/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1441.927s\n",
      "Train: 13 [9000/10009 ( 90%)]  Loss: 3.42 (3.20)  Time: 0.162s,  789.61/s  (0.161s,  794.58/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1449.980s\n",
      "Train: 13 [9050/10009 ( 90%)]  Loss: 3.28 (3.20)  Time: 0.161s,  793.96/s  (0.161s,  794.57/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1458.061s\n",
      "Train: 13 [9100/10009 ( 91%)]  Loss: 3.15 (3.20)  Time: 0.161s,  796.27/s  (0.161s,  794.56/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1466.117s\n",
      "Train: 13 [9150/10009 ( 91%)]  Loss: 3.05 (3.20)  Time: 0.160s,  797.68/s  (0.161s,  794.55/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1474.204s\n",
      "Train: 13 [9200/10009 ( 92%)]  Loss: 3.16 (3.20)  Time: 0.165s,  773.55/s  (0.161s,  794.54/s)  LR: 2.161e-06  Data: 0.010 (0.006)Time: 1482.270s\n",
      "Train: 13 [9250/10009 ( 92%)]  Loss: 3.24 (3.20)  Time: 0.162s,  789.73/s  (0.161s,  794.54/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1490.326s\n",
      "Train: 13 [9300/10009 ( 93%)]  Loss: 3.03 (3.20)  Time: 0.161s,  796.95/s  (0.161s,  794.54/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1498.392s\n",
      "Train: 13 [9350/10009 ( 93%)]  Loss: 3.28 (3.20)  Time: 0.160s,  799.80/s  (0.161s,  794.51/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1506.496s\n",
      "Train: 13 [9400/10009 ( 94%)]  Loss: 3.47 (3.20)  Time: 0.161s,  797.06/s  (0.161s,  794.50/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1514.571s\n",
      "Train: 13 [9450/10009 ( 94%)]  Loss: 3.02 (3.20)  Time: 0.161s,  794.26/s  (0.161s,  794.50/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1522.632s\n",
      "Train: 13 [9500/10009 ( 95%)]  Loss: 3.32 (3.20)  Time: 0.161s,  797.26/s  (0.161s,  794.50/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1530.685s\n",
      "Train: 13 [9550/10009 ( 95%)]  Loss: 3.47 (3.20)  Time: 0.160s,  801.37/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1538.747s\n",
      "Train: 13 [9600/10009 ( 96%)]  Loss: 3.13 (3.20)  Time: 0.162s,  792.34/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1546.806s\n",
      "Train: 13 [9650/10009 ( 96%)]  Loss: 3.19 (3.20)  Time: 0.162s,  791.85/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.007 (0.006)Time: 1554.865s\n",
      "Train: 13 [9700/10009 ( 97%)]  Loss: 3.25 (3.20)  Time: 0.161s,  796.32/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1562.926s\n",
      "Train: 13 [9750/10009 ( 97%)]  Loss: 3.25 (3.20)  Time: 0.163s,  786.38/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.008 (0.006)Time: 1570.977s\n",
      "Train: 13 [9800/10009 ( 98%)]  Loss: 3.30 (3.20)  Time: 0.162s,  790.56/s  (0.161s,  794.49/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1579.029s\n",
      "Train: 13 [9850/10009 ( 98%)]  Loss: 3.26 (3.20)  Time: 0.160s,  799.48/s  (0.161s,  794.47/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1587.126s\n",
      "Train: 13 [9900/10009 ( 99%)]  Loss: 3.57 (3.20)  Time: 0.161s,  794.83/s  (0.161s,  794.47/s)  LR: 2.161e-06  Data: 0.006 (0.006)Time: 1595.177s\n",
      "Train: 13 [9950/10009 ( 99%)]  Loss: 3.30 (3.20)  Time: 0.160s,  800.29/s  (0.161s,  794.47/s)  LR: 2.161e-06  Data: 0.005 (0.006)Time: 1603.237s\n",
      "Train: 13 [10000/10009 (100%)]  Loss: 3.12 (3.20)  Time: 0.204s,  627.09/s  (0.161s,  794.44/s)  LR: 2.161e-06  Data: 0.051 (0.006)Time: 1611.360s\n",
      "Test: [   0/390]  Time: 0.675 (0.675)  Loss:   1.272 ( 1.272)  Acc@1:  73.438 ( 73.438)  Acc@5:  89.844 ( 89.844)\n",
      "Test: [  50/390]  Time: 0.051 (0.149)  Loss:   1.183 ( 1.967)  Acc@1:  74.219 ( 58.195)  Acc@5:  92.969 ( 79.228)\n",
      "Test: [ 100/390]  Time: 0.145 (0.144)  Loss:   2.003 ( 1.993)  Acc@1:  53.906 ( 54.749)  Acc@5:  83.594 ( 79.858)\n",
      "Test: [ 150/390]  Time: 0.051 (0.144)  Loss:   1.810 ( 1.965)  Acc@1:  56.250 ( 55.552)  Acc@5:  83.594 ( 80.262)\n",
      "Test: [ 200/390]  Time: 0.052 (0.143)  Loss:   3.043 ( 2.146)  Acc@1:  29.688 ( 52.363)  Acc@5:  66.406 ( 77.235)\n",
      "Test: [ 250/390]  Time: 0.053 (0.145)  Loss:   2.273 ( 2.260)  Acc@1:  58.594 ( 50.710)  Acc@5:  71.875 ( 75.271)\n",
      "Test: [ 300/390]  Time: 0.054 (0.144)  Loss:   2.626 ( 2.355)  Acc@1:  49.219 ( 49.084)  Acc@5:  67.969 ( 73.572)\n",
      "Test: [ 350/390]  Time: 0.216 (0.143)  Loss:   2.775 ( 2.429)  Acc@1:  43.750 ( 47.723)  Acc@5:  67.188 ( 72.291)\n",
      "Test: [ 390/390]  Time: 0.034 (0.144)  Loss:   3.664 ( 2.395)  Acc@1:  21.250 ( 48.326)  Acc@5:  58.750 ( 72.896)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "Train: 14 [   0/10009 (  0%)]  Loss: 3.08 (3.08)  Time: 0.741s,  172.72/s  (0.741s,  172.72/s)  LR: 5.463e-07  Data: 0.589 (0.589)Time: 0.742s\n",
      "Train: 14 [  50/10009 (  0%)]  Loss: 3.04 (3.19)  Time: 0.159s,  803.56/s  (0.175s,  731.00/s)  LR: 5.463e-07  Data: 0.006 (0.022)Time: 8.931s\n",
      "Train: 14 [ 100/10009 (  1%)]  Loss: 3.13 (3.19)  Time: 0.162s,  791.61/s  (0.168s,  763.21/s)  LR: 5.463e-07  Data: 0.008 (0.015)Time: 16.939s\n",
      "Train: 14 [ 150/10009 (  1%)]  Loss: 3.10 (3.18)  Time: 0.160s,  801.75/s  (0.165s,  774.25/s)  LR: 5.463e-07  Data: 0.006 (0.012)Time: 24.964s\n",
      "Train: 14 [ 200/10009 (  2%)]  Loss: 2.94 (3.18)  Time: 0.161s,  793.76/s  (0.164s,  779.44/s)  LR: 5.463e-07  Data: 0.006 (0.011)Time: 33.009s\n",
      "Train: 14 [ 250/10009 (  2%)]  Loss: 2.94 (3.18)  Time: 0.161s,  795.01/s  (0.163s,  783.10/s)  LR: 5.463e-07  Data: 0.006 (0.010)Time: 41.027s\n",
      "Train: 14 [ 300/10009 (  3%)]  Loss: 3.19 (3.18)  Time: 0.160s,  802.08/s  (0.163s,  785.44/s)  LR: 5.463e-07  Data: 0.005 (0.009)Time: 49.053s\n",
      "Train: 14 [ 350/10009 (  3%)]  Loss: 3.18 (3.18)  Time: 0.160s,  802.46/s  (0.163s,  787.09/s)  LR: 5.463e-07  Data: 0.006 (0.009)Time: 57.081s\n",
      "Train: 14 [ 400/10009 (  4%)]  Loss: 3.01 (3.18)  Time: 0.162s,  789.69/s  (0.162s,  788.17/s)  LR: 5.463e-07  Data: 0.007 (0.008)Time: 65.123s\n",
      "Train: 14 [ 450/10009 (  4%)]  Loss: 3.41 (3.18)  Time: 0.160s,  798.07/s  (0.162s,  788.67/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 73.197s\n",
      "Train: 14 [ 500/10009 (  5%)]  Loss: 3.10 (3.18)  Time: 0.160s,  802.07/s  (0.162s,  789.52/s)  LR: 5.463e-07  Data: 0.005 (0.008)Time: 81.224s\n",
      "Train: 14 [ 550/10009 (  5%)]  Loss: 2.89 (3.18)  Time: 0.161s,  795.52/s  (0.162s,  790.12/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 89.262s\n",
      "Train: 14 [ 600/10009 (  6%)]  Loss: 3.19 (3.18)  Time: 0.161s,  795.90/s  (0.162s,  790.55/s)  LR: 5.463e-07  Data: 0.007 (0.008)Time: 97.310s\n",
      "Train: 14 [ 650/10009 (  6%)]  Loss: 3.37 (3.18)  Time: 0.160s,  797.53/s  (0.162s,  790.97/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 105.349s\n",
      "Train: 14 [ 700/10009 (  7%)]  Loss: 3.49 (3.18)  Time: 0.160s,  799.04/s  (0.162s,  791.28/s)  LR: 5.463e-07  Data: 0.006 (0.008)Time: 113.396s\n",
      "Train: 14 [ 750/10009 (  7%)]  Loss: 3.30 (3.18)  Time: 0.160s,  799.21/s  (0.162s,  791.64/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 121.429s\n",
      "Train: 14 [ 800/10009 (  8%)]  Loss: 3.08 (3.18)  Time: 0.160s,  799.02/s  (0.162s,  791.83/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 129.482s\n",
      "Train: 14 [ 850/10009 (  8%)]  Loss: 2.94 (3.17)  Time: 0.161s,  797.19/s  (0.162s,  792.07/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 137.523s\n",
      "Train: 14 [ 900/10009 (  9%)]  Loss: 3.26 (3.17)  Time: 0.160s,  798.85/s  (0.162s,  792.16/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 145.587s\n",
      "Train: 14 [ 950/10009 (  9%)]  Loss: 3.23 (3.17)  Time: 0.161s,  793.25/s  (0.162s,  792.27/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 153.645s\n",
      "Train: 14 [1000/10009 ( 10%)]  Loss: 3.22 (3.17)  Time: 0.166s,  768.95/s  (0.162s,  791.76/s)  LR: 5.463e-07  Data: 0.012 (0.007)Time: 161.826s\n",
      "Train: 14 [1050/10009 ( 10%)]  Loss: 3.08 (3.17)  Time: 0.161s,  793.66/s  (0.162s,  791.76/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 169.910s\n",
      "Train: 14 [1100/10009 ( 11%)]  Loss: 3.15 (3.17)  Time: 0.162s,  788.26/s  (0.162s,  791.86/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 177.972s\n",
      "Train: 14 [1150/10009 ( 11%)]  Loss: 3.06 (3.17)  Time: 0.160s,  797.75/s  (0.162s,  791.90/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 186.045s\n",
      "Train: 14 [1200/10009 ( 12%)]  Loss: 2.91 (3.17)  Time: 0.161s,  797.46/s  (0.162s,  792.03/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 194.093s\n",
      "Train: 14 [1250/10009 ( 12%)]  Loss: 3.33 (3.17)  Time: 0.160s,  799.98/s  (0.162s,  791.75/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 202.245s\n",
      "Train: 14 [1300/10009 ( 13%)]  Loss: 3.27 (3.17)  Time: 0.161s,  795.00/s  (0.162s,  791.81/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 210.312s\n",
      "Train: 14 [1350/10009 ( 13%)]  Loss: 3.33 (3.17)  Time: 0.160s,  798.55/s  (0.162s,  791.66/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 218.437s\n",
      "Train: 14 [1400/10009 ( 14%)]  Loss: 3.29 (3.17)  Time: 0.160s,  799.45/s  (0.162s,  791.80/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 226.480s\n",
      "Train: 14 [1450/10009 ( 14%)]  Loss: 3.21 (3.17)  Time: 0.160s,  799.06/s  (0.162s,  791.92/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 234.529s\n",
      "Train: 14 [1500/10009 ( 15%)]  Loss: 3.06 (3.17)  Time: 0.161s,  793.18/s  (0.162s,  791.94/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 242.604s\n",
      "Train: 14 [1550/10009 ( 15%)]  Loss: 3.07 (3.17)  Time: 0.160s,  798.48/s  (0.162s,  791.99/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 250.669s\n",
      "Train: 14 [1600/10009 ( 16%)]  Loss: 3.16 (3.17)  Time: 0.160s,  797.69/s  (0.162s,  792.07/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 258.724s\n",
      "Train: 14 [1650/10009 ( 16%)]  Loss: 3.07 (3.17)  Time: 0.161s,  796.09/s  (0.162s,  792.16/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 266.774s\n",
      "Train: 14 [1700/10009 ( 17%)]  Loss: 3.11 (3.17)  Time: 0.161s,  794.83/s  (0.162s,  792.26/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 274.817s\n",
      "Train: 14 [1750/10009 ( 17%)]  Loss: 3.07 (3.17)  Time: 0.160s,  797.56/s  (0.162s,  792.21/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 282.915s\n",
      "Train: 14 [1800/10009 ( 18%)]  Loss: 3.36 (3.17)  Time: 0.161s,  794.30/s  (0.162s,  792.29/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 290.963s\n",
      "Train: 14 [1850/10009 ( 18%)]  Loss: 3.12 (3.17)  Time: 0.160s,  798.22/s  (0.162s,  792.20/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 299.077s\n",
      "Train: 14 [1900/10009 ( 19%)]  Loss: 3.01 (3.17)  Time: 0.161s,  793.58/s  (0.162s,  792.25/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 307.134s\n",
      "Train: 14 [1950/10009 ( 19%)]  Loss: 3.32 (3.17)  Time: 0.161s,  796.75/s  (0.162s,  792.18/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 315.242s\n",
      "Train: 14 [2000/10009 ( 20%)]  Loss: 3.26 (3.17)  Time: 0.162s,  788.13/s  (0.162s,  792.11/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 323.349s\n",
      "Train: 14 [2050/10009 ( 20%)]  Loss: 2.88 (3.17)  Time: 0.160s,  798.36/s  (0.162s,  792.08/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 331.440s\n",
      "Train: 14 [2100/10009 ( 21%)]  Loss: 3.15 (3.17)  Time: 0.160s,  800.35/s  (0.162s,  792.14/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 339.496s\n",
      "Train: 14 [2150/10009 ( 21%)]  Loss: 3.13 (3.17)  Time: 0.161s,  797.42/s  (0.162s,  792.20/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 347.549s\n",
      "Train: 14 [2200/10009 ( 22%)]  Loss: 3.48 (3.17)  Time: 0.160s,  798.52/s  (0.162s,  792.28/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 355.590s\n",
      "Train: 14 [2250/10009 ( 22%)]  Loss: 3.23 (3.17)  Time: 0.161s,  793.99/s  (0.162s,  792.36/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 363.632s\n",
      "Train: 14 [2300/10009 ( 23%)]  Loss: 2.88 (3.17)  Time: 0.163s,  783.65/s  (0.162s,  792.30/s)  LR: 5.463e-07  Data: 0.009 (0.007)Time: 371.736s\n",
      "Train: 14 [2350/10009 ( 23%)]  Loss: 2.99 (3.17)  Time: 0.161s,  793.50/s  (0.162s,  792.21/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 379.857s\n",
      "Train: 14 [2400/10009 ( 24%)]  Loss: 3.30 (3.17)  Time: 0.163s,  787.42/s  (0.162s,  792.25/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 387.919s\n",
      "Train: 14 [2450/10009 ( 24%)]  Loss: 3.49 (3.17)  Time: 0.161s,  792.64/s  (0.162s,  792.32/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 395.962s\n",
      "Train: 14 [2500/10009 ( 25%)]  Loss: 3.26 (3.17)  Time: 0.162s,  789.17/s  (0.162s,  792.37/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 404.013s\n",
      "Train: 14 [2550/10009 ( 25%)]  Loss: 3.07 (3.17)  Time: 0.164s,  781.92/s  (0.162s,  792.37/s)  LR: 5.463e-07  Data: 0.009 (0.007)Time: 412.090s\n",
      "Train: 14 [2600/10009 ( 26%)]  Loss: 3.12 (3.17)  Time: 0.161s,  796.27/s  (0.162s,  792.32/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 420.193s\n",
      "Train: 14 [2650/10009 ( 26%)]  Loss: 3.09 (3.17)  Time: 0.161s,  796.82/s  (0.162s,  792.33/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 428.264s\n",
      "Train: 14 [2700/10009 ( 27%)]  Loss: 3.15 (3.17)  Time: 0.161s,  797.16/s  (0.162s,  792.36/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 436.325s\n",
      "Train: 14 [2750/10009 ( 27%)]  Loss: 3.00 (3.17)  Time: 0.160s,  797.73/s  (0.162s,  792.39/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 444.385s\n",
      "Train: 14 [2800/10009 ( 28%)]  Loss: 3.31 (3.17)  Time: 0.163s,  783.24/s  (0.162s,  792.41/s)  LR: 5.463e-07  Data: 0.009 (0.007)Time: 452.451s\n",
      "Train: 14 [2850/10009 ( 28%)]  Loss: 2.91 (3.17)  Time: 0.162s,  788.35/s  (0.162s,  792.42/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 460.523s\n",
      "Train: 14 [2900/10009 ( 29%)]  Loss: 3.24 (3.17)  Time: 0.161s,  796.65/s  (0.162s,  792.45/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 468.582s\n",
      "Train: 14 [2950/10009 ( 29%)]  Loss: 2.86 (3.17)  Time: 0.161s,  793.93/s  (0.162s,  792.50/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 476.630s\n",
      "Train: 14 [3000/10009 ( 30%)]  Loss: 3.10 (3.17)  Time: 0.160s,  802.37/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 484.677s\n",
      "Train: 14 [3050/10009 ( 30%)]  Loss: 3.58 (3.17)  Time: 0.161s,  797.42/s  (0.162s,  792.57/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 492.738s\n",
      "Train: 14 [3100/10009 ( 31%)]  Loss: 3.19 (3.17)  Time: 0.161s,  795.69/s  (0.162s,  792.55/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 500.824s\n",
      "Train: 14 [3150/10009 ( 31%)]  Loss: 2.82 (3.17)  Time: 0.161s,  795.16/s  (0.161s,  792.59/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 508.875s\n",
      "Train: 14 [3200/10009 ( 32%)]  Loss: 3.21 (3.17)  Time: 0.164s,  779.83/s  (0.162s,  792.56/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 516.964s\n",
      "Train: 14 [3250/10009 ( 32%)]  Loss: 3.10 (3.17)  Time: 0.160s,  797.85/s  (0.161s,  792.60/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 525.017s\n",
      "Train: 14 [3300/10009 ( 33%)]  Loss: 3.13 (3.17)  Time: 0.161s,  796.09/s  (0.162s,  792.52/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 533.142s\n",
      "Train: 14 [3350/10009 ( 33%)]  Loss: 3.26 (3.17)  Time: 0.161s,  796.09/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 541.208s\n",
      "Train: 14 [3400/10009 ( 34%)]  Loss: 3.06 (3.17)  Time: 0.162s,  790.27/s  (0.162s,  792.50/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 549.311s\n",
      "Train: 14 [3450/10009 ( 34%)]  Loss: 2.94 (3.17)  Time: 0.160s,  798.63/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 557.360s\n",
      "Train: 14 [3500/10009 ( 35%)]  Loss: 3.20 (3.17)  Time: 0.162s,  787.85/s  (0.161s,  792.57/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 565.408s\n",
      "Train: 14 [3550/10009 ( 35%)]  Loss: 3.24 (3.17)  Time: 0.162s,  789.97/s  (0.162s,  792.51/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 573.527s\n",
      "Train: 14 [3600/10009 ( 36%)]  Loss: 2.96 (3.17)  Time: 0.160s,  798.77/s  (0.162s,  792.54/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 581.586s\n",
      "Train: 14 [3650/10009 ( 36%)]  Loss: 3.27 (3.17)  Time: 0.160s,  800.41/s  (0.161s,  792.58/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 589.627s\n",
      "Train: 14 [3700/10009 ( 37%)]  Loss: 3.18 (3.17)  Time: 0.161s,  794.70/s  (0.161s,  792.59/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 597.695s\n",
      "Train: 14 [3750/10009 ( 37%)]  Loss: 2.98 (3.17)  Time: 0.161s,  795.10/s  (0.161s,  792.60/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 605.765s\n",
      "Train: 14 [3800/10009 ( 38%)]  Loss: 3.04 (3.17)  Time: 0.160s,  798.15/s  (0.161s,  792.62/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 613.820s\n",
      "Train: 14 [3850/10009 ( 38%)]  Loss: 3.12 (3.17)  Time: 0.160s,  798.20/s  (0.161s,  792.65/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 621.868s\n",
      "Train: 14 [3900/10009 ( 39%)]  Loss: 3.24 (3.17)  Time: 0.162s,  789.99/s  (0.161s,  792.66/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 629.937s\n",
      "Train: 14 [3950/10009 ( 39%)]  Loss: 2.96 (3.17)  Time: 0.162s,  789.22/s  (0.161s,  792.69/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 637.989s\n",
      "Train: 14 [4000/10009 ( 40%)]  Loss: 3.35 (3.17)  Time: 0.161s,  795.64/s  (0.161s,  792.73/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 646.031s\n",
      "Train: 14 [4050/10009 ( 40%)]  Loss: 3.07 (3.17)  Time: 0.161s,  793.43/s  (0.161s,  792.76/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 654.075s\n",
      "Train: 14 [4100/10009 ( 41%)]  Loss: 3.29 (3.17)  Time: 0.161s,  795.85/s  (0.161s,  792.80/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 662.120s\n",
      "Train: 14 [4150/10009 ( 41%)]  Loss: 3.06 (3.17)  Time: 0.161s,  797.42/s  (0.161s,  792.83/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 670.167s\n",
      "Train: 14 [4200/10009 ( 42%)]  Loss: 3.12 (3.17)  Time: 0.160s,  799.49/s  (0.161s,  792.84/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 678.227s\n",
      "Train: 14 [4250/10009 ( 42%)]  Loss: 2.95 (3.17)  Time: 0.161s,  792.96/s  (0.161s,  792.87/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 686.272s\n",
      "Train: 14 [4300/10009 ( 43%)]  Loss: 3.04 (3.17)  Time: 0.161s,  794.02/s  (0.161s,  792.90/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 694.319s\n",
      "Train: 14 [4350/10009 ( 43%)]  Loss: 3.24 (3.17)  Time: 0.160s,  798.65/s  (0.161s,  792.93/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 702.367s\n",
      "Train: 14 [4400/10009 ( 44%)]  Loss: 3.14 (3.17)  Time: 0.162s,  788.86/s  (0.161s,  792.92/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 710.442s\n",
      "Train: 14 [4450/10009 ( 44%)]  Loss: 3.40 (3.17)  Time: 0.162s,  787.83/s  (0.161s,  792.93/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 718.511s\n",
      "Train: 14 [4500/10009 ( 45%)]  Loss: 3.03 (3.17)  Time: 0.160s,  799.45/s  (0.161s,  792.94/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 726.571s\n",
      "Train: 14 [4550/10009 ( 45%)]  Loss: 3.51 (3.17)  Time: 0.161s,  796.82/s  (0.161s,  792.96/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 734.622s\n",
      "Train: 14 [4600/10009 ( 46%)]  Loss: 3.34 (3.17)  Time: 0.162s,  792.38/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 742.676s\n",
      "Train: 14 [4650/10009 ( 46%)]  Loss: 3.17 (3.17)  Time: 0.162s,  792.33/s  (0.161s,  792.94/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 750.785s\n",
      "Train: 14 [4700/10009 ( 47%)]  Loss: 2.92 (3.17)  Time: 0.161s,  796.06/s  (0.161s,  792.96/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 758.836s\n",
      "Train: 14 [4750/10009 ( 47%)]  Loss: 2.92 (3.17)  Time: 0.162s,  791.82/s  (0.161s,  792.99/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 766.882s\n",
      "Train: 14 [4800/10009 ( 48%)]  Loss: 3.00 (3.17)  Time: 0.162s,  791.55/s  (0.161s,  792.99/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 774.952s\n",
      "Train: 14 [4850/10009 ( 48%)]  Loss: 3.29 (3.17)  Time: 0.161s,  793.57/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 783.001s\n",
      "Train: 14 [4900/10009 ( 49%)]  Loss: 3.38 (3.17)  Time: 0.160s,  799.39/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 791.097s\n",
      "Train: 14 [4950/10009 ( 49%)]  Loss: 3.12 (3.17)  Time: 0.162s,  788.36/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 799.144s\n",
      "Train: 14 [5000/10009 ( 50%)]  Loss: 3.21 (3.17)  Time: 0.161s,  793.55/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 807.241s\n",
      "Train: 14 [5050/10009 ( 50%)]  Loss: 3.30 (3.17)  Time: 0.163s,  786.82/s  (0.161s,  792.95/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 815.339s\n",
      "Train: 14 [5100/10009 ( 51%)]  Loss: 3.16 (3.17)  Time: 0.162s,  791.24/s  (0.161s,  792.96/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 823.402s\n",
      "Train: 14 [5150/10009 ( 51%)]  Loss: 3.36 (3.17)  Time: 0.160s,  799.56/s  (0.161s,  792.98/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 831.449s\n",
      "Train: 14 [5200/10009 ( 52%)]  Loss: 3.31 (3.17)  Time: 0.162s,  791.13/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 839.496s\n",
      "Train: 14 [5250/10009 ( 52%)]  Loss: 3.03 (3.17)  Time: 0.161s,  794.51/s  (0.161s,  793.02/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 847.556s\n",
      "Train: 14 [5300/10009 ( 53%)]  Loss: 3.17 (3.17)  Time: 0.163s,  787.21/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 855.635s\n",
      "Train: 14 [5350/10009 ( 53%)]  Loss: 3.43 (3.17)  Time: 0.162s,  790.92/s  (0.161s,  793.00/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 863.714s\n",
      "Train: 14 [5400/10009 ( 54%)]  Loss: 3.00 (3.17)  Time: 0.160s,  800.36/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 871.777s\n",
      "Train: 14 [5450/10009 ( 54%)]  Loss: 3.00 (3.17)  Time: 0.161s,  797.49/s  (0.161s,  793.03/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 879.824s\n",
      "Train: 14 [5500/10009 ( 55%)]  Loss: 3.21 (3.17)  Time: 0.164s,  781.56/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.008 (0.007)Time: 887.916s\n",
      "Train: 14 [5550/10009 ( 55%)]  Loss: 3.25 (3.17)  Time: 0.161s,  794.39/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 895.990s\n",
      "Train: 14 [5600/10009 ( 56%)]  Loss: 3.17 (3.17)  Time: 0.161s,  796.30/s  (0.161s,  793.01/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 904.054s\n",
      "Train: 14 [5650/10009 ( 56%)]  Loss: 3.23 (3.17)  Time: 0.161s,  796.22/s  (0.161s,  793.02/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 912.111s\n",
      "Train: 14 [5700/10009 ( 57%)]  Loss: 3.02 (3.17)  Time: 0.161s,  797.44/s  (0.161s,  793.03/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 920.170s\n",
      "Train: 14 [5750/10009 ( 57%)]  Loss: 2.98 (3.17)  Time: 0.161s,  795.44/s  (0.161s,  793.04/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 928.229s\n",
      "Train: 14 [5800/10009 ( 58%)]  Loss: 3.45 (3.17)  Time: 0.161s,  796.57/s  (0.161s,  793.05/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 936.286s\n",
      "Train: 14 [5850/10009 ( 58%)]  Loss: 3.33 (3.17)  Time: 0.161s,  794.03/s  (0.161s,  793.07/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 944.342s\n",
      "Train: 14 [5900/10009 ( 59%)]  Loss: 3.23 (3.17)  Time: 0.160s,  798.29/s  (0.161s,  793.06/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 952.421s\n",
      "Train: 14 [5950/10009 ( 59%)]  Loss: 3.19 (3.17)  Time: 0.162s,  792.09/s  (0.161s,  793.08/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 960.465s\n",
      "Train: 14 [6000/10009 ( 60%)]  Loss: 3.60 (3.17)  Time: 0.161s,  792.94/s  (0.161s,  793.06/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 968.561s\n",
      "Train: 14 [6050/10009 ( 60%)]  Loss: 3.21 (3.17)  Time: 0.162s,  788.72/s  (0.161s,  793.08/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 976.609s\n",
      "Train: 14 [6100/10009 ( 61%)]  Loss: 2.89 (3.17)  Time: 0.160s,  797.56/s  (0.161s,  793.10/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 984.651s\n",
      "Train: 14 [6150/10009 ( 61%)]  Loss: 3.40 (3.17)  Time: 0.162s,  789.79/s  (0.161s,  793.11/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 992.711s\n",
      "Train: 14 [6200/10009 ( 62%)]  Loss: 3.62 (3.17)  Time: 0.160s,  798.12/s  (0.161s,  793.13/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1000.751s\n",
      "Train: 14 [6250/10009 ( 62%)]  Loss: 3.30 (3.17)  Time: 0.161s,  796.20/s  (0.161s,  793.12/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1008.838s\n",
      "Train: 14 [6300/10009 ( 63%)]  Loss: 3.33 (3.17)  Time: 0.162s,  792.28/s  (0.161s,  793.13/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1016.890s\n",
      "Train: 14 [6350/10009 ( 63%)]  Loss: 3.16 (3.17)  Time: 0.161s,  794.86/s  (0.161s,  793.14/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1024.946s\n",
      "Train: 14 [6400/10009 ( 64%)]  Loss: 3.12 (3.17)  Time: 0.163s,  786.76/s  (0.161s,  793.15/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1033.006s\n",
      "Train: 14 [6450/10009 ( 64%)]  Loss: 3.13 (3.17)  Time: 0.161s,  792.80/s  (0.161s,  793.16/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1041.061s\n",
      "Train: 14 [6500/10009 ( 65%)]  Loss: 3.20 (3.17)  Time: 0.160s,  800.04/s  (0.161s,  793.17/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1049.108s\n",
      "Train: 14 [6550/10009 ( 65%)]  Loss: 3.15 (3.17)  Time: 0.160s,  799.46/s  (0.161s,  793.19/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1057.154s\n",
      "Train: 14 [6600/10009 ( 66%)]  Loss: 3.21 (3.17)  Time: 0.162s,  790.89/s  (0.161s,  793.21/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1065.194s\n",
      "Train: 14 [6650/10009 ( 66%)]  Loss: 3.24 (3.17)  Time: 0.160s,  799.98/s  (0.161s,  793.22/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1073.252s\n",
      "Train: 14 [6700/10009 ( 67%)]  Loss: 3.07 (3.17)  Time: 0.160s,  798.08/s  (0.161s,  793.23/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1081.301s\n",
      "Train: 14 [6750/10009 ( 67%)]  Loss: 2.81 (3.17)  Time: 0.160s,  797.66/s  (0.161s,  793.25/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1089.350s\n",
      "Train: 14 [6800/10009 ( 68%)]  Loss: 3.17 (3.17)  Time: 0.160s,  798.89/s  (0.161s,  793.26/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1097.401s\n",
      "Train: 14 [6850/10009 ( 68%)]  Loss: 2.96 (3.17)  Time: 0.161s,  797.44/s  (0.161s,  793.27/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1105.454s\n",
      "Train: 14 [6900/10009 ( 69%)]  Loss: 3.26 (3.17)  Time: 0.162s,  789.70/s  (0.161s,  793.25/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1113.552s\n",
      "Train: 14 [6950/10009 ( 69%)]  Loss: 3.41 (3.17)  Time: 0.162s,  791.42/s  (0.161s,  793.24/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1121.629s\n",
      "Train: 14 [7000/10009 ( 70%)]  Loss: 3.37 (3.17)  Time: 0.160s,  800.74/s  (0.161s,  793.26/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1129.674s\n",
      "Train: 14 [7050/10009 ( 70%)]  Loss: 3.29 (3.17)  Time: 0.160s,  800.65/s  (0.161s,  793.28/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1137.716s\n",
      "Train: 14 [7100/10009 ( 71%)]  Loss: 3.32 (3.17)  Time: 0.161s,  796.28/s  (0.161s,  793.29/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1145.766s\n",
      "Train: 14 [7150/10009 ( 71%)]  Loss: 3.10 (3.17)  Time: 0.160s,  798.67/s  (0.161s,  793.30/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1153.813s\n",
      "Train: 14 [7200/10009 ( 72%)]  Loss: 2.95 (3.17)  Time: 0.160s,  799.58/s  (0.161s,  793.32/s)  LR: 5.463e-07  Data: 0.005 (0.007)Time: 1161.860s\n",
      "Train: 14 [7250/10009 ( 72%)]  Loss: 3.21 (3.17)  Time: 0.161s,  793.49/s  (0.161s,  793.33/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1169.910s\n",
      "Train: 14 [7300/10009 ( 73%)]  Loss: 3.49 (3.17)  Time: 0.161s,  795.07/s  (0.161s,  793.34/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1177.966s\n",
      "Train: 14 [7350/10009 ( 73%)]  Loss: 3.32 (3.17)  Time: 0.162s,  789.27/s  (0.161s,  793.36/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1186.002s\n",
      "Train: 14 [7400/10009 ( 74%)]  Loss: 3.08 (3.17)  Time: 0.161s,  794.39/s  (0.161s,  793.36/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1194.070s\n",
      "Train: 14 [7450/10009 ( 74%)]  Loss: 3.13 (3.17)  Time: 0.161s,  795.83/s  (0.161s,  793.37/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1202.125s\n",
      "Train: 14 [7500/10009 ( 75%)]  Loss: 3.20 (3.17)  Time: 0.161s,  796.17/s  (0.161s,  793.37/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1210.190s\n",
      "Train: 14 [7550/10009 ( 75%)]  Loss: 3.16 (3.17)  Time: 0.162s,  792.45/s  (0.161s,  793.38/s)  LR: 5.463e-07  Data: 0.007 (0.007)Time: 1218.240s\n",
      "Train: 14 [7600/10009 ( 76%)]  Loss: 3.36 (3.17)  Time: 0.160s,  797.64/s  (0.161s,  793.38/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1226.302s\n",
      "Train: 14 [7650/10009 ( 76%)]  Loss: 3.23 (3.17)  Time: 0.160s,  797.55/s  (0.161s,  793.39/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1234.354s\n",
      "Train: 14 [7700/10009 ( 77%)]  Loss: 3.40 (3.17)  Time: 0.160s,  798.55/s  (0.161s,  793.40/s)  LR: 5.463e-07  Data: 0.006 (0.007)Time: 1242.408s\n",
      "Train: 14 [7750/10009 ( 77%)]  Loss: 3.35 (3.17)  Time: 0.160s,  799.62/s  (0.161s,  793.42/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1250.448s\n",
      "Train: 14 [7800/10009 ( 78%)]  Loss: 3.46 (3.17)  Time: 0.160s,  799.00/s  (0.161s,  793.43/s)  LR: 5.463e-07  Data: 0.005 (0.006)Time: 1258.494s\n",
      "Train: 14 [7850/10009 ( 78%)]  Loss: 3.19 (3.17)  Time: 0.161s,  794.72/s  (0.161s,  793.44/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1266.545s\n",
      "Train: 14 [7900/10009 ( 79%)]  Loss: 3.04 (3.17)  Time: 0.160s,  798.56/s  (0.161s,  793.45/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1274.589s\n",
      "Train: 14 [7950/10009 ( 79%)]  Loss: 3.36 (3.17)  Time: 0.161s,  795.75/s  (0.161s,  793.46/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1282.642s\n",
      "Train: 14 [8000/10009 ( 80%)]  Loss: 3.35 (3.17)  Time: 0.160s,  800.21/s  (0.161s,  793.47/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1290.690s\n",
      "Train: 14 [8050/10009 ( 80%)]  Loss: 3.09 (3.17)  Time: 0.160s,  799.62/s  (0.161s,  793.48/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1298.746s\n",
      "Train: 14 [8100/10009 ( 81%)]  Loss: 3.22 (3.17)  Time: 0.162s,  790.52/s  (0.161s,  793.49/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1306.791s\n",
      "Train: 14 [8150/10009 ( 81%)]  Loss: 3.03 (3.17)  Time: 0.160s,  797.81/s  (0.161s,  793.50/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1314.833s\n",
      "Train: 14 [8200/10009 ( 82%)]  Loss: 3.14 (3.17)  Time: 0.161s,  796.06/s  (0.161s,  793.52/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1322.876s\n",
      "Train: 14 [8250/10009 ( 82%)]  Loss: 3.32 (3.17)  Time: 0.162s,  791.95/s  (0.161s,  793.53/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1330.926s\n",
      "Train: 14 [8300/10009 ( 83%)]  Loss: 3.09 (3.17)  Time: 0.162s,  792.01/s  (0.161s,  793.54/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1338.974s\n",
      "Train: 14 [8350/10009 ( 83%)]  Loss: 2.79 (3.17)  Time: 0.162s,  790.51/s  (0.161s,  793.54/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1347.027s\n",
      "Train: 14 [8400/10009 ( 84%)]  Loss: 3.36 (3.17)  Time: 0.160s,  800.31/s  (0.161s,  793.55/s)  LR: 5.463e-07  Data: 0.005 (0.006)Time: 1355.080s\n",
      "Train: 14 [8450/10009 ( 84%)]  Loss: 2.94 (3.17)  Time: 0.161s,  794.84/s  (0.161s,  793.56/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1363.134s\n",
      "Train: 14 [8500/10009 ( 85%)]  Loss: 3.07 (3.17)  Time: 0.161s,  793.94/s  (0.161s,  793.57/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1371.178s\n",
      "Train: 14 [8550/10009 ( 85%)]  Loss: 3.43 (3.17)  Time: 0.161s,  796.56/s  (0.161s,  793.58/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1379.227s\n",
      "Train: 14 [8600/10009 ( 86%)]  Loss: 3.17 (3.17)  Time: 0.161s,  794.24/s  (0.161s,  793.59/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1387.270s\n",
      "Train: 14 [8650/10009 ( 86%)]  Loss: 3.31 (3.17)  Time: 0.161s,  793.35/s  (0.161s,  793.59/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1395.329s\n",
      "Train: 14 [8700/10009 ( 87%)]  Loss: 3.44 (3.17)  Time: 0.161s,  792.60/s  (0.161s,  793.60/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1403.386s\n",
      "Train: 14 [8750/10009 ( 87%)]  Loss: 3.35 (3.17)  Time: 0.161s,  792.95/s  (0.161s,  793.61/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1411.433s\n",
      "Train: 14 [8800/10009 ( 88%)]  Loss: 3.47 (3.17)  Time: 0.162s,  791.18/s  (0.161s,  793.62/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1419.480s\n",
      "Train: 14 [8850/10009 ( 88%)]  Loss: 3.15 (3.17)  Time: 0.160s,  798.38/s  (0.161s,  793.61/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1427.560s\n",
      "Train: 14 [8900/10009 ( 89%)]  Loss: 3.15 (3.17)  Time: 0.162s,  791.99/s  (0.161s,  793.62/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1435.612s\n",
      "Train: 14 [8950/10009 ( 89%)]  Loss: 3.27 (3.17)  Time: 0.161s,  796.14/s  (0.161s,  793.62/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1443.668s\n",
      "Train: 14 [9000/10009 ( 90%)]  Loss: 3.03 (3.17)  Time: 0.160s,  799.70/s  (0.161s,  793.63/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1451.720s\n",
      "Train: 14 [9050/10009 ( 90%)]  Loss: 3.15 (3.17)  Time: 0.161s,  796.17/s  (0.161s,  793.63/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1459.781s\n",
      "Train: 14 [9100/10009 ( 91%)]  Loss: 3.25 (3.17)  Time: 0.160s,  799.12/s  (0.161s,  793.64/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1467.821s\n",
      "Train: 14 [9150/10009 ( 91%)]  Loss: 3.02 (3.17)  Time: 0.162s,  790.73/s  (0.161s,  793.65/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1475.866s\n",
      "Train: 14 [9200/10009 ( 92%)]  Loss: 3.08 (3.17)  Time: 0.162s,  790.84/s  (0.161s,  793.66/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1483.910s\n",
      "Train: 14 [9250/10009 ( 92%)]  Loss: 2.98 (3.17)  Time: 0.162s,  791.74/s  (0.161s,  793.67/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1491.967s\n",
      "Train: 14 [9300/10009 ( 93%)]  Loss: 3.09 (3.17)  Time: 0.161s,  794.34/s  (0.161s,  793.66/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1500.052s\n",
      "Train: 14 [9350/10009 ( 93%)]  Loss: 3.13 (3.17)  Time: 0.160s,  797.59/s  (0.161s,  793.67/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1508.092s\n",
      "Train: 14 [9400/10009 ( 94%)]  Loss: 3.26 (3.17)  Time: 0.161s,  795.61/s  (0.161s,  793.68/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1516.141s\n",
      "Train: 14 [9450/10009 ( 94%)]  Loss: 3.32 (3.17)  Time: 0.161s,  796.95/s  (0.161s,  793.68/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1524.194s\n",
      "Train: 14 [9500/10009 ( 95%)]  Loss: 3.16 (3.17)  Time: 0.160s,  798.85/s  (0.161s,  793.69/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1532.242s\n",
      "Train: 14 [9550/10009 ( 95%)]  Loss: 3.31 (3.17)  Time: 0.161s,  795.32/s  (0.161s,  793.69/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1540.305s\n",
      "Train: 14 [9600/10009 ( 96%)]  Loss: 3.17 (3.17)  Time: 0.161s,  794.09/s  (0.161s,  793.70/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1548.352s\n",
      "Train: 14 [9650/10009 ( 96%)]  Loss: 3.21 (3.17)  Time: 0.160s,  801.51/s  (0.161s,  793.71/s)  LR: 5.463e-07  Data: 0.005 (0.006)Time: 1556.400s\n",
      "Train: 14 [9700/10009 ( 97%)]  Loss: 3.30 (3.17)  Time: 0.162s,  790.44/s  (0.161s,  793.72/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1564.446s\n",
      "Train: 14 [9750/10009 ( 97%)]  Loss: 3.09 (3.17)  Time: 0.161s,  797.03/s  (0.161s,  793.72/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1572.496s\n",
      "Train: 14 [9800/10009 ( 98%)]  Loss: 3.28 (3.17)  Time: 0.161s,  793.67/s  (0.161s,  793.73/s)  LR: 5.463e-07  Data: 0.007 (0.006)Time: 1580.543s\n",
      "Train: 14 [9850/10009 ( 98%)]  Loss: 3.37 (3.17)  Time: 0.161s,  794.81/s  (0.161s,  793.74/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1588.587s\n",
      "Train: 14 [9900/10009 ( 99%)]  Loss: 3.02 (3.17)  Time: 0.161s,  796.07/s  (0.161s,  793.75/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1596.634s\n",
      "Train: 14 [9950/10009 ( 99%)]  Loss: 3.30 (3.17)  Time: 0.161s,  797.13/s  (0.161s,  793.76/s)  LR: 5.463e-07  Data: 0.006 (0.006)Time: 1604.677s\n",
      "Train: 14 [10000/10009 (100%)]  Loss: 3.24 (3.17)  Time: 0.203s,  630.36/s  (0.161s,  793.74/s)  LR: 5.463e-07  Data: 0.050 (0.006)Time: 1612.767s\n",
      "Test: [   0/390]  Time: 0.669 (0.669)  Loss:   1.227 ( 1.227)  Acc@1:  75.781 ( 75.781)  Acc@5:  90.625 ( 90.625)\n",
      "Test: [  50/390]  Time: 0.052 (0.140)  Loss:   1.175 ( 1.959)  Acc@1:  74.219 ( 58.594)  Acc@5:  92.969 ( 79.427)\n",
      "Test: [ 100/390]  Time: 0.236 (0.133)  Loss:   1.928 ( 1.983)  Acc@1:  57.031 ( 55.523)  Acc@5:  84.375 ( 80.067)\n",
      "Test: [ 150/390]  Time: 0.052 (0.136)  Loss:   1.791 ( 1.957)  Acc@1:  54.688 ( 56.152)  Acc@5:  82.812 ( 80.381)\n",
      "Test: [ 200/390]  Time: 0.053 (0.133)  Loss:   3.058 ( 2.135)  Acc@1:  29.688 ( 53.016)  Acc@5:  64.062 ( 77.437)\n",
      "Test: [ 250/390]  Time: 0.052 (0.133)  Loss:   2.338 ( 2.248)  Acc@1:  56.250 ( 51.264)  Acc@5:  71.094 ( 75.557)\n",
      "Test: [ 300/390]  Time: 0.195 (0.131)  Loss:   2.625 ( 2.342)  Acc@1:  51.562 ( 49.642)  Acc@5:  67.188 ( 73.845)\n",
      "Test: [ 350/390]  Time: 0.272 (0.130)  Loss:   2.639 ( 2.415)  Acc@1:  45.312 ( 48.279)  Acc@5:  71.094 ( 72.630)\n",
      "Test: [ 390/390]  Time: 0.034 (0.130)  Loss:   3.558 ( 2.381)  Acc@1:  22.500 ( 48.840)  Acc@5:  56.250 ( 73.150)\n",
      "Current checkpoints:\n",
      " ('./output/budgeted/checkpoint-14.pth.tar', 48.84)\n",
      " ('./output/budgeted/checkpoint-13.pth.tar', 48.326)\n",
      " ('./output/budgeted/checkpoint-12.pth.tar', 47.692)\n",
      " ('./output/budgeted/checkpoint-11.pth.tar', 46.698)\n",
      " ('./output/budgeted/checkpoint-10.pth.tar', 44.822)\n",
      "\n",
      "*** Best metric: 48.84 (epoch 14)\n"
     ]
    }
   ],
   "source": [
    "    try:\n",
    "        for epoch in range(start_epoch, 15):\n",
    "            if hasattr(dataset_train, 'set_epoch'):\n",
    "                dataset_train.set_epoch(epoch)\n",
    "            elif args.distributed and hasattr(loader_train.sampler, 'set_epoch'):\n",
    "                loader_train.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = train_one_epoch(\n",
    "                epoch,\n",
    "                model,\n",
    "                loader_train,\n",
    "                optimizer,\n",
    "                train_loss_fn,\n",
    "                args,\n",
    "                lr_scheduler=lr_scheduler,\n",
    "                saver=saver,\n",
    "                output_dir=output_dir,\n",
    "                amp_autocast=amp_autocast,\n",
    "                loss_scaler=loss_scaler,\n",
    "                model_ema=model_ema,\n",
    "                mixup_fn=mixup_fn,\n",
    "                # fish: add preconditioner\n",
    "                preconditioner=preconditioner,\n",
    "            )\n",
    "\n",
    "            if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                if utils.is_primary(args):\n",
    "                    _logger.info(\"Distributing BatchNorm running means and vars\")\n",
    "                utils.distribute_bn(model, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "            eval_metrics = validate(\n",
    "                model,\n",
    "                loader_eval,\n",
    "                validate_loss_fn,\n",
    "                args,\n",
    "                amp_autocast=amp_autocast,\n",
    "            )\n",
    "\n",
    "            if model_ema is not None and not args.model_ema_force_cpu:\n",
    "                if args.distributed and args.dist_bn in ('broadcast', 'reduce'):\n",
    "                    utils.distribute_bn(model_ema, args.world_size, args.dist_bn == 'reduce')\n",
    "\n",
    "                ema_eval_metrics = validate(\n",
    "                    model_ema.module,\n",
    "                    loader_eval,\n",
    "                    validate_loss_fn,\n",
    "                    args,\n",
    "                    amp_autocast=amp_autocast,\n",
    "                    log_suffix=' (EMA)',\n",
    "                )\n",
    "                eval_metrics = ema_eval_metrics\n",
    "\n",
    "            if output_dir is not None:\n",
    "                lrs = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "                utils.update_summary(\n",
    "                    epoch,\n",
    "                    train_metrics,\n",
    "                    eval_metrics,\n",
    "                    filename=os.path.join(output_dir, 'summary.csv'),\n",
    "                    lr=sum(lrs) / len(lrs),\n",
    "                    write_header=best_metric is None,\n",
    "                    log_wandb=args.log_wandb and has_wandb,\n",
    "                )\n",
    "\n",
    "            if saver is not None:\n",
    "                # save proper checkpoint with eval metric\n",
    "                save_metric = eval_metrics[eval_metric]\n",
    "                best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                # step LR for next epoch\n",
    "                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "small   1310s\n",
    "\n",
    "mid     1310s\n",
    "\n",
    "large   1650s\n",
    "\n",
    "跑小模型时显卡未达到瓶颈，利用率不高，因此对于small和mid大小可以适当放大batchsize。按时间比较，22min:27.5min=1:1.25，因此可以再训练2个epoch。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macs            params\n",
    "\n",
    "1111509504.0    6025192.0\n",
    "\n",
    "2157267456.0    11341672.0\n",
    "\n",
    "4248783360.0    21974632.0\n",
    "\n",
    "从浮点数计算的意义上来说，则是可以再训练 6 个epoch。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
